[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.11326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v1",
                "updated": "2024-09-17T16:22:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v1",
                "updated": "2024-09-16T17:59:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB)."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v1",
                "updated": "2024-09-16T17:36:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.11404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11404v1",
                "updated": "2024-09-17T17:59:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:59:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs"
                },
                "summary": "Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes ~45K\npost-edited samples, a cultural benchmark, and highlights the importance of\ntailored training to improve LLM performance in capturing the nuances of\ndiverse Arabic dialects and cultural contexts. We will release the dialectal\ntranslation models and benchmarks curated in this study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes ~45K\npost-edited samples, a cultural benchmark, and highlights the importance of\ntailored training to improve LLM performance in capturing the nuances of\ndiverse Arabic dialects and cultural contexts. We will release the dialectal\ntranslation models and benchmarks curated in this study."
                },
                "authors": [
                    {
                        "name": "Basel Mousi"
                    },
                    {
                        "name": "Nadir Durrani"
                    },
                    {
                        "name": "Fatema Ahmad"
                    },
                    {
                        "name": "Md. Arid Hasan"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Tameem Kabbani"
                    },
                    {
                        "name": "Fahim Dalvi"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Benchmarking, Culturally Informed, Large Language Models, Arabic NLP,\n  LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11403v1",
                "updated": "2024-09-17T17:59:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    22,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:59:22Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    22,
                    1,
                    261,
                    0
                ],
                "title": "UniLCD: Unified Local-Cloud Decision-Making via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniLCD: Unified Local-Cloud Decision-Making via Reinforcement Learning"
                },
                "summary": "Embodied vision-based real-world systems, such as mobile robots, require a\ncareful balance between energy consumption, compute latency, and safety\nconstraints to optimize operation across dynamic tasks and contexts. As local\ncomputation tends to be restricted, offloading the computation, ie, to a remote\nserver, can save local resources while providing access to high-quality\npredictions from powerful and large models. However, the resulting\ncommunication and latency overhead has led to limited usability of cloud models\nin dynamic, safety-critical, real-time settings. To effectively address this\ntrade-off, we introduce UniLCD, a novel hybrid inference framework for enabling\nflexible local-cloud collaboration. By efficiently optimizing a flexible\nrouting module via reinforcement learning and a suitable multi-task objective,\nUniLCD is specifically designed to support the multiple constraints of\nsafety-critical end-to-end mobile systems. We validate the proposed approach\nusing a challenging, crowded navigation task requiring frequent and timely\nswitching between local and cloud operations. UniLCD demonstrates improved\noverall performance and efficiency, by over 35% compared to state-of-the-art\nbaselines based on various split computing and early exit strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied vision-based real-world systems, such as mobile robots, require a\ncareful balance between energy consumption, compute latency, and safety\nconstraints to optimize operation across dynamic tasks and contexts. As local\ncomputation tends to be restricted, offloading the computation, ie, to a remote\nserver, can save local resources while providing access to high-quality\npredictions from powerful and large models. However, the resulting\ncommunication and latency overhead has led to limited usability of cloud models\nin dynamic, safety-critical, real-time settings. To effectively address this\ntrade-off, we introduce UniLCD, a novel hybrid inference framework for enabling\nflexible local-cloud collaboration. By efficiently optimizing a flexible\nrouting module via reinforcement learning and a suitable multi-task objective,\nUniLCD is specifically designed to support the multiple constraints of\nsafety-critical end-to-end mobile systems. We validate the proposed approach\nusing a challenging, crowded navigation task requiring frequent and timely\nswitching between local and cloud operations. UniLCD demonstrates improved\noverall performance and efficiency, by over 35% compared to state-of-the-art\nbaselines based on various split computing and early exit strategies."
                },
                "authors": [
                    {
                        "name": "Kathakoli Sengupta"
                    },
                    {
                        "name": "Zhongkai Shagguan"
                    },
                    {
                        "name": "Sandesh Bharadwaj"
                    },
                    {
                        "name": "Sanjay Arora"
                    },
                    {
                        "name": "Eshed Ohn-Bar"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "arxiv_comment": "ECCV 24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11402v1",
                "updated": "2024-09-17T17:59:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    6,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:59:06Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    6,
                    1,
                    261,
                    0
                ],
                "title": "NVLM: Open Frontier-Class Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVLM: Open Frontier-Class Multimodal LLMs"
                },
                "summary": "We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we are releasing the model weights and will open-source the code for the\ncommunity: https://nvlm-project.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we are releasing the model weights and will open-source the code for the\ncommunity: https://nvlm-project.github.io/."
                },
                "authors": [
                    {
                        "name": "Wenliang Dai"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Zhuoling Yang"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Jon Barker"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Wei Ping"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ping"
                },
                "author": "Wei Ping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11401v1",
                "updated": "2024-09-17T17:59:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    0,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:59:00Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    0,
                    1,
                    261,
                    0
                ],
                "title": "Teaching dark matter simulations to speak the halo language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching dark matter simulations to speak the halo language"
                },
                "summary": "We develop a transformer-based conditional generative model for discrete\npoint objects and their properties. We use it to build a model for populating\ncosmological simulations with gravitationally collapsed structures called dark\nmatter halos. Specifically, we condition our model with dark matter\ndistribution obtained from fast, approximate simulations to recover the correct\nthree-dimensional positions and masses of individual halos. This leads to a\nfirst model that can recover the statistical properties of the halos at small\nscales to better than 3% level using an accelerated dark matter simulation.\nThis trained model can then be applied to simulations with significantly larger\nvolumes which would otherwise be computationally prohibitive with traditional\nsimulations, and also provides a crucial missing link in making end-to-end\ndifferentiable cosmological simulations. The code, named GOTHAM (Generative\ncOnditional Transformer for Halo's Auto-regressive Modeling) is publicly\navailable at \\url{https://github.com/shivampcosmo/GOTHAM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a transformer-based conditional generative model for discrete\npoint objects and their properties. We use it to build a model for populating\ncosmological simulations with gravitationally collapsed structures called dark\nmatter halos. Specifically, we condition our model with dark matter\ndistribution obtained from fast, approximate simulations to recover the correct\nthree-dimensional positions and masses of individual halos. This leads to a\nfirst model that can recover the statistical properties of the halos at small\nscales to better than 3% level using an accelerated dark matter simulation.\nThis trained model can then be applied to simulations with significantly larger\nvolumes which would otherwise be computationally prohibitive with traditional\nsimulations, and also provides a crucial missing link in making end-to-end\ndifferentiable cosmological simulations. The code, named GOTHAM (Generative\ncOnditional Transformer for Halo's Auto-regressive Modeling) is publicly\navailable at \\url{https://github.com/shivampcosmo/GOTHAM}."
                },
                "authors": [
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "Francois Lanusse"
                    },
                    {
                        "name": "Chirag Modi"
                    },
                    {
                        "name": "Benjamin D. Wandelt"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin D. Wandelt"
                },
                "author": "Benjamin D. Wandelt",
                "arxiv_comment": "6 pages, 2 figures. Accepted by the Structured Probabilistic\n  Inference & Generative Modeling workshop of ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11393v1",
                "updated": "2024-09-17T17:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents"
                },
                "summary": "The integration of tools in LLM-based agents overcame the difficulties of\nstandalone LLMs and traditional agents' limited capabilities. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity. Indeed, they focused mainly on functionalities and\noverlooked the definition of the component's boundaries within the agent. This\ncaused terminological and architectural ambiguities between researchers which\nwe addressed in this paper by proposing a unified framework that establishes a\nclear foundation for LLM-based agents' development from both functional and\nsoftware architectural perspectives.\n  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),\nclearly distinguishes between the different components of an agent, setting\nLLMs, and tools apart from a newly introduced element: the core-agent, playing\nthe role of the central coordinator of the agent which comprises five modules:\nplanning, memory, profile, action, and security, the latter often neglected in\nprevious works. Differences in the internal structure of core-agents led us to\nclassify them into a taxonomy of passive and active types. Based on this, we\nproposed different multi-core agent architectures combining unique\ncharacteristics of various individual agents.\n  For evaluation purposes, we applied this framework to a selection of\nstate-of-the-art agents, thereby demonstrating its alignment with their\nfunctionalities and clarifying the overlooked architectural aspects. Moreover,\nwe thoroughly assessed four of our proposed architectures by integrating\ndistinctive agents into hybrid active/passive core-agents' systems. This\nanalysis provided clear insights into potential improvements and highlighted\nthe challenges involved in the combination of specific agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of tools in LLM-based agents overcame the difficulties of\nstandalone LLMs and traditional agents' limited capabilities. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity. Indeed, they focused mainly on functionalities and\noverlooked the definition of the component's boundaries within the agent. This\ncaused terminological and architectural ambiguities between researchers which\nwe addressed in this paper by proposing a unified framework that establishes a\nclear foundation for LLM-based agents' development from both functional and\nsoftware architectural perspectives.\n  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),\nclearly distinguishes between the different components of an agent, setting\nLLMs, and tools apart from a newly introduced element: the core-agent, playing\nthe role of the central coordinator of the agent which comprises five modules:\nplanning, memory, profile, action, and security, the latter often neglected in\nprevious works. Differences in the internal structure of core-agents led us to\nclassify them into a taxonomy of passive and active types. Based on this, we\nproposed different multi-core agent architectures combining unique\ncharacteristics of various individual agents.\n  For evaluation purposes, we applied this framework to a selection of\nstate-of-the-art agents, thereby demonstrating its alignment with their\nfunctionalities and clarifying the overlooked architectural aspects. Moreover,\nwe thoroughly assessed four of our proposed architectures by integrating\ndistinctive agents into hybrid active/passive core-agents' systems. This\nanalysis provided clear insights into potential improvements and highlighted\nthe challenges involved in the combination of specific agents."
                },
                "authors": [
                    {
                        "name": "Amine B. Hassouna"
                    },
                    {
                        "name": "Hana Chaari"
                    },
                    {
                        "name": "Ines Belhaj"
                    }
                ],
                "author_detail": {
                    "name": "Ines Belhaj"
                },
                "author": "Ines Belhaj",
                "arxiv_comment": "35 pages, 14 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11390v1",
                "updated": "2024-09-17T17:50:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    50,
                    15,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:50:15Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    50,
                    15,
                    1,
                    261,
                    0
                ],
                "title": "Says Who? Effective Zero-Shot Annotation of Focalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Says Who? Effective Zero-Shot Annotation of Focalization"
                },
                "summary": "Focalization, the perspective through which narrative is presented, is\nencoded via a wide range of lexico-grammatical features and is subject to\nreader interpretation. Moreover, trained readers regularly disagree on\ninterpretations, suggesting that this problem may be computationally\nintractable. In this paper, we provide experiments to test how well\ncontemporary Large Language Models (LLMs) perform when annotating literary\ntexts for focalization mode. Despite the challenging nature of the task, LLMs\nshow comparable performance to trained human annotators in our experiments. We\nprovide a case study working with the novels of Stephen King to demonstrate the\nusefulness of this approach for computational literary studies, illustrating\nhow focalization can be studied at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focalization, the perspective through which narrative is presented, is\nencoded via a wide range of lexico-grammatical features and is subject to\nreader interpretation. Moreover, trained readers regularly disagree on\ninterpretations, suggesting that this problem may be computationally\nintractable. In this paper, we provide experiments to test how well\ncontemporary Large Language Models (LLMs) perform when annotating literary\ntexts for focalization mode. Despite the challenging nature of the task, LLMs\nshow comparable performance to trained human annotators in our experiments. We\nprovide a case study working with the novels of Stephen King to demonstrate the\nusefulness of this approach for computational literary studies, illustrating\nhow focalization can be studied at scale."
                },
                "authors": [
                    {
                        "name": "Rebecca M. M. Hicke"
                    },
                    {
                        "name": "Yuri Bizzoni"
                    },
                    {
                        "name": "Pascale Feldkamp"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06173v2",
                "updated": "2024-09-17T17:42:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    42,
                    26,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-10T03:06:17Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    6,
                    17,
                    1,
                    254,
                    0
                ],
                "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks"
                },
                "summary": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors."
                },
                "authors": [
                    {
                        "name": "Georgios Chochlakis"
                    },
                    {
                        "name": "Niyantha Maruthu Pandiyan"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "arxiv_comment": "5 pages, 2 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2403.17125",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.05138v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.05138v3",
                "updated": "2024-09-17T17:31:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    31,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2023-04-11T11:03:28Z",
                "published_parsed": [
                    2023,
                    4,
                    11,
                    11,
                    3,
                    28,
                    1,
                    101,
                    0
                ],
                "title": "Cooperative Online Learning for Multi-Agent System Control via Gaussian\n  Processes with Event-Triggered Mechanism: Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Online Learning for Multi-Agent System Control via Gaussian\n  Processes with Event-Triggered Mechanism: Extended Version"
                },
                "summary": "In the realm of the cooperative control of multi-agent systems (MASs) with\nunknown dynamics, Gaussian process (GP) regression is widely used to infer the\nuncertainties due to its modeling flexibility of nonlinear functions and the\nexistence of a theoretical prediction error bound. Online learning, which\ninvolves incorporating newly acquired training data into Gaussian process\nmodels, promises to improve control performance by enhancing predictions during\nthe operation. Therefore, this paper investigates the online cooperative\nlearning algorithm for MAS control. Moreover, an event-triggered data selection\nmechanism, inspired by the analysis of a centralized event-trigger, is\nintroduced to reduce the model update frequency and enhance the data\nefficiency. With the proposed learning-based control, the practical convergence\nof the MAS is validated with guaranteed tracking performance via the Lynaponve\ntheory. Furthermore, the exclusion of the Zeno behavior for individual agents\nis shown. Finally, the effectiveness of the proposed event-triggered online\nlearning method is demonstrated in simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of the cooperative control of multi-agent systems (MASs) with\nunknown dynamics, Gaussian process (GP) regression is widely used to infer the\nuncertainties due to its modeling flexibility of nonlinear functions and the\nexistence of a theoretical prediction error bound. Online learning, which\ninvolves incorporating newly acquired training data into Gaussian process\nmodels, promises to improve control performance by enhancing predictions during\nthe operation. Therefore, this paper investigates the online cooperative\nlearning algorithm for MAS control. Moreover, an event-triggered data selection\nmechanism, inspired by the analysis of a centralized event-trigger, is\nintroduced to reduce the model update frequency and enhance the data\nefficiency. With the proposed learning-based control, the practical convergence\nof the MAS is validated with guaranteed tracking performance via the Lynaponve\ntheory. Furthermore, the exclusion of the Zeno behavior for individual agents\nis shown. Finally, the effectiveness of the proposed event-triggered online\nlearning method is demonstrated in simulations."
                },
                "authors": [
                    {
                        "name": "Xiaobing Dai"
                    },
                    {
                        "name": "Zewen Yang"
                    },
                    {
                        "name": "Sihua Zhang"
                    },
                    {
                        "name": "Di-Hua Zhai"
                    },
                    {
                        "name": "Yuanqing Xia"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.05138v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.05138v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12958v2",
                "updated": "2024-09-17T17:25:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    25,
                    40,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-19T17:57:58Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    57,
                    58,
                    1,
                    79,
                    0
                ],
                "title": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models"
                },
                "summary": "Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models."
                },
                "authors": [
                    {
                        "name": "Jeffrey Cheng"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Dawn Lawrie"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11378v1",
                "updated": "2024-09-17T17:25:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    25,
                    31,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:25:31Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    25,
                    31,
                    1,
                    261,
                    0
                ],
                "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative\n  Refinement"
                },
                "summary": "Finetuning large language models on instruction data is crucial for enhancing\npre-trained knowledge and improving instruction-following capabilities. As\ninstruction datasets proliferate, selecting optimal data for effective training\nbecomes increasingly important. This work addresses the question: How can we\ndetermine the optimal subset of data for effective training? While existing\nresearch often emphasizes local criteria like instance quality for subset\nselection, we argue that a global approach focused on data diversity is more\ncritical. Our method employs k-means clustering to ensure the selected subset\neffectively represents the full dataset. We propose an iterative refinement\nmethod inspired by active learning techniques to resample instances from\nclusters, reassessing each cluster's importance and sampling weight in every\ntraining iteration. This approach reduces the effect of outliers and\nautomatically filters out clusters containing low-quality data. Through\nextensive evaluation across natural language reasoning, general world\nknowledge, code and math reasoning tasks, and by fine-tuning models from\nvarious families, we observe consistent improvements, achieving a 7% increase\nover random selection and a 3.8% improvement over state-of-the-art sampling\nmethods. Our work highlights the significance of diversity-first sampling when\nfinetuning LLMs to enhance performance across a broad array of evaluation\ntasks. Our code is available at\nhttps://github.com/for-ai/iterative-data-selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models on instruction data is crucial for enhancing\npre-trained knowledge and improving instruction-following capabilities. As\ninstruction datasets proliferate, selecting optimal data for effective training\nbecomes increasingly important. This work addresses the question: How can we\ndetermine the optimal subset of data for effective training? While existing\nresearch often emphasizes local criteria like instance quality for subset\nselection, we argue that a global approach focused on data diversity is more\ncritical. Our method employs k-means clustering to ensure the selected subset\neffectively represents the full dataset. We propose an iterative refinement\nmethod inspired by active learning techniques to resample instances from\nclusters, reassessing each cluster's importance and sampling weight in every\ntraining iteration. This approach reduces the effect of outliers and\nautomatically filters out clusters containing low-quality data. Through\nextensive evaluation across natural language reasoning, general world\nknowledge, code and math reasoning tasks, and by fine-tuning models from\nvarious families, we observe consistent improvements, achieving a 7% increase\nover random selection and a 3.8% improvement over state-of-the-art sampling\nmethods. Our work highlights the significance of diversity-first sampling when\nfinetuning LLMs to enhance performance across a broad array of evaluation\ntasks. Our code is available at\nhttps://github.com/for-ai/iterative-data-selection."
                },
                "authors": [
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Sara Ahmadian"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Fadaee"
                },
                "author": "Marzieh Fadaee",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11376v1",
                "updated": "2024-09-17T17:23:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    23,
                    44,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:23:44Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    23,
                    44,
                    1,
                    261,
                    0
                ],
                "title": "Towards Time Series Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Time Series Reasoning with LLMs"
                },
                "summary": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains."
                },
                "authors": [
                    {
                        "name": "Winnie Chow"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Haraldur T. Hallgrímsson"
                    },
                    {
                        "name": "Maxwell A. Xu"
                    },
                    {
                        "name": "Shirley You Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shirley You Ren"
                },
                "author": "Shirley You Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11375v1",
                "updated": "2024-09-17T17:22:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    22,
                    35,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:22:35Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    22,
                    35,
                    1,
                    261,
                    0
                ],
                "title": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with\n  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with\n  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease\n  Classification"
                },
                "summary": "In the medical domain, acquiring large datasets poses significant challenges\ndue to privacy concerns. Nonetheless, the development of a robust deep-learning\nmodel for retinal disease diagnosis necessitates a substantial dataset for\ntraining. The capacity to generalize effectively on smaller datasets remains a\npersistent challenge. The scarcity of data presents a significant barrier to\nthe practical implementation of scalable medical AI solutions. To address this\nissue, we've combined a wide range of data sources to improve performance and\ngeneralization to new data by giving it a deeper understanding of the data\nrepresentation from multi-modal datasets and developed a self-supervised\nframework based on large language models (LLMs), SwinV2 to gain a deeper\nunderstanding of multi-modal dataset representations, enhancing the model's\nability to extrapolate to new data for the detection of eye diseases using\noptical coherence tomography (OCT) images. We adopt a two-phase training\nmethodology, self-supervised pre-training, and fine-tuning on a downstream\nsupervised classifier. An ablation study conducted across three datasets\nemploying various encoder backbones, without data fusion, with low data\navailability setting, and without self-supervised pre-training scenarios,\nhighlights the robustness of our method. Our findings demonstrate consistent\nperformance across these diverse conditions, showcasing superior generalization\ncapabilities compared to the baseline model, ResNet-50.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the medical domain, acquiring large datasets poses significant challenges\ndue to privacy concerns. Nonetheless, the development of a robust deep-learning\nmodel for retinal disease diagnosis necessitates a substantial dataset for\ntraining. The capacity to generalize effectively on smaller datasets remains a\npersistent challenge. The scarcity of data presents a significant barrier to\nthe practical implementation of scalable medical AI solutions. To address this\nissue, we've combined a wide range of data sources to improve performance and\ngeneralization to new data by giving it a deeper understanding of the data\nrepresentation from multi-modal datasets and developed a self-supervised\nframework based on large language models (LLMs), SwinV2 to gain a deeper\nunderstanding of multi-modal dataset representations, enhancing the model's\nability to extrapolate to new data for the detection of eye diseases using\noptical coherence tomography (OCT) images. We adopt a two-phase training\nmethodology, self-supervised pre-training, and fine-tuning on a downstream\nsupervised classifier. An ablation study conducted across three datasets\nemploying various encoder backbones, without data fusion, with low data\navailability setting, and without self-supervised pre-training scenarios,\nhighlights the robustness of our method. Our findings demonstrate consistent\nperformance across these diverse conditions, showcasing superior generalization\ncapabilities compared to the baseline model, ResNet-50."
                },
                "authors": [
                    {
                        "name": "Fatema-E- Jannat"
                    },
                    {
                        "name": "Sina Gholami"
                    },
                    {
                        "name": "Jennifer I. Lim"
                    },
                    {
                        "name": "Theodore Leng"
                    },
                    {
                        "name": "Minhaj Nur Alam"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "arxiv_comment": "25 pages, 9 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08421v2",
                "updated": "2024-09-17T17:20:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    20,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-05-14T08:30:29Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    8,
                    30,
                    29,
                    1,
                    135,
                    0
                ],
                "title": "Faster algorithms for the alignment of sparse correlated Erdös-Rényi\n  random graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster algorithms for the alignment of sparse correlated Erdös-Rényi\n  random graphs"
                },
                "summary": "The correlated Erd\\\"os-R\\'enyi random graph ensemble is a probability law on\npairs of graphs with $n$ vertices, parametrized by their average degree\n$\\lambda$ and their correlation coefficient $s$. It can be used as a benchmark\nfor the graph alignment problem, in which the labels of the vertices of one of\nthe graphs are reshuffled by an unknown permutation; the goal is to infer this\npermutation and thus properly match the pairs of vertices in both graphs. A\nseries of recent works has unveiled the role of Otter's constant $\\alpha$ (that\ncontrols the exponential rate of growth of the number of unlabeled rooted trees\nas a function of their sizes) in this problem: for $s>\\sqrt{\\alpha}$ and\n$\\lambda$ large enough it is possible to recover in a time polynomial in $n$ a\npositive fraction of the hidden permutation. The exponent of this polynomial\ngrowth is however quite large and depends on the other parameters, which limits\nthe range of applications of the algorithm. In this work we present a family of\nfaster algorithms for this task, show through numerical simulations that their\naccuracy is only slightly reduced with respect to the original one, and\nconjecture that they undergo, in the large $\\lambda$ limit, phase transitions\nat modified Otter's thresholds $\\sqrt{\\widehat{\\alpha}}>\\sqrt{\\alpha}$, with\n$\\widehat{\\alpha}$ related to the enumeration of a restricted family of trees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The correlated Erd\\\"os-R\\'enyi random graph ensemble is a probability law on\npairs of graphs with $n$ vertices, parametrized by their average degree\n$\\lambda$ and their correlation coefficient $s$. It can be used as a benchmark\nfor the graph alignment problem, in which the labels of the vertices of one of\nthe graphs are reshuffled by an unknown permutation; the goal is to infer this\npermutation and thus properly match the pairs of vertices in both graphs. A\nseries of recent works has unveiled the role of Otter's constant $\\alpha$ (that\ncontrols the exponential rate of growth of the number of unlabeled rooted trees\nas a function of their sizes) in this problem: for $s>\\sqrt{\\alpha}$ and\n$\\lambda$ large enough it is possible to recover in a time polynomial in $n$ a\npositive fraction of the hidden permutation. The exponent of this polynomial\ngrowth is however quite large and depends on the other parameters, which limits\nthe range of applications of the algorithm. In this work we present a family of\nfaster algorithms for this task, show through numerical simulations that their\naccuracy is only slightly reduced with respect to the original one, and\nconjecture that they undergo, in the large $\\lambda$ limit, phase transitions\nat modified Otter's thresholds $\\sqrt{\\widehat{\\alpha}}>\\sqrt{\\alpha}$, with\n$\\widehat{\\alpha}$ related to the enumeration of a restricted family of trees."
                },
                "authors": [
                    {
                        "name": "Andrea Muratori"
                    },
                    {
                        "name": "Guilhem Semerjian"
                    }
                ],
                "author_detail": {
                    "name": "Guilhem Semerjian"
                },
                "author": "Guilhem Semerjian",
                "arxiv_comment": "34 pages, v2 : clarification of some points, and addition of a\n  comparison with other algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11367v1",
                "updated": "2024-09-17T17:16:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    16,
                    37,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    16,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "OSV: One Step is Enough for High-Quality Image to Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSV: One Step is Enough for High-Quality Image to Video Generation"
                },
                "summary": "Video diffusion models have shown great potential in generating high-quality\nvideos, making them an increasingly popular focus. However, their inherent\niterative nature leads to substantial computational and time costs. While\nefforts have been made to accelerate video diffusion by reducing inference\nsteps (through techniques like consistency distillation) and GAN training\n(these approaches often fall short in either performance or training\nstability). In this work, we introduce a two-stage training framework that\neffectively combines consistency distillation with GAN training to address\nthese challenges. Additionally, we propose a novel video discriminator design,\nwhich eliminates the need for decoding the video latents and improves the final\nperformance. Our model is capable of producing high-quality videos in merely\none-step, with the flexibility to perform multi-step refinement for further\nperformance enhancement. Our quantitative evaluation on the OpenWebVid-1M\nbenchmark shows that our model significantly outperforms existing methods.\nNotably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of\nthe consistency distillation based method, AnimateLCM (FVD 184.79), and\napproaches the 25-step performance of advanced Stable Video Diffusion (FVD\n156.94).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion models have shown great potential in generating high-quality\nvideos, making them an increasingly popular focus. However, their inherent\niterative nature leads to substantial computational and time costs. While\nefforts have been made to accelerate video diffusion by reducing inference\nsteps (through techniques like consistency distillation) and GAN training\n(these approaches often fall short in either performance or training\nstability). In this work, we introduce a two-stage training framework that\neffectively combines consistency distillation with GAN training to address\nthese challenges. Additionally, we propose a novel video discriminator design,\nwhich eliminates the need for decoding the video latents and improves the final\nperformance. Our model is capable of producing high-quality videos in merely\none-step, with the flexibility to perform multi-step refinement for further\nperformance enhancement. Our quantitative evaluation on the OpenWebVid-1M\nbenchmark shows that our model significantly outperforms existing methods.\nNotably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of\nthe consistency distillation based method, AnimateLCM (FVD 184.79), and\napproaches the 25-step performance of advanced Stable Video Diffusion (FVD\n156.94)."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Zhengkai Jiang"
                    },
                    {
                        "name": "Fu-Yun Wang"
                    },
                    {
                        "name": "Wenbing Zhu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yabiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yabiao Wang"
                },
                "author": "Yabiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11365v1",
                "updated": "2024-09-17T17:14:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    14,
                    41,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:14:41Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    14,
                    41,
                    1,
                    261,
                    0
                ],
                "title": "CoCA: Regaining Safety-awareness of Multimodal Large Language Models\n  with Constitutional Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCA: Regaining Safety-awareness of Multimodal Large Language Models\n  with Constitutional Calibration"
                },
                "summary": "The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Tianyang Han"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguo Li"
                },
                "author": "Zhenguo Li",
                "arxiv_comment": "10 pages, COLM-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11364v1",
                "updated": "2024-09-17T17:13:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    13,
                    20,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:13:20Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    13,
                    20,
                    1,
                    261,
                    0
                ],
                "title": "On the number of elements beyond the ones actually observed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the number of elements beyond the ones actually observed"
                },
                "summary": "In this work, a variant of the birth and death chain with constant\nintensities, originally introduced by\n  Bruno de Finetti way back in 1957, is revisited. This fact is also underlined\nby the choice of the title,\n  which is clearly a literal translation of the original one. Characteristic of\nthe variant is that it\n  allows negative jumps of any magnitude. And this, as explained in the paper,\nmight be useful in offering\n  some insight into the issue, arising in numerous situations, of inferring the\nnumber of the undetected\n  elements of a given population. One thinks, for example, of problems\nconcerning abundance or richness of\n  species.\n  The author's purpose is twofold: to align the original de Finetti's\nconstruction with the modern,\n  well-established theory of the continuous-time Markov chains with discrete\nstate space and show how it\n  could be used to make probabilistic previsions on the number of the unseen\nelements of a population.\n  With the aim of enhancing the possible practical applications of the model,\none discusses the statistical\n  point estimation of the rates which characterize its infinitesimal\ndescription.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a variant of the birth and death chain with constant\nintensities, originally introduced by\n  Bruno de Finetti way back in 1957, is revisited. This fact is also underlined\nby the choice of the title,\n  which is clearly a literal translation of the original one. Characteristic of\nthe variant is that it\n  allows negative jumps of any magnitude. And this, as explained in the paper,\nmight be useful in offering\n  some insight into the issue, arising in numerous situations, of inferring the\nnumber of the undetected\n  elements of a given population. One thinks, for example, of problems\nconcerning abundance or richness of\n  species.\n  The author's purpose is twofold: to align the original de Finetti's\nconstruction with the modern,\n  well-established theory of the continuous-time Markov chains with discrete\nstate space and show how it\n  could be used to make probabilistic previsions on the number of the unseen\nelements of a population.\n  With the aim of enhancing the possible practical applications of the model,\none discusses the statistical\n  point estimation of the rates which characterize its infinitesimal\ndescription."
                },
                "authors": [
                    {
                        "name": "Eugenio Regazzini"
                    }
                ],
                "author_detail": {
                    "name": "Eugenio Regazzini"
                },
                "author": "Eugenio Regazzini",
                "arxiv_comment": "33 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60J27 (Primary) 62F10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11360v1",
                "updated": "2024-09-17T17:07:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    7,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:07:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    7,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "AI Suggestions Homogenize Writing Toward Western Styles and Diminish\n  Cultural Nuances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Suggestions Homogenize Writing Toward Western Styles and Diminish\n  Cultural Nuances"
                },
                "summary": "Large language models (LLMs) are being increasingly integrated into everyday\nproducts and services, such as coding tools and writing assistants. As these\nembedded AI applications are deployed globally, there is a growing concern that\nthe AI models underlying these applications prioritize Western values. This\npaper investigates what happens when a Western-centric AI model provides\nwriting suggestions to users from a different cultural background. We conducted\na cross-cultural controlled experiment with 118 participants from India and the\nUnited States who completed culturally grounded writing tasks with and without\nAI suggestions. Our analysis reveals that AI provided greater efficiency gains\nfor Americans compared to Indians. Moreover, AI suggestions led Indian\nparticipants to adopt Western writing styles, altering not just what is written\nbut also how it is written. These findings show that Western-centric AI models\nhomogenize writing toward Western norms, diminishing nuances that differentiate\ncultural expression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being increasingly integrated into everyday\nproducts and services, such as coding tools and writing assistants. As these\nembedded AI applications are deployed globally, there is a growing concern that\nthe AI models underlying these applications prioritize Western values. This\npaper investigates what happens when a Western-centric AI model provides\nwriting suggestions to users from a different cultural background. We conducted\na cross-cultural controlled experiment with 118 participants from India and the\nUnited States who completed culturally grounded writing tasks with and without\nAI suggestions. Our analysis reveals that AI provided greater efficiency gains\nfor Americans compared to Indians. Moreover, AI suggestions led Indian\nparticipants to adopt Western writing styles, altering not just what is written\nbut also how it is written. These findings show that Western-centric AI models\nhomogenize writing toward Western norms, diminishing nuances that differentiate\ncultural expression."
                },
                "authors": [
                    {
                        "name": "Dhruv Agarwal"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11355v1",
                "updated": "2024-09-17T16:58:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    58,
                    52,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:58:52Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    58,
                    52,
                    1,
                    261,
                    0
                ],
                "title": "Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think"
                },
                "summary": "Recent work showed that large diffusion models can be reused as highly\nprecise monocular depth estimators by casting depth estimation as an\nimage-conditional image generation task. While the proposed model achieved\nstate-of-the-art results, high computational demands due to multi-step\ninference limited its use in many scenarios. In this paper, we show that the\nperceived inefficiency was caused by a flaw in the inference pipeline that has\nso far gone unnoticed. The fixed model performs comparably to the best\npreviously reported configuration while being more than 200$\\times$ faster. To\noptimize for downstream task performance, we perform end-to-end fine-tuning on\ntop of the single-step model with task-specific losses and get a deterministic\nmodel that outperforms all other diffusion-based depth and normal estimation\nmodels on common zero-shot benchmarks. We surprisingly find that this\nfine-tuning protocol also works directly on Stable Diffusion and achieves\ncomparable performance to current state-of-the-art diffusion-based depth and\nnormal estimation models, calling into question some of the conclusions drawn\nfrom prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work showed that large diffusion models can be reused as highly\nprecise monocular depth estimators by casting depth estimation as an\nimage-conditional image generation task. While the proposed model achieved\nstate-of-the-art results, high computational demands due to multi-step\ninference limited its use in many scenarios. In this paper, we show that the\nperceived inefficiency was caused by a flaw in the inference pipeline that has\nso far gone unnoticed. The fixed model performs comparably to the best\npreviously reported configuration while being more than 200$\\times$ faster. To\noptimize for downstream task performance, we perform end-to-end fine-tuning on\ntop of the single-step model with task-specific losses and get a deterministic\nmodel that outperforms all other diffusion-based depth and normal estimation\nmodels on common zero-shot benchmarks. We surprisingly find that this\nfine-tuning protocol also works directly on Stable Diffusion and achieves\ncomparable performance to current state-of-the-art diffusion-based depth and\nnormal estimation models, calling into question some of the conclusions drawn\nfrom prior works."
                },
                "authors": [
                    {
                        "name": "Gonzalo Martin Garcia"
                    },
                    {
                        "name": "Karim Abou Zeid"
                    },
                    {
                        "name": "Christian Schmidt"
                    },
                    {
                        "name": "Daan de Geus"
                    },
                    {
                        "name": "Alexander Hermans"
                    },
                    {
                        "name": "Bastian Leibe"
                    }
                ],
                "author_detail": {
                    "name": "Bastian Leibe"
                },
                "author": "Bastian Leibe",
                "arxiv_comment": "Project page: https://vision.rwth-aachen.de/diffusion-e2e-ft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11353v1",
                "updated": "2024-09-17T16:55:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:55:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models"
                },
                "summary": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Mengfei Liang"
                    },
                    {
                        "name": "Archish Arun"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Jonathan Lutch"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Submitted to NeurIPS 2024 SoLaR (Socially Responsible Language\n  Modelling Research ) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11350v1",
                "updated": "2024-09-17T16:53:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    53,
                    47,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:53:47Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    53,
                    47,
                    1,
                    261,
                    0
                ],
                "title": "Clinical Validation of a Real-Time Machine Learning-based System for the\n  Detection of Acute Myeloid Leukemia by Flow Cytometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Validation of a Real-Time Machine Learning-based System for the\n  Detection of Acute Myeloid Leukemia by Flow Cytometry"
                },
                "summary": "Machine-learning (ML) models in flow cytometry have the potential to reduce\nerror rates, increase reproducibility, and boost the efficiency of clinical\nlabs. While numerous ML models for flow cytometry data have been proposed, few\nstudies have described the clinical deployment of such models. Realizing the\npotential gains of ML models in clinical labs requires not only an accurate\nmodel, but infrastructure for automated inference, error detection, analytics\nand monitoring, and structured data extraction. Here, we describe an ML model\nfor detection of Acute Myeloid Leukemia (AML), along with the infrastructure\nsupporting clinical implementation. Our infrastructure leverages the resilience\nand scalability of the cloud for model inference, a Kubernetes-based workflow\nsystem that provides model reproducibility and resource management, and a\nsystem for extracting structured diagnoses from full-text reports. We also\ndescribe our model monitoring and visualization platform, an essential element\nfor ensuring continued model accuracy. Finally, we present a post-deployment\nanalysis of impacts on turn-around time and compare production accuracy to the\noriginal validation statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-learning (ML) models in flow cytometry have the potential to reduce\nerror rates, increase reproducibility, and boost the efficiency of clinical\nlabs. While numerous ML models for flow cytometry data have been proposed, few\nstudies have described the clinical deployment of such models. Realizing the\npotential gains of ML models in clinical labs requires not only an accurate\nmodel, but infrastructure for automated inference, error detection, analytics\nand monitoring, and structured data extraction. Here, we describe an ML model\nfor detection of Acute Myeloid Leukemia (AML), along with the infrastructure\nsupporting clinical implementation. Our infrastructure leverages the resilience\nand scalability of the cloud for model inference, a Kubernetes-based workflow\nsystem that provides model reproducibility and resource management, and a\nsystem for extracting structured diagnoses from full-text reports. We also\ndescribe our model monitoring and visualization platform, an essential element\nfor ensuring continued model accuracy. Finally, we present a post-deployment\nanalysis of impacts on turn-around time and compare production accuracy to the\noriginal validation statistics."
                },
                "authors": [
                    {
                        "name": "Lauren M. Zuromski"
                    },
                    {
                        "name": "Jacob Durtschi"
                    },
                    {
                        "name": "Aimal Aziz"
                    },
                    {
                        "name": "Jeffrey Chumley"
                    },
                    {
                        "name": "Mark Dewey"
                    },
                    {
                        "name": "Paul English"
                    },
                    {
                        "name": "Muir Morrison"
                    },
                    {
                        "name": "Keith Simmon"
                    },
                    {
                        "name": "Blaine Whipple"
                    },
                    {
                        "name": "Brendan O'Fallon"
                    },
                    {
                        "name": "David P. Ng"
                    }
                ],
                "author_detail": {
                    "name": "David P. Ng"
                },
                "author": "David P. Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.TO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09222v2",
                "updated": "2024-09-17T16:52:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    52,
                    28,
                    1,
                    261,
                    0
                ],
                "published": "2024-02-14T15:01:21Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    1,
                    21,
                    2,
                    45,
                    0
                ],
                "title": "Integrating ytopt and libEnsemble to Autotune OpenMC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating ytopt and libEnsemble to Autotune OpenMC"
                },
                "summary": "ytopt is a Python machine-learning-based autotuning software package\ndeveloped within the ECP PROTEAS-TUNE project. The ytopt software adopts an\nasynchronous search framework that consists of sampling a small number of input\nparameter configurations and progressively fitting a surrogate model over the\ninput-output space until exhausting the user-defined maximum number of\nevaluations or the wall-clock time. libEnsemble is a Python toolkit for\ncoordinating workflows of asynchronous and dynamic ensembles of calculations\nacross massively parallel resources developed within the ECP PETSc/TAO project.\nlibEnsemble helps users take advantage of massively parallel resources to solve\ndesign, decision, and inference problems and expands the class of problems that\ncan benefit from increased parallelism. In this paper we present our\nmethodology and framework to integrate ytopt and libEnsemble to take advantage\nof massively parallel resources to accelerate the autotuning process.\nSpecifically, we focus on using the proposed framework to autotune the ECP\nExaSMR application OpenMC, an open source Monte Carlo particle transport code.\nOpenMC has seven tunable parameters some of which have large ranges such as the\nnumber of particles in-flight, which is in the range of 100,000 to 8 million,\nwith its default setting of 1 million. Setting the proper combination of these\nparameter values to achieve the best performance is extremely time-consuming.\nTherefore, we apply the proposed framework to autotune the MPI/OpenMP offload\nversion of OpenMC based on a user-defined metric such as the figure of merit\n(FoM) (particles/s) or energy efficiency energy-delay product (EDP) on Crusher\nat Oak Ridge Leadership Computing Facility. The experimental results show that\nwe achieve improvement up to 29.49\\% in FoM and up to 30.44\\% in EDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ytopt is a Python machine-learning-based autotuning software package\ndeveloped within the ECP PROTEAS-TUNE project. The ytopt software adopts an\nasynchronous search framework that consists of sampling a small number of input\nparameter configurations and progressively fitting a surrogate model over the\ninput-output space until exhausting the user-defined maximum number of\nevaluations or the wall-clock time. libEnsemble is a Python toolkit for\ncoordinating workflows of asynchronous and dynamic ensembles of calculations\nacross massively parallel resources developed within the ECP PETSc/TAO project.\nlibEnsemble helps users take advantage of massively parallel resources to solve\ndesign, decision, and inference problems and expands the class of problems that\ncan benefit from increased parallelism. In this paper we present our\nmethodology and framework to integrate ytopt and libEnsemble to take advantage\nof massively parallel resources to accelerate the autotuning process.\nSpecifically, we focus on using the proposed framework to autotune the ECP\nExaSMR application OpenMC, an open source Monte Carlo particle transport code.\nOpenMC has seven tunable parameters some of which have large ranges such as the\nnumber of particles in-flight, which is in the range of 100,000 to 8 million,\nwith its default setting of 1 million. Setting the proper combination of these\nparameter values to achieve the best performance is extremely time-consuming.\nTherefore, we apply the proposed framework to autotune the MPI/OpenMP offload\nversion of OpenMC based on a user-defined metric such as the figure of merit\n(FoM) (particles/s) or energy efficiency energy-delay product (EDP) on Crusher\nat Oak Ridge Leadership Computing Facility. The experimental results show that\nwe achieve improvement up to 29.49\\% in FoM and up to 30.44\\% in EDP."
                },
                "authors": [
                    {
                        "name": "Xingfu Wu"
                    },
                    {
                        "name": "John R. Tramm"
                    },
                    {
                        "name": "Jeffrey Larson"
                    },
                    {
                        "name": "John-Luke Navarro"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Brice Videau"
                    },
                    {
                        "name": "Michael Kruse"
                    },
                    {
                        "name": "Paul Hovland"
                    },
                    {
                        "name": "Valerie Taylor"
                    },
                    {
                        "name": "Mary Hall"
                    }
                ],
                "author_detail": {
                    "name": "Mary Hall"
                },
                "author": "Mary Hall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13322v2",
                "updated": "2024-09-17T16:29:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    29,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2023-12-20T15:11:06Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    15,
                    11,
                    6,
                    2,
                    354,
                    0
                ],
                "title": "MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks"
                },
                "summary": "With easier access to powerful compute resources, there is a growing trend in\nAI for software development to develop large language models (LLMs) to address\na variety of programming tasks. Even LLMs applied to tasks from the\nhigh-performance computing (HPC) domain are huge in size and demand expensive\ncompute resources for training. This is partly because LLMs for HPC tasks are\nobtained by finetuning existing LLMs that support several natural and/or\nprogramming languages. We found this design choice confusing - why do we need\nLLMs trained on natural languages and programming languages unrelated to HPC\nfor HPC-specific tasks? In this line of work, we aim to question choices made\nby existing LLMs by developing smaller language models (LMs) for specific\ndomains - we call them domain-specific LMs. Specifically, we start with HPC as\na domain and build an HPC-specific LM, named MonoCoder, which is orders of\nmagnitude smaller than existing LMs but delivers better performance on non-HPC\nand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific\ndataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated\nthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.\nResults demonstrate that MonoCoder, although much smaller than existing LMs,\noutperforms other LLMs on normalized-perplexity tests (in relation to model\nsize) while also delivering competing CodeBLEU scores for high-performance and\nparallel code generations. In other words, results suggest that MonoCoder\nunderstands HPC code better than state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With easier access to powerful compute resources, there is a growing trend in\nAI for software development to develop large language models (LLMs) to address\na variety of programming tasks. Even LLMs applied to tasks from the\nhigh-performance computing (HPC) domain are huge in size and demand expensive\ncompute resources for training. This is partly because LLMs for HPC tasks are\nobtained by finetuning existing LLMs that support several natural and/or\nprogramming languages. We found this design choice confusing - why do we need\nLLMs trained on natural languages and programming languages unrelated to HPC\nfor HPC-specific tasks? In this line of work, we aim to question choices made\nby existing LLMs by developing smaller language models (LMs) for specific\ndomains - we call them domain-specific LMs. Specifically, we start with HPC as\na domain and build an HPC-specific LM, named MonoCoder, which is orders of\nmagnitude smaller than existing LMs but delivers better performance on non-HPC\nand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific\ndataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated\nthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.\nResults demonstrate that MonoCoder, although much smaller than existing LMs,\noutperforms other LLMs on normalized-perplexity tests (in relation to model\nsize) while also delivering competing CodeBLEU scores for high-performance and\nparallel code generations. In other words, results suggest that MonoCoder\nunderstands HPC code better than state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Tal Kadosh"
                    },
                    {
                        "name": "Niranjan Hasabnis"
                    },
                    {
                        "name": "Vy A. Vo"
                    },
                    {
                        "name": "Nadav Schneider"
                    },
                    {
                        "name": "Neva Krien"
                    },
                    {
                        "name": "Mihai Capota"
                    },
                    {
                        "name": "Abdul Wasay"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Ted Willke"
                    },
                    {
                        "name": "Guy Tamir"
                    },
                    {
                        "name": "Yuval Pinter"
                    },
                    {
                        "name": "Timothy Mattson"
                    },
                    {
                        "name": "Gal Oren"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oren"
                },
                "author": "Gal Oren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01411v2",
                "updated": "2024-09-17T15:57:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    57,
                    6,
                    1,
                    261,
                    0
                ],
                "published": "2024-02-02T13:42:50Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    13,
                    42,
                    50,
                    4,
                    33,
                    0
                ],
                "title": "CodePori: Large-Scale System for Autonomous Software Development Using\n  Multi-Agent Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePori: Large-Scale System for Autonomous Software Development Using\n  Multi-Agent Technology"
                },
                "summary": "Context: Large Language Models (LLMs) and Generative Pre-trained Transformers\n(GPTs) have transformed the field of Software Engineering (SE). Existing\nLLM-based multi-agent models have successfully addressed basic dialogue tasks.\nHowever, the potential of LLMs for more challenging tasks, such as automated\ncode generation for large and complex projects, has been investigated in only a\nfew existing works. Objective: This paper aims to investigate the potential of\nLLM-based agents in the software industry, particularly in enhancing\nproductivity and reducing time-to-market for complex software solutions. Our\nprimary objective is to gain insights into how these agents can fundamentally\ntransform the development of large-scale software. Methods: We introduce\nCodePori, a novel system designed to automate code generation for large and\ncomplex software projects based on functional and non-functional requirements\ndefined by stakeholders. To assess the proposed system performance, we utilized\nthe HumanEval benchmark and manually tested the CodePori model, providing 20\ndifferent project descriptions as input and then evaluated the code accuracy by\nmanually executing the code. Results: CodePori is able to generate running code\nfor large-scale projects, aligned with the typical software development\nprocess. The HumanEval benchmark results indicate that CodePori improves code\naccuracy by 89%. A manual assessment conducted by the first author shows that\nthe CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the\nresults, our conclusion is that proposed system demonstrates the transformative\npotential of LLM-based agents in SE, highlighting their practical applications\nand opening new opportunities for broader adoption in both industry and\nacademia. Our project is publicly available at\nhttps://github.com/GPT-Laboratory/CodePori.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Models (LLMs) and Generative Pre-trained Transformers\n(GPTs) have transformed the field of Software Engineering (SE). Existing\nLLM-based multi-agent models have successfully addressed basic dialogue tasks.\nHowever, the potential of LLMs for more challenging tasks, such as automated\ncode generation for large and complex projects, has been investigated in only a\nfew existing works. Objective: This paper aims to investigate the potential of\nLLM-based agents in the software industry, particularly in enhancing\nproductivity and reducing time-to-market for complex software solutions. Our\nprimary objective is to gain insights into how these agents can fundamentally\ntransform the development of large-scale software. Methods: We introduce\nCodePori, a novel system designed to automate code generation for large and\ncomplex software projects based on functional and non-functional requirements\ndefined by stakeholders. To assess the proposed system performance, we utilized\nthe HumanEval benchmark and manually tested the CodePori model, providing 20\ndifferent project descriptions as input and then evaluated the code accuracy by\nmanually executing the code. Results: CodePori is able to generate running code\nfor large-scale projects, aligned with the typical software development\nprocess. The HumanEval benchmark results indicate that CodePori improves code\naccuracy by 89%. A manual assessment conducted by the first author shows that\nthe CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the\nresults, our conclusion is that proposed system demonstrates the transformative\npotential of LLM-based agents in SE, highlighting their practical applications\nand opening new opportunities for broader adoption in both industry and\nacademia. Our project is publicly available at\nhttps://github.com/GPT-Laboratory/CodePori."
                },
                "authors": [
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Malik Abdul Sami"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Kari Systä"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "18 pages, 2 figures, and 5 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11283v2",
                "updated": "2024-09-18T05:42:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    42,
                    1,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T15:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    38,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling"
                },
                "summary": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines."
                },
                "authors": [
                    {
                        "name": "Xinyue Fang"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Ziyi Pan"
                    },
                    {
                        "name": "Quntian Fang"
                    },
                    {
                        "name": "Zhihua Wen"
                    },
                    {
                        "name": "Hengyue Pan"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11282v1",
                "updated": "2024-09-17T15:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    37,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    37,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Leveraging Distillation Techniques for Document Understanding: A Case\n  Study with FLAN-T5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Distillation Techniques for Document Understanding: A Case\n  Study with FLAN-T5"
                },
                "summary": "The surge of digital documents in various formats, including less\nstandardized documents such as business reports and environmental assessments,\nunderscores the growing importance of Document Understanding. While Large\nLanguage Models (LLMs) have showcased prowess across diverse natural language\nprocessing tasks, their direct application to Document Understanding remains a\nchallenge. Previous research has demonstrated the utility of LLMs in this\ndomain, yet their significant computational demands make them challenging to\ndeploy effectively. Additionally, proprietary Blackbox LLMs often outperform\ntheir open-source counterparts, posing a barrier to widespread accessibility.\nIn this paper, we delve into the realm of document understanding, leveraging\ndistillation methods to harness the power of large LLMs while accommodating\ncomputational limitations. Specifically, we present a novel approach wherein we\ndistill document understanding knowledge from the proprietary LLM ChatGPT into\nFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms\nto facilitate efficient knowledge transfer. This work contributes to the\nadvancement of document understanding methodologies by offering a scalable\nsolution that bridges the gap between resource-intensive LLMs and practical\napplications. Our findings underscore the potential of distillation techniques\nin facilitating the deployment of sophisticated language models in real-world\nscenarios, thereby fostering advancements in natural language processing and\ndocument comprehension domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of digital documents in various formats, including less\nstandardized documents such as business reports and environmental assessments,\nunderscores the growing importance of Document Understanding. While Large\nLanguage Models (LLMs) have showcased prowess across diverse natural language\nprocessing tasks, their direct application to Document Understanding remains a\nchallenge. Previous research has demonstrated the utility of LLMs in this\ndomain, yet their significant computational demands make them challenging to\ndeploy effectively. Additionally, proprietary Blackbox LLMs often outperform\ntheir open-source counterparts, posing a barrier to widespread accessibility.\nIn this paper, we delve into the realm of document understanding, leveraging\ndistillation methods to harness the power of large LLMs while accommodating\ncomputational limitations. Specifically, we present a novel approach wherein we\ndistill document understanding knowledge from the proprietary LLM ChatGPT into\nFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms\nto facilitate efficient knowledge transfer. This work contributes to the\nadvancement of document understanding methodologies by offering a scalable\nsolution that bridges the gap between resource-intensive LLMs and practical\napplications. Our findings underscore the potential of distillation techniques\nin facilitating the deployment of sophisticated language models in real-world\nscenarios, thereby fostering advancements in natural language processing and\ndocument comprehension domains."
                },
                "authors": [
                    {
                        "name": "Marcel Lamott"
                    },
                    {
                        "name": "Muhammad Armaghan Shakir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Armaghan Shakir"
                },
                "author": "Muhammad Armaghan Shakir",
                "arxiv_comment": "Presented at AI@WORK-Workshop / Informatik-Festival (GI-Jahrestagung)\n  (Wiesbaden, Germany, 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11279v1",
                "updated": "2024-09-17T15:29:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    29,
                    34,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:29:34Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    29,
                    34,
                    1,
                    261,
                    0
                ],
                "title": "P-RAG: Progressive Retrieval Augmented Generation For Planning on\n  Embodied Everyday Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-RAG: Progressive Retrieval Augmented Generation For Planning on\n  Embodied Everyday Task"
                },
                "summary": "Embodied Everyday Task is a popular task in the embodied AI community,\nrequiring agents to make a sequence of actions based on natural language\ninstructions and visual observations. Traditional learning-based approaches\nface two challenges. Firstly, natural language instructions often lack explicit\ntask planning. Secondly, extensive training is required to equip models with\nknowledge of the task environment. Previous works based on Large Language Model\n(LLM) either suffer from poor performance due to the lack of task-specific\nknowledge or rely on ground truth as few-shot samples. To address the above\nlimitations, we propose a novel approach called Progressive Retrieval Augmented\nGeneration (P-RAG), which not only effectively leverages the powerful language\nprocessing capabilities of LLMs but also progressively accumulates\ntask-specific knowledge without ground-truth. Compared to the conventional RAG\nmethods, which retrieve relevant information from the database in a one-shot\nmanner to assist generation, P-RAG introduces an iterative approach to\nprogressively update the database. In each iteration, P-RAG retrieves the\nlatest database and obtains historical information from the previous\ninteraction as experiential references for the current interaction. Moreover,\nwe also introduce a more granular retrieval scheme that not only retrieves\nsimilar tasks but also incorporates retrieval of similar situations to provide\nmore valuable reference experiences. Extensive experiments reveal that P-RAG\nachieves competitive results without utilizing ground truth and can even\nfurther improve performance through self-iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Everyday Task is a popular task in the embodied AI community,\nrequiring agents to make a sequence of actions based on natural language\ninstructions and visual observations. Traditional learning-based approaches\nface two challenges. Firstly, natural language instructions often lack explicit\ntask planning. Secondly, extensive training is required to equip models with\nknowledge of the task environment. Previous works based on Large Language Model\n(LLM) either suffer from poor performance due to the lack of task-specific\nknowledge or rely on ground truth as few-shot samples. To address the above\nlimitations, we propose a novel approach called Progressive Retrieval Augmented\nGeneration (P-RAG), which not only effectively leverages the powerful language\nprocessing capabilities of LLMs but also progressively accumulates\ntask-specific knowledge without ground-truth. Compared to the conventional RAG\nmethods, which retrieve relevant information from the database in a one-shot\nmanner to assist generation, P-RAG introduces an iterative approach to\nprogressively update the database. In each iteration, P-RAG retrieves the\nlatest database and obtains historical information from the previous\ninteraction as experiential references for the current interaction. Moreover,\nwe also introduce a more granular retrieval scheme that not only retrieves\nsimilar tasks but also incorporates retrieval of similar situations to provide\nmore valuable reference experiences. Extensive experiments reveal that P-RAG\nachieves competitive results without utilizing ground truth and can even\nfurther improve performance through self-iterations."
                },
                "authors": [
                    {
                        "name": "Weiye Xu"
                    },
                    {
                        "name": "Min Wang"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Houqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Houqiang Li"
                },
                "author": "Houqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11276v1",
                "updated": "2024-09-17T15:28:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    28,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:28:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    28,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable potential across various\ndomains, including cybersecurity. Using commercial cloud-based LLMs may be\nundesirable due to privacy concerns, costs, and network connectivity\nconstraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be\nused as a red-team agent within network security environments. Our fine-tuned 7\nbillion parameter model can run on a single GPU card and achieves performance\ncomparable with much larger and more powerful commercial models such as GPT-4.\nHackphyr clearly outperforms other models, including GPT-3.5-turbo, and\nbaselines, such as Q-learning agents in complex, previously unseen scenarios.\nTo achieve this performance, we generated a new task-specific cybersecurity\ndataset to enhance the base model's capabilities. Finally, we conducted a\ncomprehensive analysis of the agents' behaviors that provides insights into the\nplanning abilities and potential shortcomings of such agents, contributing to\nthe broader understanding of LLM-based agents in cybersecurity contexts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable potential across various\ndomains, including cybersecurity. Using commercial cloud-based LLMs may be\nundesirable due to privacy concerns, costs, and network connectivity\nconstraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be\nused as a red-team agent within network security environments. Our fine-tuned 7\nbillion parameter model can run on a single GPU card and achieves performance\ncomparable with much larger and more powerful commercial models such as GPT-4.\nHackphyr clearly outperforms other models, including GPT-3.5-turbo, and\nbaselines, such as Q-learning agents in complex, previously unseen scenarios.\nTo achieve this performance, we generated a new task-specific cybersecurity\ndataset to enhance the base model's capabilities. Finally, we conducted a\ncomprehensive analysis of the agents' behaviors that provides insights into the\nplanning abilities and potential shortcomings of such agents, contributing to\nthe broader understanding of LLM-based agents in cybersecurity contexts"
                },
                "authors": [
                    {
                        "name": "Maria Rigaki"
                    },
                    {
                        "name": "Carlos Catania"
                    },
                    {
                        "name": "Sebastian Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Garcia"
                },
                "author": "Sebastian Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11274v1",
                "updated": "2024-09-17T15:25:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    25,
                    11,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:25:11Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    25,
                    11,
                    1,
                    261,
                    0
                ],
                "title": "Task Arithmetic for Language Expansion in Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Arithmetic for Language Expansion in Speech Translation"
                },
                "summary": "Recent advances in large language models (LLMs) have gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-based speech translation (ST). However, expanding language pairs\nfrom an existing instruction-tuned ST system is costly due to the necessity of\nre-training on a combination of new and previous datasets. We propose to expand\nnew language pairs by merging the model trained on new language pairs and the\nexisting model, using task arithmetic. We find that the direct application of\ntask arithmetic for ST causes the merged model to fail to follow instructions;\nthus, generating translation in incorrect languages. To eliminate language\nconfusion, we propose an augmented task arithmetic method that merges an\nadditional language control model. It is trained to generate the correct target\nlanguage token following the instructions. Our experiments demonstrate that our\nproposed language control model can achieve language expansion by eliminating\nlanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66\nand 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the\nuse of our task arithmetic framework can expand to a language pair where\nneither paired ST training data nor a pre-trained ST model is available. We\nfirst synthesize the ST system from machine translation (MT) systems via task\nanalogy, then merge the synthesized ST system to the existing ST model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-based speech translation (ST). However, expanding language pairs\nfrom an existing instruction-tuned ST system is costly due to the necessity of\nre-training on a combination of new and previous datasets. We propose to expand\nnew language pairs by merging the model trained on new language pairs and the\nexisting model, using task arithmetic. We find that the direct application of\ntask arithmetic for ST causes the merged model to fail to follow instructions;\nthus, generating translation in incorrect languages. To eliminate language\nconfusion, we propose an augmented task arithmetic method that merges an\nadditional language control model. It is trained to generate the correct target\nlanguage token following the instructions. Our experiments demonstrate that our\nproposed language control model can achieve language expansion by eliminating\nlanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66\nand 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the\nuse of our task arithmetic framework can expand to a language pair where\nneither paired ST training data nor a pre-trained ST model is available. We\nfirst synthesize the ST system from machine translation (MT) systems via task\nanalogy, then merge the synthesized ST system to the existing ST model."
                },
                "authors": [
                    {
                        "name": "Yao-Fei Cheng"
                    },
                    {
                        "name": "Hayato Futami"
                    },
                    {
                        "name": "Yosuke Kashiwagi"
                    },
                    {
                        "name": "Emiru Tsunoo"
                    },
                    {
                        "name": "Wen Shen Teo"
                    },
                    {
                        "name": "Siddhant Arora"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11265v2",
                "updated": "2024-09-18T07:26:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    26,
                    40,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T15:15:03Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    15,
                    3,
                    1,
                    261,
                    0
                ],
                "title": "Performance of Cross-Validated Targeted Maximum Likelihood Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Cross-Validated Targeted Maximum Likelihood Estimation"
                },
                "summary": "Background: Advanced methods for causal inference, such as targeted maximum\nlikelihood estimation (TMLE), require certain conditions for statistical\ninference. However, in situations where there is not differentiability due to\ndata sparsity or near-positivity violations, the Donsker class condition is\nviolated. In such situations, TMLE variance can suffer from inflation of the\ntype I error and poor coverage, leading to conservative confidence intervals.\nCross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve\non performance compared to TMLE in settings of positivity or Donsker class\nviolations. We aim to investigate the performance of CVTMLE compared to TMLE in\nvarious settings.\n  Methods: We utilised the data-generating mechanism as described in Leger et\nal. (2022) to run a Monte Carlo experiment under different Donsker class\nviolations. Then, we evaluated the respective statistical performances of TMLE\nand CVTMLE with different super learner libraries, with and without regression\ntree methods.\n  Results: We found that CVTMLE vastly improves confidence interval coverage\nwithout adversely affecting bias, particularly in settings with small sample\nsizes and near-positivity violations. Furthermore, incorporating regression\ntrees using standard TMLE with ensemble super learner-based initial estimates\nincreases bias and variance leading to invalid statistical inference.\n  Conclusions: It has been shown that when using CVTMLE the Donsker class\ncondition is no longer necessary to obtain valid statistical inference when\nusing regression trees and under either data sparsity or near-positivity\nviolations. We show through simulations that CVTMLE is much less sensitive to\nthe choice of the super learner library and thereby provides better estimation\nand inference in cases where the super learner library uses more flexible\ncandidates and is prone to overfitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Advanced methods for causal inference, such as targeted maximum\nlikelihood estimation (TMLE), require certain conditions for statistical\ninference. However, in situations where there is not differentiability due to\ndata sparsity or near-positivity violations, the Donsker class condition is\nviolated. In such situations, TMLE variance can suffer from inflation of the\ntype I error and poor coverage, leading to conservative confidence intervals.\nCross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve\non performance compared to TMLE in settings of positivity or Donsker class\nviolations. We aim to investigate the performance of CVTMLE compared to TMLE in\nvarious settings.\n  Methods: We utilised the data-generating mechanism as described in Leger et\nal. (2022) to run a Monte Carlo experiment under different Donsker class\nviolations. Then, we evaluated the respective statistical performances of TMLE\nand CVTMLE with different super learner libraries, with and without regression\ntree methods.\n  Results: We found that CVTMLE vastly improves confidence interval coverage\nwithout adversely affecting bias, particularly in settings with small sample\nsizes and near-positivity violations. Furthermore, incorporating regression\ntrees using standard TMLE with ensemble super learner-based initial estimates\nincreases bias and variance leading to invalid statistical inference.\n  Conclusions: It has been shown that when using CVTMLE the Donsker class\ncondition is no longer necessary to obtain valid statistical inference when\nusing regression trees and under either data sparsity or near-positivity\nviolations. We show through simulations that CVTMLE is much less sensitive to\nthe choice of the super learner library and thereby provides better estimation\nand inference in cases where the super learner library uses more flexible\ncandidates and is prone to overfitting."
                },
                "authors": [
                    {
                        "name": "Matthew J. Smith"
                    },
                    {
                        "name": "Rachael V. Phillips"
                    },
                    {
                        "name": "Camille Maringe"
                    },
                    {
                        "name": "Miguel Angel Luque-Fernandez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Angel Luque-Fernandez"
                },
                "author": "Miguel Angel Luque-Fernandez",
                "arxiv_comment": "20 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.13867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.13867v3",
                "updated": "2024-09-17T15:03:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    3,
                    42,
                    1,
                    261,
                    0
                ],
                "published": "2022-10-25T09:43:36Z",
                "published_parsed": [
                    2022,
                    10,
                    25,
                    9,
                    43,
                    36,
                    1,
                    298,
                    0
                ],
                "title": "A Dynamical System View of Langevin-Based Non-Convex Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dynamical System View of Langevin-Based Non-Convex Sampling"
                },
                "summary": "Non-convex sampling is a key challenge in machine learning, central to\nnon-convex optimization in deep learning as well as to approximate\nprobabilistic inference. Despite its significance, theoretically there remain\nmany important challenges: Existing guarantees (1) typically only hold for the\naveraged iterates rather than the more desirable last iterates, (2) lack\nconvergence metrics that capture the scales of the variables such as\nWasserstein distances, and (3) mainly apply to elementary schemes such as\nstochastic gradient Langevin dynamics. In this paper, we develop a new\nframework that lifts the above issues by harnessing several tools from the\ntheory of dynamical systems. Our key result is that, for a large class of\nstate-of-the-art sampling schemes, their last-iterate convergence in\nWasserstein distances can be reduced to the study of their continuous-time\ncounterparts, which is much better understood. Coupled with standard\nassumptions of MCMC sampling, our theory immediately yields the last-iterate\nWasserstein convergence of many advanced sampling schemes such as proximal,\nrandomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our\nframework also motivates more efficient schemes that enjoy the same rigorous\nguarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-convex sampling is a key challenge in machine learning, central to\nnon-convex optimization in deep learning as well as to approximate\nprobabilistic inference. Despite its significance, theoretically there remain\nmany important challenges: Existing guarantees (1) typically only hold for the\naveraged iterates rather than the more desirable last iterates, (2) lack\nconvergence metrics that capture the scales of the variables such as\nWasserstein distances, and (3) mainly apply to elementary schemes such as\nstochastic gradient Langevin dynamics. In this paper, we develop a new\nframework that lifts the above issues by harnessing several tools from the\ntheory of dynamical systems. Our key result is that, for a large class of\nstate-of-the-art sampling schemes, their last-iterate convergence in\nWasserstein distances can be reduced to the study of their continuous-time\ncounterparts, which is much better understood. Coupled with standard\nassumptions of MCMC sampling, our theory immediately yields the last-iterate\nWasserstein convergence of many advanced sampling schemes such as proximal,\nrandomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our\nframework also motivates more efficient schemes that enjoy the same rigorous\nguarantees."
                },
                "authors": [
                    {
                        "name": "Mohammad Reza Karimi"
                    },
                    {
                        "name": "Ya-Ping Hsieh"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "arxiv_comment": "typos corrected, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.13867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.13867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11254v1",
                "updated": "2024-09-17T15:02:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    2,
                    32,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:02:32Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    2,
                    32,
                    1,
                    261,
                    0
                ],
                "title": "Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach"
                },
                "summary": "As the complexity and connectivity of networks increase, the need for novel\nmalware detection approaches becomes imperative. Traditional security defenses\nare becoming less effective against the advanced tactics of today's\ncyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in\nstrengthening network security, offering detailed analysis of network traffic\nthat goes beyond simple metadata analysis. DPI examines not only the packet\nheaders but also the payload content within, offering a thorough insight into\nthe data traversing the network. This study proposes a novel approach that\nleverages a large language model (LLM) and few-shot learning to accurately\nrecognizes novel, unseen malware types with few labels samples. Our proposed\napproach uses a pretrained LLM on known malware types to extract the embeddings\nfrom packets. The embeddings are then used alongside few labeled samples of an\nunseen malware type. This technique is designed to acclimate the model to\ndifferent malware representations, further enabling it to generate robust\nembeddings for each trained and unseen classes. Following the extraction of\nembeddings from the LLM, few-shot learning is utilized to enhance performance\nwith minimal labeled data. Our evaluation, which utilized two renowned\ndatasets, focused on identifying malware types within network traffic and\nInternet of Things (IoT) environments. Our approach shows promising results\nwith an average accuracy of 86.35% and F1-Score of 86.40% on different malware\ntypes across the two datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity and connectivity of networks increase, the need for novel\nmalware detection approaches becomes imperative. Traditional security defenses\nare becoming less effective against the advanced tactics of today's\ncyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in\nstrengthening network security, offering detailed analysis of network traffic\nthat goes beyond simple metadata analysis. DPI examines not only the packet\nheaders but also the payload content within, offering a thorough insight into\nthe data traversing the network. This study proposes a novel approach that\nleverages a large language model (LLM) and few-shot learning to accurately\nrecognizes novel, unseen malware types with few labels samples. Our proposed\napproach uses a pretrained LLM on known malware types to extract the embeddings\nfrom packets. The embeddings are then used alongside few labeled samples of an\nunseen malware type. This technique is designed to acclimate the model to\ndifferent malware representations, further enabling it to generate robust\nembeddings for each trained and unseen classes. Following the extraction of\nembeddings from the LLM, few-shot learning is utilized to enhance performance\nwith minimal labeled data. Our evaluation, which utilized two renowned\ndatasets, focused on identifying malware types within network traffic and\nInternet of Things (IoT) environments. Our approach shows promising results\nwith an average accuracy of 86.35% and F1-Score of 86.40% on different malware\ntypes across the two datasets."
                },
                "authors": [
                    {
                        "name": "Kyle Stein"
                    },
                    {
                        "name": "Andrew A. Mahyari"
                    },
                    {
                        "name": "Guillermo Francia III"
                    },
                    {
                        "name": "Eman El-Sheikh"
                    }
                ],
                "author_detail": {
                    "name": "Eman El-Sheikh"
                },
                "author": "Eman El-Sheikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11242v1",
                "updated": "2024-09-17T14:47:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:47:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse"
                },
                "summary": "LLMs are an integral part of retrieval-augmented generation (RAG) systems.\nWhile many studies focus on evaluating the quality of end-to-end RAG systems,\nthere is a lack of research on understanding the appropriateness of an LLM for\nthe RAG task. Thus, we introduce a new metric, Trust-Score, that provides a\nholistic evaluation of the trustworthiness of LLMs in an RAG framework. We show\nthat various prompting methods, such as in-context learning, fail to adapt LLMs\neffectively to the RAG task. Thus, we propose Trust-Align, a framework to align\nLLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly\noutperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up\n29.2) and ELI5 (up 14.9). We release our code at:\nhttps://github.com/declare-lab/trust-align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are an integral part of retrieval-augmented generation (RAG) systems.\nWhile many studies focus on evaluating the quality of end-to-end RAG systems,\nthere is a lack of research on understanding the appropriateness of an LLM for\nthe RAG task. Thus, we introduce a new metric, Trust-Score, that provides a\nholistic evaluation of the trustworthiness of LLMs in an RAG framework. We show\nthat various prompting methods, such as in-context learning, fail to adapt LLMs\neffectively to the RAG task. Thus, we propose Trust-Align, a framework to align\nLLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly\noutperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up\n29.2) and ELI5 (up 14.9). We release our code at:\nhttps://github.com/declare-lab/trust-align."
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Shang Hong Sim"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Hai Leong Chieu"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09662v2",
                "updated": "2024-09-17T14:44:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    44,
                    34,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-15T08:25:24Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    8,
                    25,
                    24,
                    6,
                    259,
                    0
                ],
                "title": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models"
                },
                "summary": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. Current systems often\nlimit users' flexibility to direct their reflections. We thus present\nExploreSelf, an LLM-driven application designed to empower users to control\ntheir reflective journey. ExploreSelf allows users to receive adaptive support\nthrough dynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe balance between guided support and freedom to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss implications for designing LLM-driven tools that promote user\nempowerment through effective reflective practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. Current systems often\nlimit users' flexibility to direct their reflections. We thus present\nExploreSelf, an LLM-driven application designed to empower users to control\ntheir reflective journey. ExploreSelf allows users to receive adaptive support\nthrough dynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe balance between guided support and freedom to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss implications for designing LLM-driven tools that promote user\nempowerment through effective reflective practices."
                },
                "authors": [
                    {
                        "name": "Inhwa Song"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Jessica Lee Schleider"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "17 pages excluding reference and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11239v1",
                "updated": "2024-09-17T14:40:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:40:02Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "title": "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do"
                },
                "summary": "LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\noutside of English remains largely unexplored. In this paper, we conduct a\ncomprehensive analysis on automated evaluators, reporting key findings on their\nbehavior in a non-English environment. First, we discover that English\nevaluation capabilities significantly influence language-specific capabilities,\noften more than the language proficiency itself, enabling evaluators trained in\nEnglish to easily transfer their skills to other languages. Second, we identify\ncritical shortcomings, where LLMs fail to detect and penalize errors, such as\nfactual inaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we release Kudge, the first non-English meta-evaluation\ndataset containing 5,012 human annotations in Korean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\noutside of English remains largely unexplored. In this paper, we conduct a\ncomprehensive analysis on automated evaluators, reporting key findings on their\nbehavior in a non-English environment. First, we discover that English\nevaluation capabilities significantly influence language-specific capabilities,\noften more than the language proficiency itself, enabling evaluators trained in\nEnglish to easily transfer their skills to other languages. Second, we identify\ncritical shortcomings, where LLMs fail to detect and penalize errors, such as\nfactual inaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we release Kudge, the first non-English meta-evaluation\ndataset containing 5,012 human annotations in Korean."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Seunghyeok Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghyeok Hong"
                },
                "author": "Seunghyeok Hong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11233v1",
                "updated": "2024-09-17T14:34:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    34,
                    11,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:34:11Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    34,
                    11,
                    1,
                    261,
                    0
                ],
                "title": "Evaluating the Impact of Compression Techniques on Task-Specific\n  Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Impact of Compression Techniques on Task-Specific\n  Performance of Large Language Models"
                },
                "summary": "Large language models (LLMs) offer powerful capabilities but incur\nsubstantial computational costs, driving the need for efficient compression\ntechniques. This study evaluates the impact of popular compression methods -\nMagnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on\nthe trade-offs between model size reduction, downstream task performance, and\nthe role of calibration data. Our findings reveal that while SparseGPT and\nWanda preserve perplexity even at 50% sparsity, they suffer significant\ndegradation on downstream tasks, highlighting the inadequacy of perplexity as\nthe sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)\nDivergence as a more comprehensive metric that captures nuanced changes in\nmodel behavior post-compression. We further demonstrate that task-specific\ncalibration data significantly enhances the downstream performance of\ncompressed models compared to general calibration data. This research\nunderscores the necessity for diverse evaluation metrics and careful\ncalibration data selection to fully understand the complexities of LLM\ncompression and its implications for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer powerful capabilities but incur\nsubstantial computational costs, driving the need for efficient compression\ntechniques. This study evaluates the impact of popular compression methods -\nMagnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on\nthe trade-offs between model size reduction, downstream task performance, and\nthe role of calibration data. Our findings reveal that while SparseGPT and\nWanda preserve perplexity even at 50% sparsity, they suffer significant\ndegradation on downstream tasks, highlighting the inadequacy of perplexity as\nthe sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)\nDivergence as a more comprehensive metric that captures nuanced changes in\nmodel behavior post-compression. We further demonstrate that task-specific\ncalibration data significantly enhances the downstream performance of\ncompressed models compared to general calibration data. This research\nunderscores the necessity for diverse evaluation metrics and careful\ncalibration data selection to fully understand the complexities of LLM\ncompression and its implications for practical applications."
                },
                "authors": [
                    {
                        "name": "Bishwash Khanal"
                    },
                    {
                        "name": "Jeffery M. Capone"
                    }
                ],
                "author_detail": {
                    "name": "Jeffery M. Capone"
                },
                "author": "Jeffery M. Capone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11232v1",
                "updated": "2024-09-17T14:29:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    29,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:29:03Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    29,
                    3,
                    1,
                    261,
                    0
                ],
                "title": "Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT\n  Problem: Does the LLM Solve the Problem Itself or Call an External SAT\n  Solver?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT\n  Problem: Does the LLM Solve the Problem Itself or Call an External SAT\n  Solver?"
                },
                "summary": "In this manuscript I present an analysis on the performance of OpenAI\nO1-preview model in solving random K-SAT instances for K$\\in {2,3,4}$ as a\nfunction of $\\alpha=M/N$ where $M$ is the number of clauses and $N$ is the\nnumber of variables of the satisfiable problem. I show that the model can call\nan external SAT solver to solve the instances, rather than solving them\ndirectly. Despite using external solvers, the model reports incorrect\nassignments as output. Moreover, I propose and present an analysis to quantify\nwhether the OpenAI O1-preview model demonstrates a spark of intelligence or\nmerely makes random guesses when outputting an assignment for a Boolean\nsatisfiability problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this manuscript I present an analysis on the performance of OpenAI\nO1-preview model in solving random K-SAT instances for K$\\in {2,3,4}$ as a\nfunction of $\\alpha=M/N$ where $M$ is the number of clauses and $N$ is the\nnumber of variables of the satisfiable problem. I show that the model can call\nan external SAT solver to solve the instances, rather than solving them\ndirectly. Despite using external solvers, the model reports incorrect\nassignments as output. Moreover, I propose and present an analysis to quantify\nwhether the OpenAI O1-preview model demonstrates a spark of intelligence or\nmerely makes random guesses when outputting an assignment for a Boolean\nsatisfiability problem."
                },
                "authors": [
                    {
                        "name": "Raffaele Marino"
                    }
                ],
                "author_detail": {
                    "name": "Raffaele Marino"
                },
                "author": "Raffaele Marino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19071v2",
                "updated": "2024-09-17T14:24:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    24,
                    47,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-27T10:41:22Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    10,
                    41,
                    22,
                    3,
                    179,
                    0
                ],
                "title": "EmPO: Emotion Grounding for Empathetic Response Generation through\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmPO: Emotion Grounding for Empathetic Response Generation through\n  Preference Optimization"
                },
                "summary": "Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. We propose a novel approach where we\nconstruct theory-driven preference datasets based on emotion grounding and use\nthem to align LLMs with preference optimization algorithms to address these\nchallenges. To evaluate empathetic response generation, we employ the\nEmpatheticDialogues dataset, assessing empathy with the diff-Epitome and\nBERTscore metrics and with multi-dimensional human evaluation. Additionally, we\nmeasure diversity and emotional valence using feature-based methods. We also\nevaluate the impact of training on the generalization performance using the\nMMLU benchmark and tasks from the Open LLM Leaderboard. The results show that\nLLMs can be aligned for empathetic response generation by preference\noptimization while retaining their general performance and that emotion\ngrounding can guide preference dataset creation. We make all datasets, source\ncode, and models publicly available. https://github.com/justtherightsize/empo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. We propose a novel approach where we\nconstruct theory-driven preference datasets based on emotion grounding and use\nthem to align LLMs with preference optimization algorithms to address these\nchallenges. To evaluate empathetic response generation, we employ the\nEmpatheticDialogues dataset, assessing empathy with the diff-Epitome and\nBERTscore metrics and with multi-dimensional human evaluation. Additionally, we\nmeasure diversity and emotional valence using feature-based methods. We also\nevaluate the impact of training on the generalization performance using the\nMMLU benchmark and tasks from the Open LLM Leaderboard. The results show that\nLLMs can be aligned for empathetic response generation by preference\noptimization while retaining their general performance and that emotion\ngrounding can guide preference dataset creation. We make all datasets, source\ncode, and models publicly available. https://github.com/justtherightsize/empo"
                },
                "authors": [
                    {
                        "name": "Ondrej Sotolar"
                    },
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Alok Debnath"
                    },
                    {
                        "name": "Allison Lahnala"
                    },
                    {
                        "name": "Charles Welch"
                    },
                    {
                        "name": "Lucie FLek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie FLek"
                },
                "author": "Lucie FLek",
                "arxiv_comment": "v02, 8 pages long paper, EMNLP ACL style",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11218v1",
                "updated": "2024-09-17T14:12:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    12,
                    8,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:12:08Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    12,
                    8,
                    1,
                    261,
                    0
                ],
                "title": "Exploring ChatGPT-based Augmentation Strategies for Contrastive\n  Aspect-based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT-based Augmentation Strategies for Contrastive\n  Aspect-based Sentiment Analysis"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards\nspecific aspect terms in a sentence and allows us to uncover nuanced\nperspectives and attitudes on particular aspects of a product, service, or\ntopic. However, the scarcity of labeled data poses a significant challenge to\ntraining high-quality models. To address this issue, we explore the potential\nof data augmentation using ChatGPT, a well-performing large language model\n(LLM), to enhance the sentiment classification performance towards aspect\nterms. Specifically, we explore three data augmentation strategies based on\nChatGPT: context-focused, aspect-focused, and context-aspect data augmentation\ntechniques. Context-focused data augmentation focuses on changing the word\nexpression of context words in the sentence while keeping aspect terms\nunchanged. In contrast, aspect-focused data augmentation aims to change aspect\nterms but keep context words unchanged. Context-Aspect data augmentation\nintegrates the above two data augmentations to generate augmented samples.\nFurthermore, we incorporate contrastive learning into the ABSA tasks to improve\nperformance. Extensive experiments show that all three data augmentation\ntechniques lead to performance improvements, with the context-aspect data\naugmentation strategy performing best and surpassing the performance of the\nbaseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards\nspecific aspect terms in a sentence and allows us to uncover nuanced\nperspectives and attitudes on particular aspects of a product, service, or\ntopic. However, the scarcity of labeled data poses a significant challenge to\ntraining high-quality models. To address this issue, we explore the potential\nof data augmentation using ChatGPT, a well-performing large language model\n(LLM), to enhance the sentiment classification performance towards aspect\nterms. Specifically, we explore three data augmentation strategies based on\nChatGPT: context-focused, aspect-focused, and context-aspect data augmentation\ntechniques. Context-focused data augmentation focuses on changing the word\nexpression of context words in the sentence while keeping aspect terms\nunchanged. In contrast, aspect-focused data augmentation aims to change aspect\nterms but keep context words unchanged. Context-Aspect data augmentation\nintegrates the above two data augmentations to generate augmented samples.\nFurthermore, we incorporate contrastive learning into the ABSA tasks to improve\nperformance. Extensive experiments show that all three data augmentation\ntechniques lead to performance improvements, with the context-aspect data\naugmentation strategy performing best and surpassing the performance of the\nbaseline models."
                },
                "authors": [
                    {
                        "name": "Lingling Xu"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "S. Joe Qin"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Xiaohui Tao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Tao"
                },
                "author": "Xiaohui Tao",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11214v1",
                "updated": "2024-09-17T14:10:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    10,
                    57,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:10:57Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    10,
                    57,
                    1,
                    261,
                    0
                ],
                "title": "Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for\n  Multilingual Speech-to-Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for\n  Multilingual Speech-to-Text"
                },
                "summary": "Integrating audio encoders with LLMs through connectors has enabled these\nmodels to process and comprehend audio modalities, significantly enhancing\nspeech-to-text tasks, including automatic speech recognition (ASR) and\nautomatic speech translation (AST). However, these methods often overlook the\ncritical aspect of language adaptation in multilingual settings, relying\ninstead on multilingual data without adequately addressing language\ndifferences. To address this gap, we propose the Ideal-LLM model, which employs\ndual multilingual encoders to enrich language feature information and utilizes\na language-adapted connector to target the adaptation of each language\nspecifically. By leveraging the complementary strengths of Whisper and MMS\nencoders, our approach ensures richer multilingual representations.\nAdditionally, the language-adapted connector enhances modal transformation via\na language weight selector tailored for each language. Experimental results\ndemonstrate that Ideal-LLM significantly improves ASR performance, achieving a\n32.6% relative reduction in average word error rates compared to the standard\nspeech encoder integrated with LLMs and yields an average BLEU score of 36.78\nfor AST task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio encoders with LLMs through connectors has enabled these\nmodels to process and comprehend audio modalities, significantly enhancing\nspeech-to-text tasks, including automatic speech recognition (ASR) and\nautomatic speech translation (AST). However, these methods often overlook the\ncritical aspect of language adaptation in multilingual settings, relying\ninstead on multilingual data without adequately addressing language\ndifferences. To address this gap, we propose the Ideal-LLM model, which employs\ndual multilingual encoders to enrich language feature information and utilizes\na language-adapted connector to target the adaptation of each language\nspecifically. By leveraging the complementary strengths of Whisper and MMS\nencoders, our approach ensures richer multilingual representations.\nAdditionally, the language-adapted connector enhances modal transformation via\na language weight selector tailored for each language. Experimental results\ndemonstrate that Ideal-LLM significantly improves ASR performance, achieving a\n32.6% relative reduction in average word error rates compared to the standard\nspeech encoder integrated with LLMs and yields an average BLEU score of 36.78\nfor AST task."
                },
                "authors": [
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Wei Ren"
                    },
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Longhao Li"
                    },
                    {
                        "name": "Qijie Shao"
                    },
                    {
                        "name": "Linju Yang"
                    },
                    {
                        "name": "Kai Diao"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "5 pages, 3 figures, submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11212v1",
                "updated": "2024-09-17T14:05:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    5,
                    58,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:05:58Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    5,
                    58,
                    1,
                    261,
                    0
                ],
                "title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced\n  Preference Optimization"
                },
                "summary": "Iterative preference optimization has recently become one of the de-facto\ntraining paradigms for large language models (LLMs), but the performance is\nstill underwhelming due to too much noisy preference data yielded in the loop.\nTo combat this issue, we present an \\textbf{U}ncertainty-enhanced\n\\textbf{P}reference \\textbf{O}ptimization (UPO) framework to make the LLM\nself-evolve with reliable feedback. The key idea is mitigating the noisy\npreference data derived from the current policy and reward models by performing\npair-wise uncertainty estimation and judiciously reliable feedback sampling. To\nreach this goal, we thus introduce an estimator model, which incorporates Monte\nCarlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty\nestimation for the preference data derived from the LLM policy. Compared to the\nexisting methods that directly filter generated responses based on the reward\nscore, the estimator focuses on the model uncertainty in a pair-wise manner and\neffectively bypasses the confirmation bias problem of the reward model.\nAdditionally, we also propose an uncertainty-enhanced self-evolution algorithm\nto improve the robustness of preference optimization and encourage the LLM to\ngenerate responses with both high reward and certainty. Extensive experiments\nover multiple benchmarks demonstrate that our framework substantially\nalleviates the noisy problem and improves the performance of iterative\npreference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative preference optimization has recently become one of the de-facto\ntraining paradigms for large language models (LLMs), but the performance is\nstill underwhelming due to too much noisy preference data yielded in the loop.\nTo combat this issue, we present an \\textbf{U}ncertainty-enhanced\n\\textbf{P}reference \\textbf{O}ptimization (UPO) framework to make the LLM\nself-evolve with reliable feedback. The key idea is mitigating the noisy\npreference data derived from the current policy and reward models by performing\npair-wise uncertainty estimation and judiciously reliable feedback sampling. To\nreach this goal, we thus introduce an estimator model, which incorporates Monte\nCarlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty\nestimation for the preference data derived from the LLM policy. Compared to the\nexisting methods that directly filter generated responses based on the reward\nscore, the estimator focuses on the model uncertainty in a pair-wise manner and\neffectively bypasses the confirmation bias problem of the reward model.\nAdditionally, we also propose an uncertainty-enhanced self-evolution algorithm\nto improve the robustness of preference optimization and encourage the LLM to\ngenerate responses with both high reward and certainty. Extensive experiments\nover multiple benchmarks demonstrate that our framework substantially\nalleviates the noisy problem and improves the performance of iterative\npreference optimization."
                },
                "authors": [
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Xiaocheng Zhang"
                    },
                    {
                        "name": "Mengjiao Bao"
                    },
                    {
                        "name": "Peng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yan"
                },
                "author": "Peng Yan",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11629v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11629v4",
                "updated": "2024-09-17T14:04:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    4,
                    27,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-17T15:11:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    11,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study"
                },
                "summary": "Utilizing Large Language Models (LLMs) as evaluators for evaluating the\nperformance of LLMs has recently garnered attention. However, this kind of\nevaluation approach is affected by potential biases in LLMs, raising concerns\nabout the accuracy and reliability of the evaluation results. To mitigate this\nissue, we propose and study two many-shot ICL prompts, which rely on two\nversions of many-shot ICL prompt templates for helping LLM evaluators to\nmitigate the potential biases in LLMs, \\textbf{M}any-\\textbf{S}hot\n\\textbf{w}ith \\textbf{R}eference (\\textbf{MSwR}) and\n\\textbf{M}any-\\textbf{S}hot with\\textbf{o}ut \\textbf{R}eference\n(\\textbf{MSoR}). Concretely, the former utilizes in-context examples with\nmodel-generated rationales as guidance, and the latter without. Based on the\ndesigned prompts, we investigate the impact of scaling the number of in-context\nexamples on the consistency and quality of the evaluation results. Experimental\nresults show that advanced LLMs, such as GPT-4o, perform better in the\nmany-shot regime than in the zero-shot regime. Furthermore, we reveal the\nsymbol bias hidden in the selection bias of LLMs and propose a simple yet\neffective approach to mitigate the bias. Experimental results further verify\nthe effectiveness of the symbol bias mitigation approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models (LLMs) as evaluators for evaluating the\nperformance of LLMs has recently garnered attention. However, this kind of\nevaluation approach is affected by potential biases in LLMs, raising concerns\nabout the accuracy and reliability of the evaluation results. To mitigate this\nissue, we propose and study two many-shot ICL prompts, which rely on two\nversions of many-shot ICL prompt templates for helping LLM evaluators to\nmitigate the potential biases in LLMs, \\textbf{M}any-\\textbf{S}hot\n\\textbf{w}ith \\textbf{R}eference (\\textbf{MSwR}) and\n\\textbf{M}any-\\textbf{S}hot with\\textbf{o}ut \\textbf{R}eference\n(\\textbf{MSoR}). Concretely, the former utilizes in-context examples with\nmodel-generated rationales as guidance, and the latter without. Based on the\ndesigned prompts, we investigate the impact of scaling the number of in-context\nexamples on the consistency and quality of the evaluation results. Experimental\nresults show that advanced LLMs, such as GPT-4o, perform better in the\nmany-shot regime than in the zero-shot regime. Furthermore, we reveal the\nsymbol bias hidden in the selection bias of LLMs and propose a simple yet\neffective approach to mitigate the bias. Experimental results further verify\nthe effectiveness of the symbol bias mitigation approach."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Xuan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Luo"
                },
                "author": "Xuan Luo",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11629v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11629v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05804v3",
                "updated": "2024-09-17T14:02:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    2,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-09T14:42:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    42,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning"
                },
                "summary": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles (policy models, evaluators, and dynamic models), and\nuniversally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on LMPR implementations and workflow usage\nacross different agent paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles (policy models, evaluators, and dynamic models), and\nuniversally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on LMPR implementations and workflow usage\nacross different agent paradigms."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09590v2",
                "updated": "2024-09-17T13:48:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    48,
                    50,
                    1,
                    261,
                    0
                ],
                "published": "2024-07-12T17:25:02Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    17,
                    25,
                    2,
                    4,
                    194,
                    0
                ],
                "title": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts"
                },
                "summary": "By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "13pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11190v1",
                "updated": "2024-09-17T13:44:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    44,
                    42,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:44:42Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    44,
                    42,
                    1,
                    261,
                    0
                ],
                "title": "SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as\n  Autonomous Programmer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as\n  Autonomous Programmer"
                },
                "summary": "We present SuperCoder2.0, an advanced autonomous system designed to enhance\nsoftware development through artificial intelligence. The system combines an\nAI-native development approach with intelligent agents to enable fully\nautonomous coding. Key focus areas include a retry mechanism with error output\ntraceback, comprehensive code rewriting and replacement using Abstract Syntax\nTree (ast) parsing to minimize linting issues, code embedding technique for\nretrieval-augmented generation, and a focus on localizing methods for\nproblem-solving rather than identifying specific line numbers. The methodology\nemploys a three-step hierarchical search space reduction approach for code base\nnavigation and bug localization:utilizing Retrieval Augmented Generation (RAG)\nand a Repository File Level Map to identify candidate files, (2) narrowing down\nto the most relevant files using a File Level Schematic Map, and (3) extracting\n'relevant locations' within these files. Code editing is performed through a\ntwo-part module comprising CodeGeneration and CodeEditing, which generates\nmultiple solutions at different temperature values and replaces entire methods\nor classes to maintain code integrity. A feedback loop executes\nrepository-level test cases to validate and refine solutions. Experiments\nconducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's\neffectiveness, achieving correct file localization in 84.33% of cases within\nthe top 5 candidates and successfully resolving 34% of test instances. This\nperformance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.\nThe system's ability to handle diverse repositories and problem types\nhighlights its potential as a versatile tool for autonomous software\ndevelopment. Future work will focus on refining the code editing process and\nexploring advanced embedding models for improved natural language to code\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SuperCoder2.0, an advanced autonomous system designed to enhance\nsoftware development through artificial intelligence. The system combines an\nAI-native development approach with intelligent agents to enable fully\nautonomous coding. Key focus areas include a retry mechanism with error output\ntraceback, comprehensive code rewriting and replacement using Abstract Syntax\nTree (ast) parsing to minimize linting issues, code embedding technique for\nretrieval-augmented generation, and a focus on localizing methods for\nproblem-solving rather than identifying specific line numbers. The methodology\nemploys a three-step hierarchical search space reduction approach for code base\nnavigation and bug localization:utilizing Retrieval Augmented Generation (RAG)\nand a Repository File Level Map to identify candidate files, (2) narrowing down\nto the most relevant files using a File Level Schematic Map, and (3) extracting\n'relevant locations' within these files. Code editing is performed through a\ntwo-part module comprising CodeGeneration and CodeEditing, which generates\nmultiple solutions at different temperature values and replaces entire methods\nor classes to maintain code integrity. A feedback loop executes\nrepository-level test cases to validate and refine solutions. Experiments\nconducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's\neffectiveness, achieving correct file localization in 84.33% of cases within\nthe top 5 candidates and successfully resolving 34% of test instances. This\nperformance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.\nThe system's ability to handle diverse repositories and problem types\nhighlights its potential as a versatile tool for autonomous software\ndevelopment. Future work will focus on refining the code editing process and\nexploring advanced embedding models for improved natural language to code\nmapping."
                },
                "authors": [
                    {
                        "name": "Anmol Gautam"
                    },
                    {
                        "name": "Kishore Kumar"
                    },
                    {
                        "name": "Adarsh Jha"
                    },
                    {
                        "name": "Mukunda NS"
                    },
                    {
                        "name": "Ishaan Bhola"
                    }
                ],
                "author_detail": {
                    "name": "Ishaan Bhola"
                },
                "author": "Ishaan Bhola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11172v2",
                "updated": "2024-09-18T09:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    9,
                    35,
                    15,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T13:26:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    26,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "Annealed Winner-Takes-All for Motion Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annealed Winner-Takes-All for Motion Forecasting"
                },
                "summary": "In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nwill be made publicly available upon acceptance:\nhttps://github.com/valeoai/MF_aWTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nwill be made publicly available upon acceptance:\nhttps://github.com/valeoai/MF_aWTA."
                },
                "authors": [
                    {
                        "name": "Yihong Xu"
                    },
                    {
                        "name": "Victor Letzelter"
                    },
                    {
                        "name": "Mickaël Chen"
                    },
                    {
                        "name": "Éloi Zablocki"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11163v1",
                "updated": "2024-09-17T13:17:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    17,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:17:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    17,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "Fast radio bursts as a probe of gravity on cosmological scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast radio bursts as a probe of gravity on cosmological scales"
                },
                "summary": "We explore the potential for improving constraints on gravity by leveraging\ncorrelations in the dispersion measure derived from Fast Radio Bursts (FRBs) in\ncombination with cosmic shear. Specifically, we focus on Horndeski gravity,\ninferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic\nshear mock survey alongside a survey comprising $10^4$ FRBs. For the inference\npipeline, we utilise hi_class to predict the linear matter power spectrum in\nmodified gravity scenarios, while non-linear corrections are modelled with\nHMcode, including feedback mechanisms. Our findings indicate that FRBs can\ndisentangle degeneracies between baryonic feedback and cosmological parameters,\nas well as the mass of massive neutrinos. Since these parameters are also\ndegenerate with modified gravity parameters, the inclusion of FRBs can enhance\nconstraints on Horndeski parameters by up to $40$ percent, despite being a less\nsignificant measurement. Additionally, we apply our model to current FRB data\nand use the uncertainty in the $\\mathrm{DM}-z$ relation to impose limits on\ngravity. However, due to the limited sample size of current data, constraints\nare predominantly influenced by theoretical priors. Despite this, our study\ndemonstrates that FRBs will significantly augment the limited set of\ncosmological probes available, playing a critical role in providing alternative\ntests of feedback, cosmology, and gravity. All codes used in this work are made\npublically available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the potential for improving constraints on gravity by leveraging\ncorrelations in the dispersion measure derived from Fast Radio Bursts (FRBs) in\ncombination with cosmic shear. Specifically, we focus on Horndeski gravity,\ninferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic\nshear mock survey alongside a survey comprising $10^4$ FRBs. For the inference\npipeline, we utilise hi_class to predict the linear matter power spectrum in\nmodified gravity scenarios, while non-linear corrections are modelled with\nHMcode, including feedback mechanisms. Our findings indicate that FRBs can\ndisentangle degeneracies between baryonic feedback and cosmological parameters,\nas well as the mass of massive neutrinos. Since these parameters are also\ndegenerate with modified gravity parameters, the inclusion of FRBs can enhance\nconstraints on Horndeski parameters by up to $40$ percent, despite being a less\nsignificant measurement. Additionally, we apply our model to current FRB data\nand use the uncertainty in the $\\mathrm{DM}-z$ relation to impose limits on\ngravity. However, due to the limited sample size of current data, constraints\nare predominantly influenced by theoretical priors. Despite this, our study\ndemonstrates that FRBs will significantly augment the limited set of\ncosmological probes available, playing a critical role in providing alternative\ntests of feedback, cosmology, and gravity. All codes used in this work are made\npublically available."
                },
                "authors": [
                    {
                        "name": "Dennis Neumann"
                    },
                    {
                        "name": "Robert Reischke"
                    },
                    {
                        "name": "Steffen Hagstotz"
                    },
                    {
                        "name": "Hendrik Hildebrandt"
                    }
                ],
                "author_detail": {
                    "name": "Hendrik Hildebrandt"
                },
                "author": "Hendrik Hildebrandt",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11162v1",
                "updated": "2024-09-17T13:16:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    16,
                    9,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:16:09Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    16,
                    9,
                    1,
                    261,
                    0
                ],
                "title": "Chasing Shadows: How Implausible Assumptions Skew Our Understanding of\n  Causal Estimands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chasing Shadows: How Implausible Assumptions Skew Our Understanding of\n  Causal Estimands"
                },
                "summary": "The ICH E9 (R1) addendum on estimands, coupled with recent advancements in\ncausal inference, has prompted a shift towards using model-free treatment\neffect estimands that are more closely aligned with the underlying scientific\nquestion. This represents a departure from traditional, model-dependent\napproaches where the statistical model often overshadows the inquiry itself.\nWhile this shift is a positive development, it has unintentionally led to the\nprioritization of an estimand's theoretical appeal over its practical\nlearnability from data under plausible assumptions. We illustrate this by\nscrutinizing assumptions in the recent clinical trials literature on principal\nstratum estimands, demonstrating that some popular assumptions are not only\nimplausible but often inevitably violated. We advocate for a more balanced\napproach to estimand formulation, one that carefully considers both the\nscientific relevance and the practical feasibility of estimation under\nrealistic conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ICH E9 (R1) addendum on estimands, coupled with recent advancements in\ncausal inference, has prompted a shift towards using model-free treatment\neffect estimands that are more closely aligned with the underlying scientific\nquestion. This represents a departure from traditional, model-dependent\napproaches where the statistical model often overshadows the inquiry itself.\nWhile this shift is a positive development, it has unintentionally led to the\nprioritization of an estimand's theoretical appeal over its practical\nlearnability from data under plausible assumptions. We illustrate this by\nscrutinizing assumptions in the recent clinical trials literature on principal\nstratum estimands, demonstrating that some popular assumptions are not only\nimplausible but often inevitably violated. We advocate for a more balanced\napproach to estimand formulation, one that carefully considers both the\nscientific relevance and the practical feasibility of estimation under\nrealistic conditions."
                },
                "authors": [
                    {
                        "name": "Stijn Vansteelandt"
                    },
                    {
                        "name": "Kelly Van Lancker"
                    }
                ],
                "author_detail": {
                    "name": "Kelly Van Lancker"
                },
                "author": "Kelly Van Lancker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11147v1",
                "updated": "2024-09-17T12:58:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    58,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T12:58:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    58,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning"
                },
                "summary": "Large language models(LLMs) have exhibited remarkable few-shot learning\ncapabilities and unified the paradigm of NLP tasks through the in-context\nlearning(ICL) technique. Despite the success of ICL, the quality of the\nexemplar demonstrations can significantly influence the LLM's performance.\nExisting exemplar selection methods mainly focus on the semantic similarity\nbetween queries and candidate exemplars. On the other hand, the logical\nconnections between reasoning steps can be beneficial to depict the\nproblem-solving process as well. In this paper, we proposes a novel method\nnamed Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM\nto generate an initial response, then expresses intermediate problem-solving\nsteps to a graph structure. After that, it employs graph kernel to select\nexemplars with semantic and structural similarity. Extensive experiments\ndemonstrate the structural relationship is helpful to the alignment of queries\nand candidate exemplars. The efficacy of RGER on math and logit reasoning tasks\nshowcases its superiority over state-of-the-art retrieval-based approaches. Our\ncode is released at https://github.com/Yukang-Lin/RGER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) have exhibited remarkable few-shot learning\ncapabilities and unified the paradigm of NLP tasks through the in-context\nlearning(ICL) technique. Despite the success of ICL, the quality of the\nexemplar demonstrations can significantly influence the LLM's performance.\nExisting exemplar selection methods mainly focus on the semantic similarity\nbetween queries and candidate exemplars. On the other hand, the logical\nconnections between reasoning steps can be beneficial to depict the\nproblem-solving process as well. In this paper, we proposes a novel method\nnamed Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM\nto generate an initial response, then expresses intermediate problem-solving\nsteps to a graph structure. After that, it employs graph kernel to select\nexemplars with semantic and structural similarity. Extensive experiments\ndemonstrate the structural relationship is helpful to the alignment of queries\nand candidate exemplars. The efficacy of RGER on math and logit reasoning tasks\nshowcases its superiority over state-of-the-art retrieval-based approaches. Our\ncode is released at https://github.com/Yukang-Lin/RGER."
                },
                "authors": [
                    {
                        "name": "Yukang Lin"
                    },
                    {
                        "name": "Bingchen Zhong"
                    },
                    {
                        "name": "Shuoran Jiang"
                    },
                    {
                        "name": "Joanna Siebert"
                    },
                    {
                        "name": "Qingcai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingcai Chen"
                },
                "author": "Qingcai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11134v1",
                "updated": "2024-09-17T12:37:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    37,
                    48,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T12:37:48Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    37,
                    48,
                    1,
                    261,
                    0
                ],
                "title": "E-Values for Exponential Families: the General Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-Values for Exponential Families: the General Case"
                },
                "summary": "We analyze common types of e-variables and e-processes for composite\nexponential family nulls: the optimal e-variable based on the reverse\ninformation projection (RIPr), the conditional (COND) e-variable, and the\nuniversal inference (UI) and sequen\\-tialized RIPr e-processes. We characterize\nthe RIPr prior for simple and Bayes-mixture based alternatives, either\nprecisely (for Gaussian nulls and alternatives) or in an approximate sense\n(general exponential families). We provide conditions under which the RIPr\ne-variable is (again exactly vs. approximately) equal to the COND e-variable.\nBased on these and other interrelations which we establish, we determine the\ne-power of the four e-statistics as a function of sample size, exactly for\nGaussian and up to $o(1)$ in general. For $d$-dimensional null and alternative,\nthe e-power of UI tends to be smaller by a term of $(d/2) \\log n + O(1)$ than\nthat of the COND e-variable, which is the clear winner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze common types of e-variables and e-processes for composite\nexponential family nulls: the optimal e-variable based on the reverse\ninformation projection (RIPr), the conditional (COND) e-variable, and the\nuniversal inference (UI) and sequen\\-tialized RIPr e-processes. We characterize\nthe RIPr prior for simple and Bayes-mixture based alternatives, either\nprecisely (for Gaussian nulls and alternatives) or in an approximate sense\n(general exponential families). We provide conditions under which the RIPr\ne-variable is (again exactly vs. approximately) equal to the COND e-variable.\nBased on these and other interrelations which we establish, we determine the\ne-power of the four e-statistics as a function of sample size, exactly for\nGaussian and up to $o(1)$ in general. For $d$-dimensional null and alternative,\nthe e-power of UI tends to be smaller by a term of $(d/2) \\log n + O(1)$ than\nthat of the COND e-variable, which is the clear winner."
                },
                "authors": [
                    {
                        "name": "Yunda Hao"
                    },
                    {
                        "name": "Peter Grünwald"
                    }
                ],
                "author_detail": {
                    "name": "Peter Grünwald"
                },
                "author": "Peter Grünwald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18697v3",
                "updated": "2024-09-17T12:27:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    27,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-27T15:46:25Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    15,
                    46,
                    25,
                    2,
                    87,
                    0
                ],
                "title": "The Invalsi Benchmarks: measuring Linguistic and Mathematical\n  understanding of Large Language Models in Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Invalsi Benchmarks: measuring Linguistic and Mathematical\n  understanding of Large Language Models in Italian"
                },
                "summary": "While Italian is a high-resource language, there are few Italian-native\nbenchmarks to evaluate generative Large Language Models (LLMs) in this\nlanguage. This work presents three new benchmarks: Invalsi MATE to evaluate\nmodels performance on mathematical understanding in Italian, Invalsi ITA to\nevaluate language understanding in Italian and Olimpiadi MATE for more complex\nmathematical understanding.\n  The first two benchmarks are based on the Invalsi tests, which are\nadministered to students of age between 6 and 18 within the Italian school\nsystem and have been validated by several experts in teaching and pedagogy, the\nthird one comes from the Italian high school math Olympics.\n  We evaluate 10 powerful language models on these benchmarks and find that\nthey are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b\ninstruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we\ncompare LLMs with the average performance of Italian students to show that\nLlama 3.1 is the only one to outperform them on Invalsi MATE while most models\ndo so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than\nInvalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is\n45%.\n  We will make data and evaluation code openly available upon acceptance of the\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Italian is a high-resource language, there are few Italian-native\nbenchmarks to evaluate generative Large Language Models (LLMs) in this\nlanguage. This work presents three new benchmarks: Invalsi MATE to evaluate\nmodels performance on mathematical understanding in Italian, Invalsi ITA to\nevaluate language understanding in Italian and Olimpiadi MATE for more complex\nmathematical understanding.\n  The first two benchmarks are based on the Invalsi tests, which are\nadministered to students of age between 6 and 18 within the Italian school\nsystem and have been validated by several experts in teaching and pedagogy, the\nthird one comes from the Italian high school math Olympics.\n  We evaluate 10 powerful language models on these benchmarks and find that\nthey are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b\ninstruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we\ncompare LLMs with the average performance of Italian students to show that\nLlama 3.1 is the only one to outperform them on Invalsi MATE while most models\ndo so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than\nInvalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is\n45%.\n  We will make data and evaluation code openly available upon acceptance of the\npaper."
                },
                "authors": [
                    {
                        "name": "Giovanni Puccetti"
                    },
                    {
                        "name": "Maria Cassese"
                    },
                    {
                        "name": "Andrea Esuli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Esuli"
                },
                "author": "Andrea Esuli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10482v2",
                "updated": "2024-09-17T12:10:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    10,
                    49,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T17:18:11Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "title": "Schrodinger's Memory: Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schrodinger's Memory: Large Language Models"
                },
                "summary": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11114v1",
                "updated": "2024-09-17T12:07:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    7,
                    17,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T12:07:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    7,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "Diversity-grounded Channel Prototypical Learning for Out-of-Distribution\n  Intent Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-grounded Channel Prototypical Learning for Out-of-Distribution\n  Intent Detection"
                },
                "summary": "In the realm of task-oriented dialogue systems, a robust intent detection\nmechanism must effectively handle malformed utterances encountered in\nreal-world scenarios. This study presents a novel fine-tuning framework for\nlarge language models (LLMs) aimed at enhancing in-distribution (ID) intent\nclassification and out-of-distribution (OOD) intent detection, which utilizes\nsemantic matching with prototypes derived from ID class names. By harnessing\nthe highly distinguishable representations of LLMs, we construct semantic\nprototypes for each ID class using a diversity-grounded prompt tuning approach.\nWe rigorously test our framework in a challenging OOD context, where ID and OOD\nclasses are semantically close yet distinct, referred to as \\emph{near} OOD\ndetection. For a thorough assessment, we benchmark our method against the\nprevalent fine-tuning approaches. The experimental findings reveal that our\nmethod demonstrates superior performance in both few-shot ID intent\nclassification and near-OOD intent detection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of task-oriented dialogue systems, a robust intent detection\nmechanism must effectively handle malformed utterances encountered in\nreal-world scenarios. This study presents a novel fine-tuning framework for\nlarge language models (LLMs) aimed at enhancing in-distribution (ID) intent\nclassification and out-of-distribution (OOD) intent detection, which utilizes\nsemantic matching with prototypes derived from ID class names. By harnessing\nthe highly distinguishable representations of LLMs, we construct semantic\nprototypes for each ID class using a diversity-grounded prompt tuning approach.\nWe rigorously test our framework in a challenging OOD context, where ID and OOD\nclasses are semantically close yet distinct, referred to as \\emph{near} OOD\ndetection. For a thorough assessment, we benchmark our method against the\nprevalent fine-tuning approaches. The experimental findings reveal that our\nmethod demonstrates superior performance in both few-shot ID intent\nclassification and near-OOD intent detection tasks."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Liming Zhan"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Chengqiang Xie"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Albert Y. S. Lam"
                    }
                ],
                "author_detail": {
                    "name": "Albert Y. S. Lam"
                },
                "author": "Albert Y. S. Lam",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02421v2",
                "updated": "2024-09-17T12:07:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    7,
                    16,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-04T19:13:23Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    19,
                    13,
                    23,
                    0,
                    64,
                    0
                ],
                "title": "Situated Understanding of Errors in Older Adults' Interactions with\n  Voice Assistants: A Month-Long, In-Home Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situated Understanding of Errors in Older Adults' Interactions with\n  Voice Assistants: A Month-Long, In-Home Study"
                },
                "summary": "Our work addresses the challenges older adults face with commercial Voice\nAssistants (VAs), notably in conversation breakdowns and error handling.\nTraditional methods of collecting user experiences-usage logs and post-hoc\ninterviews-do not fully capture the intricacies of older adults' interactions\nwith VAs, particularly regarding their reactions to errors. To bridge this gap,\nwe equipped 15 older adults' homes with smart speakers integrated with custom\naudio recorders to collect ``in-the-wild'' audio interaction data for detailed\nerror analysis. Recognizing the conversational limitations of current VAs, our\nstudy also explored the capabilities of Large Language Models (LLMs) to handle\nnatural and imperfect text for improving VAs. Midway through our study, we\ndeployed ChatGPT-powered VA to investigate its efficacy for older adults. Our\nresearch suggests leveraging vocal and verbal responses combined with LLMs'\ncontextual capabilities for enhanced error prevention and management in VAs,\nwhile proposing design considerations to align VA capabilities with older\nadults' expectations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work addresses the challenges older adults face with commercial Voice\nAssistants (VAs), notably in conversation breakdowns and error handling.\nTraditional methods of collecting user experiences-usage logs and post-hoc\ninterviews-do not fully capture the intricacies of older adults' interactions\nwith VAs, particularly regarding their reactions to errors. To bridge this gap,\nwe equipped 15 older adults' homes with smart speakers integrated with custom\naudio recorders to collect ``in-the-wild'' audio interaction data for detailed\nerror analysis. Recognizing the conversational limitations of current VAs, our\nstudy also explored the capabilities of Large Language Models (LLMs) to handle\nnatural and imperfect text for improving VAs. Midway through our study, we\ndeployed ChatGPT-powered VA to investigate its efficacy for older adults. Our\nresearch suggests leveraging vocal and verbal responses combined with LLMs'\ncontextual capabilities for enhanced error prevention and management in VAs,\nwhile proposing design considerations to align VA capabilities with older\nadults' expectations."
                },
                "authors": [
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15796v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15796v3",
                "updated": "2024-09-17T12:00:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    0,
                    10,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-22T09:40:07Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    9,
                    40,
                    7,
                    5,
                    174,
                    0
                ],
                "title": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis"
                },
                "summary": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs."
                },
                "authors": [
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15796v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15796v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11104v1",
                "updated": "2024-09-17T11:59:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    59,
                    34,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:59:34Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    59,
                    34,
                    1,
                    261,
                    0
                ],
                "title": "Depth-based Privileged Information for Boosting 3D Human Pose Estimation\n  on RGB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-based Privileged Information for Boosting 3D Human Pose Estimation\n  on RGB"
                },
                "summary": "Despite the recent advances in computer vision research, estimating the 3D\nhuman pose from single RGB images remains a challenging task, as multiple 3D\nposes can correspond to the same 2D projection on the image. In this context,\ndepth data could help to disambiguate the 2D information by providing\nadditional constraints about the distance between objects in the scene and the\ncamera. Unfortunately, the acquisition of accurate depth data is limited to\nindoor spaces and usually is tied to specific depth technologies and devices,\nthus limiting generalization capabilities. In this paper, we propose a method\nable to leverage the benefits of depth information without compromising its\nbroader applicability and adaptability in a predominantly RGB-camera-centric\nlandscape. Our approach consists of a heatmap-based 3D pose estimator that,\nleveraging the paradigm of Privileged Information, is able to hallucinate depth\ninformation from the RGB frames given at inference time. More precisely, depth\ninformation is used exclusively during training by enforcing our RGB-based\nhallucination network to learn similar features to a backbone pre-trained only\non depth data. This approach proves to be effective even when dealing with\nlimited and small datasets. Experimental results reveal that the paradigm of\nPrivileged Information significantly enhances the model's performance, enabling\nefficient extraction of depth information by using only RGB images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advances in computer vision research, estimating the 3D\nhuman pose from single RGB images remains a challenging task, as multiple 3D\nposes can correspond to the same 2D projection on the image. In this context,\ndepth data could help to disambiguate the 2D information by providing\nadditional constraints about the distance between objects in the scene and the\ncamera. Unfortunately, the acquisition of accurate depth data is limited to\nindoor spaces and usually is tied to specific depth technologies and devices,\nthus limiting generalization capabilities. In this paper, we propose a method\nable to leverage the benefits of depth information without compromising its\nbroader applicability and adaptability in a predominantly RGB-camera-centric\nlandscape. Our approach consists of a heatmap-based 3D pose estimator that,\nleveraging the paradigm of Privileged Information, is able to hallucinate depth\ninformation from the RGB frames given at inference time. More precisely, depth\ninformation is used exclusively during training by enforcing our RGB-based\nhallucination network to learn similar features to a backbone pre-trained only\non depth data. This approach proves to be effective even when dealing with\nlimited and small datasets. Experimental results reveal that the paradigm of\nPrivileged Information significantly enhances the model's performance, enabling\nefficient extraction of depth information by using only RGB images."
                },
                "authors": [
                    {
                        "name": "Alessandro Simoni"
                    },
                    {
                        "name": "Francesco Marchetti"
                    },
                    {
                        "name": "Guido Borghi"
                    },
                    {
                        "name": "Federico Becattini"
                    },
                    {
                        "name": "Davide Davoli"
                    },
                    {
                        "name": "Lorenzo Garattoni"
                    },
                    {
                        "name": "Gianpiero Francesca"
                    },
                    {
                        "name": "Lorenzo Seidenari"
                    },
                    {
                        "name": "Roberto Vezzani"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Vezzani"
                },
                "author": "Roberto Vezzani",
                "arxiv_comment": "ECCV 2024 Workshop T-CAP: TOWARDS A COMPLETE ANALYSIS OF PEOPLE:\n  FINE-GRAINED UNDERSTANDING FOR REAL-WORLD APPLICATIONS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11103v1",
                "updated": "2024-09-17T11:56:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    56,
                    13,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:56:13Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    56,
                    13,
                    1,
                    261,
                    0
                ],
                "title": "Vetting quark-star models with gravitational waves in the hierarchical\n  Bayesian framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vetting quark-star models with gravitational waves in the hierarchical\n  Bayesian framework"
                },
                "summary": "The recent discovery of gravitational waves (GWs) has opened a new avenue for\ninvestigating the equation of state (EOS) of dense matter in compact stars,\nwhich is an outstanding problem in astronomy and nuclear physics. In the\nfuture, next-generation (XG) GW detectors will be constructed, deemed to\nprovide a large number of high-precision observations. We investigate the\npotential of constraining the EOS of quark stars (QSs) with high-precision\nmeasurements of mass $m$ and tidal deformability $\\Lambda$ from the XG GW\nobservatories. We adopt the widely-used bag model for QSs, consisting of four\nmicroscopic parameters: the effective bag constant $B_{\\rm eff}$, the\nperturbative quantum chromodynamics correction parameter $a_4$, the strange\nquark mass $m_s$, and the pairing energy gap $\\Delta$. With the help of\nhierarchical Bayesian inference, for the first time we are able to infer the\nEOS of QSs combining multiple GW observations. Using the top 25 loudest GW\nevents in our simulation, we find that, the constraints on $B_{\\rm eff}$ and\n$\\Delta$ are tightened by several times, while $a_4$ and $m_s$ are still poorly\nconstrained. We also study a simplified 2-dimensional (2-d) EOS model which was\nrecently proposed in literature. The 2-d model is found to exhibit significant\nparameter-estimation biases as more GW events are analyzed, while the predicted\n$m$-$\\Lambda$ relation remains consistent with the full model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent discovery of gravitational waves (GWs) has opened a new avenue for\ninvestigating the equation of state (EOS) of dense matter in compact stars,\nwhich is an outstanding problem in astronomy and nuclear physics. In the\nfuture, next-generation (XG) GW detectors will be constructed, deemed to\nprovide a large number of high-precision observations. We investigate the\npotential of constraining the EOS of quark stars (QSs) with high-precision\nmeasurements of mass $m$ and tidal deformability $\\Lambda$ from the XG GW\nobservatories. We adopt the widely-used bag model for QSs, consisting of four\nmicroscopic parameters: the effective bag constant $B_{\\rm eff}$, the\nperturbative quantum chromodynamics correction parameter $a_4$, the strange\nquark mass $m_s$, and the pairing energy gap $\\Delta$. With the help of\nhierarchical Bayesian inference, for the first time we are able to infer the\nEOS of QSs combining multiple GW observations. Using the top 25 loudest GW\nevents in our simulation, we find that, the constraints on $B_{\\rm eff}$ and\n$\\Delta$ are tightened by several times, while $a_4$ and $m_s$ are still poorly\nconstrained. We also study a simplified 2-dimensional (2-d) EOS model which was\nrecently proposed in literature. The 2-d model is found to exhibit significant\nparameter-estimation biases as more GW events are analyzed, while the predicted\n$m$-$\\Lambda$ relation remains consistent with the full model."
                },
                "authors": [
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Yong Gao"
                    },
                    {
                        "name": "Dicong Liang"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Lijing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Shao"
                },
                "author": "Lijing Shao",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v4",
                "updated": "2024-09-18T02:03:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    3,
                    35,
                    2,
                    262,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_comment": "Accepted in: AI and Ethics. 20 pages, 1 figure, 3 tables, 2 boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11087v1",
                "updated": "2024-09-17T11:34:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    34,
                    1,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:34:01Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    34,
                    1,
                    1,
                    261,
                    0
                ],
                "title": "Reactive Environments for Active Inference Agents with RxEnvironments.jl",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reactive Environments for Active Inference Agents with RxEnvironments.jl"
                },
                "summary": "Active Inference is a framework that emphasizes the interaction between\nagents and their environment. While the framework has seen significant\nadvancements in the development of agents, the environmental models are often\nborrowed from reinforcement learning problems, which may not fully capture the\ncomplexity of multi-agent interactions or allow complex, conditional\ncommunication. This paper introduces Reactive Environments, a comprehensive\nparadigm that facilitates complex multi-agent communication. In this paradigm,\nboth agents and environments are defined as entities encapsulated by boundaries\nwith interfaces. This setup facilitates a robust framework for communication in\nnonequilibrium-Steady-State systems, allowing for complex interactions and\ninformation exchange. We present a Julia package RxEnvironments.jl, which is a\nspecific implementation of Reactive Environments, where we utilize a Reactive\nProgramming style for efficient implementation. The flexibility of this\nparadigm is demonstrated through its application to several complex,\nmulti-agent environments. These case studies highlight the potential of\nReactive Environments in modeling sophisticated systems of interacting agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference is a framework that emphasizes the interaction between\nagents and their environment. While the framework has seen significant\nadvancements in the development of agents, the environmental models are often\nborrowed from reinforcement learning problems, which may not fully capture the\ncomplexity of multi-agent interactions or allow complex, conditional\ncommunication. This paper introduces Reactive Environments, a comprehensive\nparadigm that facilitates complex multi-agent communication. In this paradigm,\nboth agents and environments are defined as entities encapsulated by boundaries\nwith interfaces. This setup facilitates a robust framework for communication in\nnonequilibrium-Steady-State systems, allowing for complex interactions and\ninformation exchange. We present a Julia package RxEnvironments.jl, which is a\nspecific implementation of Reactive Environments, where we utilize a Reactive\nProgramming style for efficient implementation. The flexibility of this\nparadigm is demonstrated through its application to several complex,\nmulti-agent environments. These case studies highlight the potential of\nReactive Environments in modeling sophisticated systems of interacting agents."
                },
                "authors": [
                    {
                        "name": "Wouter W. L. Nuijten"
                    },
                    {
                        "name": "Bert de Vries"
                    }
                ],
                "author_detail": {
                    "name": "Bert de Vries"
                },
                "author": "Bert de Vries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09605v2",
                "updated": "2024-09-17T11:25:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    25,
                    18,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-14T17:47:01Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    47,
                    1,
                    3,
                    74,
                    0
                ],
                "title": "Counterfactual contrastive learning: robust representations via causal\n  image synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual contrastive learning: robust representations via causal\n  image synthesis"
                },
                "summary": "Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training."
                },
                "authors": [
                    {
                        "name": "Melanie Roschewitz"
                    },
                    {
                        "name": "Fabio De Sousa Ribeiro"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Galvin Khara"
                    },
                    {
                        "name": "Ben Glocker"
                    }
                ],
                "author_detail": {
                    "name": "Ben Glocker"
                },
                "author": "Ben Glocker",
                "arxiv_comment": "Accepted for publication at the MICCAI 2024 Data Engineering in\n  Medical Imaging workshop. Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive. Extended version\n  of this work available at arXiv:2409.10365",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10679v2",
                "updated": "2024-09-17T10:47:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    47,
                    51,
                    1,
                    261,
                    0
                ],
                "published": "2023-10-12T11:17:23Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    11,
                    17,
                    23,
                    3,
                    285,
                    0
                ],
                "title": "Large language models can replicate cross-cultural differences in\n  personality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can replicate cross-cultural differences in\n  personality"
                },
                "summary": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Paweł Niszczota"
                    },
                    {
                        "name": "Mateusz Janczak"
                    },
                    {
                        "name": "Michał Misiak"
                    }
                ],
                "author_detail": {
                    "name": "Michał Misiak"
                },
                "author": "Michał Misiak",
                "arxiv_comment": "27 pages: 12 pages of manuscript + 15 pages of supplementary\n  materials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11056v1",
                "updated": "2024-09-17T10:33:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    33,
                    27,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:33:27Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    33,
                    27,
                    1,
                    261,
                    0
                ],
                "title": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts"
                },
                "summary": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiongwei Han"
                },
                "author": "Xiongwei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11055v1",
                "updated": "2024-09-17T10:31:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    31,
                    37,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    31,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language\n  Models: An Experimental Analysis up to 405B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language\n  Models: An Experimental Analysis up to 405B"
                },
                "summary": "Prior research works have evaluated quantized LLMs using limited metrics such\nas perplexity or a few basic knowledge tasks and old datasets. Additionally,\nrecent large-scale models such as Llama 3.1 with up to 405B have not been\nthoroughly examined. This paper evaluates the performance of instruction-tuned\nLLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on\nmodels ranging from 7B to 405B. Using 13 benchmarks, we assess performance\nacross six task types: commonsense Q\\&A, knowledge and language understanding,\ninstruction following, hallucination detection, mathematics, and dialogue. Our\nkey findings reveal that (1) quantizing a larger LLM to a similar size as a\nsmaller FP16 LLM generally performs better across most benchmarks, except for\nhallucination detection and instruction following; (2) performance varies\nsignificantly with different quantization methods, model size, and bit-width,\nwith weight-only methods often yielding better results in larger models; (3)\ntask difficulty does not significantly impact accuracy degradation due to\nquantization; and (4) the MT-Bench evaluation method has limited discriminatory\npower among recent high-performing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research works have evaluated quantized LLMs using limited metrics such\nas perplexity or a few basic knowledge tasks and old datasets. Additionally,\nrecent large-scale models such as Llama 3.1 with up to 405B have not been\nthoroughly examined. This paper evaluates the performance of instruction-tuned\nLLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on\nmodels ranging from 7B to 405B. Using 13 benchmarks, we assess performance\nacross six task types: commonsense Q\\&A, knowledge and language understanding,\ninstruction following, hallucination detection, mathematics, and dialogue. Our\nkey findings reveal that (1) quantizing a larger LLM to a similar size as a\nsmaller FP16 LLM generally performs better across most benchmarks, except for\nhallucination detection and instruction following; (2) performance varies\nsignificantly with different quantization methods, model size, and bit-width,\nwith weight-only methods often yielding better results in larger models; (3)\ntask difficulty does not significantly impact accuracy degradation due to\nquantization; and (4) the MT-Bench evaluation method has limited discriminatory\npower among recent high-performing LLMs."
                },
                "authors": [
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Jinse Kwon"
                    },
                    {
                        "name": "Jihun Oh"
                    },
                    {
                        "name": "Yongin Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Yongin Kwon"
                },
                "author": "Yongin Kwon",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v2",
                "updated": "2024-09-17T10:16:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    16,
                    14,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models"
                },
                "summary": "Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01558v2",
                "updated": "2024-09-17T10:15:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    15,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-05-05T19:10:19Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    19,
                    10,
                    19,
                    6,
                    126,
                    0
                ],
                "title": "Visual grounding for desktop graphical user interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding for desktop graphical user interfaces"
                },
                "summary": "Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel."
                },
                "authors": [
                    {
                        "name": "Tassnim Dardouri"
                    },
                    {
                        "name": "Laura Minkova"
                    },
                    {
                        "name": "Jessica López Espejel"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Preprint submitted to Computer Vision and Image Understanding journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11047v1",
                "updated": "2024-09-17T10:13:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    13,
                    9,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:13:09Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    13,
                    9,
                    1,
                    261,
                    0
                ],
                "title": "TacDiffusion: Force-domain Diffusion Policy for Precise Tactile\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TacDiffusion: Force-domain Diffusion Policy for Precise Tactile\n  Manipulation"
                },
                "summary": "Assembly is a crucial skill for robots in both modern manufacturing and\nservice robotics. However, mastering transferable insertion skills that can\nhandle a variety of high-precision assembly tasks remains a significant\nchallenge. This paper presents a novel framework that utilizes diffusion models\nto generate 6D wrench for high-precision tactile robotic insertion tasks. It\nlearns from demonstrations performed on a single task and achieves a zero-shot\ntransfer success rate of 95.7% across various novel high-precision tasks. Our\nmethod effectively inherits the self-adaptability demonstrated by our previous\nwork. In this framework, we address the frequency misalignment between the\ndiffusion policy and the real-time control loop with a dynamic system-based\nfilter, significantly improving the task success rate by 9.15%. Furthermore, we\nprovide a practical guideline regarding the trade-off between diffusion models'\ninference ability and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assembly is a crucial skill for robots in both modern manufacturing and\nservice robotics. However, mastering transferable insertion skills that can\nhandle a variety of high-precision assembly tasks remains a significant\nchallenge. This paper presents a novel framework that utilizes diffusion models\nto generate 6D wrench for high-precision tactile robotic insertion tasks. It\nlearns from demonstrations performed on a single task and achieves a zero-shot\ntransfer success rate of 95.7% across various novel high-precision tasks. Our\nmethod effectively inherits the self-adaptability demonstrated by our previous\nwork. In this framework, we address the frequency misalignment between the\ndiffusion policy and the real-time control loop with a dynamic system-based\nfilter, significantly improving the task success rate by 9.15%. Furthermore, we\nprovide a practical guideline regarding the trade-off between diffusion models'\ninference ability and speed."
                },
                "authors": [
                    {
                        "name": "Yansong Wu"
                    },
                    {
                        "name": "Zongxie Chen"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Lingyun Chen"
                    },
                    {
                        "name": "Liding Zhang"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Alois Knoll"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11044v1",
                "updated": "2024-09-17T10:08:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    8,
                    14,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:08:14Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    8,
                    14,
                    1,
                    261,
                    0
                ],
                "title": "Computation and Complexity of Preference Inference Based on Hierarchical\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation and Complexity of Preference Inference Based on Hierarchical\n  Models"
                },
                "summary": "Preference Inference involves inferring additional user preferences from\nelicited or observed preferences, based on assumptions regarding the form of\nthe user's preference relation. In this paper we consider a situation in which\nalternatives have an associated vector of costs, each component corresponding\nto a different criterion, and are compared using a kind of lexicographic order,\nsimilar to the way alternatives are compared in a Hierarchical Constraint Logic\nProgramming model. It is assumed that the user has some (unknown) importance\nordering on criteria, and that to compare two alternatives, firstly, the\ncombined cost of each alternative with respect to the most important criteria\nare compared; only if these combined costs are equal, are the next most\nimportant criteria considered. The preference inference problem then consists\nof determining whether a preference statement can be inferred from a set of\ninput preferences. We show that this problem is coNP-complete, even if one\nrestricts the cardinality of the equal-importance sets to have at most two\nelements, and one only considers non-strict preferences. However, it is\npolynomial if it is assumed that the user's ordering of criteria is a total\nordering; it is also polynomial if the sets of equally important criteria are\nall equivalence classes of a given fixed equivalence relation. We give an\nefficient polynomial algorithm for these cases, which also throws light on the\nstructure of the inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Inference involves inferring additional user preferences from\nelicited or observed preferences, based on assumptions regarding the form of\nthe user's preference relation. In this paper we consider a situation in which\nalternatives have an associated vector of costs, each component corresponding\nto a different criterion, and are compared using a kind of lexicographic order,\nsimilar to the way alternatives are compared in a Hierarchical Constraint Logic\nProgramming model. It is assumed that the user has some (unknown) importance\nordering on criteria, and that to compare two alternatives, firstly, the\ncombined cost of each alternative with respect to the most important criteria\nare compared; only if these combined costs are equal, are the next most\nimportant criteria considered. The preference inference problem then consists\nof determining whether a preference statement can be inferred from a set of\ninput preferences. We show that this problem is coNP-complete, even if one\nrestricts the cardinality of the equal-importance sets to have at most two\nelements, and one only considers non-strict preferences. However, it is\npolynomial if it is assumed that the user's ordering of criteria is a total\nordering; it is also polynomial if the sets of equally important criteria are\nall equivalence classes of a given fixed equivalence relation. We give an\nefficient polynomial algorithm for these cases, which also throws light on the\nstructure of the inference."
                },
                "authors": [
                    {
                        "name": "Nic Wilson"
                    },
                    {
                        "name": "Anne-Marie George"
                    },
                    {
                        "name": "Barry O'Sullivan"
                    }
                ],
                "author_detail": {
                    "name": "Barry O'Sullivan"
                },
                "author": "Barry O'Sullivan",
                "arxiv_comment": "Longer Version of IJCAI'15 publication\n  https://www.ijcai.org/Proceedings/15/Papers/461.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11041v2",
                "updated": "2024-09-18T07:17:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    17,
                    17,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T10:04:50Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    50,
                    1,
                    261,
                    0
                ],
                "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming"
                },
                "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00454v2",
                "updated": "2024-09-17T10:04:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    22,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-29T14:40:23Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    14,
                    40,
                    23,
                    5,
                    181,
                    0
                ],
                "title": "Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language\n  Models via Inherent Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language\n  Models via Inherent Capability"
                },
                "summary": "Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained\nmodels shows promise for low-resource languages, but often suffers from\nmisalignment of internal representations between languages. We hypothesize that\neven when the model cannot generalize across languages effectively in\nfine-tuning, it still captures cross-lingual correspondence useful for\ncross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a\nmethod that lets large language models (LLMs) to translate training data into\nthe target language and fine-tunes the model on its own generated data. By\ndemonstrating that Self-Translate-Train outperforms zero-shot transfer, we\nencourage further exploration of better methods to elicit cross-lingual\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained\nmodels shows promise for low-resource languages, but often suffers from\nmisalignment of internal representations between languages. We hypothesize that\neven when the model cannot generalize across languages effectively in\nfine-tuning, it still captures cross-lingual correspondence useful for\ncross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a\nmethod that lets large language models (LLMs) to translate training data into\nthe target language and fine-tunes the model on its own generated data. By\ndemonstrating that Self-Translate-Train outperforms zero-shot transfer, we\nencourage further exploration of better methods to elicit cross-lingual\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Ryokan Ri"
                    },
                    {
                        "name": "Shun Kiyono"
                    },
                    {
                        "name": "Sho Takase"
                    }
                ],
                "author_detail": {
                    "name": "Sho Takase"
                },
                "author": "Sho Takase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11032v1",
                "updated": "2024-09-17T09:56:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    56,
                    12,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:56:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    56,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI"
                },
                "summary": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement."
                },
                "authors": [
                    {
                        "name": "Riona Matsuoka"
                    },
                    {
                        "name": "Hiroki Matsumoto"
                    },
                    {
                        "name": "Takahiro Yoshida"
                    },
                    {
                        "name": "Tomohiro Watanabe"
                    },
                    {
                        "name": "Ryoma Kondo"
                    },
                    {
                        "name": "Ryohei Hisano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Hisano"
                },
                "author": "Ryohei Hisano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11026v1",
                "updated": "2024-09-17T09:43:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:43:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Prompt Obfuscation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Obfuscation for Large Language Models"
                },
                "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying large language model (LLM) can easily transform\nfoundation models into tools and services with minimal overhead. Because of\ntheir crucial impact on the utility, they are often considered intellectual\nproperty, similar to the code of a software product. However, extracting system\nprompts is easily possible by using prompt injection. As of today, there is no\neffective countermeasure to prevent the stealing of system prompts and all\nsafeguarding efforts could be evaded with carefully crafted prompt injections\nthat bypass all protection mechanisms.In this work, we propose an alternative\nto conventional system prompts. We introduce prompt obfuscation to prevent the\nextraction of the system prompt while maintaining the utility of the system\nitself with only little overhead. The core idea is to find a representation of\nthe original system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We implement an\noptimization-based method to find an obfuscated prompt representation while\nmaintaining the functionality. To evaluate our approach, we investigate eight\ndifferent metrics to compare the performance of a system using the original and\nthe obfuscated system prompts, and we show that the obfuscated version is\nconstantly on par with the original one. We further perform three different\ndeobfuscation attacks and show that with access to the obfuscated prompt and\nthe LLM itself, we are not able to consistently extract meaningful information.\nOverall, we showed that prompt obfuscation can be an effective method to\nprotect intellectual property while maintaining the same utility as the\noriginal system prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts that include detailed instructions to describe the task\nperformed by the underlying large language model (LLM) can easily transform\nfoundation models into tools and services with minimal overhead. Because of\ntheir crucial impact on the utility, they are often considered intellectual\nproperty, similar to the code of a software product. However, extracting system\nprompts is easily possible by using prompt injection. As of today, there is no\neffective countermeasure to prevent the stealing of system prompts and all\nsafeguarding efforts could be evaded with carefully crafted prompt injections\nthat bypass all protection mechanisms.In this work, we propose an alternative\nto conventional system prompts. We introduce prompt obfuscation to prevent the\nextraction of the system prompt while maintaining the utility of the system\nitself with only little overhead. The core idea is to find a representation of\nthe original system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We implement an\noptimization-based method to find an obfuscated prompt representation while\nmaintaining the functionality. To evaluate our approach, we investigate eight\ndifferent metrics to compare the performance of a system using the original and\nthe obfuscated system prompts, and we show that the obfuscated version is\nconstantly on par with the original one. We further perform three different\ndeobfuscation attacks and show that with access to the obfuscated prompt and\nthe LLM itself, we are not able to consistently extract meaningful information.\nOverall, we showed that prompt obfuscation can be an effective method to\nprotect intellectual property while maintaining the same utility as the\noriginal system prompt."
                },
                "authors": [
                    {
                        "name": "David Pape"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Lea Schönherr"
                    }
                ],
                "author_detail": {
                    "name": "Lea Schönherr"
                },
                "author": "Lea Schönherr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11022v2",
                "updated": "2024-09-18T10:05:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    5,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T09:32:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Tong Shang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09785v2",
                "updated": "2024-09-17T09:32:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-15T16:32:49Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    16,
                    32,
                    49,
                    6,
                    259,
                    0
                ],
                "title": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition"
                },
                "summary": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations."
                },
                "authors": [
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Yuanchao Li"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Yen-Ting Lin"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Sabato Marco Siniscalchi"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Catherine Lai"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community:\n  https://huggingface.co/GenSEC-LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14889v3",
                "updated": "2024-09-17T09:24:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    24,
                    42,
                    1,
                    261,
                    0
                ],
                "published": "2024-02-22T10:46:11Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    10,
                    46,
                    11,
                    3,
                    53,
                    0
                ],
                "title": "COBIAS: Contextual Reliability in Bias Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBIAS: Contextual Reliability in Bias Assessment"
                },
                "summary": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augment 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65$, $p = 3.4 *\n10^{-60}$) and can be used to create reliable datasets, which would assist bias\nmitigation works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augment 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65$, $p = 3.4 *\n10^{-60}$) and can be used to create reliable datasets, which would assist bias\nmitigation works."
                },
                "authors": [
                    {
                        "name": "Priyanshul Govil"
                    },
                    {
                        "name": "Hemang Jain"
                    },
                    {
                        "name": "Vamshi Krishna Bonagiri"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Manas Gaur"
                    },
                    {
                        "name": "Sanorita Dey"
                    }
                ],
                "author_detail": {
                    "name": "Sanorita Dey"
                },
                "author": "Sanorita Dey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11010v1",
                "updated": "2024-09-17T09:21:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    21,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:21:07Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    21,
                    7,
                    1,
                    261,
                    0
                ],
                "title": "MM2Latent: Text-to-facial image generation and editing in GANs with\n  multimodal assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM2Latent: Text-to-facial image generation and editing in GANs with\n  multimodal assistance"
                },
                "summary": "Generating human portraits is a hot topic in the image generation area, e.g.\nmask-to-face generation and text-to-face generation. However, these unimodal\ngeneration methods lack controllability in image generation. Controllability\ncan be enhanced by exploring the advantages and complementarities of various\nmodalities. For instance, we can utilize the advantages of text in controlling\ndiverse attributes and masks in controlling spatial locations. Current\nstate-of-the-art methods in multimodal generation face limitations due to their\nreliance on extensive hyperparameters, manual operations during the inference\nstage, substantial computational demands during training and inference, or\ninability to edit real images. In this paper, we propose a practical framework\n- MM2Latent - for multimodal image generation and editing. We use StyleGAN2 as\nour image generator, FaRL for text encoding, and train an autoencoders for\nspatial modalities like mask, sketch and 3DMM. We propose a strategy that\ninvolves training a mapping network to map the multimodal input into the w\nlatent space of StyleGAN. The proposed framework 1) eliminates hyperparameters\nand manual operations in the inference stage, 2) ensures fast inference speeds,\nand 3) enables the editing of real images. Extensive experiments demonstrate\nthat our method exhibits superior performance in multimodal image generation,\nsurpassing recent GAN- and diffusion-based methods. Also, it proves effective\nin multimodal image editing and is faster than GAN- and diffusion-based\nmethods. We make the code publicly available at:\nhttps://github.com/Open-Debin/MM2Latent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating human portraits is a hot topic in the image generation area, e.g.\nmask-to-face generation and text-to-face generation. However, these unimodal\ngeneration methods lack controllability in image generation. Controllability\ncan be enhanced by exploring the advantages and complementarities of various\nmodalities. For instance, we can utilize the advantages of text in controlling\ndiverse attributes and masks in controlling spatial locations. Current\nstate-of-the-art methods in multimodal generation face limitations due to their\nreliance on extensive hyperparameters, manual operations during the inference\nstage, substantial computational demands during training and inference, or\ninability to edit real images. In this paper, we propose a practical framework\n- MM2Latent - for multimodal image generation and editing. We use StyleGAN2 as\nour image generator, FaRL for text encoding, and train an autoencoders for\nspatial modalities like mask, sketch and 3DMM. We propose a strategy that\ninvolves training a mapping network to map the multimodal input into the w\nlatent space of StyleGAN. The proposed framework 1) eliminates hyperparameters\nand manual operations in the inference stage, 2) ensures fast inference speeds,\nand 3) enables the editing of real images. Extensive experiments demonstrate\nthat our method exhibits superior performance in multimodal image generation,\nsurpassing recent GAN- and diffusion-based methods. Also, it proves effective\nin multimodal image editing and is faster than GAN- and diffusion-based\nmethods. We make the code publicly available at:\nhttps://github.com/Open-Debin/MM2Latent"
                },
                "authors": [
                    {
                        "name": "Debin Meng"
                    },
                    {
                        "name": "Christos Tzelepis"
                    },
                    {
                        "name": "Ioannis Patras"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tzimiropoulos"
                },
                "author": "Georgios Tzimiropoulos",
                "arxiv_comment": "Accepted at ECCV 2024 AIM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11008v1",
                "updated": "2024-09-17T09:16:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    16,
                    38,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:16:38Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    16,
                    38,
                    1,
                    261,
                    0
                ],
                "title": "Latent mixed-effect models for high-dimensional longitudinal data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent mixed-effect models for high-dimensional longitudinal data"
                },
                "summary": "Modelling longitudinal data is an important yet challenging task. These\ndatasets can be high-dimensional, contain non-linear effects and time-varying\ncovariates. Gaussian process (GP) prior-based variational autoencoders (VAEs)\nhave emerged as a promising approach due to their ability to model time-series\ndata. However, they are costly to train and struggle to fully exploit the rich\ncovariates characteristic of longitudinal data, making them difficult for\npractitioners to use effectively. In this work, we leverage linear mixed models\n(LMMs) and amortized variational inference to provide conditional priors for\nVAEs, and propose LMM-VAE, a scalable, interpretable and identifiable model. We\nhighlight theoretical connections between it and GP-based techniques, providing\na unified framework for this class of methods. Our proposal performs\ncompetitively compared to existing approaches across simulated and real-world\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling longitudinal data is an important yet challenging task. These\ndatasets can be high-dimensional, contain non-linear effects and time-varying\ncovariates. Gaussian process (GP) prior-based variational autoencoders (VAEs)\nhave emerged as a promising approach due to their ability to model time-series\ndata. However, they are costly to train and struggle to fully exploit the rich\ncovariates characteristic of longitudinal data, making them difficult for\npractitioners to use effectively. In this work, we leverage linear mixed models\n(LMMs) and amortized variational inference to provide conditional priors for\nVAEs, and propose LMM-VAE, a scalable, interpretable and identifiable model. We\nhighlight theoretical connections between it and GP-based techniques, providing\na unified framework for this class of methods. Our proposal performs\ncompetitively compared to existing approaches across simulated and real-world\ndatasets."
                },
                "authors": [
                    {
                        "name": "Priscilla Ong"
                    },
                    {
                        "name": "Manuel Haußmann"
                    },
                    {
                        "name": "Otto Lönnroth"
                    },
                    {
                        "name": "Harri Lähdesmäki"
                    }
                ],
                "author_detail": {
                    "name": "Harri Lähdesmäki"
                },
                "author": "Harri Lähdesmäki",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10999v1",
                "updated": "2024-09-17T09:04:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    4,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:04:03Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    4,
                    3,
                    1,
                    261,
                    0
                ],
                "title": "Enhancing Low-Resource Language and Instruction Following Capabilities\n  of Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Low-Resource Language and Instruction Following Capabilities\n  of Audio Language Models"
                },
                "summary": "Audio language models can understand audio inputs and perform a range of\naudio-related tasks based on instructions, such as speech recognition and audio\ncaptioning, where the instructions are usually textual prompts. Audio language\nmodels are mostly initialized from pre-trained audio encoders and large\nlanguage models (LLMs). Although these pre-trained components were developed to\nsupport multiple languages, audio-language models are trained predominantly on\nEnglish data, which may limit their usability to only English instructions or\nEnglish speech inputs. First, this paper examines the performance of existing\naudio language models in an underserved language using Thai as an example. This\npaper demonstrates that, despite being built on multilingual backbones, audio\nlanguage models do not exhibit cross-lingual emergent abilities to low-resource\nlanguages. Second, this paper studies data mixture for developing audio\nlanguage models that are optimized for a target language as well as English. In\naddition. this paper integrates audio comprehension and speech\ninstruction-following capabilities into a single unified model. Our experiments\nprovide insights into data mixture for enhancing instruction-following\ncapabilities in both a low-resource language and English. Our model,\nTyphoon-Audio, outperforms existing open-source audio language models by a\nconsiderable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in\nboth English and Thai languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio language models can understand audio inputs and perform a range of\naudio-related tasks based on instructions, such as speech recognition and audio\ncaptioning, where the instructions are usually textual prompts. Audio language\nmodels are mostly initialized from pre-trained audio encoders and large\nlanguage models (LLMs). Although these pre-trained components were developed to\nsupport multiple languages, audio-language models are trained predominantly on\nEnglish data, which may limit their usability to only English instructions or\nEnglish speech inputs. First, this paper examines the performance of existing\naudio language models in an underserved language using Thai as an example. This\npaper demonstrates that, despite being built on multilingual backbones, audio\nlanguage models do not exhibit cross-lingual emergent abilities to low-resource\nlanguages. Second, this paper studies data mixture for developing audio\nlanguage models that are optimized for a target language as well as English. In\naddition. this paper integrates audio comprehension and speech\ninstruction-following capabilities into a single unified model. Our experiments\nprovide insights into data mixture for enhancing instruction-following\ncapabilities in both a low-resource language and English. Our model,\nTyphoon-Audio, outperforms existing open-source audio language models by a\nconsiderable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in\nboth English and Thai languages."
                },
                "authors": [
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Warit Sirichotedumrong"
                    },
                    {
                        "name": "Kasima Tharnpipitchai"
                    },
                    {
                        "name": "Kunat Pipatanakul"
                    }
                ],
                "author_detail": {
                    "name": "Kunat Pipatanakul"
                },
                "author": "Kunat Pipatanakul",
                "arxiv_comment": "5 pages. Preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10994v1",
                "updated": "2024-09-17T08:56:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    56,
                    27,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T08:56:27Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    56,
                    27,
                    1,
                    261,
                    0
                ],
                "title": "Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs"
                },
                "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models."
                },
                "authors": [
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Wenjun Wang"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Michael Guan"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "9 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01208v2",
                "updated": "2024-09-17T08:42:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    42,
                    50,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-02T12:36:35Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    12,
                    36,
                    35,
                    0,
                    246,
                    0
                ],
                "title": "Statistical Jump Model for Mixed-Type Data with Missing Data Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Jump Model for Mixed-Type Data with Missing Data Imputation"
                },
                "summary": "In this paper, we address the challenge of clustering mixed-type data with\ntemporal evolution by introducing the statistical jump model for mixed-type\ndata. This novel framework incorporates regime persistence, enhancing\ninterpretability and reducing the frequency of state switches, and efficiently\nhandles missing data. The model is easily interpretable through its\nstate-conditional means and modes, making it accessible to practitioners and\npolicymakers. We validate our approach through extensive simulation studies and\nan empirical application to air quality data, demonstrating its superiority in\ninferring persistent air quality regimes compared to the traditional air\nquality index. Our contributions include a robust method for mixed-type\ntemporal clustering, effective missing data management, and practical insights\nfor environmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of clustering mixed-type data with\ntemporal evolution by introducing the statistical jump model for mixed-type\ndata. This novel framework incorporates regime persistence, enhancing\ninterpretability and reducing the frequency of state switches, and efficiently\nhandles missing data. The model is easily interpretable through its\nstate-conditional means and modes, making it accessible to practitioners and\npolicymakers. We validate our approach through extensive simulation studies and\nan empirical application to air quality data, demonstrating its superiority in\ninferring persistent air quality regimes compared to the traditional air\nquality index. Our contributions include a robust method for mixed-type\ntemporal clustering, effective missing data management, and practical insights\nfor environmental monitoring."
                },
                "authors": [
                    {
                        "name": "Federico P. Cortese"
                    },
                    {
                        "name": "Antonio Pievatolo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pievatolo"
                },
                "author": "Antonio Pievatolo",
                "arxiv_comment": "25 pages, 5 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "37M10, 62D10, 62H30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10983v1",
                "updated": "2024-09-17T08:34:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    34,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T08:34:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    34,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "MoDex: Planning High-Dimensional Dexterous Control via Learning Neural\n  Hand Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDex: Planning High-Dimensional Dexterous Control via Learning Neural\n  Hand Models"
                },
                "summary": "Controlling hands in the high-dimensional action space has been a\nlongstanding challenge, yet humans naturally perform dexterous tasks with ease.\nIn this paper, we draw inspiration from the human embodied cognition and\nreconsider dexterous hands as learnable systems. Specifically, we introduce\nMoDex, a framework which employs a neural hand model to capture the dynamical\ncharacteristics of hand movements. Based on the model, a bidirectional planning\nmethod is developed, which demonstrates efficiency in both training and\ninference. The method is further integrated with a large language model to\ngenerate various gestures such as ``Scissorshand\" and ``Rock\\&Roll.\" Moreover,\nwe show that decomposing the system dynamics into a pretrained hand model and\nan external model improves data efficiency, as supported by both theoretical\nanalysis and empirical experiments. Additional visualization results are\navailable at https://tongwu19.github.io/MoDex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling hands in the high-dimensional action space has been a\nlongstanding challenge, yet humans naturally perform dexterous tasks with ease.\nIn this paper, we draw inspiration from the human embodied cognition and\nreconsider dexterous hands as learnable systems. Specifically, we introduce\nMoDex, a framework which employs a neural hand model to capture the dynamical\ncharacteristics of hand movements. Based on the model, a bidirectional planning\nmethod is developed, which demonstrates efficiency in both training and\ninference. The method is further integrated with a large language model to\ngenerate various gestures such as ``Scissorshand\" and ``Rock\\&Roll.\" Moreover,\nwe show that decomposing the system dynamics into a pretrained hand model and\nan external model improves data efficiency, as supported by both theoretical\nanalysis and empirical experiments. Additional visualization results are\navailable at https://tongwu19.github.io/MoDex."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Shoujie Li"
                    },
                    {
                        "name": "Chuqiao Lyu"
                    },
                    {
                        "name": "Kit-Wa Sou"
                    },
                    {
                        "name": "Wang-Sing Chan"
                    },
                    {
                        "name": "Wenbo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Ding"
                },
                "author": "Wenbo Ding",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12423v3",
                "updated": "2024-09-17T08:32:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    32,
                    2,
                    1,
                    261,
                    0
                ],
                "published": "2024-07-17T09:20:44Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    9,
                    20,
                    44,
                    2,
                    199,
                    0
                ],
                "title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT\n  Interactions"
                },
                "summary": "The integration of Large Language Models (LLMs), especially ChatGPT, into\neducation is poised to revolutionize students' learning experiences by\nintroducing innovative conversational learning methodologies. To empower\nstudents to fully leverage the capabilities of ChatGPT in educational\nscenarios, understanding students' interaction patterns with ChatGPT is crucial\nfor instructors. However, this endeavor is challenging due to the absence of\ndatasets focused on student-ChatGPT conversations and the complexities in\nidentifying and analyzing the evolutional interaction patterns within\nconversations. To address these challenges, we collected conversational data\nfrom 48 students interacting with ChatGPT in a master's level data\nvisualization course over one semester. We then developed a coding scheme,\ngrounded in the literature on cognitive levels and thematic analysis, to\ncategorize students' interaction patterns with ChatGPT. Furthermore, we present\na visual analytics system, StuGPTViz, that tracks and compares temporal\npatterns in student prompts and the quality of ChatGPT's responses at multiple\nscales, revealing significant pedagogical insights for instructors. We\nvalidated the system's effectiveness through expert interviews with six data\nvisualization instructors and three case studies. The results confirmed\nStuGPTViz's capacity to enhance educators' insights into the pedagogical value\nof ChatGPT. We also discussed the potential research opportunities of applying\nvisual analytics in education and developing AI-driven personalized learning\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs), especially ChatGPT, into\neducation is poised to revolutionize students' learning experiences by\nintroducing innovative conversational learning methodologies. To empower\nstudents to fully leverage the capabilities of ChatGPT in educational\nscenarios, understanding students' interaction patterns with ChatGPT is crucial\nfor instructors. However, this endeavor is challenging due to the absence of\ndatasets focused on student-ChatGPT conversations and the complexities in\nidentifying and analyzing the evolutional interaction patterns within\nconversations. To address these challenges, we collected conversational data\nfrom 48 students interacting with ChatGPT in a master's level data\nvisualization course over one semester. We then developed a coding scheme,\ngrounded in the literature on cognitive levels and thematic analysis, to\ncategorize students' interaction patterns with ChatGPT. Furthermore, we present\na visual analytics system, StuGPTViz, that tracks and compares temporal\npatterns in student prompts and the quality of ChatGPT's responses at multiple\nscales, revealing significant pedagogical insights for instructors. We\nvalidated the system's effectiveness through expert interviews with six data\nvisualization instructors and three case studies. The results confirmed\nStuGPTViz's capacity to enhance educators' insights into the pedagogical value\nof ChatGPT. We also discussed the potential research opportunities of applying\nvisual analytics in education and developing AI-driven personalized learning\nsolutions."
                },
                "authors": [
                    {
                        "name": "Zixin Chen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Meng Xia"
                    },
                    {
                        "name": "Kento Shigyo"
                    },
                    {
                        "name": "Dingdong Liu"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "11 pages. To be published at IEEE Visualization 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04067v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04067v4",
                "updated": "2024-09-17T08:19:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    19,
                    59,
                    1,
                    261,
                    0
                ],
                "published": "2024-04-05T12:51:37Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    12,
                    51,
                    37,
                    4,
                    96,
                    0
                ],
                "title": "Does Biomedical Training Lead to Better Medical Performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Biomedical Training Lead to Better Medical Performance?"
                },
                "summary": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, biomedical\ntraining has not been systematically evaluated on medical tasks. This study\ninvestigates the effect of biomedical training in the context of six practical\nmedical tasks evaluating $25$ models. In contrast to previous evaluations, our\nresults reveal a performance decline in nine out of twelve biomedical models\nafter fine-tuning, particularly on tasks involving hallucinations, ICD10\ncoding, and instruction adherence. General-domain models like\nMeta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts,\nindicating a trade-off between domain-specific fine-tuning and general medical\ntask performance. We open-source all evaluation scripts and datasets at\nhttps://github.com/TIO-IKIM/CLUE to support further research in this critical\narea.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, biomedical\ntraining has not been systematically evaluated on medical tasks. This study\ninvestigates the effect of biomedical training in the context of six practical\nmedical tasks evaluating $25$ models. In contrast to previous evaluations, our\nresults reveal a performance decline in nine out of twelve biomedical models\nafter fine-tuning, particularly on tasks involving hallucinations, ICD10\ncoding, and instruction adherence. General-domain models like\nMeta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts,\nindicating a trade-off between domain-specific fine-tuning and general medical\ntask performance. We open-source all evaluation scripts and datasets at\nhttps://github.com/TIO-IKIM/CLUE to support further research in this critical\narea."
                },
                "authors": [
                    {
                        "name": "Amin Dada"
                    },
                    {
                        "name": "Marie Bauer"
                    },
                    {
                        "name": "Amanda Butler Contreras"
                    },
                    {
                        "name": "Osman Alperen Koraş"
                    },
                    {
                        "name": "Constantin Marc Seibold"
                    },
                    {
                        "name": "Kaleb E Smith"
                    },
                    {
                        "name": "Jens Kleesiek"
                    }
                ],
                "author_detail": {
                    "name": "Jens Kleesiek"
                },
                "author": "Jens Kleesiek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04067v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04067v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10969v1",
                "updated": "2024-09-17T08:11:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    11,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T08:11:07Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    11,
                    7,
                    1,
                    261,
                    0
                ],
                "title": "Enhancing Multilingual Speech Generation and Recognition Abilities in\n  LLMs with Constructed Code-switched Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multilingual Speech Generation and Recognition Abilities in\n  LLMs with Constructed Code-switched Data"
                },
                "summary": "While large language models (LLMs) have been explored in the speech domain\nfor both generation and recognition tasks, their applications are predominantly\nconfined to the monolingual scenario, with limited exploration in multilingual\nand code-switched (CS) contexts. Additionally, speech generation and\nrecognition tasks are often handled separately, such as VALL-E and Qwen-Audio.\nIn this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating\nmultilingual speech generation and recognition tasks within the single LLM.\nFurthermore, we develop an effective data construction approach that splits and\nconcatenates words from different languages to equip LLMs with CS synthesis\nability without relying on CS data. The experimental results demonstrate that\nour model outperforms other baselines with a comparable data scale.\nFurthermore, our data construction approach not only equips LLMs with CS speech\nsynthesis capability with comparable speaker consistency and similarity to any\ngiven speaker, but also improves the performance of LLMs in multilingual speech\ngeneration and recognition tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have been explored in the speech domain\nfor both generation and recognition tasks, their applications are predominantly\nconfined to the monolingual scenario, with limited exploration in multilingual\nand code-switched (CS) contexts. Additionally, speech generation and\nrecognition tasks are often handled separately, such as VALL-E and Qwen-Audio.\nIn this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating\nmultilingual speech generation and recognition tasks within the single LLM.\nFurthermore, we develop an effective data construction approach that splits and\nconcatenates words from different languages to equip LLMs with CS synthesis\nability without relying on CS data. The experimental results demonstrate that\nour model outperforms other baselines with a comparable data scale.\nFurthermore, our data construction approach not only equips LLMs with CS speech\nsynthesis capability with comparable speaker consistency and similarity to any\ngiven speaker, but also improves the performance of LLMs in multilingual speech\ngeneration and recognition tasks."
                },
                "authors": [
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Daxin Tan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Chen"
                },
                "author": "Xiao Chen",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12422v2",
                "updated": "2024-09-17T07:58:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    58,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-18T09:14:58Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    14,
                    58,
                    1,
                    170,
                    0
                ],
                "title": "Open-Source Web Service with Morphological Dictionary-Supplemented Deep\n  Learning for Morphosyntactic Analysis of Czech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source Web Service with Morphological Dictionary-Supplemented Deep\n  Learning for Morphosyntactic Analysis of Czech"
                },
                "summary": "We present an open-source web service for Czech morphosyntactic analysis. The\nsystem combines a deep learning model with rescoring by a high-precision\nmorphological dictionary at inference time. We show that our hybrid method\nsurpasses two competitive baselines: While the deep learning model ensures\ngeneralization for out-of-vocabulary words and better disambiguation, an\nimprovement over an existing morphological analyser MorphoDiTa, at the same\ntime, the deep learning model benefits from inference-time guidance of a\nmanually curated morphological dictionary. We achieve 50% error reduction in\nlemmatization and 58% error reduction in POS tagging over MorphoDiTa, while\nalso offering dependency parsing. The model is trained on one of the currently\nlargest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models\navailable at https://hdl.handle.net/11234/1-5293. We provide the tool as a web\nservice deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source\ncode is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2),\nalong with a Python client for a simple use. The documentation for the models\ncan be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an open-source web service for Czech morphosyntactic analysis. The\nsystem combines a deep learning model with rescoring by a high-precision\nmorphological dictionary at inference time. We show that our hybrid method\nsurpasses two competitive baselines: While the deep learning model ensures\ngeneralization for out-of-vocabulary words and better disambiguation, an\nimprovement over an existing morphological analyser MorphoDiTa, at the same\ntime, the deep learning model benefits from inference-time guidance of a\nmanually curated morphological dictionary. We achieve 50% error reduction in\nlemmatization and 58% error reduction in POS tagging over MorphoDiTa, while\nalso offering dependency parsing. The model is trained on one of the currently\nlargest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models\navailable at https://hdl.handle.net/11234/1-5293. We provide the tool as a web\nservice deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source\ncode is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2),\nalong with a Python client for a simple use. The documentation for the models\ncan be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model."
                },
                "authors": [
                    {
                        "name": "Milan Straka"
                    },
                    {
                        "name": "Jana Straková"
                    }
                ],
                "author_detail": {
                    "name": "Jana Straková"
                },
                "author": "Jana Straková",
                "arxiv_doi": "10.1007/978-3-031-70563-2_22",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-70563-2_22",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.12422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to TSD 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10955v1",
                "updated": "2024-09-17T07:44:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    44,
                    6,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:44:06Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    44,
                    6,
                    1,
                    261,
                    0
                ],
                "title": "Investigating Context-Faithfulness in Large Language Models: The Roles\n  of Memory Strength and Evidence Style",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Context-Faithfulness in Large Language Models: The Roles\n  of Memory Strength and Evidence Style"
                },
                "summary": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs'\ncontext-faithfulness remain largely unexplored. In this study, we investigate\nthe impact of memory strength and evidence presentation on LLMs' receptiveness\nto external evidence. We introduce a method to quantify the memory strength of\nLLMs by measuring the divergence in LLMs' responses to different paraphrases of\nthe same question, which is not considered by previous works. We also generate\nevidence in various styles to evaluate the effects of evidence in different\nstyles. Two datasets are used for evaluation: Natural Questions (NQ) with\npopular questions and popQA featuring long-tail questions. Our results show\nthat for questions with high memory strength, LLMs are more likely to rely on\ninternal memory, particularly for larger LLMs such as GPT-4. On the other hand,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs'\ncontext-faithfulness remain largely unexplored. In this study, we investigate\nthe impact of memory strength and evidence presentation on LLMs' receptiveness\nto external evidence. We introduce a method to quantify the memory strength of\nLLMs by measuring the divergence in LLMs' responses to different paraphrases of\nthe same question, which is not considered by previous works. We also generate\nevidence in various styles to evaluate the effects of evidence in different\nstyles. Two datasets are used for evaluation: Natural Questions (NQ) with\npopular questions and popQA featuring long-tail questions. Our results show\nthat for questions with high memory strength, LLMs are more likely to rely on\ninternal memory, particularly for larger LLMs such as GPT-4. On the other hand,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details."
                },
                "authors": [
                    {
                        "name": "Yuepei Li"
                    },
                    {
                        "name": "Kang Zhou"
                    },
                    {
                        "name": "Qiao Qiao"
                    },
                    {
                        "name": "Bach Nguyen"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14887v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14887v4",
                "updated": "2024-09-17T07:42:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    42,
                    58,
                    1,
                    261,
                    0
                ],
                "published": "2024-02-22T08:32:31Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    8,
                    32,
                    31,
                    3,
                    53,
                    0
                ],
                "title": "Infer metabolic velocities from moment differences of molecular weight\n  distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infer metabolic velocities from moment differences of molecular weight\n  distributions"
                },
                "summary": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic balance changes over time or\nafter different treatments. Isotopic labeling experiment is the standard method\nto infer metabolic flux, which is currently defined as the flow of a single\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic balance changes after different treatments in an\nexperiment. This study introduces a different concept: molecular weight\ndistribution, which is the empirical distribution of the molecular weights of\nall metabolites of interest. By estimating the differences of the location and\nscale estimates of these distributions, it becomes possible to quantitatively\ninfer the metabolic balance changes even without requiring knowledge of the\nexact chemical structures of these compounds and their related pathways. This\nresearch article provides a mathematical framing for a classic biological\nconcept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic balance changes over time or\nafter different treatments. Isotopic labeling experiment is the standard method\nto infer metabolic flux, which is currently defined as the flow of a single\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic balance changes after different treatments in an\nexperiment. This study introduces a different concept: molecular weight\ndistribution, which is the empirical distribution of the molecular weights of\nall metabolites of interest. By estimating the differences of the location and\nscale estimates of these distributions, it becomes possible to quantitatively\ninfer the metabolic balance changes even without requiring knowledge of the\nexact chemical structures of these compounds and their related pathways. This\nresearch article provides a mathematical framing for a classic biological\nconcept."
                },
                "authors": [
                    {
                        "name": "Li Tuobang"
                    }
                ],
                "author_detail": {
                    "name": "Li Tuobang"
                },
                "author": "Li Tuobang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14887v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14887v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10942v1",
                "updated": "2024-09-17T07:21:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    21,
                    49,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:21:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    21,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Optimizing TinyML: The Impact of Reduced Data Acquisition Rates for Time\n  Series Classification on Microcontrollers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing TinyML: The Impact of Reduced Data Acquisition Rates for Time\n  Series Classification on Microcontrollers"
                },
                "summary": "Tiny Machine Learning (TinyML) enables efficient, lowcost, and privacy\npreserving machine learning inference directly on microcontroller units (MCUs)\nconnected to sensors. Optimizing models for these constrained environments is\ncrucial. This paper investigates how reducing data acquisition rates affects\nTinyML models for time series classification, focusing on resource-constrained,\nbattery operated IoT devices. By lowering data sampling frequency, we aim to\nreduce computational demands RAM usage, energy consumption, latency, and MAC\noperations by approximately fourfold while maintaining similar classification\naccuracies. Our experiments with six benchmark datasets (UCIHAR, WISDM, PAMAP2,\nMHEALTH, MITBIH, and PTB) showed that reducing data acquisition rates\nsignificantly cut energy consumption and computational load, with minimal\naccuracy loss. For example, a 75\\% reduction in acquisition rate for MITBIH and\nPTB datasets led to a 60\\% decrease in RAM usage, 75\\% reduction in MAC\noperations, 74\\% decrease in latency, and 70\\% reduction in energy consumption,\nwithout accuracy loss. These results offer valuable insights for deploying\nefficient TinyML models in constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Machine Learning (TinyML) enables efficient, lowcost, and privacy\npreserving machine learning inference directly on microcontroller units (MCUs)\nconnected to sensors. Optimizing models for these constrained environments is\ncrucial. This paper investigates how reducing data acquisition rates affects\nTinyML models for time series classification, focusing on resource-constrained,\nbattery operated IoT devices. By lowering data sampling frequency, we aim to\nreduce computational demands RAM usage, energy consumption, latency, and MAC\noperations by approximately fourfold while maintaining similar classification\naccuracies. Our experiments with six benchmark datasets (UCIHAR, WISDM, PAMAP2,\nMHEALTH, MITBIH, and PTB) showed that reducing data acquisition rates\nsignificantly cut energy consumption and computational load, with minimal\naccuracy loss. For example, a 75\\% reduction in acquisition rate for MITBIH and\nPTB datasets led to a 60\\% decrease in RAM usage, 75\\% reduction in MAC\noperations, 74\\% decrease in latency, and 70\\% reduction in energy consumption,\nwithout accuracy loss. These results offer valuable insights for deploying\nefficient TinyML models in constrained environments."
                },
                "authors": [
                    {
                        "name": "Riya Samanta"
                    },
                    {
                        "name": "Bidyut Saha"
                    },
                    {
                        "name": "Soumya K. Ghosh"
                    },
                    {
                        "name": "Ram Babu Roy"
                    }
                ],
                "author_detail": {
                    "name": "Ram Babu Roy"
                },
                "author": "Ram Babu Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10937v1",
                "updated": "2024-09-17T07:15:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    15,
                    15,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:15:15Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    15,
                    15,
                    1,
                    261,
                    0
                ],
                "title": "Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering\n  Part I: Galaxies in the Restframe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering\n  Part I: Galaxies in the Restframe"
                },
                "summary": "Forward models of the galaxy density field enable simulation based inference\nas well as field level inference of galaxy clustering. However, these analysis\ntechniques require forward models that are both computationally fast and robust\nto modeling uncertainties in the relation between galaxies and matter. Both\nrequirements can be addressed with the Effective Field Theory of Large Scale\nStructure. Here, we focus on the physical and numerical convergence of the\nLEFTfield model. Based on the perturbative nature of the forward model, we\nderive an analytic understanding of the leading numerical errors, and we\ncompare our estimates to high-resolution and N-body references. This allows us\nto derive a set of best-practice recommendations for the numerical accuracy\nparameters, which are completely specified by the desired order of the\nperturbative solution and the cut-off scale. We verify these recommendations by\nan extended set of parameter recovery tests from fully nonlinear mock data and\nfind very consistent results. A single evaluation of the forward model takes\nseconds, making cosmological analyses of galaxy clustering data based on\nforward models computationally feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward models of the galaxy density field enable simulation based inference\nas well as field level inference of galaxy clustering. However, these analysis\ntechniques require forward models that are both computationally fast and robust\nto modeling uncertainties in the relation between galaxies and matter. Both\nrequirements can be addressed with the Effective Field Theory of Large Scale\nStructure. Here, we focus on the physical and numerical convergence of the\nLEFTfield model. Based on the perturbative nature of the forward model, we\nderive an analytic understanding of the leading numerical errors, and we\ncompare our estimates to high-resolution and N-body references. This allows us\nto derive a set of best-practice recommendations for the numerical accuracy\nparameters, which are completely specified by the desired order of the\nperturbative solution and the cut-off scale. We verify these recommendations by\nan extended set of parameter recovery tests from fully nonlinear mock data and\nfind very consistent results. A single evaluation of the forward model takes\nseconds, making cosmological analyses of galaxy clustering data based on\nforward models computationally feasible."
                },
                "authors": [
                    {
                        "name": "Julia Stadler"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Martin Reinecke"
                    }
                ],
                "author_detail": {
                    "name": "Martin Reinecke"
                },
                "author": "Martin Reinecke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10927v2",
                "updated": "2024-09-18T07:23:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    23,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T06:51:59Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    51,
                    59,
                    1,
                    261,
                    0
                ],
                "title": "Propulsion: Steering LLM with Tiny Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propulsion: Steering LLM with Tiny Fine-Tuning"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "Prakash Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Bhat"
                },
                "author": "Prakash Bhat",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14036v2",
                "updated": "2024-09-17T06:33:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    33,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-05-22T22:10:40Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    22,
                    10,
                    40,
                    2,
                    143,
                    0
                ],
                "title": "Remote Keylogging Attacks in Multi-user VR Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote Keylogging Attacks in Multi-user VR Applications"
                },
                "summary": "As Virtual Reality (VR) applications grow in popularity, they have bridged\ndistances and brought users closer together. However, with this growth, there\nhave been increasing concerns about security and privacy, especially related to\nthe motion data used to create immersive experiences. In this study, we\nhighlight a significant security threat in multi-user VR applications, which\nare applications that allow multiple users to interact with each other in the\nsame virtual space. Specifically, we propose a remote attack that utilizes the\navatar rendering information collected from an adversary's game clients to\nextract user-typed secrets like credit card information, passwords, or private\nconversations. We do this by (1) extracting motion data from network packets,\nand (2) mapping motion data to keystroke entries. We conducted a user study to\nverify the attack's effectiveness, in which our attack successfully inferred\n97.62% of the keystrokes. Besides, we performed an additional experiment to\nunderline that our attack is practical, confirming its effectiveness even when\n(1) there are multiple users in a room, and (2) the attacker cannot see the\nvictims. Moreover, we replicated our proposed attack on four applications to\ndemonstrate the generalizability of the attack. Lastly, we proposed a defense\nagainst the attack, which has been implemented by major players in the VR\nindustry. These results underscore the severity of the vulnerability and its\npotential impact on millions of VR social platform users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Virtual Reality (VR) applications grow in popularity, they have bridged\ndistances and brought users closer together. However, with this growth, there\nhave been increasing concerns about security and privacy, especially related to\nthe motion data used to create immersive experiences. In this study, we\nhighlight a significant security threat in multi-user VR applications, which\nare applications that allow multiple users to interact with each other in the\nsame virtual space. Specifically, we propose a remote attack that utilizes the\navatar rendering information collected from an adversary's game clients to\nextract user-typed secrets like credit card information, passwords, or private\nconversations. We do this by (1) extracting motion data from network packets,\nand (2) mapping motion data to keystroke entries. We conducted a user study to\nverify the attack's effectiveness, in which our attack successfully inferred\n97.62% of the keystrokes. Besides, we performed an additional experiment to\nunderline that our attack is practical, confirming its effectiveness even when\n(1) there are multiple users in a room, and (2) the attacker cannot see the\nvictims. Moreover, we replicated our proposed attack on four applications to\ndemonstrate the generalizability of the attack. Lastly, we proposed a defense\nagainst the attack, which has been implemented by major players in the VR\nindustry. These results underscore the severity of the vulnerability and its\npotential impact on millions of VR social platform users."
                },
                "authors": [
                    {
                        "name": "Zihao Su"
                    },
                    {
                        "name": "Kunlin Cai"
                    },
                    {
                        "name": "Reuben Beeler"
                    },
                    {
                        "name": "Lukas Dresel"
                    },
                    {
                        "name": "Allan Garcia"
                    },
                    {
                        "name": "Ilya Grishchenko"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Christopher Kruegel"
                    },
                    {
                        "name": "Giovanni Vigna"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Vigna"
                },
                "author": "Giovanni Vigna",
                "arxiv_comment": "Accepted for Usenix 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10177v2",
                "updated": "2024-09-17T06:30:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    30,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T11:13:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    13,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "Augmenting Automatic Speech Recognition Models with Disfluency Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Automatic Speech Recognition Models with Disfluency Detection"
                },
                "summary": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Robin Amann"
                    },
                    {
                        "name": "Zhaolin Li"
                    },
                    {
                        "name": "Barbara Bruno"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "Accepted by SLT2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03612v2",
                "updated": "2024-09-17T06:25:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    25,
                    38,
                    1,
                    261,
                    0
                ],
                "published": "2024-08-07T08:08:08Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    8,
                    8,
                    8,
                    2,
                    220,
                    0
                ],
                "title": "JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling"
                },
                "summary": "Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21."
                },
                "authors": [
                    {
                        "name": "Seok Hwan Lee"
                    },
                    {
                        "name": "Taein Son"
                    },
                    {
                        "name": "Soo Won Seo"
                    },
                    {
                        "name": "Jisong Kim"
                    },
                    {
                        "name": "Jun Won Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jun Won Choi"
                },
                "author": "Jun Won Choi",
                "arxiv_comment": "31 pages, 10 figures, update references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10913v1",
                "updated": "2024-09-17T06:11:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T06:11:22Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers"
                },
                "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Mehak Chhokar"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Mahendra Meena"
                    },
                    {
                        "name": "Hamid Abdullah"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Ruchit Nagar"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10909v1",
                "updated": "2024-09-17T05:59:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    59,
                    32,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T05:59:32Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    59,
                    32,
                    1,
                    261,
                    0
                ],
                "title": "GenCRF: Generative Clustering and Reformulation Framework for Enhanced\n  Intent-Driven Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenCRF: Generative Clustering and Reformulation Framework for Enhanced\n  Intent-Driven Information Retrieval"
                },
                "summary": "Query reformulation is a well-known problem in Information Retrieval (IR)\naimed at enhancing single search successful completion rate by automatically\nmodifying user's input query. Recent methods leverage Large Language Models\n(LLMs) to improve query reformulation, but often generate limited and redundant\nexpansions, potentially constraining their effectiveness in capturing diverse\nintents. In this paper, we propose GenCRF: a Generative Clustering and\nReformulation Framework to capture diverse intentions adaptively based on\nmultiple differentiated, well-generated queries in the retrieval phase for the\nfirst time. GenCRF leverages LLMs to generate variable queries from the initial\nquery using customized prompts, then clusters them into groups to distinctly\nrepresent diverse intents. Furthermore, the framework explores to combine\ndiverse intents query with innovative weighted aggregation strategies to\noptimize retrieval performance and crucially integrates a novel Query\nEvaluation Rewarding Model (QERM) to refine the process through feedback loops.\nEmpirical experiments on the BEIR benchmark demonstrate that GenCRF achieves\nstate-of-the-art performance, surpassing previous query reformulation SOTAs by\nup to 12% on nDCG@10. These techniques can be adapted to various LLMs,\nsignificantly boosting retriever performance and advancing the field of\nInformation Retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query reformulation is a well-known problem in Information Retrieval (IR)\naimed at enhancing single search successful completion rate by automatically\nmodifying user's input query. Recent methods leverage Large Language Models\n(LLMs) to improve query reformulation, but often generate limited and redundant\nexpansions, potentially constraining their effectiveness in capturing diverse\nintents. In this paper, we propose GenCRF: a Generative Clustering and\nReformulation Framework to capture diverse intentions adaptively based on\nmultiple differentiated, well-generated queries in the retrieval phase for the\nfirst time. GenCRF leverages LLMs to generate variable queries from the initial\nquery using customized prompts, then clusters them into groups to distinctly\nrepresent diverse intents. Furthermore, the framework explores to combine\ndiverse intents query with innovative weighted aggregation strategies to\noptimize retrieval performance and crucially integrates a novel Query\nEvaluation Rewarding Model (QERM) to refine the process through feedback loops.\nEmpirical experiments on the BEIR benchmark demonstrate that GenCRF achieves\nstate-of-the-art performance, surpassing previous query reformulation SOTAs by\nup to 12% on nDCG@10. These techniques can be adapted to various LLMs,\nsignificantly boosting retriever performance and advancing the field of\nInformation Retrieval."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Haojie Zhang"
                    },
                    {
                        "name": "Yueyang Zhang"
                    },
                    {
                        "name": "Changhao Zhang"
                    },
                    {
                        "name": "Songyao Duan"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14043v3",
                "updated": "2024-09-17T04:56:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    56,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-01-25T09:47:55Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    9,
                    47,
                    55,
                    3,
                    25,
                    0
                ],
                "title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Goal-oriented Prompt Engineering for Large Language Models: A\n  Survey"
                },
                "summary": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields."
                },
                "authors": [
                    {
                        "name": "Haochen Li"
                    },
                    {
                        "name": "Jonathan Leung"
                    },
                    {
                        "name": "Zhiqi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Shen"
                },
                "author": "Zhiqi Shen",
                "arxiv_comment": "An up-to-date resource including papers and tasks is maintained at\n  https://github.com/Alex-HaochenLi/Goal-oriented-Prompt-Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10883v1",
                "updated": "2024-09-17T04:39:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    20,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T04:39:20Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    20,
                    1,
                    261,
                    0
                ],
                "title": "CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation\n  for Meeting Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation\n  for Meeting Summarization"
                },
                "summary": "Large Language Models (LLMs) have spurred interest in automatic evaluation\nmethods for summarization, offering a faster, more cost-effective alternative\nto human evaluation. However, existing methods often fall short when applied to\ncomplex tasks like long-context summarizations and dialogue-based meeting\nsummarizations. In this paper, we introduce CREAM (Comparison-Based\nReference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a\nnovel framework that addresses the unique challenges of evaluating meeting\nsummaries. CREAM leverages a combination of chain-of-thought reasoning and key\nfacts alignment to assess conciseness and completeness of model-generated\nsummaries without requiring reference. By employing an ELO ranking system, our\napproach provides a robust mechanism for comparing the quality of different\nmodels or prompt configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have spurred interest in automatic evaluation\nmethods for summarization, offering a faster, more cost-effective alternative\nto human evaluation. However, existing methods often fall short when applied to\ncomplex tasks like long-context summarizations and dialogue-based meeting\nsummarizations. In this paper, we introduce CREAM (Comparison-Based\nReference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a\nnovel framework that addresses the unique challenges of evaluating meeting\nsummaries. CREAM leverages a combination of chain-of-thought reasoning and key\nfacts alignment to assess conciseness and completeness of model-generated\nsummaries without requiring reference. By employing an ELO ranking system, our\napproach provides a robust mechanism for comparing the quality of different\nmodels or prompt configurations."
                },
                "authors": [
                    {
                        "name": "Ziwei Gong"
                    },
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Harshsaiprasad Deshpande"
                    },
                    {
                        "name": "Alexander Johnson"
                    },
                    {
                        "name": "Emmy Phung"
                    },
                    {
                        "name": "Zehui Wu"
                    },
                    {
                        "name": "Ahmad Emami"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10870v1",
                "updated": "2024-09-17T03:46:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    46,
                    1,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T03:46:01Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    46,
                    1,
                    1,
                    261,
                    0
                ],
                "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Large Language Models By Layerwise Attention Shortcuts"
                },
                "summary": "Transformer architectures are the backbone of the modern AI revolution.\nHowever, they are based on simply stacking the same blocks in dozens of layers\nand processing information sequentially from one block to another. In this\npaper, we propose to challenge this and introduce adaptive computations for\nLLM-like setups, which allow the final layer to attend to all of the\nintermediate layers as it deems fit through the attention mechanism, thereby\nintroducing computational \\textbf{attention shortcuts}. These shortcuts can\nthus make the architecture depth and context adaptive. We showcase four\ndifferent datasets, namely acoustic tokens, natural language, and symbolic\nmusic, and we achieve superior performance for GPT-like architecture. We give\nevidence via attention maps that the models learn complex dependencies across\nlayers that are adaptive in context and depth depending on the input tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer architectures are the backbone of the modern AI revolution.\nHowever, they are based on simply stacking the same blocks in dozens of layers\nand processing information sequentially from one block to another. In this\npaper, we propose to challenge this and introduce adaptive computations for\nLLM-like setups, which allow the final layer to attend to all of the\nintermediate layers as it deems fit through the attention mechanism, thereby\nintroducing computational \\textbf{attention shortcuts}. These shortcuts can\nthus make the architecture depth and context adaptive. We showcase four\ndifferent datasets, namely acoustic tokens, natural language, and symbolic\nmusic, and we achieve superior performance for GPT-like architecture. We give\nevidence via attention maps that the models learn complex dependencies across\nlayers that are adaptive in context and depth depending on the input tokens."
                },
                "authors": [
                    {
                        "name": "Prateek Verma"
                    },
                    {
                        "name": "Mert Pilanci"
                    }
                ],
                "author_detail": {
                    "name": "Mert Pilanci"
                },
                "author": "Mert Pilanci",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10869v1",
                "updated": "2024-09-17T03:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    44,
                    12,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T03:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    44,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "Stable Case BB/BC Mass Transfer to Form GW190425-like Massive Binary\n  Neutron Star Mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Case BB/BC Mass Transfer to Form GW190425-like Massive Binary\n  Neutron Star Mergers"
                },
                "summary": "On April 25th, 2019, the LIGO-Virgo Collaboration discovered a\nGravitational-wave (GW) signal from a binary neutron star (BNS) merger, i.e.,\nGW190425. Due to the inferred large total mass, the origin of GW190425 remains\nunclear. We perform detailed stellar structure and binary evolution\ncalculations that take into account mass-loss, internal differential rotation,\nand tidal interactions between a He-rich star and a NS companion. We explore\nthe parameter space of the initial binary properties, including initial NS and\nHe-rich masses and initial orbital period. We find that the immediate\npost-common-envelope progenitor system, consisting of a primary\n$\\sim2.0\\,M_\\odot$ ($\\sim1.7\\,M_\\odot$) NS and a secondary He-rich star with an\ninitial mass of $\\sim3.0-5.5\\,M_\\odot$ ($\\sim5.5-6.0\\,M_\\odot$) in a close\nbinary with an initial period of $\\sim0.08-0.5\\,{\\rm{days}}$ ($\\sim\n0.08-0.4\\,{\\rm{days}}$), that experiences stable Case BB/BC mass transfer (MT)\nduring binary evolution, can reproduce the formation of GW190425-like BNS\nevents. Our studies reveal that the secondary He-rich star of the GW190425's\nprogenitor before its core collapse can be efficiently spun up through tidal\ninteraction, finally remaining as a NS with rotational energy even reaching\n$\\sim10^{52}\\,{\\rm{erg}}$, which is always much higher than the neutrino-driven\nenergy of the supernova (SN) explosion. If the newborn secondary NS is a\nmagnetar, we expect that GW190425 can be the remnant of a magnetar-driven SN,\ne.g., a magnetar-driven ultra-stripped SN, a superluminous SN, or a broad-line\nType Ic SN. Our results show that GW190425 could be formed through the isolated\nbinary evolution, which involves a stable Case BB/BC MT just after the common\nenvelope phase. On top of that, we show the He-rich star can be tidally spun\nup, potentially forming a spinning magnetized NS (magnetar) during the second\nSN explosion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On April 25th, 2019, the LIGO-Virgo Collaboration discovered a\nGravitational-wave (GW) signal from a binary neutron star (BNS) merger, i.e.,\nGW190425. Due to the inferred large total mass, the origin of GW190425 remains\nunclear. We perform detailed stellar structure and binary evolution\ncalculations that take into account mass-loss, internal differential rotation,\nand tidal interactions between a He-rich star and a NS companion. We explore\nthe parameter space of the initial binary properties, including initial NS and\nHe-rich masses and initial orbital period. We find that the immediate\npost-common-envelope progenitor system, consisting of a primary\n$\\sim2.0\\,M_\\odot$ ($\\sim1.7\\,M_\\odot$) NS and a secondary He-rich star with an\ninitial mass of $\\sim3.0-5.5\\,M_\\odot$ ($\\sim5.5-6.0\\,M_\\odot$) in a close\nbinary with an initial period of $\\sim0.08-0.5\\,{\\rm{days}}$ ($\\sim\n0.08-0.4\\,{\\rm{days}}$), that experiences stable Case BB/BC mass transfer (MT)\nduring binary evolution, can reproduce the formation of GW190425-like BNS\nevents. Our studies reveal that the secondary He-rich star of the GW190425's\nprogenitor before its core collapse can be efficiently spun up through tidal\ninteraction, finally remaining as a NS with rotational energy even reaching\n$\\sim10^{52}\\,{\\rm{erg}}$, which is always much higher than the neutrino-driven\nenergy of the supernova (SN) explosion. If the newborn secondary NS is a\nmagnetar, we expect that GW190425 can be the remnant of a magnetar-driven SN,\ne.g., a magnetar-driven ultra-stripped SN, a superluminous SN, or a broad-line\nType Ic SN. Our results show that GW190425 could be formed through the isolated\nbinary evolution, which involves a stable Case BB/BC MT just after the common\nenvelope phase. On top of that, we show the He-rich star can be tidally spun\nup, potentially forming a spinning magnetized NS (magnetar) during the second\nSN explosion."
                },
                "authors": [
                    {
                        "name": "Ying Qin"
                    },
                    {
                        "name": "Jin-Ping Zhu"
                    },
                    {
                        "name": "Georges Meynet"
                    },
                    {
                        "name": "Bing Zhang"
                    },
                    {
                        "name": "Fa-Yin Wang"
                    },
                    {
                        "name": "Xin-Wen Shu"
                    },
                    {
                        "name": "Han-Feng Song"
                    },
                    {
                        "name": "Yuan-Zhu Wang"
                    },
                    {
                        "name": "Liang Yuan"
                    },
                    {
                        "name": "Zhen-Han-Tao Wang"
                    },
                    {
                        "name": "Rui-Chong Hu"
                    },
                    {
                        "name": "Dong-Hong Wu"
                    },
                    {
                        "name": "Shuang-Xi Yi"
                    },
                    {
                        "name": "Qing-Wen Tang"
                    },
                    {
                        "name": "Jun-Jie Wei"
                    },
                    {
                        "name": "Xue-Feng Wu"
                    },
                    {
                        "name": "En-Wei Liang"
                    }
                ],
                "author_detail": {
                    "name": "En-Wei Liang"
                },
                "author": "En-Wei Liang",
                "arxiv_comment": "7 pages, 8 figures, resubmited to the journal of Astronomy and\n  Astrophysics with minor revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10354v2",
                "updated": "2024-09-17T03:22:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    22,
                    45,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T15:04:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    4,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot"
                },
                "summary": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots."
                },
                "authors": [
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "arxiv_comment": "The first two authors contributed equally to this research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14378v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14378v3",
                "updated": "2024-09-17T02:48:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    2,
                    48,
                    13,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-20T14:46:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    14,
                    46,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "X-ray view of dissipative warm corona in active galactic nuclei",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray view of dissipative warm corona in active galactic nuclei"
                },
                "summary": "In the X-ray spectra of AGNs, a noticeable excess of soft X-rays is typically\ndetected beyond the extrapolation of the power-law trend observed between 2-10\nkeV. In the scenario of warm Comptonization, observations propose a warm corona\ntemperature ranging from 0.1-1 keV and an optical depth of approximately 10-20.\nFurthermore, according to radiative constraints derived from spectral analyses\nemploying Comptonization models, it is suggested that the majority of the\naccretion power is released within the warm corona, while the disk beneath it\nis largely non-dissipative, emitting mainly the reprocessed radiation from the\ncorona. We test the dissipative warm corona model using the radiative transfer\ncode-TITAN/NOAR on a sample of 82 XMM-Newton observations of AGNs. Through\nspectral modeling of the X-ray data, we aim to estimate the total amount of\ninternal heating inside the warm corona situated on top of the accretion disk.\nBy modeling the 0.3-10 keV EPIC-pn spectra, we estimate the internal heating\nand optical depth of the warm corona and check their correlations with global\nparameters blackhole parameters. From model normalization, we compute the\nradial extent of warm corona on top of cold accretion disk. Our model infers\nthe presence of dissipative warm corona, with optical depths distributed in the\nrange 6-30 and total internal heating in the range 1-29 x 1e-23 erg/s-cm3. The\nextent of warm corona is spread across a large range from 7-408 gravitational\nradii, and we find that warm corona is more extended for larger accretion\nrates. Soft excess emission is ubiquitous in AGNs across wide mass range and\naccretion rate. We confirm that warm corona responsible for producing the\nsoft-excess is highly dissipative in nature with larger optical depths being\nassociated with lower internal heating and vice versa. The cold standard\naccretion disk regulates the extent of warm corona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the X-ray spectra of AGNs, a noticeable excess of soft X-rays is typically\ndetected beyond the extrapolation of the power-law trend observed between 2-10\nkeV. In the scenario of warm Comptonization, observations propose a warm corona\ntemperature ranging from 0.1-1 keV and an optical depth of approximately 10-20.\nFurthermore, according to radiative constraints derived from spectral analyses\nemploying Comptonization models, it is suggested that the majority of the\naccretion power is released within the warm corona, while the disk beneath it\nis largely non-dissipative, emitting mainly the reprocessed radiation from the\ncorona. We test the dissipative warm corona model using the radiative transfer\ncode-TITAN/NOAR on a sample of 82 XMM-Newton observations of AGNs. Through\nspectral modeling of the X-ray data, we aim to estimate the total amount of\ninternal heating inside the warm corona situated on top of the accretion disk.\nBy modeling the 0.3-10 keV EPIC-pn spectra, we estimate the internal heating\nand optical depth of the warm corona and check their correlations with global\nparameters blackhole parameters. From model normalization, we compute the\nradial extent of warm corona on top of cold accretion disk. Our model infers\nthe presence of dissipative warm corona, with optical depths distributed in the\nrange 6-30 and total internal heating in the range 1-29 x 1e-23 erg/s-cm3. The\nextent of warm corona is spread across a large range from 7-408 gravitational\nradii, and we find that warm corona is more extended for larger accretion\nrates. Soft excess emission is ubiquitous in AGNs across wide mass range and\naccretion rate. We confirm that warm corona responsible for producing the\nsoft-excess is highly dissipative in nature with larger optical depths being\nassociated with lower internal heating and vice versa. The cold standard\naccretion disk regulates the extent of warm corona."
                },
                "authors": [
                    {
                        "name": "B. Palit"
                    },
                    {
                        "name": "A. Rozanska"
                    },
                    {
                        "name": "P. O. Petrucci"
                    },
                    {
                        "name": "D. Gronkiewicz"
                    },
                    {
                        "name": "S. Barnier"
                    },
                    {
                        "name": "S. Bianchi"
                    },
                    {
                        "name": "D. R. Ballantyne"
                    },
                    {
                        "name": "V. E. Gianolli"
                    },
                    {
                        "name": "R. Middei"
                    },
                    {
                        "name": "R. Belmont"
                    },
                    {
                        "name": "F. Ursini"
                    }
                ],
                "author_detail": {
                    "name": "F. Ursini"
                },
                "author": "F. Ursini",
                "arxiv_comment": "24 pages, 13 figures; Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14378v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14378v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.11404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11404v1",
                "updated": "2024-09-17T17:59:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:59:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs"
                },
                "summary": "Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes ~45K\npost-edited samples, a cultural benchmark, and highlights the importance of\ntailored training to improve LLM performance in capturing the nuances of\ndiverse Arabic dialects and cultural contexts. We will release the dialectal\ntranslation models and benchmarks curated in this study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic, with its rich diversity of dialects, remains significantly\nunderrepresented in Large Language Models, particularly in dialectal\nvariations. We address this gap by introducing seven synthetic datasets in\ndialects alongside Modern Standard Arabic (MSA), created using Machine\nTranslation (MT) combined with human post-editing. We present AraDiCE, a\nbenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on\ndialect comprehension and generation, focusing specifically on low-resource\nArabic dialects. Additionally, we introduce the first-ever fine-grained\nbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, and\nLevant regions, providing a novel dimension to LLM evaluation. Our findings\ndemonstrate that while Arabic-specific models like Jais and AceGPT outperform\nmultilingual models on dialectal tasks, significant challenges persist in\ndialect identification, generation, and translation. This work contributes ~45K\npost-edited samples, a cultural benchmark, and highlights the importance of\ntailored training to improve LLM performance in capturing the nuances of\ndiverse Arabic dialects and cultural contexts. We will release the dialectal\ntranslation models and benchmarks curated in this study."
                },
                "authors": [
                    {
                        "name": "Basel Mousi"
                    },
                    {
                        "name": "Nadir Durrani"
                    },
                    {
                        "name": "Fatema Ahmad"
                    },
                    {
                        "name": "Md. Arid Hasan"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Tameem Kabbani"
                    },
                    {
                        "name": "Fahim Dalvi"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Benchmarking, Culturally Informed, Large Language Models, Arabic NLP,\n  LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11402v1",
                "updated": "2024-09-17T17:59:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    6,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:59:06Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    59,
                    6,
                    1,
                    261,
                    0
                ],
                "title": "NVLM: Open Frontier-Class Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVLM: Open Frontier-Class Multimodal LLMs"
                },
                "summary": "We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we are releasing the model weights and will open-source the code for the\ncommunity: https://nvlm-project.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we are releasing the model weights and will open-source the code for the\ncommunity: https://nvlm-project.github.io/."
                },
                "authors": [
                    {
                        "name": "Wenliang Dai"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Zhuoling Yang"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Jon Barker"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Wei Ping"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ping"
                },
                "author": "Wei Ping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11393v1",
                "updated": "2024-09-17T17:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    54,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents"
                },
                "summary": "The integration of tools in LLM-based agents overcame the difficulties of\nstandalone LLMs and traditional agents' limited capabilities. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity. Indeed, they focused mainly on functionalities and\noverlooked the definition of the component's boundaries within the agent. This\ncaused terminological and architectural ambiguities between researchers which\nwe addressed in this paper by proposing a unified framework that establishes a\nclear foundation for LLM-based agents' development from both functional and\nsoftware architectural perspectives.\n  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),\nclearly distinguishes between the different components of an agent, setting\nLLMs, and tools apart from a newly introduced element: the core-agent, playing\nthe role of the central coordinator of the agent which comprises five modules:\nplanning, memory, profile, action, and security, the latter often neglected in\nprevious works. Differences in the internal structure of core-agents led us to\nclassify them into a taxonomy of passive and active types. Based on this, we\nproposed different multi-core agent architectures combining unique\ncharacteristics of various individual agents.\n  For evaluation purposes, we applied this framework to a selection of\nstate-of-the-art agents, thereby demonstrating its alignment with their\nfunctionalities and clarifying the overlooked architectural aspects. Moreover,\nwe thoroughly assessed four of our proposed architectures by integrating\ndistinctive agents into hybrid active/passive core-agents' systems. This\nanalysis provided clear insights into potential improvements and highlighted\nthe challenges involved in the combination of specific agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of tools in LLM-based agents overcame the difficulties of\nstandalone LLMs and traditional agents' limited capabilities. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity. Indeed, they focused mainly on functionalities and\noverlooked the definition of the component's boundaries within the agent. This\ncaused terminological and architectural ambiguities between researchers which\nwe addressed in this paper by proposing a unified framework that establishes a\nclear foundation for LLM-based agents' development from both functional and\nsoftware architectural perspectives.\n  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),\nclearly distinguishes between the different components of an agent, setting\nLLMs, and tools apart from a newly introduced element: the core-agent, playing\nthe role of the central coordinator of the agent which comprises five modules:\nplanning, memory, profile, action, and security, the latter often neglected in\nprevious works. Differences in the internal structure of core-agents led us to\nclassify them into a taxonomy of passive and active types. Based on this, we\nproposed different multi-core agent architectures combining unique\ncharacteristics of various individual agents.\n  For evaluation purposes, we applied this framework to a selection of\nstate-of-the-art agents, thereby demonstrating its alignment with their\nfunctionalities and clarifying the overlooked architectural aspects. Moreover,\nwe thoroughly assessed four of our proposed architectures by integrating\ndistinctive agents into hybrid active/passive core-agents' systems. This\nanalysis provided clear insights into potential improvements and highlighted\nthe challenges involved in the combination of specific agents."
                },
                "authors": [
                    {
                        "name": "Amine B. Hassouna"
                    },
                    {
                        "name": "Hana Chaari"
                    },
                    {
                        "name": "Ines Belhaj"
                    }
                ],
                "author_detail": {
                    "name": "Ines Belhaj"
                },
                "author": "Ines Belhaj",
                "arxiv_comment": "35 pages, 14 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02839v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02839v4",
                "updated": "2024-09-17T17:51:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    51,
                    43,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-04T16:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "title": "Jäger: Automated Telephone Call Traceback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jäger: Automated Telephone Call Traceback"
                },
                "summary": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators."
                },
                "authors": [
                    {
                        "name": "David Adei"
                    },
                    {
                        "name": "Varun Madathil"
                    },
                    {
                        "name": "Sathvik Prasad"
                    },
                    {
                        "name": "Bradley Reaves"
                    },
                    {
                        "name": "Alessandra Scafuro"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Scafuro"
                },
                "author": "Alessandra Scafuro",
                "arxiv_doi": "10.1145/3658644.3690290",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690290",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02839v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02839v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 2024 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '24), October 14---18, 2024, Salt Lake City, UT,\n  USA. ACM, New York, NY, USA, 24 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11390v1",
                "updated": "2024-09-17T17:50:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    50,
                    15,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:50:15Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    50,
                    15,
                    1,
                    261,
                    0
                ],
                "title": "Says Who? Effective Zero-Shot Annotation of Focalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Says Who? Effective Zero-Shot Annotation of Focalization"
                },
                "summary": "Focalization, the perspective through which narrative is presented, is\nencoded via a wide range of lexico-grammatical features and is subject to\nreader interpretation. Moreover, trained readers regularly disagree on\ninterpretations, suggesting that this problem may be computationally\nintractable. In this paper, we provide experiments to test how well\ncontemporary Large Language Models (LLMs) perform when annotating literary\ntexts for focalization mode. Despite the challenging nature of the task, LLMs\nshow comparable performance to trained human annotators in our experiments. We\nprovide a case study working with the novels of Stephen King to demonstrate the\nusefulness of this approach for computational literary studies, illustrating\nhow focalization can be studied at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focalization, the perspective through which narrative is presented, is\nencoded via a wide range of lexico-grammatical features and is subject to\nreader interpretation. Moreover, trained readers regularly disagree on\ninterpretations, suggesting that this problem may be computationally\nintractable. In this paper, we provide experiments to test how well\ncontemporary Large Language Models (LLMs) perform when annotating literary\ntexts for focalization mode. Despite the challenging nature of the task, LLMs\nshow comparable performance to trained human annotators in our experiments. We\nprovide a case study working with the novels of Stephen King to demonstrate the\nusefulness of this approach for computational literary studies, illustrating\nhow focalization can be studied at scale."
                },
                "authors": [
                    {
                        "name": "Rebecca M. M. Hicke"
                    },
                    {
                        "name": "Yuri Bizzoni"
                    },
                    {
                        "name": "Pascale Feldkamp"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06173v2",
                "updated": "2024-09-17T17:42:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    42,
                    26,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-10T03:06:17Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    3,
                    6,
                    17,
                    1,
                    254,
                    0
                ],
                "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks"
                },
                "summary": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors."
                },
                "authors": [
                    {
                        "name": "Georgios Chochlakis"
                    },
                    {
                        "name": "Niyantha Maruthu Pandiyan"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "arxiv_comment": "5 pages, 2 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2403.17125",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12958v2",
                "updated": "2024-09-17T17:25:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    25,
                    40,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-19T17:57:58Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    57,
                    58,
                    1,
                    79,
                    0
                ],
                "title": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models"
                },
                "summary": "Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models."
                },
                "authors": [
                    {
                        "name": "Jeffrey Cheng"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Dawn Lawrie"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11378v1",
                "updated": "2024-09-17T17:25:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    25,
                    31,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:25:31Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    25,
                    31,
                    1,
                    261,
                    0
                ],
                "title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative\n  Refinement"
                },
                "summary": "Finetuning large language models on instruction data is crucial for enhancing\npre-trained knowledge and improving instruction-following capabilities. As\ninstruction datasets proliferate, selecting optimal data for effective training\nbecomes increasingly important. This work addresses the question: How can we\ndetermine the optimal subset of data for effective training? While existing\nresearch often emphasizes local criteria like instance quality for subset\nselection, we argue that a global approach focused on data diversity is more\ncritical. Our method employs k-means clustering to ensure the selected subset\neffectively represents the full dataset. We propose an iterative refinement\nmethod inspired by active learning techniques to resample instances from\nclusters, reassessing each cluster's importance and sampling weight in every\ntraining iteration. This approach reduces the effect of outliers and\nautomatically filters out clusters containing low-quality data. Through\nextensive evaluation across natural language reasoning, general world\nknowledge, code and math reasoning tasks, and by fine-tuning models from\nvarious families, we observe consistent improvements, achieving a 7% increase\nover random selection and a 3.8% improvement over state-of-the-art sampling\nmethods. Our work highlights the significance of diversity-first sampling when\nfinetuning LLMs to enhance performance across a broad array of evaluation\ntasks. Our code is available at\nhttps://github.com/for-ai/iterative-data-selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning large language models on instruction data is crucial for enhancing\npre-trained knowledge and improving instruction-following capabilities. As\ninstruction datasets proliferate, selecting optimal data for effective training\nbecomes increasingly important. This work addresses the question: How can we\ndetermine the optimal subset of data for effective training? While existing\nresearch often emphasizes local criteria like instance quality for subset\nselection, we argue that a global approach focused on data diversity is more\ncritical. Our method employs k-means clustering to ensure the selected subset\neffectively represents the full dataset. We propose an iterative refinement\nmethod inspired by active learning techniques to resample instances from\nclusters, reassessing each cluster's importance and sampling weight in every\ntraining iteration. This approach reduces the effect of outliers and\nautomatically filters out clusters containing low-quality data. Through\nextensive evaluation across natural language reasoning, general world\nknowledge, code and math reasoning tasks, and by fine-tuning models from\nvarious families, we observe consistent improvements, achieving a 7% increase\nover random selection and a 3.8% improvement over state-of-the-art sampling\nmethods. Our work highlights the significance of diversity-first sampling when\nfinetuning LLMs to enhance performance across a broad array of evaluation\ntasks. Our code is available at\nhttps://github.com/for-ai/iterative-data-selection."
                },
                "authors": [
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Sara Ahmadian"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Fadaee"
                },
                "author": "Marzieh Fadaee",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11376v1",
                "updated": "2024-09-17T17:23:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    23,
                    44,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:23:44Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    23,
                    44,
                    1,
                    261,
                    0
                ],
                "title": "Towards Time Series Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Time Series Reasoning with LLMs"
                },
                "summary": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains."
                },
                "authors": [
                    {
                        "name": "Winnie Chow"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Haraldur T. Hallgrímsson"
                    },
                    {
                        "name": "Maxwell A. Xu"
                    },
                    {
                        "name": "Shirley You Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shirley You Ren"
                },
                "author": "Shirley You Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11375v1",
                "updated": "2024-09-17T17:22:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    22,
                    35,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:22:35Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    22,
                    35,
                    1,
                    261,
                    0
                ],
                "title": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with\n  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-OCT-SelfNet: Integrating Self-Supervised Learning with\n  Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease\n  Classification"
                },
                "summary": "In the medical domain, acquiring large datasets poses significant challenges\ndue to privacy concerns. Nonetheless, the development of a robust deep-learning\nmodel for retinal disease diagnosis necessitates a substantial dataset for\ntraining. The capacity to generalize effectively on smaller datasets remains a\npersistent challenge. The scarcity of data presents a significant barrier to\nthe practical implementation of scalable medical AI solutions. To address this\nissue, we've combined a wide range of data sources to improve performance and\ngeneralization to new data by giving it a deeper understanding of the data\nrepresentation from multi-modal datasets and developed a self-supervised\nframework based on large language models (LLMs), SwinV2 to gain a deeper\nunderstanding of multi-modal dataset representations, enhancing the model's\nability to extrapolate to new data for the detection of eye diseases using\noptical coherence tomography (OCT) images. We adopt a two-phase training\nmethodology, self-supervised pre-training, and fine-tuning on a downstream\nsupervised classifier. An ablation study conducted across three datasets\nemploying various encoder backbones, without data fusion, with low data\navailability setting, and without self-supervised pre-training scenarios,\nhighlights the robustness of our method. Our findings demonstrate consistent\nperformance across these diverse conditions, showcasing superior generalization\ncapabilities compared to the baseline model, ResNet-50.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the medical domain, acquiring large datasets poses significant challenges\ndue to privacy concerns. Nonetheless, the development of a robust deep-learning\nmodel for retinal disease diagnosis necessitates a substantial dataset for\ntraining. The capacity to generalize effectively on smaller datasets remains a\npersistent challenge. The scarcity of data presents a significant barrier to\nthe practical implementation of scalable medical AI solutions. To address this\nissue, we've combined a wide range of data sources to improve performance and\ngeneralization to new data by giving it a deeper understanding of the data\nrepresentation from multi-modal datasets and developed a self-supervised\nframework based on large language models (LLMs), SwinV2 to gain a deeper\nunderstanding of multi-modal dataset representations, enhancing the model's\nability to extrapolate to new data for the detection of eye diseases using\noptical coherence tomography (OCT) images. We adopt a two-phase training\nmethodology, self-supervised pre-training, and fine-tuning on a downstream\nsupervised classifier. An ablation study conducted across three datasets\nemploying various encoder backbones, without data fusion, with low data\navailability setting, and without self-supervised pre-training scenarios,\nhighlights the robustness of our method. Our findings demonstrate consistent\nperformance across these diverse conditions, showcasing superior generalization\ncapabilities compared to the baseline model, ResNet-50."
                },
                "authors": [
                    {
                        "name": "Fatema-E- Jannat"
                    },
                    {
                        "name": "Sina Gholami"
                    },
                    {
                        "name": "Jennifer I. Lim"
                    },
                    {
                        "name": "Theodore Leng"
                    },
                    {
                        "name": "Minhaj Nur Alam"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "arxiv_comment": "25 pages, 9 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11365v1",
                "updated": "2024-09-17T17:14:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    14,
                    41,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:14:41Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    14,
                    41,
                    1,
                    261,
                    0
                ],
                "title": "CoCA: Regaining Safety-awareness of Multimodal Large Language Models\n  with Constitutional Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCA: Regaining Safety-awareness of Multimodal Large Language Models\n  with Constitutional Calibration"
                },
                "summary": "The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Tianyang Han"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Lanqing Hong"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguo Li"
                },
                "author": "Zhenguo Li",
                "arxiv_comment": "10 pages, COLM-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11360v1",
                "updated": "2024-09-17T17:07:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    7,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T17:07:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    7,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "AI Suggestions Homogenize Writing Toward Western Styles and Diminish\n  Cultural Nuances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Suggestions Homogenize Writing Toward Western Styles and Diminish\n  Cultural Nuances"
                },
                "summary": "Large language models (LLMs) are being increasingly integrated into everyday\nproducts and services, such as coding tools and writing assistants. As these\nembedded AI applications are deployed globally, there is a growing concern that\nthe AI models underlying these applications prioritize Western values. This\npaper investigates what happens when a Western-centric AI model provides\nwriting suggestions to users from a different cultural background. We conducted\na cross-cultural controlled experiment with 118 participants from India and the\nUnited States who completed culturally grounded writing tasks with and without\nAI suggestions. Our analysis reveals that AI provided greater efficiency gains\nfor Americans compared to Indians. Moreover, AI suggestions led Indian\nparticipants to adopt Western writing styles, altering not just what is written\nbut also how it is written. These findings show that Western-centric AI models\nhomogenize writing toward Western norms, diminishing nuances that differentiate\ncultural expression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being increasingly integrated into everyday\nproducts and services, such as coding tools and writing assistants. As these\nembedded AI applications are deployed globally, there is a growing concern that\nthe AI models underlying these applications prioritize Western values. This\npaper investigates what happens when a Western-centric AI model provides\nwriting suggestions to users from a different cultural background. We conducted\na cross-cultural controlled experiment with 118 participants from India and the\nUnited States who completed culturally grounded writing tasks with and without\nAI suggestions. Our analysis reveals that AI provided greater efficiency gains\nfor Americans compared to Indians. Moreover, AI suggestions led Indian\nparticipants to adopt Western writing styles, altering not just what is written\nbut also how it is written. These findings show that Western-centric AI models\nhomogenize writing toward Western norms, diminishing nuances that differentiate\ncultural expression."
                },
                "authors": [
                    {
                        "name": "Dhruv Agarwal"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11353v1",
                "updated": "2024-09-17T16:55:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:55:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models"
                },
                "summary": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Mengfei Liang"
                    },
                    {
                        "name": "Archish Arun"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Jonathan Lutch"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Submitted to NeurIPS 2024 SoLaR (Socially Responsible Language\n  Modelling Research ) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11350v1",
                "updated": "2024-09-17T16:53:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    53,
                    47,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:53:47Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    53,
                    47,
                    1,
                    261,
                    0
                ],
                "title": "Clinical Validation of a Real-Time Machine Learning-based System for the\n  Detection of Acute Myeloid Leukemia by Flow Cytometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Validation of a Real-Time Machine Learning-based System for the\n  Detection of Acute Myeloid Leukemia by Flow Cytometry"
                },
                "summary": "Machine-learning (ML) models in flow cytometry have the potential to reduce\nerror rates, increase reproducibility, and boost the efficiency of clinical\nlabs. While numerous ML models for flow cytometry data have been proposed, few\nstudies have described the clinical deployment of such models. Realizing the\npotential gains of ML models in clinical labs requires not only an accurate\nmodel, but infrastructure for automated inference, error detection, analytics\nand monitoring, and structured data extraction. Here, we describe an ML model\nfor detection of Acute Myeloid Leukemia (AML), along with the infrastructure\nsupporting clinical implementation. Our infrastructure leverages the resilience\nand scalability of the cloud for model inference, a Kubernetes-based workflow\nsystem that provides model reproducibility and resource management, and a\nsystem for extracting structured diagnoses from full-text reports. We also\ndescribe our model monitoring and visualization platform, an essential element\nfor ensuring continued model accuracy. Finally, we present a post-deployment\nanalysis of impacts on turn-around time and compare production accuracy to the\noriginal validation statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-learning (ML) models in flow cytometry have the potential to reduce\nerror rates, increase reproducibility, and boost the efficiency of clinical\nlabs. While numerous ML models for flow cytometry data have been proposed, few\nstudies have described the clinical deployment of such models. Realizing the\npotential gains of ML models in clinical labs requires not only an accurate\nmodel, but infrastructure for automated inference, error detection, analytics\nand monitoring, and structured data extraction. Here, we describe an ML model\nfor detection of Acute Myeloid Leukemia (AML), along with the infrastructure\nsupporting clinical implementation. Our infrastructure leverages the resilience\nand scalability of the cloud for model inference, a Kubernetes-based workflow\nsystem that provides model reproducibility and resource management, and a\nsystem for extracting structured diagnoses from full-text reports. We also\ndescribe our model monitoring and visualization platform, an essential element\nfor ensuring continued model accuracy. Finally, we present a post-deployment\nanalysis of impacts on turn-around time and compare production accuracy to the\noriginal validation statistics."
                },
                "authors": [
                    {
                        "name": "Lauren M. Zuromski"
                    },
                    {
                        "name": "Jacob Durtschi"
                    },
                    {
                        "name": "Aimal Aziz"
                    },
                    {
                        "name": "Jeffrey Chumley"
                    },
                    {
                        "name": "Mark Dewey"
                    },
                    {
                        "name": "Paul English"
                    },
                    {
                        "name": "Muir Morrison"
                    },
                    {
                        "name": "Keith Simmon"
                    },
                    {
                        "name": "Blaine Whipple"
                    },
                    {
                        "name": "Brendan O'Fallon"
                    },
                    {
                        "name": "David P. Ng"
                    }
                ],
                "author_detail": {
                    "name": "David P. Ng"
                },
                "author": "David P. Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.TO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11337v1",
                "updated": "2024-09-17T16:39:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    39,
                    9,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T16:39:09Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    39,
                    9,
                    1,
                    261,
                    0
                ],
                "title": "Meijer-G Function with Continued Product and Integer Exponent:\n  Performance of Multi-Aperture UOWC System over EGG Turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meijer-G Function with Continued Product and Integer Exponent:\n  Performance of Multi-Aperture UOWC System over EGG Turbulence"
                },
                "summary": "Signal transmission over underwater optical wireless communication (UOWC)\nexperiences the combined effect of oceanic turbulence and pointing errors\nstatistically modeled using the sum of two Meijer-G functions. There is a\nresearch gap in the exact statistical analysis of multi-aperture UOWC systems\nthat use selection combining diversity techniques to enhance performance\ncompared to single-aperture systems. In this paper, we develop a general\nframework for the continued product and positive integer exponent for the sum\nof Meijer-G functions to analyze the exact statistical performance of the UOWC\nsystem in terms of multivariate Fox-H function for both independent and\nnon-identically distributed (i.ni.d.) and independent and identically\ndistributed (i.i.d.) channels. We also approximate the performance of a\nmulti-aperture UOWC system with i.i.d. channels using the single-variate Fox-H\nfunction. Using the generalized approach, we present analytical expressions for\naverage bit-error rate (BER) and ergodic capacity for the considered system\noperating over exponential generalized gamma (EGG) oceanic turbulence combined\nwith zero-boresight pointing errors. We also develop asymptotic expressions for\nthe average BER at a high signal-to-noise (SNR) to capture insights into the\nsystem's performance. Our simulation findings confirm the accuracy of our\nderived expressions and illustrate the impact of turbulence parameters for\ni.ni.d. and i.i.d. models for the average BER and ergodic capacity, which may\nprovide a better estimate for the efficient deployment of UOWC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signal transmission over underwater optical wireless communication (UOWC)\nexperiences the combined effect of oceanic turbulence and pointing errors\nstatistically modeled using the sum of two Meijer-G functions. There is a\nresearch gap in the exact statistical analysis of multi-aperture UOWC systems\nthat use selection combining diversity techniques to enhance performance\ncompared to single-aperture systems. In this paper, we develop a general\nframework for the continued product and positive integer exponent for the sum\nof Meijer-G functions to analyze the exact statistical performance of the UOWC\nsystem in terms of multivariate Fox-H function for both independent and\nnon-identically distributed (i.ni.d.) and independent and identically\ndistributed (i.i.d.) channels. We also approximate the performance of a\nmulti-aperture UOWC system with i.i.d. channels using the single-variate Fox-H\nfunction. Using the generalized approach, we present analytical expressions for\naverage bit-error rate (BER) and ergodic capacity for the considered system\noperating over exponential generalized gamma (EGG) oceanic turbulence combined\nwith zero-boresight pointing errors. We also develop asymptotic expressions for\nthe average BER at a high signal-to-noise (SNR) to capture insights into the\nsystem's performance. Our simulation findings confirm the accuracy of our\nderived expressions and illustrate the impact of turbulence parameters for\ni.ni.d. and i.i.d. models for the average BER and ergodic capacity, which may\nprovide a better estimate for the efficient deployment of UOWC."
                },
                "authors": [
                    {
                        "name": "Arvind Kumar"
                    },
                    {
                        "name": "Nikumani Choudhury"
                    },
                    {
                        "name": "Jayendra N. Bandyopadhyay"
                    },
                    {
                        "name": "S. M. Zafaruddin"
                    }
                ],
                "author_detail": {
                    "name": "S. M. Zafaruddin"
                },
                "author": "S. M. Zafaruddin",
                "arxiv_comment": "This work has been submitted to IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01804v2",
                "updated": "2024-09-17T16:31:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    31,
                    26,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-03T11:37:30Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    11,
                    37,
                    30,
                    1,
                    247,
                    0
                ],
                "title": "Strengthening Solidity Invariant Generation: From Post- to\n  Pre-Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strengthening Solidity Invariant Generation: From Post- to\n  Pre-Deployment"
                },
                "summary": "Invariants are essential for ensuring the security and correctness of\nSolidity smart contracts, particularly in the context of blockchain's\nimmutability and decentralized execution. This paper introduces InvSol, a novel\nframework for pre-deployment invariant generation tailored specifically for\nSolidity smart contracts. Unlike existing solutions, namely InvCon, InvCon+,\nand Trace2Inv, that rely on post-deployment transaction histories on Ethereum\nmainnet, InvSol identifies invariants before deployment and offers\ncomprehensive coverage of Solidity language constructs, including loops.\nAdditionally, InvSol incorporates custom templates to effectively prevent\ncritical issues such as reentrancy, out-of-gas errors, and exceptions during\ninvariant generation. We rigorously evaluate InvSol using a benchmark set of\nsmart contracts and compare its performance with state-of-the-art solutions.\nOur findings reveal that InvSol significantly outperforms these tools,\ndemonstrating its effectiveness in handling new contracts with limited\ntransaction histories. Notably, InvSol achieves a 15% improvement in\nidentifying common vulnerabilities compared to InvCon+ and is able to address\ncertain crucial vulnerabilities using specific invariant templates, better than\nTrace2Inv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariants are essential for ensuring the security and correctness of\nSolidity smart contracts, particularly in the context of blockchain's\nimmutability and decentralized execution. This paper introduces InvSol, a novel\nframework for pre-deployment invariant generation tailored specifically for\nSolidity smart contracts. Unlike existing solutions, namely InvCon, InvCon+,\nand Trace2Inv, that rely on post-deployment transaction histories on Ethereum\nmainnet, InvSol identifies invariants before deployment and offers\ncomprehensive coverage of Solidity language constructs, including loops.\nAdditionally, InvSol incorporates custom templates to effectively prevent\ncritical issues such as reentrancy, out-of-gas errors, and exceptions during\ninvariant generation. We rigorously evaluate InvSol using a benchmark set of\nsmart contracts and compare its performance with state-of-the-art solutions.\nOur findings reveal that InvSol significantly outperforms these tools,\ndemonstrating its effectiveness in handling new contracts with limited\ntransaction histories. Notably, InvSol achieves a 15% improvement in\nidentifying common vulnerabilities compared to InvCon+ and is able to address\ncertain crucial vulnerabilities using specific invariant templates, better than\nTrace2Inv."
                },
                "authors": [
                    {
                        "name": "Kartik Kaushik"
                    },
                    {
                        "name": "Raju Halder"
                    },
                    {
                        "name": "Samrat Mondal"
                    }
                ],
                "author_detail": {
                    "name": "Samrat Mondal"
                },
                "author": "Samrat Mondal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13322v2",
                "updated": "2024-09-17T16:29:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    29,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2023-12-20T15:11:06Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    15,
                    11,
                    6,
                    2,
                    354,
                    0
                ],
                "title": "MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks"
                },
                "summary": "With easier access to powerful compute resources, there is a growing trend in\nAI for software development to develop large language models (LLMs) to address\na variety of programming tasks. Even LLMs applied to tasks from the\nhigh-performance computing (HPC) domain are huge in size and demand expensive\ncompute resources for training. This is partly because LLMs for HPC tasks are\nobtained by finetuning existing LLMs that support several natural and/or\nprogramming languages. We found this design choice confusing - why do we need\nLLMs trained on natural languages and programming languages unrelated to HPC\nfor HPC-specific tasks? In this line of work, we aim to question choices made\nby existing LLMs by developing smaller language models (LMs) for specific\ndomains - we call them domain-specific LMs. Specifically, we start with HPC as\na domain and build an HPC-specific LM, named MonoCoder, which is orders of\nmagnitude smaller than existing LMs but delivers better performance on non-HPC\nand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific\ndataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated\nthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.\nResults demonstrate that MonoCoder, although much smaller than existing LMs,\noutperforms other LLMs on normalized-perplexity tests (in relation to model\nsize) while also delivering competing CodeBLEU scores for high-performance and\nparallel code generations. In other words, results suggest that MonoCoder\nunderstands HPC code better than state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With easier access to powerful compute resources, there is a growing trend in\nAI for software development to develop large language models (LLMs) to address\na variety of programming tasks. Even LLMs applied to tasks from the\nhigh-performance computing (HPC) domain are huge in size and demand expensive\ncompute resources for training. This is partly because LLMs for HPC tasks are\nobtained by finetuning existing LLMs that support several natural and/or\nprogramming languages. We found this design choice confusing - why do we need\nLLMs trained on natural languages and programming languages unrelated to HPC\nfor HPC-specific tasks? In this line of work, we aim to question choices made\nby existing LLMs by developing smaller language models (LMs) for specific\ndomains - we call them domain-specific LMs. Specifically, we start with HPC as\na domain and build an HPC-specific LM, named MonoCoder, which is orders of\nmagnitude smaller than existing LMs but delivers better performance on non-HPC\nand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific\ndataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated\nthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.\nResults demonstrate that MonoCoder, although much smaller than existing LMs,\noutperforms other LLMs on normalized-perplexity tests (in relation to model\nsize) while also delivering competing CodeBLEU scores for high-performance and\nparallel code generations. In other words, results suggest that MonoCoder\nunderstands HPC code better than state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Tal Kadosh"
                    },
                    {
                        "name": "Niranjan Hasabnis"
                    },
                    {
                        "name": "Vy A. Vo"
                    },
                    {
                        "name": "Nadav Schneider"
                    },
                    {
                        "name": "Neva Krien"
                    },
                    {
                        "name": "Mihai Capota"
                    },
                    {
                        "name": "Abdul Wasay"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Ted Willke"
                    },
                    {
                        "name": "Guy Tamir"
                    },
                    {
                        "name": "Yuval Pinter"
                    },
                    {
                        "name": "Timothy Mattson"
                    },
                    {
                        "name": "Gal Oren"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oren"
                },
                "author": "Gal Oren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01411v2",
                "updated": "2024-09-17T15:57:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    57,
                    6,
                    1,
                    261,
                    0
                ],
                "published": "2024-02-02T13:42:50Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    13,
                    42,
                    50,
                    4,
                    33,
                    0
                ],
                "title": "CodePori: Large-Scale System for Autonomous Software Development Using\n  Multi-Agent Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePori: Large-Scale System for Autonomous Software Development Using\n  Multi-Agent Technology"
                },
                "summary": "Context: Large Language Models (LLMs) and Generative Pre-trained Transformers\n(GPTs) have transformed the field of Software Engineering (SE). Existing\nLLM-based multi-agent models have successfully addressed basic dialogue tasks.\nHowever, the potential of LLMs for more challenging tasks, such as automated\ncode generation for large and complex projects, has been investigated in only a\nfew existing works. Objective: This paper aims to investigate the potential of\nLLM-based agents in the software industry, particularly in enhancing\nproductivity and reducing time-to-market for complex software solutions. Our\nprimary objective is to gain insights into how these agents can fundamentally\ntransform the development of large-scale software. Methods: We introduce\nCodePori, a novel system designed to automate code generation for large and\ncomplex software projects based on functional and non-functional requirements\ndefined by stakeholders. To assess the proposed system performance, we utilized\nthe HumanEval benchmark and manually tested the CodePori model, providing 20\ndifferent project descriptions as input and then evaluated the code accuracy by\nmanually executing the code. Results: CodePori is able to generate running code\nfor large-scale projects, aligned with the typical software development\nprocess. The HumanEval benchmark results indicate that CodePori improves code\naccuracy by 89%. A manual assessment conducted by the first author shows that\nthe CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the\nresults, our conclusion is that proposed system demonstrates the transformative\npotential of LLM-based agents in SE, highlighting their practical applications\nand opening new opportunities for broader adoption in both industry and\nacademia. Our project is publicly available at\nhttps://github.com/GPT-Laboratory/CodePori.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Models (LLMs) and Generative Pre-trained Transformers\n(GPTs) have transformed the field of Software Engineering (SE). Existing\nLLM-based multi-agent models have successfully addressed basic dialogue tasks.\nHowever, the potential of LLMs for more challenging tasks, such as automated\ncode generation for large and complex projects, has been investigated in only a\nfew existing works. Objective: This paper aims to investigate the potential of\nLLM-based agents in the software industry, particularly in enhancing\nproductivity and reducing time-to-market for complex software solutions. Our\nprimary objective is to gain insights into how these agents can fundamentally\ntransform the development of large-scale software. Methods: We introduce\nCodePori, a novel system designed to automate code generation for large and\ncomplex software projects based on functional and non-functional requirements\ndefined by stakeholders. To assess the proposed system performance, we utilized\nthe HumanEval benchmark and manually tested the CodePori model, providing 20\ndifferent project descriptions as input and then evaluated the code accuracy by\nmanually executing the code. Results: CodePori is able to generate running code\nfor large-scale projects, aligned with the typical software development\nprocess. The HumanEval benchmark results indicate that CodePori improves code\naccuracy by 89%. A manual assessment conducted by the first author shows that\nthe CodePori system achieved an accuracy rate of 85%. Conclusion: Based on the\nresults, our conclusion is that proposed system demonstrates the transformative\npotential of LLM-based agents in SE, highlighting their practical applications\nand opening new opportunities for broader adoption in both industry and\nacademia. Our project is publicly available at\nhttps://github.com/GPT-Laboratory/CodePori."
                },
                "authors": [
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Malik Abdul Sami"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Kari Systä"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "18 pages, 2 figures, and 5 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11283v2",
                "updated": "2024-09-18T05:42:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    5,
                    42,
                    1,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T15:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    38,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling"
                },
                "summary": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines."
                },
                "authors": [
                    {
                        "name": "Xinyue Fang"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Ziyi Pan"
                    },
                    {
                        "name": "Quntian Fang"
                    },
                    {
                        "name": "Zhihua Wen"
                    },
                    {
                        "name": "Hengyue Pan"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11282v1",
                "updated": "2024-09-17T15:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    37,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    37,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Leveraging Distillation Techniques for Document Understanding: A Case\n  Study with FLAN-T5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Distillation Techniques for Document Understanding: A Case\n  Study with FLAN-T5"
                },
                "summary": "The surge of digital documents in various formats, including less\nstandardized documents such as business reports and environmental assessments,\nunderscores the growing importance of Document Understanding. While Large\nLanguage Models (LLMs) have showcased prowess across diverse natural language\nprocessing tasks, their direct application to Document Understanding remains a\nchallenge. Previous research has demonstrated the utility of LLMs in this\ndomain, yet their significant computational demands make them challenging to\ndeploy effectively. Additionally, proprietary Blackbox LLMs often outperform\ntheir open-source counterparts, posing a barrier to widespread accessibility.\nIn this paper, we delve into the realm of document understanding, leveraging\ndistillation methods to harness the power of large LLMs while accommodating\ncomputational limitations. Specifically, we present a novel approach wherein we\ndistill document understanding knowledge from the proprietary LLM ChatGPT into\nFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms\nto facilitate efficient knowledge transfer. This work contributes to the\nadvancement of document understanding methodologies by offering a scalable\nsolution that bridges the gap between resource-intensive LLMs and practical\napplications. Our findings underscore the potential of distillation techniques\nin facilitating the deployment of sophisticated language models in real-world\nscenarios, thereby fostering advancements in natural language processing and\ndocument comprehension domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of digital documents in various formats, including less\nstandardized documents such as business reports and environmental assessments,\nunderscores the growing importance of Document Understanding. While Large\nLanguage Models (LLMs) have showcased prowess across diverse natural language\nprocessing tasks, their direct application to Document Understanding remains a\nchallenge. Previous research has demonstrated the utility of LLMs in this\ndomain, yet their significant computational demands make them challenging to\ndeploy effectively. Additionally, proprietary Blackbox LLMs often outperform\ntheir open-source counterparts, posing a barrier to widespread accessibility.\nIn this paper, we delve into the realm of document understanding, leveraging\ndistillation methods to harness the power of large LLMs while accommodating\ncomputational limitations. Specifically, we present a novel approach wherein we\ndistill document understanding knowledge from the proprietary LLM ChatGPT into\nFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms\nto facilitate efficient knowledge transfer. This work contributes to the\nadvancement of document understanding methodologies by offering a scalable\nsolution that bridges the gap between resource-intensive LLMs and practical\napplications. Our findings underscore the potential of distillation techniques\nin facilitating the deployment of sophisticated language models in real-world\nscenarios, thereby fostering advancements in natural language processing and\ndocument comprehension domains."
                },
                "authors": [
                    {
                        "name": "Marcel Lamott"
                    },
                    {
                        "name": "Muhammad Armaghan Shakir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Armaghan Shakir"
                },
                "author": "Muhammad Armaghan Shakir",
                "arxiv_comment": "Presented at AI@WORK-Workshop / Informatik-Festival (GI-Jahrestagung)\n  (Wiesbaden, Germany, 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11279v1",
                "updated": "2024-09-17T15:29:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    29,
                    34,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:29:34Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    29,
                    34,
                    1,
                    261,
                    0
                ],
                "title": "P-RAG: Progressive Retrieval Augmented Generation For Planning on\n  Embodied Everyday Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-RAG: Progressive Retrieval Augmented Generation For Planning on\n  Embodied Everyday Task"
                },
                "summary": "Embodied Everyday Task is a popular task in the embodied AI community,\nrequiring agents to make a sequence of actions based on natural language\ninstructions and visual observations. Traditional learning-based approaches\nface two challenges. Firstly, natural language instructions often lack explicit\ntask planning. Secondly, extensive training is required to equip models with\nknowledge of the task environment. Previous works based on Large Language Model\n(LLM) either suffer from poor performance due to the lack of task-specific\nknowledge or rely on ground truth as few-shot samples. To address the above\nlimitations, we propose a novel approach called Progressive Retrieval Augmented\nGeneration (P-RAG), which not only effectively leverages the powerful language\nprocessing capabilities of LLMs but also progressively accumulates\ntask-specific knowledge without ground-truth. Compared to the conventional RAG\nmethods, which retrieve relevant information from the database in a one-shot\nmanner to assist generation, P-RAG introduces an iterative approach to\nprogressively update the database. In each iteration, P-RAG retrieves the\nlatest database and obtains historical information from the previous\ninteraction as experiential references for the current interaction. Moreover,\nwe also introduce a more granular retrieval scheme that not only retrieves\nsimilar tasks but also incorporates retrieval of similar situations to provide\nmore valuable reference experiences. Extensive experiments reveal that P-RAG\nachieves competitive results without utilizing ground truth and can even\nfurther improve performance through self-iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Everyday Task is a popular task in the embodied AI community,\nrequiring agents to make a sequence of actions based on natural language\ninstructions and visual observations. Traditional learning-based approaches\nface two challenges. Firstly, natural language instructions often lack explicit\ntask planning. Secondly, extensive training is required to equip models with\nknowledge of the task environment. Previous works based on Large Language Model\n(LLM) either suffer from poor performance due to the lack of task-specific\nknowledge or rely on ground truth as few-shot samples. To address the above\nlimitations, we propose a novel approach called Progressive Retrieval Augmented\nGeneration (P-RAG), which not only effectively leverages the powerful language\nprocessing capabilities of LLMs but also progressively accumulates\ntask-specific knowledge without ground-truth. Compared to the conventional RAG\nmethods, which retrieve relevant information from the database in a one-shot\nmanner to assist generation, P-RAG introduces an iterative approach to\nprogressively update the database. In each iteration, P-RAG retrieves the\nlatest database and obtains historical information from the previous\ninteraction as experiential references for the current interaction. Moreover,\nwe also introduce a more granular retrieval scheme that not only retrieves\nsimilar tasks but also incorporates retrieval of similar situations to provide\nmore valuable reference experiences. Extensive experiments reveal that P-RAG\nachieves competitive results without utilizing ground truth and can even\nfurther improve performance through self-iterations."
                },
                "authors": [
                    {
                        "name": "Weiye Xu"
                    },
                    {
                        "name": "Min Wang"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Houqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Houqiang Li"
                },
                "author": "Houqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11276v1",
                "updated": "2024-09-17T15:28:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    28,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:28:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    28,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable potential across various\ndomains, including cybersecurity. Using commercial cloud-based LLMs may be\nundesirable due to privacy concerns, costs, and network connectivity\nconstraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be\nused as a red-team agent within network security environments. Our fine-tuned 7\nbillion parameter model can run on a single GPU card and achieves performance\ncomparable with much larger and more powerful commercial models such as GPT-4.\nHackphyr clearly outperforms other models, including GPT-3.5-turbo, and\nbaselines, such as Q-learning agents in complex, previously unseen scenarios.\nTo achieve this performance, we generated a new task-specific cybersecurity\ndataset to enhance the base model's capabilities. Finally, we conducted a\ncomprehensive analysis of the agents' behaviors that provides insights into the\nplanning abilities and potential shortcomings of such agents, contributing to\nthe broader understanding of LLM-based agents in cybersecurity contexts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable potential across various\ndomains, including cybersecurity. Using commercial cloud-based LLMs may be\nundesirable due to privacy concerns, costs, and network connectivity\nconstraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be\nused as a red-team agent within network security environments. Our fine-tuned 7\nbillion parameter model can run on a single GPU card and achieves performance\ncomparable with much larger and more powerful commercial models such as GPT-4.\nHackphyr clearly outperforms other models, including GPT-3.5-turbo, and\nbaselines, such as Q-learning agents in complex, previously unseen scenarios.\nTo achieve this performance, we generated a new task-specific cybersecurity\ndataset to enhance the base model's capabilities. Finally, we conducted a\ncomprehensive analysis of the agents' behaviors that provides insights into the\nplanning abilities and potential shortcomings of such agents, contributing to\nthe broader understanding of LLM-based agents in cybersecurity contexts"
                },
                "authors": [
                    {
                        "name": "Maria Rigaki"
                    },
                    {
                        "name": "Carlos Catania"
                    },
                    {
                        "name": "Sebastian Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Garcia"
                },
                "author": "Sebastian Garcia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11274v1",
                "updated": "2024-09-17T15:25:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    25,
                    11,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:25:11Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    25,
                    11,
                    1,
                    261,
                    0
                ],
                "title": "Task Arithmetic for Language Expansion in Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Arithmetic for Language Expansion in Speech Translation"
                },
                "summary": "Recent advances in large language models (LLMs) have gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-based speech translation (ST). However, expanding language pairs\nfrom an existing instruction-tuned ST system is costly due to the necessity of\nre-training on a combination of new and previous datasets. We propose to expand\nnew language pairs by merging the model trained on new language pairs and the\nexisting model, using task arithmetic. We find that the direct application of\ntask arithmetic for ST causes the merged model to fail to follow instructions;\nthus, generating translation in incorrect languages. To eliminate language\nconfusion, we propose an augmented task arithmetic method that merges an\nadditional language control model. It is trained to generate the correct target\nlanguage token following the instructions. Our experiments demonstrate that our\nproposed language control model can achieve language expansion by eliminating\nlanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66\nand 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the\nuse of our task arithmetic framework can expand to a language pair where\nneither paired ST training data nor a pre-trained ST model is available. We\nfirst synthesize the ST system from machine translation (MT) systems via task\nanalogy, then merge the synthesized ST system to the existing ST model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have gained interest in\nspeech-text multimodal foundation models, achieving strong performance on\ninstruction-based speech translation (ST). However, expanding language pairs\nfrom an existing instruction-tuned ST system is costly due to the necessity of\nre-training on a combination of new and previous datasets. We propose to expand\nnew language pairs by merging the model trained on new language pairs and the\nexisting model, using task arithmetic. We find that the direct application of\ntask arithmetic for ST causes the merged model to fail to follow instructions;\nthus, generating translation in incorrect languages. To eliminate language\nconfusion, we propose an augmented task arithmetic method that merges an\nadditional language control model. It is trained to generate the correct target\nlanguage token following the instructions. Our experiments demonstrate that our\nproposed language control model can achieve language expansion by eliminating\nlanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66\nand 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the\nuse of our task arithmetic framework can expand to a language pair where\nneither paired ST training data nor a pre-trained ST model is available. We\nfirst synthesize the ST system from machine translation (MT) systems via task\nanalogy, then merge the synthesized ST system to the existing ST model."
                },
                "authors": [
                    {
                        "name": "Yao-Fei Cheng"
                    },
                    {
                        "name": "Hayato Futami"
                    },
                    {
                        "name": "Yosuke Kashiwagi"
                    },
                    {
                        "name": "Emiru Tsunoo"
                    },
                    {
                        "name": "Wen Shen Teo"
                    },
                    {
                        "name": "Siddhant Arora"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11262v1",
                "updated": "2024-09-17T15:10:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    10,
                    36,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:10:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    10,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound\n  Event Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound\n  Event Detection"
                },
                "summary": "This paper presents a residential audio dataset to support sound event\ndetection research for smart home applications aimed at promoting wellbeing for\nolder adults. The dataset is constructed by deploying audio recording systems\nin the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic\ncharacteristics are documented through detailed floor plans and construction\nmaterial information to enable replication of the recording environments for AI\nmodel deployment. A novel automated speech removal pipeline is developed, using\npre-trained audio neural networks to detect and remove segments containing\nspoken voice, while preserving segments containing other sound events. The\nresulting dataset consists of privacy-compliant audio recordings that\naccurately capture the soundscapes and activities of daily living within\nresidential spaces. The paper details the dataset creation methodology, the\nspeech removal pipeline utilizing cascaded model architectures, and an analysis\nof the vocal label distribution to validate the speech removal process. This\ndataset enables the development and benchmarking of sound event detection\nmodels tailored specifically for in-home applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a residential audio dataset to support sound event\ndetection research for smart home applications aimed at promoting wellbeing for\nolder adults. The dataset is constructed by deploying audio recording systems\nin the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic\ncharacteristics are documented through detailed floor plans and construction\nmaterial information to enable replication of the recording environments for AI\nmodel deployment. A novel automated speech removal pipeline is developed, using\npre-trained audio neural networks to detect and remove segments containing\nspoken voice, while preserving segments containing other sound events. The\nresulting dataset consists of privacy-compliant audio recordings that\naccurately capture the soundscapes and activities of daily living within\nresidential spaces. The paper details the dataset creation methodology, the\nspeech removal pipeline utilizing cascaded model architectures, and an analysis\nof the vocal label distribution to validate the speech removal process. This\ndataset enables the development and benchmarking of sound event detection\nmodels tailored specifically for in-home applications."
                },
                "authors": [
                    {
                        "name": "Gabriel Bibbó"
                    },
                    {
                        "name": "Thomas Deacon"
                    },
                    {
                        "name": "Arshdeep Singh"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    }
                ],
                "author_detail": {
                    "name": "Mark D. Plumbley"
                },
                "author": "Mark D. Plumbley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11256v1",
                "updated": "2024-09-17T15:05:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    5,
                    33,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:05:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    5,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained\n  Image Denoisers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained\n  Image Denoisers"
                },
                "summary": "Recent advancements in deep learning have shown impressive results in image\nand video denoising, leveraging extensive pairs of noisy and noise-free data\nfor supervision. However, the challenge of acquiring paired videos for dynamic\nscenes hampers the practical deployment of deep video denoising techniques. In\ncontrast, this obstacle is less pronounced in image denoising, where paired\ndata is more readily available. Thus, a well-trained image denoiser could serve\nas a reliable spatial prior for video denoising. In this paper, we propose a\nnovel unsupervised video denoising framework, named ``Temporal As a Plugin''\n(TAP), which integrates tunable temporal modules into a pre-trained image\ndenoiser. By incorporating temporal modules, our method can harness temporal\ninformation across noisy frames, complementing its power of spatial denoising.\nFurthermore, we introduce a progressive fine-tuning strategy that refines each\ntemporal module using the generated pseudo clean video frames, progressively\nenhancing the network's denoising performance. Compared to other unsupervised\nvideo denoising methods, our framework demonstrates superior performance on\nboth sRGB and raw video denoising datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in deep learning have shown impressive results in image\nand video denoising, leveraging extensive pairs of noisy and noise-free data\nfor supervision. However, the challenge of acquiring paired videos for dynamic\nscenes hampers the practical deployment of deep video denoising techniques. In\ncontrast, this obstacle is less pronounced in image denoising, where paired\ndata is more readily available. Thus, a well-trained image denoiser could serve\nas a reliable spatial prior for video denoising. In this paper, we propose a\nnovel unsupervised video denoising framework, named ``Temporal As a Plugin''\n(TAP), which integrates tunable temporal modules into a pre-trained image\ndenoiser. By incorporating temporal modules, our method can harness temporal\ninformation across noisy frames, complementing its power of spatial denoising.\nFurthermore, we introduce a progressive fine-tuning strategy that refines each\ntemporal module using the generated pseudo clean video frames, progressively\nenhancing the network's denoising performance. Compared to other unsupervised\nvideo denoising methods, our framework demonstrates superior performance on\nboth sRGB and raw video denoising datasets."
                },
                "authors": [
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11254v1",
                "updated": "2024-09-17T15:02:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    2,
                    32,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:02:32Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    2,
                    32,
                    1,
                    261,
                    0
                ],
                "title": "Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach"
                },
                "summary": "As the complexity and connectivity of networks increase, the need for novel\nmalware detection approaches becomes imperative. Traditional security defenses\nare becoming less effective against the advanced tactics of today's\ncyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in\nstrengthening network security, offering detailed analysis of network traffic\nthat goes beyond simple metadata analysis. DPI examines not only the packet\nheaders but also the payload content within, offering a thorough insight into\nthe data traversing the network. This study proposes a novel approach that\nleverages a large language model (LLM) and few-shot learning to accurately\nrecognizes novel, unseen malware types with few labels samples. Our proposed\napproach uses a pretrained LLM on known malware types to extract the embeddings\nfrom packets. The embeddings are then used alongside few labeled samples of an\nunseen malware type. This technique is designed to acclimate the model to\ndifferent malware representations, further enabling it to generate robust\nembeddings for each trained and unseen classes. Following the extraction of\nembeddings from the LLM, few-shot learning is utilized to enhance performance\nwith minimal labeled data. Our evaluation, which utilized two renowned\ndatasets, focused on identifying malware types within network traffic and\nInternet of Things (IoT) environments. Our approach shows promising results\nwith an average accuracy of 86.35% and F1-Score of 86.40% on different malware\ntypes across the two datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity and connectivity of networks increase, the need for novel\nmalware detection approaches becomes imperative. Traditional security defenses\nare becoming less effective against the advanced tactics of today's\ncyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in\nstrengthening network security, offering detailed analysis of network traffic\nthat goes beyond simple metadata analysis. DPI examines not only the packet\nheaders but also the payload content within, offering a thorough insight into\nthe data traversing the network. This study proposes a novel approach that\nleverages a large language model (LLM) and few-shot learning to accurately\nrecognizes novel, unseen malware types with few labels samples. Our proposed\napproach uses a pretrained LLM on known malware types to extract the embeddings\nfrom packets. The embeddings are then used alongside few labeled samples of an\nunseen malware type. This technique is designed to acclimate the model to\ndifferent malware representations, further enabling it to generate robust\nembeddings for each trained and unseen classes. Following the extraction of\nembeddings from the LLM, few-shot learning is utilized to enhance performance\nwith minimal labeled data. Our evaluation, which utilized two renowned\ndatasets, focused on identifying malware types within network traffic and\nInternet of Things (IoT) environments. Our approach shows promising results\nwith an average accuracy of 86.35% and F1-Score of 86.40% on different malware\ntypes across the two datasets."
                },
                "authors": [
                    {
                        "name": "Kyle Stein"
                    },
                    {
                        "name": "Andrew A. Mahyari"
                    },
                    {
                        "name": "Guillermo Francia III"
                    },
                    {
                        "name": "Eman El-Sheikh"
                    }
                ],
                "author_detail": {
                    "name": "Eman El-Sheikh"
                },
                "author": "Eman El-Sheikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05272v3",
                "updated": "2024-09-17T14:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    49,
                    21,
                    1,
                    261,
                    0
                ],
                "published": "2023-12-07T23:31:42Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    23,
                    31,
                    42,
                    3,
                    341,
                    0
                ],
                "title": "GenQ: Quantization in Low Data Regimes with Generative Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenQ: Quantization in Low Data Regimes with Generative Synthetic Data"
                },
                "summary": "In the realm of deep neural network deployment, low-bit quantization presents\na promising avenue for enhancing computational efficiency. However, it often\nhinges on the availability of training data to mitigate quantization errors, a\nsignificant challenge when data availability is scarce or restricted due to\nprivacy or copyright concerns. Addressing this, we introduce GenQ, a novel\napproach employing an advanced Generative AI model to generate photorealistic,\nhigh-resolution synthetic data, overcoming the limitations of traditional\nmethods that struggle to accurately mimic complex objects in extensive datasets\nlike ImageNet. Our methodology is underscored by two robust filtering\nmechanisms designed to ensure the synthetic data closely aligns with the\nintrinsic characteristics of the actual training data. In case of limited data\navailability, the actual data is used to guide the synthetic data generation\nprocess, enhancing fidelity through the inversion of learnable token\nembeddings. Through rigorous experimentation, GenQ establishes new benchmarks\nin data-free and data-scarce quantization, significantly outperforming existing\nmethods in accuracy and efficiency, thereby setting a new standard for\nquantization in low data regimes. Code is released at\n\\url{https://github.com/Intelligent-Computing-Lab-Yale/GenQ}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of deep neural network deployment, low-bit quantization presents\na promising avenue for enhancing computational efficiency. However, it often\nhinges on the availability of training data to mitigate quantization errors, a\nsignificant challenge when data availability is scarce or restricted due to\nprivacy or copyright concerns. Addressing this, we introduce GenQ, a novel\napproach employing an advanced Generative AI model to generate photorealistic,\nhigh-resolution synthetic data, overcoming the limitations of traditional\nmethods that struggle to accurately mimic complex objects in extensive datasets\nlike ImageNet. Our methodology is underscored by two robust filtering\nmechanisms designed to ensure the synthetic data closely aligns with the\nintrinsic characteristics of the actual training data. In case of limited data\navailability, the actual data is used to guide the synthetic data generation\nprocess, enhancing fidelity through the inversion of learnable token\nembeddings. Through rigorous experimentation, GenQ establishes new benchmarks\nin data-free and data-scarce quantization, significantly outperforming existing\nmethods in accuracy and efficiency, thereby setting a new standard for\nquantization in low data regimes. Code is released at\n\\url{https://github.com/Intelligent-Computing-Lab-Yale/GenQ}."
                },
                "authors": [
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Donghyun Lee"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Priyadarshini Panda"
                    }
                ],
                "author_detail": {
                    "name": "Priyadarshini Panda"
                },
                "author": "Priyadarshini Panda",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11242v1",
                "updated": "2024-09-17T14:47:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:47:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse"
                },
                "summary": "LLMs are an integral part of retrieval-augmented generation (RAG) systems.\nWhile many studies focus on evaluating the quality of end-to-end RAG systems,\nthere is a lack of research on understanding the appropriateness of an LLM for\nthe RAG task. Thus, we introduce a new metric, Trust-Score, that provides a\nholistic evaluation of the trustworthiness of LLMs in an RAG framework. We show\nthat various prompting methods, such as in-context learning, fail to adapt LLMs\neffectively to the RAG task. Thus, we propose Trust-Align, a framework to align\nLLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly\noutperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up\n29.2) and ELI5 (up 14.9). We release our code at:\nhttps://github.com/declare-lab/trust-align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are an integral part of retrieval-augmented generation (RAG) systems.\nWhile many studies focus on evaluating the quality of end-to-end RAG systems,\nthere is a lack of research on understanding the appropriateness of an LLM for\nthe RAG task. Thus, we introduce a new metric, Trust-Score, that provides a\nholistic evaluation of the trustworthiness of LLMs in an RAG framework. We show\nthat various prompting methods, such as in-context learning, fail to adapt LLMs\neffectively to the RAG task. Thus, we propose Trust-Align, a framework to align\nLLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly\noutperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up\n29.2) and ELI5 (up 14.9). We release our code at:\nhttps://github.com/declare-lab/trust-align."
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Shang Hong Sim"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Hai Leong Chieu"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09662v2",
                "updated": "2024-09-17T14:44:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    44,
                    34,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-15T08:25:24Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    8,
                    25,
                    24,
                    6,
                    259,
                    0
                ],
                "title": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models"
                },
                "summary": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. Current systems often\nlimit users' flexibility to direct their reflections. We thus present\nExploreSelf, an LLM-driven application designed to empower users to control\ntheir reflective journey. ExploreSelf allows users to receive adaptive support\nthrough dynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe balance between guided support and freedom to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss implications for designing LLM-driven tools that promote user\nempowerment through effective reflective practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. Current systems often\nlimit users' flexibility to direct their reflections. We thus present\nExploreSelf, an LLM-driven application designed to empower users to control\ntheir reflective journey. ExploreSelf allows users to receive adaptive support\nthrough dynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe balance between guided support and freedom to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss implications for designing LLM-driven tools that promote user\nempowerment through effective reflective practices."
                },
                "authors": [
                    {
                        "name": "Inhwa Song"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Jessica Lee Schleider"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "17 pages excluding reference and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11239v1",
                "updated": "2024-09-17T14:40:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:40:02Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "title": "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge & Reward Model: What They Can and Cannot Do"
                },
                "summary": "LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\noutside of English remains largely unexplored. In this paper, we conduct a\ncomprehensive analysis on automated evaluators, reporting key findings on their\nbehavior in a non-English environment. First, we discover that English\nevaluation capabilities significantly influence language-specific capabilities,\noften more than the language proficiency itself, enabling evaluators trained in\nEnglish to easily transfer their skills to other languages. Second, we identify\ncritical shortcomings, where LLMs fail to detect and penalize errors, such as\nfactual inaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we release Kudge, the first non-English meta-evaluation\ndataset containing 5,012 human annotations in Korean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge and reward models are widely used alternatives of\nmultiple-choice questions or human annotators for large language model (LLM)\nevaluation. Their efficacy shines in evaluating long-form responses, serving a\ncritical role as evaluators of leaderboards and as proxies to align LLMs via\nreinforcement learning. However, despite their popularity, their effectiveness\noutside of English remains largely unexplored. In this paper, we conduct a\ncomprehensive analysis on automated evaluators, reporting key findings on their\nbehavior in a non-English environment. First, we discover that English\nevaluation capabilities significantly influence language-specific capabilities,\noften more than the language proficiency itself, enabling evaluators trained in\nEnglish to easily transfer their skills to other languages. Second, we identify\ncritical shortcomings, where LLMs fail to detect and penalize errors, such as\nfactual inaccuracies, cultural misrepresentations, and the presence of unwanted\nlanguage. Finally, we release Kudge, the first non-English meta-evaluation\ndataset containing 5,012 human annotations in Korean."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Seunghyeok Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghyeok Hong"
                },
                "author": "Seunghyeok Hong",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11233v1",
                "updated": "2024-09-17T14:34:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    34,
                    11,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:34:11Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    34,
                    11,
                    1,
                    261,
                    0
                ],
                "title": "Evaluating the Impact of Compression Techniques on Task-Specific\n  Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Impact of Compression Techniques on Task-Specific\n  Performance of Large Language Models"
                },
                "summary": "Large language models (LLMs) offer powerful capabilities but incur\nsubstantial computational costs, driving the need for efficient compression\ntechniques. This study evaluates the impact of popular compression methods -\nMagnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on\nthe trade-offs between model size reduction, downstream task performance, and\nthe role of calibration data. Our findings reveal that while SparseGPT and\nWanda preserve perplexity even at 50% sparsity, they suffer significant\ndegradation on downstream tasks, highlighting the inadequacy of perplexity as\nthe sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)\nDivergence as a more comprehensive metric that captures nuanced changes in\nmodel behavior post-compression. We further demonstrate that task-specific\ncalibration data significantly enhances the downstream performance of\ncompressed models compared to general calibration data. This research\nunderscores the necessity for diverse evaluation metrics and careful\ncalibration data selection to fully understand the complexities of LLM\ncompression and its implications for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer powerful capabilities but incur\nsubstantial computational costs, driving the need for efficient compression\ntechniques. This study evaluates the impact of popular compression methods -\nMagnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on\nthe trade-offs between model size reduction, downstream task performance, and\nthe role of calibration data. Our findings reveal that while SparseGPT and\nWanda preserve perplexity even at 50% sparsity, they suffer significant\ndegradation on downstream tasks, highlighting the inadequacy of perplexity as\nthe sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)\nDivergence as a more comprehensive metric that captures nuanced changes in\nmodel behavior post-compression. We further demonstrate that task-specific\ncalibration data significantly enhances the downstream performance of\ncompressed models compared to general calibration data. This research\nunderscores the necessity for diverse evaluation metrics and careful\ncalibration data selection to fully understand the complexities of LLM\ncompression and its implications for practical applications."
                },
                "authors": [
                    {
                        "name": "Bishwash Khanal"
                    },
                    {
                        "name": "Jeffery M. Capone"
                    }
                ],
                "author_detail": {
                    "name": "Jeffery M. Capone"
                },
                "author": "Jeffery M. Capone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11232v1",
                "updated": "2024-09-17T14:29:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    29,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:29:03Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    29,
                    3,
                    1,
                    261,
                    0
                ],
                "title": "Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT\n  Problem: Does the LLM Solve the Problem Itself or Call an External SAT\n  Solver?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT\n  Problem: Does the LLM Solve the Problem Itself or Call an External SAT\n  Solver?"
                },
                "summary": "In this manuscript I present an analysis on the performance of OpenAI\nO1-preview model in solving random K-SAT instances for K$\\in {2,3,4}$ as a\nfunction of $\\alpha=M/N$ where $M$ is the number of clauses and $N$ is the\nnumber of variables of the satisfiable problem. I show that the model can call\nan external SAT solver to solve the instances, rather than solving them\ndirectly. Despite using external solvers, the model reports incorrect\nassignments as output. Moreover, I propose and present an analysis to quantify\nwhether the OpenAI O1-preview model demonstrates a spark of intelligence or\nmerely makes random guesses when outputting an assignment for a Boolean\nsatisfiability problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this manuscript I present an analysis on the performance of OpenAI\nO1-preview model in solving random K-SAT instances for K$\\in {2,3,4}$ as a\nfunction of $\\alpha=M/N$ where $M$ is the number of clauses and $N$ is the\nnumber of variables of the satisfiable problem. I show that the model can call\nan external SAT solver to solve the instances, rather than solving them\ndirectly. Despite using external solvers, the model reports incorrect\nassignments as output. Moreover, I propose and present an analysis to quantify\nwhether the OpenAI O1-preview model demonstrates a spark of intelligence or\nmerely makes random guesses when outputting an assignment for a Boolean\nsatisfiability problem."
                },
                "authors": [
                    {
                        "name": "Raffaele Marino"
                    }
                ],
                "author_detail": {
                    "name": "Raffaele Marino"
                },
                "author": "Raffaele Marino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19071v2",
                "updated": "2024-09-17T14:24:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    24,
                    47,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-27T10:41:22Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    10,
                    41,
                    22,
                    3,
                    179,
                    0
                ],
                "title": "EmPO: Emotion Grounding for Empathetic Response Generation through\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmPO: Emotion Grounding for Empathetic Response Generation through\n  Preference Optimization"
                },
                "summary": "Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. We propose a novel approach where we\nconstruct theory-driven preference datasets based on emotion grounding and use\nthem to align LLMs with preference optimization algorithms to address these\nchallenges. To evaluate empathetic response generation, we employ the\nEmpatheticDialogues dataset, assessing empathy with the diff-Epitome and\nBERTscore metrics and with multi-dimensional human evaluation. Additionally, we\nmeasure diversity and emotional valence using feature-based methods. We also\nevaluate the impact of training on the generalization performance using the\nMMLU benchmark and tasks from the Open LLM Leaderboard. The results show that\nLLMs can be aligned for empathetic response generation by preference\noptimization while retaining their general performance and that emotion\ngrounding can guide preference dataset creation. We make all datasets, source\ncode, and models publicly available. https://github.com/justtherightsize/empo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. We propose a novel approach where we\nconstruct theory-driven preference datasets based on emotion grounding and use\nthem to align LLMs with preference optimization algorithms to address these\nchallenges. To evaluate empathetic response generation, we employ the\nEmpatheticDialogues dataset, assessing empathy with the diff-Epitome and\nBERTscore metrics and with multi-dimensional human evaluation. Additionally, we\nmeasure diversity and emotional valence using feature-based methods. We also\nevaluate the impact of training on the generalization performance using the\nMMLU benchmark and tasks from the Open LLM Leaderboard. The results show that\nLLMs can be aligned for empathetic response generation by preference\noptimization while retaining their general performance and that emotion\ngrounding can guide preference dataset creation. We make all datasets, source\ncode, and models publicly available. https://github.com/justtherightsize/empo"
                },
                "authors": [
                    {
                        "name": "Ondrej Sotolar"
                    },
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Alok Debnath"
                    },
                    {
                        "name": "Allison Lahnala"
                    },
                    {
                        "name": "Charles Welch"
                    },
                    {
                        "name": "Lucie FLek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie FLek"
                },
                "author": "Lucie FLek",
                "arxiv_comment": "v02, 8 pages long paper, EMNLP ACL style",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11224v1",
                "updated": "2024-09-17T14:18:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    18,
                    21,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:18:21Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    18,
                    21,
                    1,
                    261,
                    0
                ],
                "title": "A Human-Centered Risk Evaluation of Biometric Systems Using Conjoint\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Human-Centered Risk Evaluation of Biometric Systems Using Conjoint\n  Analysis"
                },
                "summary": "Biometric recognition systems, known for their convenience, are widely\nadopted across various fields. However, their security faces risks depending on\nthe authentication algorithm and deployment environment. Current risk\nassessment methods faces significant challenges in incorporating the crucial\nfactor of attacker's motivation, leading to incomplete evaluations. This paper\npresents a novel human-centered risk evaluation framework using conjoint\nanalysis to quantify the impact of risk factors, such as surveillance cameras,\non attacker's motivation. Our framework calculates risk values incorporating\nthe False Acceptance Rate (FAR) and attack probability, allowing comprehensive\ncomparisons across use cases. A survey of 600 Japanese participants\ndemonstrates our method's effectiveness, showing how security measures\ninfluence attacker's motivation. This approach helps decision-makers customize\nbiometric systems to enhance security while maintaining usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biometric recognition systems, known for their convenience, are widely\nadopted across various fields. However, their security faces risks depending on\nthe authentication algorithm and deployment environment. Current risk\nassessment methods faces significant challenges in incorporating the crucial\nfactor of attacker's motivation, leading to incomplete evaluations. This paper\npresents a novel human-centered risk evaluation framework using conjoint\nanalysis to quantify the impact of risk factors, such as surveillance cameras,\non attacker's motivation. Our framework calculates risk values incorporating\nthe False Acceptance Rate (FAR) and attack probability, allowing comprehensive\ncomparisons across use cases. A survey of 600 Japanese participants\ndemonstrates our method's effectiveness, showing how security measures\ninfluence attacker's motivation. This approach helps decision-makers customize\nbiometric systems to enhance security while maintaining usability."
                },
                "authors": [
                    {
                        "name": "Tetsushi Ohki"
                    },
                    {
                        "name": "Narishige Abe"
                    },
                    {
                        "name": "Hidetsugu Uchida"
                    },
                    {
                        "name": "Shigefumi Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Shigefumi Yamada"
                },
                "author": "Shigefumi Yamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09641v2",
                "updated": "2024-09-17T14:16:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    16,
                    16,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-15T07:23:07Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    7,
                    23,
                    7,
                    6,
                    259,
                    0
                ],
                "title": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation"
                },
                "summary": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction."
                },
                "authors": [
                    {
                        "name": "Dasom Choi"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Kyungah Lee"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "19 pages excluding reference and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11218v1",
                "updated": "2024-09-17T14:12:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    12,
                    8,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:12:08Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    12,
                    8,
                    1,
                    261,
                    0
                ],
                "title": "Exploring ChatGPT-based Augmentation Strategies for Contrastive\n  Aspect-based Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ChatGPT-based Augmentation Strategies for Contrastive\n  Aspect-based Sentiment Analysis"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards\nspecific aspect terms in a sentence and allows us to uncover nuanced\nperspectives and attitudes on particular aspects of a product, service, or\ntopic. However, the scarcity of labeled data poses a significant challenge to\ntraining high-quality models. To address this issue, we explore the potential\nof data augmentation using ChatGPT, a well-performing large language model\n(LLM), to enhance the sentiment classification performance towards aspect\nterms. Specifically, we explore three data augmentation strategies based on\nChatGPT: context-focused, aspect-focused, and context-aspect data augmentation\ntechniques. Context-focused data augmentation focuses on changing the word\nexpression of context words in the sentence while keeping aspect terms\nunchanged. In contrast, aspect-focused data augmentation aims to change aspect\nterms but keep context words unchanged. Context-Aspect data augmentation\nintegrates the above two data augmentations to generate augmented samples.\nFurthermore, we incorporate contrastive learning into the ABSA tasks to improve\nperformance. Extensive experiments show that all three data augmentation\ntechniques lead to performance improvements, with the context-aspect data\naugmentation strategy performing best and surpassing the performance of the\nbaseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards\nspecific aspect terms in a sentence and allows us to uncover nuanced\nperspectives and attitudes on particular aspects of a product, service, or\ntopic. However, the scarcity of labeled data poses a significant challenge to\ntraining high-quality models. To address this issue, we explore the potential\nof data augmentation using ChatGPT, a well-performing large language model\n(LLM), to enhance the sentiment classification performance towards aspect\nterms. Specifically, we explore three data augmentation strategies based on\nChatGPT: context-focused, aspect-focused, and context-aspect data augmentation\ntechniques. Context-focused data augmentation focuses on changing the word\nexpression of context words in the sentence while keeping aspect terms\nunchanged. In contrast, aspect-focused data augmentation aims to change aspect\nterms but keep context words unchanged. Context-Aspect data augmentation\nintegrates the above two data augmentations to generate augmented samples.\nFurthermore, we incorporate contrastive learning into the ABSA tasks to improve\nperformance. Extensive experiments show that all three data augmentation\ntechniques lead to performance improvements, with the context-aspect data\naugmentation strategy performing best and surpassing the performance of the\nbaseline models."
                },
                "authors": [
                    {
                        "name": "Lingling Xu"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "S. Joe Qin"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Xiaohui Tao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Tao"
                },
                "author": "Xiaohui Tao",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11214v1",
                "updated": "2024-09-17T14:10:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    10,
                    57,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:10:57Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    10,
                    57,
                    1,
                    261,
                    0
                ],
                "title": "Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for\n  Multilingual Speech-to-Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for\n  Multilingual Speech-to-Text"
                },
                "summary": "Integrating audio encoders with LLMs through connectors has enabled these\nmodels to process and comprehend audio modalities, significantly enhancing\nspeech-to-text tasks, including automatic speech recognition (ASR) and\nautomatic speech translation (AST). However, these methods often overlook the\ncritical aspect of language adaptation in multilingual settings, relying\ninstead on multilingual data without adequately addressing language\ndifferences. To address this gap, we propose the Ideal-LLM model, which employs\ndual multilingual encoders to enrich language feature information and utilizes\na language-adapted connector to target the adaptation of each language\nspecifically. By leveraging the complementary strengths of Whisper and MMS\nencoders, our approach ensures richer multilingual representations.\nAdditionally, the language-adapted connector enhances modal transformation via\na language weight selector tailored for each language. Experimental results\ndemonstrate that Ideal-LLM significantly improves ASR performance, achieving a\n32.6% relative reduction in average word error rates compared to the standard\nspeech encoder integrated with LLMs and yields an average BLEU score of 36.78\nfor AST task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio encoders with LLMs through connectors has enabled these\nmodels to process and comprehend audio modalities, significantly enhancing\nspeech-to-text tasks, including automatic speech recognition (ASR) and\nautomatic speech translation (AST). However, these methods often overlook the\ncritical aspect of language adaptation in multilingual settings, relying\ninstead on multilingual data without adequately addressing language\ndifferences. To address this gap, we propose the Ideal-LLM model, which employs\ndual multilingual encoders to enrich language feature information and utilizes\na language-adapted connector to target the adaptation of each language\nspecifically. By leveraging the complementary strengths of Whisper and MMS\nencoders, our approach ensures richer multilingual representations.\nAdditionally, the language-adapted connector enhances modal transformation via\na language weight selector tailored for each language. Experimental results\ndemonstrate that Ideal-LLM significantly improves ASR performance, achieving a\n32.6% relative reduction in average word error rates compared to the standard\nspeech encoder integrated with LLMs and yields an average BLEU score of 36.78\nfor AST task."
                },
                "authors": [
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Wei Ren"
                    },
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Longhao Li"
                    },
                    {
                        "name": "Qijie Shao"
                    },
                    {
                        "name": "Linju Yang"
                    },
                    {
                        "name": "Kai Diao"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "5 pages, 3 figures, submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11212v1",
                "updated": "2024-09-17T14:05:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    5,
                    58,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:05:58Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    5,
                    58,
                    1,
                    261,
                    0
                ],
                "title": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Evolutionary Large Language Models through Uncertainty-Enhanced\n  Preference Optimization"
                },
                "summary": "Iterative preference optimization has recently become one of the de-facto\ntraining paradigms for large language models (LLMs), but the performance is\nstill underwhelming due to too much noisy preference data yielded in the loop.\nTo combat this issue, we present an \\textbf{U}ncertainty-enhanced\n\\textbf{P}reference \\textbf{O}ptimization (UPO) framework to make the LLM\nself-evolve with reliable feedback. The key idea is mitigating the noisy\npreference data derived from the current policy and reward models by performing\npair-wise uncertainty estimation and judiciously reliable feedback sampling. To\nreach this goal, we thus introduce an estimator model, which incorporates Monte\nCarlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty\nestimation for the preference data derived from the LLM policy. Compared to the\nexisting methods that directly filter generated responses based on the reward\nscore, the estimator focuses on the model uncertainty in a pair-wise manner and\neffectively bypasses the confirmation bias problem of the reward model.\nAdditionally, we also propose an uncertainty-enhanced self-evolution algorithm\nto improve the robustness of preference optimization and encourage the LLM to\ngenerate responses with both high reward and certainty. Extensive experiments\nover multiple benchmarks demonstrate that our framework substantially\nalleviates the noisy problem and improves the performance of iterative\npreference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative preference optimization has recently become one of the de-facto\ntraining paradigms for large language models (LLMs), but the performance is\nstill underwhelming due to too much noisy preference data yielded in the loop.\nTo combat this issue, we present an \\textbf{U}ncertainty-enhanced\n\\textbf{P}reference \\textbf{O}ptimization (UPO) framework to make the LLM\nself-evolve with reliable feedback. The key idea is mitigating the noisy\npreference data derived from the current policy and reward models by performing\npair-wise uncertainty estimation and judiciously reliable feedback sampling. To\nreach this goal, we thus introduce an estimator model, which incorporates Monte\nCarlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty\nestimation for the preference data derived from the LLM policy. Compared to the\nexisting methods that directly filter generated responses based on the reward\nscore, the estimator focuses on the model uncertainty in a pair-wise manner and\neffectively bypasses the confirmation bias problem of the reward model.\nAdditionally, we also propose an uncertainty-enhanced self-evolution algorithm\nto improve the robustness of preference optimization and encourage the LLM to\ngenerate responses with both high reward and certainty. Extensive experiments\nover multiple benchmarks demonstrate that our framework substantially\nalleviates the noisy problem and improves the performance of iterative\npreference optimization."
                },
                "authors": [
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Xiaocheng Zhang"
                    },
                    {
                        "name": "Mengjiao Bao"
                    },
                    {
                        "name": "Peng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Peng Yan"
                },
                "author": "Peng Yan",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11629v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11629v4",
                "updated": "2024-09-17T14:04:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    4,
                    27,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-17T15:11:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    11,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study"
                },
                "summary": "Utilizing Large Language Models (LLMs) as evaluators for evaluating the\nperformance of LLMs has recently garnered attention. However, this kind of\nevaluation approach is affected by potential biases in LLMs, raising concerns\nabout the accuracy and reliability of the evaluation results. To mitigate this\nissue, we propose and study two many-shot ICL prompts, which rely on two\nversions of many-shot ICL prompt templates for helping LLM evaluators to\nmitigate the potential biases in LLMs, \\textbf{M}any-\\textbf{S}hot\n\\textbf{w}ith \\textbf{R}eference (\\textbf{MSwR}) and\n\\textbf{M}any-\\textbf{S}hot with\\textbf{o}ut \\textbf{R}eference\n(\\textbf{MSoR}). Concretely, the former utilizes in-context examples with\nmodel-generated rationales as guidance, and the latter without. Based on the\ndesigned prompts, we investigate the impact of scaling the number of in-context\nexamples on the consistency and quality of the evaluation results. Experimental\nresults show that advanced LLMs, such as GPT-4o, perform better in the\nmany-shot regime than in the zero-shot regime. Furthermore, we reveal the\nsymbol bias hidden in the selection bias of LLMs and propose a simple yet\neffective approach to mitigate the bias. Experimental results further verify\nthe effectiveness of the symbol bias mitigation approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models (LLMs) as evaluators for evaluating the\nperformance of LLMs has recently garnered attention. However, this kind of\nevaluation approach is affected by potential biases in LLMs, raising concerns\nabout the accuracy and reliability of the evaluation results. To mitigate this\nissue, we propose and study two many-shot ICL prompts, which rely on two\nversions of many-shot ICL prompt templates for helping LLM evaluators to\nmitigate the potential biases in LLMs, \\textbf{M}any-\\textbf{S}hot\n\\textbf{w}ith \\textbf{R}eference (\\textbf{MSwR}) and\n\\textbf{M}any-\\textbf{S}hot with\\textbf{o}ut \\textbf{R}eference\n(\\textbf{MSoR}). Concretely, the former utilizes in-context examples with\nmodel-generated rationales as guidance, and the latter without. Based on the\ndesigned prompts, we investigate the impact of scaling the number of in-context\nexamples on the consistency and quality of the evaluation results. Experimental\nresults show that advanced LLMs, such as GPT-4o, perform better in the\nmany-shot regime than in the zero-shot regime. Furthermore, we reveal the\nsymbol bias hidden in the selection bias of LLMs and propose a simple yet\neffective approach to mitigate the bias. Experimental results further verify\nthe effectiveness of the symbol bias mitigation approach."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Xuan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Luo"
                },
                "author": "Xuan Luo",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11629v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11629v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11208v1",
                "updated": "2024-09-17T14:02:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    2,
                    58,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T14:02:58Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    2,
                    58,
                    1,
                    261,
                    0
                ],
                "title": "Energy Efficiency Support for Software Defined Networks: a Serverless\n  Computing Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Efficiency Support for Software Defined Networks: a Serverless\n  Computing Approach"
                },
                "summary": "Automatic network management strategies have become paramount for meeting the\nneeds of innovative real-time and data-intensive applications, such as in the\nInternet of Things. However, meeting the ever-growing and fluctuating demands\nfor data and services in such applications requires more than ever an efficient\nand scalable network resource management approach. Such approach should enable\nthe automated provisioning of services while incentivising energy-efficient\nresource usage that expands throughout the edge-to-cloud continuum. This paper\nis the first to realise the concept of modular Software-Defined Networks based\non serverless functions in an energy-aware environment. By adopting Function as\na Service, the approach enables on-demand deployment of network functions,\nresulting in cost reduction through fine resource provisioning granularity. An\nanalytical model is presented to approximate the service delivery time and\npower consumption, as well as an open-source prototype implementation supported\nby an extensive experimental evaluation. The experiments demonstrate not only\nthe practical applicability of the proposed approach but significant\nimprovement in terms of energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic network management strategies have become paramount for meeting the\nneeds of innovative real-time and data-intensive applications, such as in the\nInternet of Things. However, meeting the ever-growing and fluctuating demands\nfor data and services in such applications requires more than ever an efficient\nand scalable network resource management approach. Such approach should enable\nthe automated provisioning of services while incentivising energy-efficient\nresource usage that expands throughout the edge-to-cloud continuum. This paper\nis the first to realise the concept of modular Software-Defined Networks based\non serverless functions in an energy-aware environment. By adopting Function as\na Service, the approach enables on-demand deployment of network functions,\nresulting in cost reduction through fine resource provisioning granularity. An\nanalytical model is presented to approximate the service delivery time and\npower consumption, as well as an open-source prototype implementation supported\nby an extensive experimental evaluation. The experiments demonstrate not only\nthe practical applicability of the proposed approach but significant\nimprovement in terms of energy efficiency."
                },
                "authors": [
                    {
                        "name": "Fatemeh Banaie"
                    },
                    {
                        "name": "Karim Djemame"
                    },
                    {
                        "name": "Abdulaziz Alhindi"
                    },
                    {
                        "name": "Vasilios Kelefouras"
                    }
                ],
                "author_detail": {
                    "name": "Vasilios Kelefouras"
                },
                "author": "Vasilios Kelefouras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05804v3",
                "updated": "2024-09-17T14:02:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    2,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-09T14:42:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    42,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning"
                },
                "summary": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles (policy models, evaluators, and dynamic models), and\nuniversally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on LMPR implementations and workflow usage\nacross different agent paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles (policy models, evaluators, and dynamic models), and\nuniversally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on LMPR implementations and workflow usage\nacross different agent paradigms."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11200v1",
                "updated": "2024-09-17T13:55:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    55,
                    44,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:55:44Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    55,
                    44,
                    1,
                    261,
                    0
                ],
                "title": "LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and\n  Future Directions"
                },
                "summary": "The emerging field of smart agriculture leverages the Internet of Things\n(IoT) to revolutionize farming practices. This paper investigates the\ntransformative potential of Long Range (LoRa) technology as a key enabler of\nlong-range wireless communication for agricultural IoT systems. By reviewing\nexisting literature, we identify a gap in research specifically focused on\nLoRa's prospects and challenges from a communication perspective in smart\nagriculture. We delve into the details of LoRa-based agricultural networks,\ncovering network architecture design, Physical Layer (PHY) considerations\ntailored to the agricultural environment, and channel modeling techniques that\naccount for soil characteristics. The paper further explores relaying and\nrouting mechanisms that address the challenges of extending network coverage\nand optimizing data transmission in vast agricultural landscapes. Transitioning\nto practical aspects, we discuss sensor deployment strategies and energy\nmanagement techniques, offering insights for real-world deployments. A\ncomparative analysis of LoRa with other wireless communication technologies\nemployed in agricultural IoT applications highlights its strengths and\nweaknesses in this context. Furthermore, the paper outlines several future\nresearch directions to leverage the potential of LoRa-based agriculture 4.0.\nThese include advancements in channel modeling for diverse farming\nenvironments, novel relay routing algorithms, integrating emerging sensor\ntechnologies like hyper-spectral imaging and drone-based sensing, on-device\nArtificial Intelligence (AI) models, and sustainable solutions. This survey can\nguide researchers, technologists, and practitioners to understand, implement,\nand propel smart agriculture initiatives using LoRa technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging field of smart agriculture leverages the Internet of Things\n(IoT) to revolutionize farming practices. This paper investigates the\ntransformative potential of Long Range (LoRa) technology as a key enabler of\nlong-range wireless communication for agricultural IoT systems. By reviewing\nexisting literature, we identify a gap in research specifically focused on\nLoRa's prospects and challenges from a communication perspective in smart\nagriculture. We delve into the details of LoRa-based agricultural networks,\ncovering network architecture design, Physical Layer (PHY) considerations\ntailored to the agricultural environment, and channel modeling techniques that\naccount for soil characteristics. The paper further explores relaying and\nrouting mechanisms that address the challenges of extending network coverage\nand optimizing data transmission in vast agricultural landscapes. Transitioning\nto practical aspects, we discuss sensor deployment strategies and energy\nmanagement techniques, offering insights for real-world deployments. A\ncomparative analysis of LoRa with other wireless communication technologies\nemployed in agricultural IoT applications highlights its strengths and\nweaknesses in this context. Furthermore, the paper outlines several future\nresearch directions to leverage the potential of LoRa-based agriculture 4.0.\nThese include advancements in channel modeling for diverse farming\nenvironments, novel relay routing algorithms, integrating emerging sensor\ntechnologies like hyper-spectral imaging and drone-based sensing, on-device\nArtificial Intelligence (AI) models, and sustainable solutions. This survey can\nguide researchers, technologists, and practitioners to understand, implement,\nand propel smart agriculture initiatives using LoRa technology."
                },
                "authors": [
                    {
                        "name": "Lameya Aldhaheri"
                    },
                    {
                        "name": "Noor Alshehhi"
                    },
                    {
                        "name": "Irfana Ilyas Jameela Manzil"
                    },
                    {
                        "name": "Ruhul Amin Khalil"
                    },
                    {
                        "name": "Shumaila Javaid"
                    },
                    {
                        "name": "Nasir Saeed"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09590v2",
                "updated": "2024-09-17T13:48:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    48,
                    50,
                    1,
                    261,
                    0
                ],
                "published": "2024-07-12T17:25:02Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    17,
                    25,
                    2,
                    4,
                    194,
                    0
                ],
                "title": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse\n  Mixture-of-Experts"
                },
                "summary": "By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "13pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11192v1",
                "updated": "2024-09-17T13:48:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    48,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:48:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    48,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Towards Ethical Personal AI Applications: Practical Considerations for\n  AI Assistants with Long-Term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Ethical Personal AI Applications: Practical Considerations for\n  AI Assistants with Long-Term Memory"
                },
                "summary": "One application area of long-term memory (LTM) capabilities with increasing\ntraction is personal AI companions and assistants. With the ability to retain\nand contextualize past interactions and adapt to user preferences, personal AI\ncompanions and assistants promise a profound shift in how we interact with AI\nand are on track to become indispensable in personal and professional settings.\nHowever, this advancement introduces new challenges and vulnerabilities that\nrequire careful consideration regarding the deployment and widespread use of\nthese systems. The goal of this paper is to explore the broader implications of\nbuilding and deploying personal AI applications with LTM capabilities using a\nholistic evaluation approach. This will be done in three ways: 1) reviewing the\ntechnological underpinnings of LTM in Large Language Models, 2) surveying\ncurrent personal AI companions and assistants, and 3) analyzing critical\nconsiderations and implications of deploying and using these applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One application area of long-term memory (LTM) capabilities with increasing\ntraction is personal AI companions and assistants. With the ability to retain\nand contextualize past interactions and adapt to user preferences, personal AI\ncompanions and assistants promise a profound shift in how we interact with AI\nand are on track to become indispensable in personal and professional settings.\nHowever, this advancement introduces new challenges and vulnerabilities that\nrequire careful consideration regarding the deployment and widespread use of\nthese systems. The goal of this paper is to explore the broader implications of\nbuilding and deploying personal AI applications with LTM capabilities using a\nholistic evaluation approach. This will be done in three ways: 1) reviewing the\ntechnological underpinnings of LTM in Large Language Models, 2) surveying\ncurrent personal AI companions and assistants, and 3) analyzing critical\nconsiderations and implications of deploying and using these applications."
                },
                "authors": [
                    {
                        "name": "Eunhae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Eunhae Lee"
                },
                "author": "Eunhae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11190v1",
                "updated": "2024-09-17T13:44:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    44,
                    42,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:44:42Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    44,
                    42,
                    1,
                    261,
                    0
                ],
                "title": "SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as\n  Autonomous Programmer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as\n  Autonomous Programmer"
                },
                "summary": "We present SuperCoder2.0, an advanced autonomous system designed to enhance\nsoftware development through artificial intelligence. The system combines an\nAI-native development approach with intelligent agents to enable fully\nautonomous coding. Key focus areas include a retry mechanism with error output\ntraceback, comprehensive code rewriting and replacement using Abstract Syntax\nTree (ast) parsing to minimize linting issues, code embedding technique for\nretrieval-augmented generation, and a focus on localizing methods for\nproblem-solving rather than identifying specific line numbers. The methodology\nemploys a three-step hierarchical search space reduction approach for code base\nnavigation and bug localization:utilizing Retrieval Augmented Generation (RAG)\nand a Repository File Level Map to identify candidate files, (2) narrowing down\nto the most relevant files using a File Level Schematic Map, and (3) extracting\n'relevant locations' within these files. Code editing is performed through a\ntwo-part module comprising CodeGeneration and CodeEditing, which generates\nmultiple solutions at different temperature values and replaces entire methods\nor classes to maintain code integrity. A feedback loop executes\nrepository-level test cases to validate and refine solutions. Experiments\nconducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's\neffectiveness, achieving correct file localization in 84.33% of cases within\nthe top 5 candidates and successfully resolving 34% of test instances. This\nperformance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.\nThe system's ability to handle diverse repositories and problem types\nhighlights its potential as a versatile tool for autonomous software\ndevelopment. Future work will focus on refining the code editing process and\nexploring advanced embedding models for improved natural language to code\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SuperCoder2.0, an advanced autonomous system designed to enhance\nsoftware development through artificial intelligence. The system combines an\nAI-native development approach with intelligent agents to enable fully\nautonomous coding. Key focus areas include a retry mechanism with error output\ntraceback, comprehensive code rewriting and replacement using Abstract Syntax\nTree (ast) parsing to minimize linting issues, code embedding technique for\nretrieval-augmented generation, and a focus on localizing methods for\nproblem-solving rather than identifying specific line numbers. The methodology\nemploys a three-step hierarchical search space reduction approach for code base\nnavigation and bug localization:utilizing Retrieval Augmented Generation (RAG)\nand a Repository File Level Map to identify candidate files, (2) narrowing down\nto the most relevant files using a File Level Schematic Map, and (3) extracting\n'relevant locations' within these files. Code editing is performed through a\ntwo-part module comprising CodeGeneration and CodeEditing, which generates\nmultiple solutions at different temperature values and replaces entire methods\nor classes to maintain code integrity. A feedback loop executes\nrepository-level test cases to validate and refine solutions. Experiments\nconducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's\neffectiveness, achieving correct file localization in 84.33% of cases within\nthe top 5 candidates and successfully resolving 34% of test instances. This\nperformance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.\nThe system's ability to handle diverse repositories and problem types\nhighlights its potential as a versatile tool for autonomous software\ndevelopment. Future work will focus on refining the code editing process and\nexploring advanced embedding models for improved natural language to code\nmapping."
                },
                "authors": [
                    {
                        "name": "Anmol Gautam"
                    },
                    {
                        "name": "Kishore Kumar"
                    },
                    {
                        "name": "Adarsh Jha"
                    },
                    {
                        "name": "Mukunda NS"
                    },
                    {
                        "name": "Ishaan Bhola"
                    }
                ],
                "author_detail": {
                    "name": "Ishaan Bhola"
                },
                "author": "Ishaan Bhola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03351v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03351v3",
                "updated": "2024-09-17T13:25:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    25,
                    12,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-05T08:53:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    53,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "Digital Ecosystem for FAIR Time Series Data Management in Environmental\n  System Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Ecosystem for FAIR Time Series Data Management in Environmental\n  System Science"
                },
                "summary": "Addressing the challenges posed by climate change, biodiversity loss, and\nenvironmental pollution requires comprehensive monitoring and effective data\nmanagement strategies that are applicable across various scales in\nenvironmental system science. This paper introduces a versatile and\ntransferable digital ecosystem for managing time series data, designed to\nadhere to the FAIR principles (Findable, Accessible, Interoperable, and\nReusable). The system is highly adaptable, cloud-ready, and suitable for\ndeployment in a wide range of settings, from small-scale projects to\nlarge-scale monitoring initiatives. The ecosystem comprises three core\ncomponents: the Sensor Management System (SMS) for detailed metadata\nregistration and management; timeIO, a platform for efficient time series data\nstorage, transfer, and real-time visualization; and the System for Automated\nQuality Control (SaQC), which ensures data integrity through real-time analysis\nand quality assurance. The modular architecture, combined with standardized\nprotocols and interfaces, ensures that the ecosystem can be easily transferred\nand deployed across different environments and institutions. This approach\nenhances data accessibility for a broad spectrum of stakeholders, including\nresearchers, policymakers, and the public, while fostering collaboration and\nadvancing scientific research in environmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the challenges posed by climate change, biodiversity loss, and\nenvironmental pollution requires comprehensive monitoring and effective data\nmanagement strategies that are applicable across various scales in\nenvironmental system science. This paper introduces a versatile and\ntransferable digital ecosystem for managing time series data, designed to\nadhere to the FAIR principles (Findable, Accessible, Interoperable, and\nReusable). The system is highly adaptable, cloud-ready, and suitable for\ndeployment in a wide range of settings, from small-scale projects to\nlarge-scale monitoring initiatives. The ecosystem comprises three core\ncomponents: the Sensor Management System (SMS) for detailed metadata\nregistration and management; timeIO, a platform for efficient time series data\nstorage, transfer, and real-time visualization; and the System for Automated\nQuality Control (SaQC), which ensures data integrity through real-time analysis\nand quality assurance. The modular architecture, combined with standardized\nprotocols and interfaces, ensures that the ecosystem can be easily transferred\nand deployed across different environments and institutions. This approach\nenhances data accessibility for a broad spectrum of stakeholders, including\nresearchers, policymakers, and the public, while fostering collaboration and\nadvancing scientific research in environmental monitoring."
                },
                "authors": [
                    {
                        "name": "J. Bumberger"
                    },
                    {
                        "name": "M. Abbrent"
                    },
                    {
                        "name": "N. Brinckmann"
                    },
                    {
                        "name": "J. Hemmen"
                    },
                    {
                        "name": "R. Kunkel"
                    },
                    {
                        "name": "C. Lorenz"
                    },
                    {
                        "name": "P. Lünenschloß"
                    },
                    {
                        "name": "B. Palm"
                    },
                    {
                        "name": "T. Schnicke"
                    },
                    {
                        "name": "C. Schulz"
                    },
                    {
                        "name": "H. van der Schaaf"
                    },
                    {
                        "name": "D. Schäfer"
                    }
                ],
                "author_detail": {
                    "name": "D. Schäfer"
                },
                "author": "D. Schäfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03351v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03351v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11160v1",
                "updated": "2024-09-17T13:14:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    14,
                    13,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:14:13Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    14,
                    13,
                    1,
                    261,
                    0
                ],
                "title": "UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D\n  Object Detection via Channel2height",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltimateDO: An Efficient Framework to Marry Occupancy Prediction with 3D\n  Object Detection via Channel2height"
                },
                "summary": "Occupancy and 3D object detection are characterized as two standard tasks in\nmodern autonomous driving system. In order to deploy them on a series of edge\nchips with better precision and time-consuming trade-off, contemporary\napproaches either deploy standalone models for individual tasks, or design a\nmulti-task paradigm with separate heads. However, they might suffer from\ndeployment difficulties (i.e., 3D convolution, transformer and so on) or\ndeficiencies in task coordination. Instead, we argue that a favorable framework\nshould be devised in pursuit of ease deployment on diverse chips and high\nprecision with little time-consuming. Oriented at this, we revisit the paradigm\nfor interaction between 3D object detection and occupancy prediction,\nreformulate the model with 2D convolution and prioritize the tasks such that\neach contributes to other. Thus, we propose a method to achieve fast 3D object\ndetection and occupancy prediction (UltimateDO), wherein the light occupancy\nprediction head in FlashOcc is married to 3D object detection network, with\nnegligible additional timeconsuming of only 1.1ms while facilitating each\nother. We instantiate UltimateDO on the challenging nuScenes-series benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupancy and 3D object detection are characterized as two standard tasks in\nmodern autonomous driving system. In order to deploy them on a series of edge\nchips with better precision and time-consuming trade-off, contemporary\napproaches either deploy standalone models for individual tasks, or design a\nmulti-task paradigm with separate heads. However, they might suffer from\ndeployment difficulties (i.e., 3D convolution, transformer and so on) or\ndeficiencies in task coordination. Instead, we argue that a favorable framework\nshould be devised in pursuit of ease deployment on diverse chips and high\nprecision with little time-consuming. Oriented at this, we revisit the paradigm\nfor interaction between 3D object detection and occupancy prediction,\nreformulate the model with 2D convolution and prioritize the tasks such that\neach contributes to other. Thus, we propose a method to achieve fast 3D object\ndetection and occupancy prediction (UltimateDO), wherein the light occupancy\nprediction head in FlashOcc is married to 3D object detection network, with\nnegligible additional timeconsuming of only 1.1ms while facilitating each\nother. We instantiate UltimateDO on the challenging nuScenes-series benchmarks."
                },
                "authors": [
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Changyong Shu"
                    }
                ],
                "author_detail": {
                    "name": "Changyong Shu"
                },
                "author": "Changyong Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10652v2",
                "updated": "2024-09-17T13:14:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    14,
                    0,
                    1,
                    261,
                    0
                ],
                "published": "2023-11-17T17:14:32Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    17,
                    14,
                    32,
                    4,
                    321,
                    0
                ],
                "title": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems"
                },
                "summary": "AI models are constantly evolving, with new versions released frequently.\nThis raises a key question: how should AI-infused systems integrate updates\nwhen the downstream impact on user experience and performance is unclear?\nHuman-AI interaction guidelines encourage notifying users about (changes in)\nmodel capabilities, ideally supported by thorough benchmarking. Yet, as AI\nmodels integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical or expensive, often leading to invisible or minimally\ncommunicated updates. In this work, we explore the impact of such updates\nthrough two complementary studies on facial recognition for historical person\nidentification. First, we conducted an online experiment to understand how\nusers distinguish between models, followed by a diary study examining user\nperceptions in a real-world deployment. Our findings reveal how model changes\nimpact human-AI performance, downstream user behavior, and the folk theories\nthey develop. Based on these insights, we discuss implications for updating\nmodels in AI-infused systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models are constantly evolving, with new versions released frequently.\nThis raises a key question: how should AI-infused systems integrate updates\nwhen the downstream impact on user experience and performance is unclear?\nHuman-AI interaction guidelines encourage notifying users about (changes in)\nmodel capabilities, ideally supported by thorough benchmarking. Yet, as AI\nmodels integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical or expensive, often leading to invisible or minimally\ncommunicated updates. In this work, we explore the impact of such updates\nthrough two complementary studies on facial recognition for historical person\nidentification. First, we conducted an online experiment to understand how\nusers distinguish between models, followed by a diary study examining user\nperceptions in a real-world deployment. Our findings reveal how model changes\nimpact human-AI performance, downstream user behavior, and the folk theories\nthey develop. Based on these insights, we discuss implications for updating\nmodels in AI-infused systems."
                },
                "authors": [
                    {
                        "name": "Vikram Mohanty"
                    },
                    {
                        "name": "Jude Lim"
                    },
                    {
                        "name": "Kurt Luther"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Luther"
                },
                "author": "Kurt Luther",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11156v1",
                "updated": "2024-09-17T13:10:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    10,
                    8,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T13:10:08Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    10,
                    8,
                    1,
                    261,
                    0
                ],
                "title": "On Performance of Distributed RIS-aided Communication in Random Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Performance of Distributed RIS-aided Communication in Random Networks"
                },
                "summary": "This paper evaluates the geometrically averaged performance of a wireless\ncommunication network assisted by a multitude of distributed reconfigurable\nintelligent surfaces (RISs), where the RIS locations are randomly dropped\nobeying a homogeneous Poisson point process. By exploiting stochastic geometry\nand then averaging over the random locations of RISs as well as the serving\nuser, we first derive a closed-form expression for the spatially ergodic rate\nin the presence of phase errors at the RISs in practice. Armed with this\nclosed-form characterization, we then optimize the RIS deployment under a\nreasonable and fair constraint of a total number of RIS elements per unit area.\nThe optimal configurations in terms of key network parameters, including the\nRIS deployment density and the array sizes of RISs, are disclosed for the\nspatially ergodic rate maximization. Our findings suggest that deploying\nlarger-size RISs with reduced deployment density is theoretically preferred to\nsupport extended RIS coverages, under the cases of bounded phase shift errors.\nHowever, when dealing with random phase shifts, the reflecting elements are\nrecommended to spread out as much as possible, disregarding the deployment\ncost. Furthermore, the spatially ergodic rate loss due to the phase shift\nerrors is quantitatively characterized. For bounded phase shift errors, the\nrate loss is eventually upper bounded by a constant as $N\\rightarrow\\infty$,\nwhere $N$ is the number of reflecting elements at each RIS. While for random\nphase shifts, this rate loss scales up in the order of $\\log N$. These\nanalytical observations are validated through numerical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the geometrically averaged performance of a wireless\ncommunication network assisted by a multitude of distributed reconfigurable\nintelligent surfaces (RISs), where the RIS locations are randomly dropped\nobeying a homogeneous Poisson point process. By exploiting stochastic geometry\nand then averaging over the random locations of RISs as well as the serving\nuser, we first derive a closed-form expression for the spatially ergodic rate\nin the presence of phase errors at the RISs in practice. Armed with this\nclosed-form characterization, we then optimize the RIS deployment under a\nreasonable and fair constraint of a total number of RIS elements per unit area.\nThe optimal configurations in terms of key network parameters, including the\nRIS deployment density and the array sizes of RISs, are disclosed for the\nspatially ergodic rate maximization. Our findings suggest that deploying\nlarger-size RISs with reduced deployment density is theoretically preferred to\nsupport extended RIS coverages, under the cases of bounded phase shift errors.\nHowever, when dealing with random phase shifts, the reflecting elements are\nrecommended to spread out as much as possible, disregarding the deployment\ncost. Furthermore, the spatially ergodic rate loss due to the phase shift\nerrors is quantitatively characterized. For bounded phase shift errors, the\nrate loss is eventually upper bounded by a constant as $N\\rightarrow\\infty$,\nwhere $N$ is the number of reflecting elements at each RIS. While for random\nphase shifts, this rate loss scales up in the order of $\\log N$. These\nanalytical observations are validated through numerical results."
                },
                "authors": [
                    {
                        "name": "Jindan Xu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "39 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11147v1",
                "updated": "2024-09-17T12:58:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    58,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T12:58:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    58,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning"
                },
                "summary": "Large language models(LLMs) have exhibited remarkable few-shot learning\ncapabilities and unified the paradigm of NLP tasks through the in-context\nlearning(ICL) technique. Despite the success of ICL, the quality of the\nexemplar demonstrations can significantly influence the LLM's performance.\nExisting exemplar selection methods mainly focus on the semantic similarity\nbetween queries and candidate exemplars. On the other hand, the logical\nconnections between reasoning steps can be beneficial to depict the\nproblem-solving process as well. In this paper, we proposes a novel method\nnamed Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM\nto generate an initial response, then expresses intermediate problem-solving\nsteps to a graph structure. After that, it employs graph kernel to select\nexemplars with semantic and structural similarity. Extensive experiments\ndemonstrate the structural relationship is helpful to the alignment of queries\nand candidate exemplars. The efficacy of RGER on math and logit reasoning tasks\nshowcases its superiority over state-of-the-art retrieval-based approaches. Our\ncode is released at https://github.com/Yukang-Lin/RGER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) have exhibited remarkable few-shot learning\ncapabilities and unified the paradigm of NLP tasks through the in-context\nlearning(ICL) technique. Despite the success of ICL, the quality of the\nexemplar demonstrations can significantly influence the LLM's performance.\nExisting exemplar selection methods mainly focus on the semantic similarity\nbetween queries and candidate exemplars. On the other hand, the logical\nconnections between reasoning steps can be beneficial to depict the\nproblem-solving process as well. In this paper, we proposes a novel method\nnamed Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM\nto generate an initial response, then expresses intermediate problem-solving\nsteps to a graph structure. After that, it employs graph kernel to select\nexemplars with semantic and structural similarity. Extensive experiments\ndemonstrate the structural relationship is helpful to the alignment of queries\nand candidate exemplars. The efficacy of RGER on math and logit reasoning tasks\nshowcases its superiority over state-of-the-art retrieval-based approaches. Our\ncode is released at https://github.com/Yukang-Lin/RGER."
                },
                "authors": [
                    {
                        "name": "Yukang Lin"
                    },
                    {
                        "name": "Bingchen Zhong"
                    },
                    {
                        "name": "Shuoran Jiang"
                    },
                    {
                        "name": "Joanna Siebert"
                    },
                    {
                        "name": "Qingcai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingcai Chen"
                },
                "author": "Qingcai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15886v2",
                "updated": "2024-09-17T12:42:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    42,
                    43,
                    1,
                    261,
                    0
                ],
                "published": "2024-04-24T14:12:56Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    14,
                    12,
                    56,
                    2,
                    115,
                    0
                ],
                "title": "Privacy-Preserving Billing for Local Energy Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Billing for Local Energy Markets"
                },
                "summary": "We propose a privacy-preserving billing protocol for local energy markets\n(PBP-LEM) that takes into account market participants' energy volume deviations\nfrom their bids. PBP-LEM enables a group of market entities to jointly compute\nparticipants' bills in a decentralized and privacy-preserving manner without\nsacrificing correctness. It also mitigates risks on individuals' privacy\narising from any potential internal collusion. We first propose an efficient\nand privacy-preserving individual billing scheme, achieving\ninformation-theoretic security, which serves as a building block. PBP-LEM\nutilizes this scheme, along with other techniques such as multiparty\ncomputation, inner product functional encryption and Pedersen commitments to\nensure data confidentiality and accuracy. Additionally, we present three\napproaches, resulting in different levels of privacy protection and\nperformance. We prove that the protocol meets its security and privacy\nrequirements and is feasible for deployment in real LEMs: bills can be computed\nin less than five minutes for 4,000 users using the most computationally\nintensive approach, and in just 0.18 seconds using the least intensive one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a privacy-preserving billing protocol for local energy markets\n(PBP-LEM) that takes into account market participants' energy volume deviations\nfrom their bids. PBP-LEM enables a group of market entities to jointly compute\nparticipants' bills in a decentralized and privacy-preserving manner without\nsacrificing correctness. It also mitigates risks on individuals' privacy\narising from any potential internal collusion. We first propose an efficient\nand privacy-preserving individual billing scheme, achieving\ninformation-theoretic security, which serves as a building block. PBP-LEM\nutilizes this scheme, along with other techniques such as multiparty\ncomputation, inner product functional encryption and Pedersen commitments to\nensure data confidentiality and accuracy. Additionally, we present three\napproaches, resulting in different levels of privacy protection and\nperformance. We prove that the protocol meets its security and privacy\nrequirements and is feasible for deployment in real LEMs: bills can be computed\nin less than five minutes for 4,000 users using the most computationally\nintensive approach, and in just 0.18 seconds using the least intensive one."
                },
                "authors": [
                    {
                        "name": "Eman Alqahtani"
                    },
                    {
                        "name": "Mustafa A. Mustafa"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Mustafa"
                },
                "author": "Mustafa A. Mustafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18697v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18697v3",
                "updated": "2024-09-17T12:27:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    27,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-27T15:46:25Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    15,
                    46,
                    25,
                    2,
                    87,
                    0
                ],
                "title": "The Invalsi Benchmarks: measuring Linguistic and Mathematical\n  understanding of Large Language Models in Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Invalsi Benchmarks: measuring Linguistic and Mathematical\n  understanding of Large Language Models in Italian"
                },
                "summary": "While Italian is a high-resource language, there are few Italian-native\nbenchmarks to evaluate generative Large Language Models (LLMs) in this\nlanguage. This work presents three new benchmarks: Invalsi MATE to evaluate\nmodels performance on mathematical understanding in Italian, Invalsi ITA to\nevaluate language understanding in Italian and Olimpiadi MATE for more complex\nmathematical understanding.\n  The first two benchmarks are based on the Invalsi tests, which are\nadministered to students of age between 6 and 18 within the Italian school\nsystem and have been validated by several experts in teaching and pedagogy, the\nthird one comes from the Italian high school math Olympics.\n  We evaluate 10 powerful language models on these benchmarks and find that\nthey are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b\ninstruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we\ncompare LLMs with the average performance of Italian students to show that\nLlama 3.1 is the only one to outperform them on Invalsi MATE while most models\ndo so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than\nInvalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is\n45%.\n  We will make data and evaluation code openly available upon acceptance of the\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Italian is a high-resource language, there are few Italian-native\nbenchmarks to evaluate generative Large Language Models (LLMs) in this\nlanguage. This work presents three new benchmarks: Invalsi MATE to evaluate\nmodels performance on mathematical understanding in Italian, Invalsi ITA to\nevaluate language understanding in Italian and Olimpiadi MATE for more complex\nmathematical understanding.\n  The first two benchmarks are based on the Invalsi tests, which are\nadministered to students of age between 6 and 18 within the Italian school\nsystem and have been validated by several experts in teaching and pedagogy, the\nthird one comes from the Italian high school math Olympics.\n  We evaluate 10 powerful language models on these benchmarks and find that\nthey are bound by 71% accuracy on Invasli MATE, achieved by Llama 3.1 70b\ninstruct and by 88% on Invalsi ITA. For both Invalsi MATE and Invalsi ITA we\ncompare LLMs with the average performance of Italian students to show that\nLlama 3.1 is the only one to outperform them on Invalsi MATE while most models\ndo so on Invalsi ITA, we then show that Olimpiadi MATE is more challenging than\nInvalsi MATE and the highest accuracy, achieved by Llama 3.1 405b instruct is\n45%.\n  We will make data and evaluation code openly available upon acceptance of the\npaper."
                },
                "authors": [
                    {
                        "name": "Giovanni Puccetti"
                    },
                    {
                        "name": "Maria Cassese"
                    },
                    {
                        "name": "Andrea Esuli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Esuli"
                },
                "author": "Andrea Esuli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18697v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18697v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10482v2",
                "updated": "2024-09-17T12:10:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    10,
                    49,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T17:18:11Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "title": "Schrodinger's Memory: Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schrodinger's Memory: Large Language Models"
                },
                "summary": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11114v1",
                "updated": "2024-09-17T12:07:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    7,
                    17,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T12:07:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    7,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "Diversity-grounded Channel Prototypical Learning for Out-of-Distribution\n  Intent Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity-grounded Channel Prototypical Learning for Out-of-Distribution\n  Intent Detection"
                },
                "summary": "In the realm of task-oriented dialogue systems, a robust intent detection\nmechanism must effectively handle malformed utterances encountered in\nreal-world scenarios. This study presents a novel fine-tuning framework for\nlarge language models (LLMs) aimed at enhancing in-distribution (ID) intent\nclassification and out-of-distribution (OOD) intent detection, which utilizes\nsemantic matching with prototypes derived from ID class names. By harnessing\nthe highly distinguishable representations of LLMs, we construct semantic\nprototypes for each ID class using a diversity-grounded prompt tuning approach.\nWe rigorously test our framework in a challenging OOD context, where ID and OOD\nclasses are semantically close yet distinct, referred to as \\emph{near} OOD\ndetection. For a thorough assessment, we benchmark our method against the\nprevalent fine-tuning approaches. The experimental findings reveal that our\nmethod demonstrates superior performance in both few-shot ID intent\nclassification and near-OOD intent detection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of task-oriented dialogue systems, a robust intent detection\nmechanism must effectively handle malformed utterances encountered in\nreal-world scenarios. This study presents a novel fine-tuning framework for\nlarge language models (LLMs) aimed at enhancing in-distribution (ID) intent\nclassification and out-of-distribution (OOD) intent detection, which utilizes\nsemantic matching with prototypes derived from ID class names. By harnessing\nthe highly distinguishable representations of LLMs, we construct semantic\nprototypes for each ID class using a diversity-grounded prompt tuning approach.\nWe rigorously test our framework in a challenging OOD context, where ID and OOD\nclasses are semantically close yet distinct, referred to as \\emph{near} OOD\ndetection. For a thorough assessment, we benchmark our method against the\nprevalent fine-tuning approaches. The experimental findings reveal that our\nmethod demonstrates superior performance in both few-shot ID intent\nclassification and near-OOD intent detection tasks."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Liming Zhan"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Chengqiang Xie"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Albert Y. S. Lam"
                    }
                ],
                "author_detail": {
                    "name": "Albert Y. S. Lam"
                },
                "author": "Albert Y. S. Lam",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02421v2",
                "updated": "2024-09-17T12:07:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    7,
                    16,
                    1,
                    261,
                    0
                ],
                "published": "2024-03-04T19:13:23Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    19,
                    13,
                    23,
                    0,
                    64,
                    0
                ],
                "title": "Situated Understanding of Errors in Older Adults' Interactions with\n  Voice Assistants: A Month-Long, In-Home Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situated Understanding of Errors in Older Adults' Interactions with\n  Voice Assistants: A Month-Long, In-Home Study"
                },
                "summary": "Our work addresses the challenges older adults face with commercial Voice\nAssistants (VAs), notably in conversation breakdowns and error handling.\nTraditional methods of collecting user experiences-usage logs and post-hoc\ninterviews-do not fully capture the intricacies of older adults' interactions\nwith VAs, particularly regarding their reactions to errors. To bridge this gap,\nwe equipped 15 older adults' homes with smart speakers integrated with custom\naudio recorders to collect ``in-the-wild'' audio interaction data for detailed\nerror analysis. Recognizing the conversational limitations of current VAs, our\nstudy also explored the capabilities of Large Language Models (LLMs) to handle\nnatural and imperfect text for improving VAs. Midway through our study, we\ndeployed ChatGPT-powered VA to investigate its efficacy for older adults. Our\nresearch suggests leveraging vocal and verbal responses combined with LLMs'\ncontextual capabilities for enhanced error prevention and management in VAs,\nwhile proposing design considerations to align VA capabilities with older\nadults' expectations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work addresses the challenges older adults face with commercial Voice\nAssistants (VAs), notably in conversation breakdowns and error handling.\nTraditional methods of collecting user experiences-usage logs and post-hoc\ninterviews-do not fully capture the intricacies of older adults' interactions\nwith VAs, particularly regarding their reactions to errors. To bridge this gap,\nwe equipped 15 older adults' homes with smart speakers integrated with custom\naudio recorders to collect ``in-the-wild'' audio interaction data for detailed\nerror analysis. Recognizing the conversational limitations of current VAs, our\nstudy also explored the capabilities of Large Language Models (LLMs) to handle\nnatural and imperfect text for improving VAs. Midway through our study, we\ndeployed ChatGPT-powered VA to investigate its efficacy for older adults. Our\nresearch suggests leveraging vocal and verbal responses combined with LLMs'\ncontextual capabilities for enhanced error prevention and management in VAs,\nwhile proposing design considerations to align VA capabilities with older\nadults' expectations."
                },
                "authors": [
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15796v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15796v3",
                "updated": "2024-09-17T12:00:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    12,
                    0,
                    10,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-22T09:40:07Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    9,
                    40,
                    7,
                    5,
                    174,
                    0
                ],
                "title": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis"
                },
                "summary": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs."
                },
                "authors": [
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Weihong Zhong"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Yangfan Ye"
                    },
                    {
                        "name": "Xiachong Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15796v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15796v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v4",
                "updated": "2024-09-18T02:03:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    2,
                    3,
                    35,
                    2,
                    262,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_comment": "Accepted in: AI and Ethics. 20 pages, 1 figure, 3 tables, 2 boxes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10679v2",
                "updated": "2024-09-17T10:47:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    47,
                    51,
                    1,
                    261,
                    0
                ],
                "published": "2023-10-12T11:17:23Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    11,
                    17,
                    23,
                    3,
                    285,
                    0
                ],
                "title": "Large language models can replicate cross-cultural differences in\n  personality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can replicate cross-cultural differences in\n  personality"
                },
                "summary": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use a large-scale experiment (N=8000) to determine whether GPT-4 can\nreplicate cross-cultural differences in the Big Five, measured using the\nTen-Item Personality Inventory. We used the US and South Korea as the cultural\npair, given that prior research suggests substantial personality differences\nbetween people from these two countries. We manipulated the target of the\nsimulation (US vs. Korean), the language of the inventory (English vs. Korean),\nand the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4\nreplicated the cross-cultural differences for each factor. However, mean\nratings had an upward bias and exhibited lower variation than in the human\nsamples, as well as lower structural validity. We provide preliminary evidence\nthat LLMs can aid cross-cultural researchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Paweł Niszczota"
                    },
                    {
                        "name": "Mateusz Janczak"
                    },
                    {
                        "name": "Michał Misiak"
                    }
                ],
                "author_detail": {
                    "name": "Michał Misiak"
                },
                "author": "Michał Misiak",
                "arxiv_comment": "27 pages: 12 pages of manuscript + 15 pages of supplementary\n  materials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11056v1",
                "updated": "2024-09-17T10:33:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    33,
                    27,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:33:27Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    33,
                    27,
                    1,
                    261,
                    0
                ],
                "title": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet\n  Cross-lingual Prompts"
                },
                "summary": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of Large Language Models (LLMs), generating rule-based data\nfor real-world applications has become more accessible. Due to the inherent\nambiguity of natural language and the complexity of rule sets, especially in\nlong contexts, LLMs often struggle to follow all specified rules, frequently\nomitting at least one. To enhance the reasoning and understanding of LLMs on\nlong and complex contexts, we propose a novel prompting strategy Multi-Lingual\nPrompt, namely MLPrompt, which automatically translates the error-prone rule\nthat an LLM struggles to follow into another language, thus drawing greater\nattention to it. Experimental results on public datasets across various tasks\nhave shown MLPrompt can outperform state-of-the-art prompting methods such as\nChain of Thought, Tree of Thought, and Self-Consistency. Additionally, we\nintroduce a framework integrating MLPrompt with an auto-checking mechanism for\nstructured data generation, with a specific case study in text-to-MIP\ninstances. Further, we extend the proposed framework for text-to-SQL to\ndemonstrate its generation ability towards structured data synthesis."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Wing-Yin Yu"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiongwei Han"
                },
                "author": "Xiongwei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11055v1",
                "updated": "2024-09-17T10:31:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    31,
                    37,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    31,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language\n  Models: An Experimental Analysis up to 405B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language\n  Models: An Experimental Analysis up to 405B"
                },
                "summary": "Prior research works have evaluated quantized LLMs using limited metrics such\nas perplexity or a few basic knowledge tasks and old datasets. Additionally,\nrecent large-scale models such as Llama 3.1 with up to 405B have not been\nthoroughly examined. This paper evaluates the performance of instruction-tuned\nLLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on\nmodels ranging from 7B to 405B. Using 13 benchmarks, we assess performance\nacross six task types: commonsense Q\\&A, knowledge and language understanding,\ninstruction following, hallucination detection, mathematics, and dialogue. Our\nkey findings reveal that (1) quantizing a larger LLM to a similar size as a\nsmaller FP16 LLM generally performs better across most benchmarks, except for\nhallucination detection and instruction following; (2) performance varies\nsignificantly with different quantization methods, model size, and bit-width,\nwith weight-only methods often yielding better results in larger models; (3)\ntask difficulty does not significantly impact accuracy degradation due to\nquantization; and (4) the MT-Bench evaluation method has limited discriminatory\npower among recent high-performing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior research works have evaluated quantized LLMs using limited metrics such\nas perplexity or a few basic knowledge tasks and old datasets. Additionally,\nrecent large-scale models such as Llama 3.1 with up to 405B have not been\nthoroughly examined. This paper evaluates the performance of instruction-tuned\nLLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on\nmodels ranging from 7B to 405B. Using 13 benchmarks, we assess performance\nacross six task types: commonsense Q\\&A, knowledge and language understanding,\ninstruction following, hallucination detection, mathematics, and dialogue. Our\nkey findings reveal that (1) quantizing a larger LLM to a similar size as a\nsmaller FP16 LLM generally performs better across most benchmarks, except for\nhallucination detection and instruction following; (2) performance varies\nsignificantly with different quantization methods, model size, and bit-width,\nwith weight-only methods often yielding better results in larger models; (3)\ntask difficulty does not significantly impact accuracy degradation due to\nquantization; and (4) the MT-Bench evaluation method has limited discriminatory\npower among recent high-performing LLMs."
                },
                "authors": [
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Jinse Kwon"
                    },
                    {
                        "name": "Jihun Oh"
                    },
                    {
                        "name": "Yongin Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Yongin Kwon"
                },
                "author": "Yongin Kwon",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v2",
                "updated": "2024-09-17T10:16:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    16,
                    14,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models"
                },
                "summary": "Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01558v2",
                "updated": "2024-09-17T10:15:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    15,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-05-05T19:10:19Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    19,
                    10,
                    19,
                    6,
                    126,
                    0
                ],
                "title": "Visual grounding for desktop graphical user interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding for desktop graphical user interfaces"
                },
                "summary": "Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel."
                },
                "authors": [
                    {
                        "name": "Tassnim Dardouri"
                    },
                    {
                        "name": "Laura Minkova"
                    },
                    {
                        "name": "Jessica López Espejel"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Preprint submitted to Computer Vision and Image Understanding journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11041v2",
                "updated": "2024-09-18T07:17:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    17,
                    17,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T10:04:50Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    50,
                    1,
                    261,
                    0
                ],
                "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis\n  by Large Code Models for Conversational Programming"
                },
                "summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops)."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sherzod Hakimov"
                    },
                    {
                        "name": "David Schlangen"
                    }
                ],
                "author_detail": {
                    "name": "David Schlangen"
                },
                "author": "David Schlangen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00454v2",
                "updated": "2024-09-17T10:04:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    4,
                    22,
                    1,
                    261,
                    0
                ],
                "published": "2024-06-29T14:40:23Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    14,
                    40,
                    23,
                    5,
                    181,
                    0
                ],
                "title": "Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language\n  Models via Inherent Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language\n  Models via Inherent Capability"
                },
                "summary": "Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained\nmodels shows promise for low-resource languages, but often suffers from\nmisalignment of internal representations between languages. We hypothesize that\neven when the model cannot generalize across languages effectively in\nfine-tuning, it still captures cross-lingual correspondence useful for\ncross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a\nmethod that lets large language models (LLMs) to translate training data into\nthe target language and fine-tunes the model on its own generated data. By\ndemonstrating that Self-Translate-Train outperforms zero-shot transfer, we\nencourage further exploration of better methods to elicit cross-lingual\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot cross-lingual transfer by fine-tuning multilingual pretrained\nmodels shows promise for low-resource languages, but often suffers from\nmisalignment of internal representations between languages. We hypothesize that\neven when the model cannot generalize across languages effectively in\nfine-tuning, it still captures cross-lingual correspondence useful for\ncross-lingual transfer. We explore this hypothesis with Self-Translate-Train, a\nmethod that lets large language models (LLMs) to translate training data into\nthe target language and fine-tunes the model on its own generated data. By\ndemonstrating that Self-Translate-Train outperforms zero-shot transfer, we\nencourage further exploration of better methods to elicit cross-lingual\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Ryokan Ri"
                    },
                    {
                        "name": "Shun Kiyono"
                    },
                    {
                        "name": "Sho Takase"
                    }
                ],
                "author_detail": {
                    "name": "Sho Takase"
                },
                "author": "Sho Takase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11032v1",
                "updated": "2024-09-17T09:56:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    56,
                    12,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:56:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    56,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI"
                },
                "summary": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement."
                },
                "authors": [
                    {
                        "name": "Riona Matsuoka"
                    },
                    {
                        "name": "Hiroki Matsumoto"
                    },
                    {
                        "name": "Takahiro Yoshida"
                    },
                    {
                        "name": "Tomohiro Watanabe"
                    },
                    {
                        "name": "Ryoma Kondo"
                    },
                    {
                        "name": "Ryohei Hisano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Hisano"
                },
                "author": "Ryohei Hisano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11026v1",
                "updated": "2024-09-17T09:43:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:43:29Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    43,
                    29,
                    1,
                    261,
                    0
                ],
                "title": "Prompt Obfuscation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Obfuscation for Large Language Models"
                },
                "summary": "System prompts that include detailed instructions to describe the task\nperformed by the underlying large language model (LLM) can easily transform\nfoundation models into tools and services with minimal overhead. Because of\ntheir crucial impact on the utility, they are often considered intellectual\nproperty, similar to the code of a software product. However, extracting system\nprompts is easily possible by using prompt injection. As of today, there is no\neffective countermeasure to prevent the stealing of system prompts and all\nsafeguarding efforts could be evaded with carefully crafted prompt injections\nthat bypass all protection mechanisms.In this work, we propose an alternative\nto conventional system prompts. We introduce prompt obfuscation to prevent the\nextraction of the system prompt while maintaining the utility of the system\nitself with only little overhead. The core idea is to find a representation of\nthe original system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We implement an\noptimization-based method to find an obfuscated prompt representation while\nmaintaining the functionality. To evaluate our approach, we investigate eight\ndifferent metrics to compare the performance of a system using the original and\nthe obfuscated system prompts, and we show that the obfuscated version is\nconstantly on par with the original one. We further perform three different\ndeobfuscation attacks and show that with access to the obfuscated prompt and\nthe LLM itself, we are not able to consistently extract meaningful information.\nOverall, we showed that prompt obfuscation can be an effective method to\nprotect intellectual property while maintaining the same utility as the\noriginal system prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts that include detailed instructions to describe the task\nperformed by the underlying large language model (LLM) can easily transform\nfoundation models into tools and services with minimal overhead. Because of\ntheir crucial impact on the utility, they are often considered intellectual\nproperty, similar to the code of a software product. However, extracting system\nprompts is easily possible by using prompt injection. As of today, there is no\neffective countermeasure to prevent the stealing of system prompts and all\nsafeguarding efforts could be evaded with carefully crafted prompt injections\nthat bypass all protection mechanisms.In this work, we propose an alternative\nto conventional system prompts. We introduce prompt obfuscation to prevent the\nextraction of the system prompt while maintaining the utility of the system\nitself with only little overhead. The core idea is to find a representation of\nthe original system prompt that leads to the same functionality, while the\nobfuscated system prompt does not contain any information that allows\nconclusions to be drawn about the original system prompt. We implement an\noptimization-based method to find an obfuscated prompt representation while\nmaintaining the functionality. To evaluate our approach, we investigate eight\ndifferent metrics to compare the performance of a system using the original and\nthe obfuscated system prompts, and we show that the obfuscated version is\nconstantly on par with the original one. We further perform three different\ndeobfuscation attacks and show that with access to the obfuscated prompt and\nthe LLM itself, we are not able to consistently extract meaningful information.\nOverall, we showed that prompt obfuscation can be an effective method to\nprotect intellectual property while maintaining the same utility as the\noriginal system prompt."
                },
                "authors": [
                    {
                        "name": "David Pape"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Lea Schönherr"
                    }
                ],
                "author_detail": {
                    "name": "Lea Schönherr"
                },
                "author": "Lea Schönherr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11022v2",
                "updated": "2024-09-18T10:05:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    10,
                    5,
                    2,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T09:32:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Tong Shang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09785v2",
                "updated": "2024-09-17T09:32:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-15T16:32:49Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    16,
                    32,
                    49,
                    6,
                    259,
                    0
                ],
                "title": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition"
                },
                "summary": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations."
                },
                "authors": [
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Yuanchao Li"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Yen-Ting Lin"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Sabato Marco Siniscalchi"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Catherine Lai"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community:\n  https://huggingface.co/GenSEC-LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14889v3",
                "updated": "2024-09-17T09:24:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    24,
                    42,
                    1,
                    261,
                    0
                ],
                "published": "2024-02-22T10:46:11Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    10,
                    46,
                    11,
                    3,
                    53,
                    0
                ],
                "title": "COBIAS: Contextual Reliability in Bias Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBIAS: Contextual Reliability in Bias Assessment"
                },
                "summary": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augment 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65$, $p = 3.4 *\n10^{-60}$) and can be used to create reliable datasets, which would assist bias\nmitigation works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augment 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65$, $p = 3.4 *\n10^{-60}$) and can be used to create reliable datasets, which would assist bias\nmitigation works."
                },
                "authors": [
                    {
                        "name": "Priyanshul Govil"
                    },
                    {
                        "name": "Hemang Jain"
                    },
                    {
                        "name": "Vamshi Krishna Bonagiri"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Manas Gaur"
                    },
                    {
                        "name": "Sanorita Dey"
                    }
                ],
                "author_detail": {
                    "name": "Sanorita Dey"
                },
                "author": "Sanorita Dey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10999v1",
                "updated": "2024-09-17T09:04:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    4,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T09:04:03Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    4,
                    3,
                    1,
                    261,
                    0
                ],
                "title": "Enhancing Low-Resource Language and Instruction Following Capabilities\n  of Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Low-Resource Language and Instruction Following Capabilities\n  of Audio Language Models"
                },
                "summary": "Audio language models can understand audio inputs and perform a range of\naudio-related tasks based on instructions, such as speech recognition and audio\ncaptioning, where the instructions are usually textual prompts. Audio language\nmodels are mostly initialized from pre-trained audio encoders and large\nlanguage models (LLMs). Although these pre-trained components were developed to\nsupport multiple languages, audio-language models are trained predominantly on\nEnglish data, which may limit their usability to only English instructions or\nEnglish speech inputs. First, this paper examines the performance of existing\naudio language models in an underserved language using Thai as an example. This\npaper demonstrates that, despite being built on multilingual backbones, audio\nlanguage models do not exhibit cross-lingual emergent abilities to low-resource\nlanguages. Second, this paper studies data mixture for developing audio\nlanguage models that are optimized for a target language as well as English. In\naddition. this paper integrates audio comprehension and speech\ninstruction-following capabilities into a single unified model. Our experiments\nprovide insights into data mixture for enhancing instruction-following\ncapabilities in both a low-resource language and English. Our model,\nTyphoon-Audio, outperforms existing open-source audio language models by a\nconsiderable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in\nboth English and Thai languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio language models can understand audio inputs and perform a range of\naudio-related tasks based on instructions, such as speech recognition and audio\ncaptioning, where the instructions are usually textual prompts. Audio language\nmodels are mostly initialized from pre-trained audio encoders and large\nlanguage models (LLMs). Although these pre-trained components were developed to\nsupport multiple languages, audio-language models are trained predominantly on\nEnglish data, which may limit their usability to only English instructions or\nEnglish speech inputs. First, this paper examines the performance of existing\naudio language models in an underserved language using Thai as an example. This\npaper demonstrates that, despite being built on multilingual backbones, audio\nlanguage models do not exhibit cross-lingual emergent abilities to low-resource\nlanguages. Second, this paper studies data mixture for developing audio\nlanguage models that are optimized for a target language as well as English. In\naddition. this paper integrates audio comprehension and speech\ninstruction-following capabilities into a single unified model. Our experiments\nprovide insights into data mixture for enhancing instruction-following\ncapabilities in both a low-resource language and English. Our model,\nTyphoon-Audio, outperforms existing open-source audio language models by a\nconsiderable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in\nboth English and Thai languages."
                },
                "authors": [
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Warit Sirichotedumrong"
                    },
                    {
                        "name": "Kasima Tharnpipitchai"
                    },
                    {
                        "name": "Kunat Pipatanakul"
                    }
                ],
                "author_detail": {
                    "name": "Kunat Pipatanakul"
                },
                "author": "Kunat Pipatanakul",
                "arxiv_comment": "5 pages. Preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09441v2",
                "updated": "2024-09-17T08:57:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    57,
                    42,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T13:51:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    13,
                    51,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for\n  Quadrupedal Robot Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for\n  Quadrupedal Robot Locomotion"
                },
                "summary": "A core strength of Model Predictive Control (MPC) for quadrupedal locomotion\nhas been its ability to enforce constraints and provide interpretability of the\nsequence of commands over the horizon. However, despite being able to plan, MPC\nstruggles to scale with task complexity, often failing to achieve robust\nbehavior on rapidly changing surfaces. On the other hand, model-free\nReinforcement Learning (RL) methods have outperformed MPC on multiple terrains,\nshowing emergent motions but inherently lack any ability to handle constraints\nor perform planning. To address these limitations, we propose a framework that\nintegrates proprioceptive planning with RL, allowing for agile and safe\nlocomotion behaviors through the horizon. Inspired by MPC, we incorporate an\ninternal model that includes a velocity estimator and a Dreamer module. During\ntraining, the framework learns an expert policy and an internal model that are\nco-dependent, facilitating exploration for improved locomotion behaviors.\nDuring deployment, the Dreamer module solves an infinite-horizon MPC problem,\nadapting actions and velocity commands to respect the constraints. We validate\nthe robustness of our training framework through ablation studies on internal\nmodel components and demonstrate improved robustness to training noise.\nFinally, we evaluate our approach across multi-terrain scenarios in both\nsimulation and hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A core strength of Model Predictive Control (MPC) for quadrupedal locomotion\nhas been its ability to enforce constraints and provide interpretability of the\nsequence of commands over the horizon. However, despite being able to plan, MPC\nstruggles to scale with task complexity, often failing to achieve robust\nbehavior on rapidly changing surfaces. On the other hand, model-free\nReinforcement Learning (RL) methods have outperformed MPC on multiple terrains,\nshowing emergent motions but inherently lack any ability to handle constraints\nor perform planning. To address these limitations, we propose a framework that\nintegrates proprioceptive planning with RL, allowing for agile and safe\nlocomotion behaviors through the horizon. Inspired by MPC, we incorporate an\ninternal model that includes a velocity estimator and a Dreamer module. During\ntraining, the framework learns an expert policy and an internal model that are\nco-dependent, facilitating exploration for improved locomotion behaviors.\nDuring deployment, the Dreamer module solves an infinite-horizon MPC problem,\nadapting actions and velocity commands to respect the constraints. We validate\nthe robustness of our training framework through ablation studies on internal\nmodel components and demonstrate improved robustness to training noise.\nFinally, we evaluate our approach across multi-terrain scenarios in both\nsimulation and hardware."
                },
                "authors": [
                    {
                        "name": "Aditya Shirwatkar"
                    },
                    {
                        "name": "Naman Saxena"
                    },
                    {
                        "name": "Kishore Chandra"
                    },
                    {
                        "name": "Shishir Kolathaya"
                    }
                ],
                "author_detail": {
                    "name": "Shishir Kolathaya"
                },
                "author": "Shishir Kolathaya",
                "arxiv_comment": "Preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10994v1",
                "updated": "2024-09-17T08:56:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    56,
                    27,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T08:56:27Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    56,
                    27,
                    1,
                    261,
                    0
                ],
                "title": "Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs"
                },
                "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models."
                },
                "authors": [
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Wenjun Wang"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Michael Guan"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "9 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12423v3",
                "updated": "2024-09-17T08:32:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    32,
                    2,
                    1,
                    261,
                    0
                ],
                "published": "2024-07-17T09:20:44Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    9,
                    20,
                    44,
                    2,
                    199,
                    0
                ],
                "title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT\n  Interactions"
                },
                "summary": "The integration of Large Language Models (LLMs), especially ChatGPT, into\neducation is poised to revolutionize students' learning experiences by\nintroducing innovative conversational learning methodologies. To empower\nstudents to fully leverage the capabilities of ChatGPT in educational\nscenarios, understanding students' interaction patterns with ChatGPT is crucial\nfor instructors. However, this endeavor is challenging due to the absence of\ndatasets focused on student-ChatGPT conversations and the complexities in\nidentifying and analyzing the evolutional interaction patterns within\nconversations. To address these challenges, we collected conversational data\nfrom 48 students interacting with ChatGPT in a master's level data\nvisualization course over one semester. We then developed a coding scheme,\ngrounded in the literature on cognitive levels and thematic analysis, to\ncategorize students' interaction patterns with ChatGPT. Furthermore, we present\na visual analytics system, StuGPTViz, that tracks and compares temporal\npatterns in student prompts and the quality of ChatGPT's responses at multiple\nscales, revealing significant pedagogical insights for instructors. We\nvalidated the system's effectiveness through expert interviews with six data\nvisualization instructors and three case studies. The results confirmed\nStuGPTViz's capacity to enhance educators' insights into the pedagogical value\nof ChatGPT. We also discussed the potential research opportunities of applying\nvisual analytics in education and developing AI-driven personalized learning\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs), especially ChatGPT, into\neducation is poised to revolutionize students' learning experiences by\nintroducing innovative conversational learning methodologies. To empower\nstudents to fully leverage the capabilities of ChatGPT in educational\nscenarios, understanding students' interaction patterns with ChatGPT is crucial\nfor instructors. However, this endeavor is challenging due to the absence of\ndatasets focused on student-ChatGPT conversations and the complexities in\nidentifying and analyzing the evolutional interaction patterns within\nconversations. To address these challenges, we collected conversational data\nfrom 48 students interacting with ChatGPT in a master's level data\nvisualization course over one semester. We then developed a coding scheme,\ngrounded in the literature on cognitive levels and thematic analysis, to\ncategorize students' interaction patterns with ChatGPT. Furthermore, we present\na visual analytics system, StuGPTViz, that tracks and compares temporal\npatterns in student prompts and the quality of ChatGPT's responses at multiple\nscales, revealing significant pedagogical insights for instructors. We\nvalidated the system's effectiveness through expert interviews with six data\nvisualization instructors and three case studies. The results confirmed\nStuGPTViz's capacity to enhance educators' insights into the pedagogical value\nof ChatGPT. We also discussed the potential research opportunities of applying\nvisual analytics in education and developing AI-driven personalized learning\nsolutions."
                },
                "authors": [
                    {
                        "name": "Zixin Chen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Meng Xia"
                    },
                    {
                        "name": "Kento Shigyo"
                    },
                    {
                        "name": "Dingdong Liu"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "11 pages. To be published at IEEE Visualization 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04067v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04067v4",
                "updated": "2024-09-17T08:19:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    19,
                    59,
                    1,
                    261,
                    0
                ],
                "published": "2024-04-05T12:51:37Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    12,
                    51,
                    37,
                    4,
                    96,
                    0
                ],
                "title": "Does Biomedical Training Lead to Better Medical Performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Biomedical Training Lead to Better Medical Performance?"
                },
                "summary": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, biomedical\ntraining has not been systematically evaluated on medical tasks. This study\ninvestigates the effect of biomedical training in the context of six practical\nmedical tasks evaluating $25$ models. In contrast to previous evaluations, our\nresults reveal a performance decline in nine out of twelve biomedical models\nafter fine-tuning, particularly on tasks involving hallucinations, ICD10\ncoding, and instruction adherence. General-domain models like\nMeta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts,\nindicating a trade-off between domain-specific fine-tuning and general medical\ntask performance. We open-source all evaluation scripts and datasets at\nhttps://github.com/TIO-IKIM/CLUE to support further research in this critical\narea.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, biomedical\ntraining has not been systematically evaluated on medical tasks. This study\ninvestigates the effect of biomedical training in the context of six practical\nmedical tasks evaluating $25$ models. In contrast to previous evaluations, our\nresults reveal a performance decline in nine out of twelve biomedical models\nafter fine-tuning, particularly on tasks involving hallucinations, ICD10\ncoding, and instruction adherence. General-domain models like\nMeta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts,\nindicating a trade-off between domain-specific fine-tuning and general medical\ntask performance. We open-source all evaluation scripts and datasets at\nhttps://github.com/TIO-IKIM/CLUE to support further research in this critical\narea."
                },
                "authors": [
                    {
                        "name": "Amin Dada"
                    },
                    {
                        "name": "Marie Bauer"
                    },
                    {
                        "name": "Amanda Butler Contreras"
                    },
                    {
                        "name": "Osman Alperen Koraş"
                    },
                    {
                        "name": "Constantin Marc Seibold"
                    },
                    {
                        "name": "Kaleb E Smith"
                    },
                    {
                        "name": "Jens Kleesiek"
                    }
                ],
                "author_detail": {
                    "name": "Jens Kleesiek"
                },
                "author": "Jens Kleesiek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04067v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04067v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10969v1",
                "updated": "2024-09-17T08:11:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    11,
                    7,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T08:11:07Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    11,
                    7,
                    1,
                    261,
                    0
                ],
                "title": "Enhancing Multilingual Speech Generation and Recognition Abilities in\n  LLMs with Constructed Code-switched Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multilingual Speech Generation and Recognition Abilities in\n  LLMs with Constructed Code-switched Data"
                },
                "summary": "While large language models (LLMs) have been explored in the speech domain\nfor both generation and recognition tasks, their applications are predominantly\nconfined to the monolingual scenario, with limited exploration in multilingual\nand code-switched (CS) contexts. Additionally, speech generation and\nrecognition tasks are often handled separately, such as VALL-E and Qwen-Audio.\nIn this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating\nmultilingual speech generation and recognition tasks within the single LLM.\nFurthermore, we develop an effective data construction approach that splits and\nconcatenates words from different languages to equip LLMs with CS synthesis\nability without relying on CS data. The experimental results demonstrate that\nour model outperforms other baselines with a comparable data scale.\nFurthermore, our data construction approach not only equips LLMs with CS speech\nsynthesis capability with comparable speaker consistency and similarity to any\ngiven speaker, but also improves the performance of LLMs in multilingual speech\ngeneration and recognition tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have been explored in the speech domain\nfor both generation and recognition tasks, their applications are predominantly\nconfined to the monolingual scenario, with limited exploration in multilingual\nand code-switched (CS) contexts. Additionally, speech generation and\nrecognition tasks are often handled separately, such as VALL-E and Qwen-Audio.\nIn this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating\nmultilingual speech generation and recognition tasks within the single LLM.\nFurthermore, we develop an effective data construction approach that splits and\nconcatenates words from different languages to equip LLMs with CS synthesis\nability without relying on CS data. The experimental results demonstrate that\nour model outperforms other baselines with a comparable data scale.\nFurthermore, our data construction approach not only equips LLMs with CS speech\nsynthesis capability with comparable speaker consistency and similarity to any\ngiven speaker, but also improves the performance of LLMs in multilingual speech\ngeneration and recognition tasks."
                },
                "authors": [
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Daxin Tan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Chen"
                },
                "author": "Xiao Chen",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13324v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13324v5",
                "updated": "2024-09-17T08:08:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    8,
                    8,
                    0,
                    1,
                    261,
                    0
                ],
                "published": "2024-01-24T09:39:39Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    9,
                    39,
                    39,
                    2,
                    24,
                    0
                ],
                "title": "Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions"
                },
                "summary": "Every AI system that makes decisions about people has a group of stakeholders\nthat are personally affected by these decisions. However, explanations of AI\nsystems rarely address the information needs of this stakeholder group, who\noften are AI novices. This creates a gap between conveyed information and\ninformation that matters to those who are impacted by the system's decisions,\nsuch as domain experts and decision subjects. To address this, we present the\n\"XAI Novice Question Bank,\" an extension of the XAI Question Bank containing a\ncatalog of information needs from AI novices in two use cases: employment\nprediction and health monitoring. The catalog covers the categories of data,\nsystem context, system usage, and system specifications. We gathered\ninformation needs through task-based interviews where participants asked\nquestions about two AI systems to decide on their adoption and received verbal\nexplanations in response. Our analysis showed that participants' confidence\nincreased after receiving explanations but that their understanding faced\nchallenges. These included difficulties in locating information and in\nassessing their own understanding, as well as attempts to outsource\nunderstanding. Additionally, participants' prior perceptions of the systems'\nrisks and benefits influenced their information needs. Participants who\nperceived high risks sought explanations about the intentions behind a system's\ndeployment, while those who perceived low risks rather asked about the system's\noperation. Our work aims to support the inclusion of AI novices in\nexplainability efforts by highlighting their information needs, aims, and\nchallenges. We summarize our findings as five key implications that can inform\nthe design of future explanations for lay stakeholder audiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Every AI system that makes decisions about people has a group of stakeholders\nthat are personally affected by these decisions. However, explanations of AI\nsystems rarely address the information needs of this stakeholder group, who\noften are AI novices. This creates a gap between conveyed information and\ninformation that matters to those who are impacted by the system's decisions,\nsuch as domain experts and decision subjects. To address this, we present the\n\"XAI Novice Question Bank,\" an extension of the XAI Question Bank containing a\ncatalog of information needs from AI novices in two use cases: employment\nprediction and health monitoring. The catalog covers the categories of data,\nsystem context, system usage, and system specifications. We gathered\ninformation needs through task-based interviews where participants asked\nquestions about two AI systems to decide on their adoption and received verbal\nexplanations in response. Our analysis showed that participants' confidence\nincreased after receiving explanations but that their understanding faced\nchallenges. These included difficulties in locating information and in\nassessing their own understanding, as well as attempts to outsource\nunderstanding. Additionally, participants' prior perceptions of the systems'\nrisks and benefits influenced their information needs. Participants who\nperceived high risks sought explanations about the intentions behind a system's\ndeployment, while those who perceived low risks rather asked about the system's\noperation. Our work aims to support the inclusion of AI novices in\nexplainability efforts by highlighting their information needs, aims, and\nchallenges. We summarize our findings as five key implications that can inform\nthe design of future explanations for lay stakeholder audiences."
                },
                "authors": [
                    {
                        "name": "Timothée Schmude"
                    },
                    {
                        "name": "Laura Koesten"
                    },
                    {
                        "name": "Torsten Möller"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Tschiatschek"
                },
                "author": "Sebastian Tschiatschek",
                "arxiv_comment": "Main text: 26 pages, 4 figures. Supplementary material is provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13324v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13324v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10955v1",
                "updated": "2024-09-17T07:44:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    44,
                    6,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:44:06Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    44,
                    6,
                    1,
                    261,
                    0
                ],
                "title": "Investigating Context-Faithfulness in Large Language Models: The Roles\n  of Memory Strength and Evidence Style",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Context-Faithfulness in Large Language Models: The Roles\n  of Memory Strength and Evidence Style"
                },
                "summary": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs'\ncontext-faithfulness remain largely unexplored. In this study, we investigate\nthe impact of memory strength and evidence presentation on LLMs' receptiveness\nto external evidence. We introduce a method to quantify the memory strength of\nLLMs by measuring the divergence in LLMs' responses to different paraphrases of\nthe same question, which is not considered by previous works. We also generate\nevidence in various styles to evaluate the effects of evidence in different\nstyles. Two datasets are used for evaluation: Natural Questions (NQ) with\npopular questions and popQA featuring long-tail questions. Our results show\nthat for questions with high memory strength, LLMs are more likely to rely on\ninternal memory, particularly for larger LLMs such as GPT-4. On the other hand,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs'\ncontext-faithfulness remain largely unexplored. In this study, we investigate\nthe impact of memory strength and evidence presentation on LLMs' receptiveness\nto external evidence. We introduce a method to quantify the memory strength of\nLLMs by measuring the divergence in LLMs' responses to different paraphrases of\nthe same question, which is not considered by previous works. We also generate\nevidence in various styles to evaluate the effects of evidence in different\nstyles. Two datasets are used for evaluation: Natural Questions (NQ) with\npopular questions and popQA featuring long-tail questions. Our results show\nthat for questions with high memory strength, LLMs are more likely to rely on\ninternal memory, particularly for larger LLMs such as GPT-4. On the other hand,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details."
                },
                "authors": [
                    {
                        "name": "Yuepei Li"
                    },
                    {
                        "name": "Kang Zhou"
                    },
                    {
                        "name": "Qiao Qiao"
                    },
                    {
                        "name": "Bach Nguyen"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10927v2",
                "updated": "2024-09-18T07:23:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    7,
                    23,
                    50,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T06:51:59Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    51,
                    59,
                    1,
                    261,
                    0
                ],
                "title": "Propulsion: Steering LLM with Tiny Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propulsion: Steering LLM with Tiny Fine-Tuning"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have revolutionized\nnatural language processing (NLP) and related fields. However, fine-tuning\nthese models for specific tasks remains computationally expensive and risks\ndegrading pre-learned features. To address these challenges, we propose\nPropulsion, a novel parameter efficient fine-tuning (PEFT) method designed to\noptimize task-specific performance while drastically reducing computational\noverhead. Inspired by the concept of controlled adjustments in physical motion,\nPropulsion selectively re-scales specific dimensions of a pre-trained model,\nguiding output predictions toward task objectives without modifying the model's\nparameters. By introducing lightweight, trainable Propulsion parameters at the\npre-trained layer, we minimize the number of parameters updated during\nfine-tuning, preventing overfitting or overwriting of existing knowledge. Our\ntheoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows\nthat Propulsion approximates the performance of full fine-tuning with far fewer\ntrainable parameters. Empirically, Propulsion reduces the parameter count from\n355.3 million to just 0.086 million, achieving over a 10x reduction compared to\nstandard approaches like LoRA while maintaining competitive performance across\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "Prakash Bhat"
                    }
                ],
                "author_detail": {
                    "name": "Prakash Bhat"
                },
                "author": "Prakash Bhat",
                "arxiv_comment": "26 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10913v1",
                "updated": "2024-09-17T06:11:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T06:11:22Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers"
                },
                "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Mehak Chhokar"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Mahendra Meena"
                    },
                    {
                        "name": "Hamid Abdullah"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Ruchit Nagar"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10909v1",
                "updated": "2024-09-17T05:59:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    59,
                    32,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T05:59:32Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    59,
                    32,
                    1,
                    261,
                    0
                ],
                "title": "GenCRF: Generative Clustering and Reformulation Framework for Enhanced\n  Intent-Driven Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenCRF: Generative Clustering and Reformulation Framework for Enhanced\n  Intent-Driven Information Retrieval"
                },
                "summary": "Query reformulation is a well-known problem in Information Retrieval (IR)\naimed at enhancing single search successful completion rate by automatically\nmodifying user's input query. Recent methods leverage Large Language Models\n(LLMs) to improve query reformulation, but often generate limited and redundant\nexpansions, potentially constraining their effectiveness in capturing diverse\nintents. In this paper, we propose GenCRF: a Generative Clustering and\nReformulation Framework to capture diverse intentions adaptively based on\nmultiple differentiated, well-generated queries in the retrieval phase for the\nfirst time. GenCRF leverages LLMs to generate variable queries from the initial\nquery using customized prompts, then clusters them into groups to distinctly\nrepresent diverse intents. Furthermore, the framework explores to combine\ndiverse intents query with innovative weighted aggregation strategies to\noptimize retrieval performance and crucially integrates a novel Query\nEvaluation Rewarding Model (QERM) to refine the process through feedback loops.\nEmpirical experiments on the BEIR benchmark demonstrate that GenCRF achieves\nstate-of-the-art performance, surpassing previous query reformulation SOTAs by\nup to 12% on nDCG@10. These techniques can be adapted to various LLMs,\nsignificantly boosting retriever performance and advancing the field of\nInformation Retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query reformulation is a well-known problem in Information Retrieval (IR)\naimed at enhancing single search successful completion rate by automatically\nmodifying user's input query. Recent methods leverage Large Language Models\n(LLMs) to improve query reformulation, but often generate limited and redundant\nexpansions, potentially constraining their effectiveness in capturing diverse\nintents. In this paper, we propose GenCRF: a Generative Clustering and\nReformulation Framework to capture diverse intentions adaptively based on\nmultiple differentiated, well-generated queries in the retrieval phase for the\nfirst time. GenCRF leverages LLMs to generate variable queries from the initial\nquery using customized prompts, then clusters them into groups to distinctly\nrepresent diverse intents. Furthermore, the framework explores to combine\ndiverse intents query with innovative weighted aggregation strategies to\noptimize retrieval performance and crucially integrates a novel Query\nEvaluation Rewarding Model (QERM) to refine the process through feedback loops.\nEmpirical experiments on the BEIR benchmark demonstrate that GenCRF achieves\nstate-of-the-art performance, surpassing previous query reformulation SOTAs by\nup to 12% on nDCG@10. These techniques can be adapted to various LLMs,\nsignificantly boosting retriever performance and advancing the field of\nInformation Retrieval."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Haojie Zhang"
                    },
                    {
                        "name": "Yueyang Zhang"
                    },
                    {
                        "name": "Changhao Zhang"
                    },
                    {
                        "name": "Songyao Duan"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10906v1",
                "updated": "2024-09-17T05:53:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    53,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T05:53:04Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    53,
                    4,
                    1,
                    261,
                    0
                ],
                "title": "Multi-Floor Zero-Shot Object Navigation Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Floor Zero-Shot Object Navigation Policy"
                },
                "summary": "Object navigation in multi-floor environments presents a formidable challenge\nin robotics, requiring sophisticated spatial reasoning and adaptive exploration\nstrategies. Traditional approaches have primarily focused on single-floor\nscenarios, overlooking the complexities introduced by multi-floor structures.\nTo address these challenges, we first propose a Multi-floor Navigation Policy\n(MFNP) and implement it in Zero-Shot object navigation tasks. Our framework\ncomprises three key components: (i) Multi-floor Navigation Policy, which\nenables an agent to explore across multiple floors; (ii) Multi-modal Large\nLanguage Models (MLLMs) for reasoning in the navigation process; and (iii)\nInter-Floor Navigation, ensuring efficient floor transitions. We evaluate MFNP\non the Habitat-Matterport 3D (HM3D) and Matterport 3D (MP3D) datasets, both\ninclude multi-floor scenes. Our experiment results demonstrate that MFNP\nsignificantly outperforms all the existing methods in Zero-Shot object\nnavigation, achieving higher success rates and improved exploration efficiency.\nAblation studies further highlight the effectiveness of each component in\naddressing the unique challenges of multi-floor navigation. Meanwhile, we\nconducted real-world experiments to evaluate the feasibility of our policy.\nUpon deployment of MFNP, the Unitree quadruped robot demonstrated successful\nmulti-floor navigation and found the target object in a completely unseen\nenvironment. By introducing MFNP, we offer a new paradigm for tackling complex,\nmulti-floor environments in object navigation tasks, opening avenues for future\nresearch in visual-based navigation in realistic, multi-floor settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object navigation in multi-floor environments presents a formidable challenge\nin robotics, requiring sophisticated spatial reasoning and adaptive exploration\nstrategies. Traditional approaches have primarily focused on single-floor\nscenarios, overlooking the complexities introduced by multi-floor structures.\nTo address these challenges, we first propose a Multi-floor Navigation Policy\n(MFNP) and implement it in Zero-Shot object navigation tasks. Our framework\ncomprises three key components: (i) Multi-floor Navigation Policy, which\nenables an agent to explore across multiple floors; (ii) Multi-modal Large\nLanguage Models (MLLMs) for reasoning in the navigation process; and (iii)\nInter-Floor Navigation, ensuring efficient floor transitions. We evaluate MFNP\non the Habitat-Matterport 3D (HM3D) and Matterport 3D (MP3D) datasets, both\ninclude multi-floor scenes. Our experiment results demonstrate that MFNP\nsignificantly outperforms all the existing methods in Zero-Shot object\nnavigation, achieving higher success rates and improved exploration efficiency.\nAblation studies further highlight the effectiveness of each component in\naddressing the unique challenges of multi-floor navigation. Meanwhile, we\nconducted real-world experiments to evaluate the feasibility of our policy.\nUpon deployment of MFNP, the Unitree quadruped robot demonstrated successful\nmulti-floor navigation and found the target object in a completely unseen\nenvironment. By introducing MFNP, we offer a new paradigm for tackling complex,\nmulti-floor environments in object navigation tasks, opening avenues for future\nresearch in visual-based navigation in realistic, multi-floor settings."
                },
                "authors": [
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Xinyao Zhang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zixuan Jiang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14043v3",
                "updated": "2024-09-17T04:56:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    56,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-01-25T09:47:55Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    9,
                    47,
                    55,
                    3,
                    25,
                    0
                ],
                "title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Goal-oriented Prompt Engineering for Large Language Models: A\n  Survey"
                },
                "summary": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields."
                },
                "authors": [
                    {
                        "name": "Haochen Li"
                    },
                    {
                        "name": "Jonathan Leung"
                    },
                    {
                        "name": "Zhiqi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Shen"
                },
                "author": "Zhiqi Shen",
                "arxiv_comment": "An up-to-date resource including papers and tasks is maintained at\n  https://github.com/Alex-HaochenLi/Goal-oriented-Prompt-Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10883v1",
                "updated": "2024-09-17T04:39:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    20,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T04:39:20Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    20,
                    1,
                    261,
                    0
                ],
                "title": "CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation\n  for Meeting Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation\n  for Meeting Summarization"
                },
                "summary": "Large Language Models (LLMs) have spurred interest in automatic evaluation\nmethods for summarization, offering a faster, more cost-effective alternative\nto human evaluation. However, existing methods often fall short when applied to\ncomplex tasks like long-context summarizations and dialogue-based meeting\nsummarizations. In this paper, we introduce CREAM (Comparison-Based\nReference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a\nnovel framework that addresses the unique challenges of evaluating meeting\nsummaries. CREAM leverages a combination of chain-of-thought reasoning and key\nfacts alignment to assess conciseness and completeness of model-generated\nsummaries without requiring reference. By employing an ELO ranking system, our\napproach provides a robust mechanism for comparing the quality of different\nmodels or prompt configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have spurred interest in automatic evaluation\nmethods for summarization, offering a faster, more cost-effective alternative\nto human evaluation. However, existing methods often fall short when applied to\ncomplex tasks like long-context summarizations and dialogue-based meeting\nsummarizations. In this paper, we introduce CREAM (Comparison-Based\nReference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a\nnovel framework that addresses the unique challenges of evaluating meeting\nsummaries. CREAM leverages a combination of chain-of-thought reasoning and key\nfacts alignment to assess conciseness and completeness of model-generated\nsummaries without requiring reference. By employing an ELO ranking system, our\napproach provides a robust mechanism for comparing the quality of different\nmodels or prompt configurations."
                },
                "authors": [
                    {
                        "name": "Ziwei Gong"
                    },
                    {
                        "name": "Lin Ai"
                    },
                    {
                        "name": "Harshsaiprasad Deshpande"
                    },
                    {
                        "name": "Alexander Johnson"
                    },
                    {
                        "name": "Emmy Phung"
                    },
                    {
                        "name": "Zehui Wu"
                    },
                    {
                        "name": "Ahmad Emami"
                    },
                    {
                        "name": "Julia Hirschberg"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hirschberg"
                },
                "author": "Julia Hirschberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10870v1",
                "updated": "2024-09-17T03:46:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    46,
                    1,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T03:46:01Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    46,
                    1,
                    1,
                    261,
                    0
                ],
                "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Large Language Models By Layerwise Attention Shortcuts"
                },
                "summary": "Transformer architectures are the backbone of the modern AI revolution.\nHowever, they are based on simply stacking the same blocks in dozens of layers\nand processing information sequentially from one block to another. In this\npaper, we propose to challenge this and introduce adaptive computations for\nLLM-like setups, which allow the final layer to attend to all of the\nintermediate layers as it deems fit through the attention mechanism, thereby\nintroducing computational \\textbf{attention shortcuts}. These shortcuts can\nthus make the architecture depth and context adaptive. We showcase four\ndifferent datasets, namely acoustic tokens, natural language, and symbolic\nmusic, and we achieve superior performance for GPT-like architecture. We give\nevidence via attention maps that the models learn complex dependencies across\nlayers that are adaptive in context and depth depending on the input tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer architectures are the backbone of the modern AI revolution.\nHowever, they are based on simply stacking the same blocks in dozens of layers\nand processing information sequentially from one block to another. In this\npaper, we propose to challenge this and introduce adaptive computations for\nLLM-like setups, which allow the final layer to attend to all of the\nintermediate layers as it deems fit through the attention mechanism, thereby\nintroducing computational \\textbf{attention shortcuts}. These shortcuts can\nthus make the architecture depth and context adaptive. We showcase four\ndifferent datasets, namely acoustic tokens, natural language, and symbolic\nmusic, and we achieve superior performance for GPT-like architecture. We give\nevidence via attention maps that the models learn complex dependencies across\nlayers that are adaptive in context and depth depending on the input tokens."
                },
                "authors": [
                    {
                        "name": "Prateek Verma"
                    },
                    {
                        "name": "Mert Pilanci"
                    }
                ],
                "author_detail": {
                    "name": "Mert Pilanci"
                },
                "author": "Mert Pilanci",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10354v2",
                "updated": "2024-09-17T03:22:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    22,
                    45,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T15:04:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    4,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot"
                },
                "summary": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots."
                },
                "authors": [
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "arxiv_comment": "The first two authors contributed equally to this research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10863v1",
                "updated": "2024-09-17T03:07:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    7,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T03:07:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    7,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Dynamic Range Reduction via Branch-and-Bound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Range Reduction via Branch-and-Bound"
                },
                "summary": "The demand for high-performance computing in machine learning and artificial\nintelligence has led to the development of specialized hardware accelerators\nlike Tensor Processing Units (TPUs), Graphics Processing Units (GPUs), and\nField-Programmable Gate Arrays (FPGAs). A key strategy to enhance these\naccelerators is the reduction of precision in arithmetic operations, which\nincreases processing speed and lowers latency - crucial for real-time AI\napplications. Precision reduction minimizes memory bandwidth requirements and\nenergy consumption, essential for large-scale and mobile deployments, and\nincreases throughput by enabling more parallel operations per cycle, maximizing\nhardware resource utilization. This strategy is equally vital for solving\nNP-hard quadratic unconstrained binary optimization (QUBO) problems common in\nmachine learning, which often require high precision for accurate\nrepresentation. Special hardware solvers, such as quantum annealers, benefit\nsignificantly from precision reduction. This paper introduces a fully\nprincipled Branch-and-Bound algorithm for reducing precision needs in QUBO\nproblems by utilizing dynamic range as a measure of complexity. Experiments\nvalidate our algorithm's effectiveness on an actual quantum annealer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for high-performance computing in machine learning and artificial\nintelligence has led to the development of specialized hardware accelerators\nlike Tensor Processing Units (TPUs), Graphics Processing Units (GPUs), and\nField-Programmable Gate Arrays (FPGAs). A key strategy to enhance these\naccelerators is the reduction of precision in arithmetic operations, which\nincreases processing speed and lowers latency - crucial for real-time AI\napplications. Precision reduction minimizes memory bandwidth requirements and\nenergy consumption, essential for large-scale and mobile deployments, and\nincreases throughput by enabling more parallel operations per cycle, maximizing\nhardware resource utilization. This strategy is equally vital for solving\nNP-hard quadratic unconstrained binary optimization (QUBO) problems common in\nmachine learning, which often require high precision for accurate\nrepresentation. Special hardware solvers, such as quantum annealers, benefit\nsignificantly from precision reduction. This paper introduces a fully\nprincipled Branch-and-Bound algorithm for reducing precision needs in QUBO\nproblems by utilizing dynamic range as a measure of complexity. Experiments\nvalidate our algorithm's effectiveness on an actual quantum annealer."
                },
                "authors": [
                    {
                        "name": "Thore Gerlach"
                    },
                    {
                        "name": "Nico Piatkowski"
                    }
                ],
                "author_detail": {
                    "name": "Nico Piatkowski"
                },
                "author": "Nico Piatkowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10838v1",
                "updated": "2024-09-17T02:07:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    2,
                    7,
                    14,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T02:07:14Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    2,
                    7,
                    14,
                    1,
                    261,
                    0
                ],
                "title": "Machine Learning for Public Good: Predicting Urban Crime Patterns to\n  Enhance Community Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning for Public Good: Predicting Urban Crime Patterns to\n  Enhance Community Safety"
                },
                "summary": "In recent years, urban safety has become a paramount concern for city\nplanners and law enforcement agencies. Accurate prediction of likely crime\noccurrences can significantly enhance preventive measures and resource\nallocation. However, many law enforcement departments lack the tools to analyze\nand apply advanced AI and ML techniques that can support city planners, watch\nprograms, and safety leaders to take proactive steps towards overall community\nsafety.\n  This paper explores the effectiveness of ML techniques to predict spatial and\ntemporal patterns of crimes in urban areas. Leveraging police dispatch call\ndata from San Jose, CA, the research goal is to achieve a high degree of\naccuracy in categorizing calls into priority levels particularly for more\ndangerous situations that require an immediate law enforcement response. This\ncategorization is informed by the time, place, and nature of the call. The\nresearch steps include data extraction, preprocessing, feature engineering,\nexploratory data analysis, implementation, optimization and tuning of different\nsupervised machine learning models and neural networks. The accuracy and\nprecision are examined for different models and features at varying granularity\nof crime categories and location precision.\n  The results demonstrate that when compared to a variety of other models,\nRandom Forest classification models are most effective in identifying dangerous\nsituations and their corresponding priority levels with high accuracy (Accuracy\n= 85%, AUC = 0.92) at a local level while ensuring a minimum amount of false\nnegatives. While further research and data gathering is needed to include other\nsocial and economic factors, these results provide valuable insights for law\nenforcement agencies to optimize resources, develop proactive deployment\napproaches, and adjust response patterns to enhance overall public safety\noutcomes in an unbiased way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, urban safety has become a paramount concern for city\nplanners and law enforcement agencies. Accurate prediction of likely crime\noccurrences can significantly enhance preventive measures and resource\nallocation. However, many law enforcement departments lack the tools to analyze\nand apply advanced AI and ML techniques that can support city planners, watch\nprograms, and safety leaders to take proactive steps towards overall community\nsafety.\n  This paper explores the effectiveness of ML techniques to predict spatial and\ntemporal patterns of crimes in urban areas. Leveraging police dispatch call\ndata from San Jose, CA, the research goal is to achieve a high degree of\naccuracy in categorizing calls into priority levels particularly for more\ndangerous situations that require an immediate law enforcement response. This\ncategorization is informed by the time, place, and nature of the call. The\nresearch steps include data extraction, preprocessing, feature engineering,\nexploratory data analysis, implementation, optimization and tuning of different\nsupervised machine learning models and neural networks. The accuracy and\nprecision are examined for different models and features at varying granularity\nof crime categories and location precision.\n  The results demonstrate that when compared to a variety of other models,\nRandom Forest classification models are most effective in identifying dangerous\nsituations and their corresponding priority levels with high accuracy (Accuracy\n= 85%, AUC = 0.92) at a local level while ensuring a minimum amount of false\nnegatives. While further research and data gathering is needed to include other\nsocial and economic factors, these results provide valuable insights for law\nenforcement agencies to optimize resources, develop proactive deployment\napproaches, and adjust response patterns to enhance overall public safety\noutcomes in an unbiased way."
                },
                "authors": [
                    {
                        "name": "Sia Gupta"
                    },
                    {
                        "name": "Simeon Sayer"
                    }
                ],
                "author_detail": {
                    "name": "Simeon Sayer"
                },
                "author": "Simeon Sayer",
                "arxiv_comment": "19 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17439v2",
                "updated": "2024-09-17T01:57:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    57,
                    36,
                    1,
                    261,
                    0
                ],
                "published": "2024-05-09T03:07:59Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    3,
                    7,
                    59,
                    3,
                    130,
                    0
                ],
                "title": "An Overview of Machine Learning-Enabled Optimization for Reconfigurable\n  Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Overview of Machine Learning-Enabled Optimization for Reconfigurable\n  Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large\n  Language Models"
                },
                "summary": "Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G\nnetworks by reshaping signal propagation in smart radio environments. However,\nit also leads to significant complexity for network management due to the large\nnumber of elements and dedicated phase-shift optimization. In this work, we\nprovide an overview of machine learning (ML)-enabled optimization for RIS-aided\n6G networks. In particular, we focus on various reinforcement learning (RL)\ntechniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer\nreinforcement learning, hierarchical reinforcement learning, and offline\nreinforcement learning. Different from existing studies, this work further\ndiscusses how large language models (LLMs) can be combined with RL to handle\nnetwork optimization problems. It shows that LLM offers new opportunities to\nenhance the capabilities of RL algorithms in terms of generalization, reward\nfunction design, multi-modal information processing, etc. Finally, we identify\nthe future challenges and directions of ML-enabled optimization for RIS-aided\n6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G\nnetworks by reshaping signal propagation in smart radio environments. However,\nit also leads to significant complexity for network management due to the large\nnumber of elements and dedicated phase-shift optimization. In this work, we\nprovide an overview of machine learning (ML)-enabled optimization for RIS-aided\n6G networks. In particular, we focus on various reinforcement learning (RL)\ntechniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer\nreinforcement learning, hierarchical reinforcement learning, and offline\nreinforcement learning. Different from existing studies, this work further\ndiscusses how large language models (LLMs) can be combined with RL to handle\nnetwork optimization problems. It shows that LLM offers new opportunities to\nenhance the capabilities of RL algorithms in terms of generalization, reward\nfunction design, multi-modal information processing, etc. Finally, we identify\nthe future challenges and directions of ML-enabled optimization for RIS-aided\n6G networks."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Chengming Hu"
                    },
                    {
                        "name": "Xue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xue Liu"
                },
                "author": "Xue Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10033v2",
                "updated": "2024-09-17T01:49:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    49,
                    17,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T06:51:32Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    51,
                    32,
                    0,
                    260,
                    0
                ],
                "title": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs"
                },
                "summary": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have long demonstrated remarkable effectiveness in automatic program\nrepair (APR), with OpenAI's ChatGPT being one of the most widely used models in\nthis domain. Through continuous iterations and upgrades of GPT-family models,\ntheir performance in fixing bugs has already reached state-of-the-art levels.\nHowever, there are few works comparing the effectiveness and variations of\ndifferent versions of GPT-family models on APR. In this work, inspired by the\nrecent public release of the GPT-o1 models, we conduct the first study to\ncompare the effectiveness of different versions of the GPT-family models in\nAPR. We evaluate the performance of the latest version of the GPT-family models\n(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT\non APR. We conduct an empirical study of the four GPT-family models against\nother LLMs and APR techniques on the QuixBugs benchmark from multiple\nevaluation perspectives, including repair success rate, repair cost, response\nlength, and behavior patterns. The results demonstrate that O1's repair\ncapability exceeds that of prior GPT-family models, successfully fixing all 40\nbugs in the benchmark. Our work can serve as a foundation for further in-depth\nexploration of the applications of GPT-family models in APR."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Ye Shang"
                    },
                    {
                        "name": "Guolin Xu"
                    },
                    {
                        "name": "Congqing He"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10825v1",
                "updated": "2024-09-17T01:37:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    57,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T01:37:57Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    57,
                    1,
                    261,
                    0
                ],
                "title": "Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based\n  Recommendations"
                },
                "summary": "Large Language Model (LLM)-based recommendation systems provide more\ncomprehensive recommendations than traditional systems by deeply analyzing\ncontent and user behavior. However, these systems often exhibit biases,\nfavoring mainstream content while marginalizing non-traditional options due to\nskewed training data. This study investigates the intricate relationship\nbetween bias and LLM-based recommendation systems, with a focus on music, song,\nand book recommendations across diverse demographic and cultural groups.\nThrough a comprehensive analysis conducted over different LLM-models, this\npaper evaluates the impact of bias on recommendation outcomes. Our findings\nreveal that bias is so deeply ingrained within these systems that even a\nsimpler intervention like prompt engineering can significantly reduce bias,\nunderscoring the pervasive nature of the issue. Moreover, factors like\nintersecting identities and contextual information, such as socioeconomic\nstatus, further amplify these biases, demonstrating the complexity and depth of\nthe challenges faced in creating fair recommendations across different groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based recommendation systems provide more\ncomprehensive recommendations than traditional systems by deeply analyzing\ncontent and user behavior. However, these systems often exhibit biases,\nfavoring mainstream content while marginalizing non-traditional options due to\nskewed training data. This study investigates the intricate relationship\nbetween bias and LLM-based recommendation systems, with a focus on music, song,\nand book recommendations across diverse demographic and cultural groups.\nThrough a comprehensive analysis conducted over different LLM-models, this\npaper evaluates the impact of bias on recommendation outcomes. Our findings\nreveal that bias is so deeply ingrained within these systems that even a\nsimpler intervention like prompt engineering can significantly reduce bias,\nunderscoring the pervasive nature of the issue. Moreover, factors like\nintersecting identities and contextual information, such as socioeconomic\nstatus, further amplify these biases, demonstrating the complexity and depth of\nthe challenges faced in creating fair recommendations across different groups."
                },
                "authors": [
                    {
                        "name": "Shahnewaz Karim Sakib"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    }
                ],
                "author_detail": {
                    "name": "Anindya Bijoy Das"
                },
                "author": "Anindya Bijoy Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07066v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07066v4",
                "updated": "2024-09-17T01:37:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    18,
                    1,
                    261,
                    0
                ],
                "published": "2024-04-10T14:56:40Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    14,
                    56,
                    40,
                    2,
                    101,
                    0
                ],
                "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?"
                },
                "summary": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07066v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07066v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09383v2",
                "updated": "2024-09-17T01:35:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    35,
                    25,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T09:21:46Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    9,
                    21,
                    46,
                    5,
                    258,
                    0
                ],
                "title": "LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free\n  Approach"
                },
                "summary": "We participated in the KDD CUP 2024 paper source tracing competition and\nachieved the 3rd place. This competition tasked participants with identifying\nthe reference sources (i.e., ref-sources, as referred to by the organizers of\nthe competition) of given academic papers. Unlike most teams that addressed\nthis challenge by fine-tuning pre-trained neural language models such as BERT\nor ChatGLM, our primary approach utilized closed-source large language models\n(LLMs). With recent advancements in LLM technology, closed-source LLMs have\ndemonstrated the capability to tackle complex reasoning tasks in zero-shot or\nfew-shot scenarios. Consequently, in the absence of GPUs, we employed\nclosed-source LLMs to directly generate predicted reference sources from the\nprovided papers. We further refined these predictions through ensemble\nlearning. Notably, our method was the only one among the award-winning\napproaches that did not require the use of GPUs for model training. Code\navailable at https://github.com/Cklwanfifa/KDDCUP2024-PST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We participated in the KDD CUP 2024 paper source tracing competition and\nachieved the 3rd place. This competition tasked participants with identifying\nthe reference sources (i.e., ref-sources, as referred to by the organizers of\nthe competition) of given academic papers. Unlike most teams that addressed\nthis challenge by fine-tuning pre-trained neural language models such as BERT\nor ChatGLM, our primary approach utilized closed-source large language models\n(LLMs). With recent advancements in LLM technology, closed-source LLMs have\ndemonstrated the capability to tackle complex reasoning tasks in zero-shot or\nfew-shot scenarios. Consequently, in the absence of GPUs, we employed\nclosed-source LLMs to directly generate predicted reference sources from the\nprovided papers. We further refined these predictions through ensemble\nlearning. Notably, our method was the only one among the award-winning\napproaches that did not require the use of GPUs for model training. Code\navailable at https://github.com/Cklwanfifa/KDDCUP2024-PST."
                },
                "authors": [
                    {
                        "name": "Kunlong Chen"
                    },
                    {
                        "name": "Junjun Wang"
                    },
                    {
                        "name": "Zhaoqun Chen"
                    },
                    {
                        "name": "Kunjin Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yitian Chen"
                },
                "author": "Yitian Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10797v1",
                "updated": "2024-09-17T00:27:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    0,
                    27,
                    19,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T00:27:19Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    0,
                    27,
                    19,
                    1,
                    261,
                    0
                ],
                "title": "ArticulatePro: A Comparative Study on a Proactive and Non-Proactive\n  Assistant in a Climate Data Exploration Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArticulatePro: A Comparative Study on a Proactive and Non-Proactive\n  Assistant in a Climate Data Exploration Task"
                },
                "summary": "Recent advances in Natural Language Interfaces (NLIs) and Large Language\nModels (LLMs) have transformed our approach to NLP tasks, allowing us to focus\nmore on a Pragmatics-based approach. This shift enables more natural\ninteractions between humans and voice assistants, which have been challenging\nto achieve. Pragmatics describes how users often talk out of turn, interrupt\neach other, or provide relevant information without being explicitly asked\n(maxim of quantity). To explore this, we developed a digital assistant that\nconstantly listens to conversations and proactively generates relevant\nvisualizations during data exploration tasks. In a within-subject study,\nparticipants interacted with both proactive and non-proactive versions of a\nvoice assistant while exploring the Hawaii Climate Data Portal (HCDP). Results\nsuggest that the proactive assistant enhanced user engagement and facilitated\nquicker insights. Our study highlights the potential of Pragmatic, proactive AI\nin NLIs and identifies key challenges in its implementation, offering insights\nfor future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Natural Language Interfaces (NLIs) and Large Language\nModels (LLMs) have transformed our approach to NLP tasks, allowing us to focus\nmore on a Pragmatics-based approach. This shift enables more natural\ninteractions between humans and voice assistants, which have been challenging\nto achieve. Pragmatics describes how users often talk out of turn, interrupt\neach other, or provide relevant information without being explicitly asked\n(maxim of quantity). To explore this, we developed a digital assistant that\nconstantly listens to conversations and proactively generates relevant\nvisualizations during data exploration tasks. In a within-subject study,\nparticipants interacted with both proactive and non-proactive versions of a\nvoice assistant while exploring the Hawaii Climate Data Portal (HCDP). Results\nsuggest that the proactive assistant enhanced user engagement and facilitated\nquicker insights. Our study highlights the potential of Pragmatic, proactive AI\nin NLIs and identifies key challenges in its implementation, offering insights\nfor future research."
                },
                "authors": [
                    {
                        "name": "Roderick Tabalba"
                    },
                    {
                        "name": "Christopher J. Lee"
                    },
                    {
                        "name": "Giorgio Tran"
                    },
                    {
                        "name": "Nurit Kirshenbaum"
                    },
                    {
                        "name": "Jason Leigh"
                    }
                ],
                "author_detail": {
                    "name": "Jason Leigh"
                },
                "author": "Jason Leigh",
                "arxiv_comment": "14 pages, 8 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10790v1",
                "updated": "2024-09-16T23:52:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    23,
                    52,
                    41,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T23:52:41Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    23,
                    52,
                    41,
                    0,
                    260,
                    0
                ],
                "title": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic\n  Attention Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic\n  Attention Steering"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious real-world tasks. However, they often struggle to fully comprehend and\neffectively utilize their input contexts, resulting in responses that are\nunfaithful or hallucinated. This difficulty increases for contexts that are\nlong or contain distracting information, which can divert LLMs from fully\ncapturing essential evidence. To address this issue, many works use prompting\nto help LLMs utilize contextual information more faithfully. For instance,\niterative prompting highlights key information in two steps that first ask the\nLLM to identify important pieces of context and then derive answers\naccordingly. However, prompting methods are constrained to highlighting key\ninformation implicitly in token space, which is often insufficient to fully\nsteer the model's attention. To improve model faithfulness more reliably, we\npropose AutoPASTA, a method that automatically identifies key contextual\ninformation and explicitly highlights it by steering an LLM's attention scores.\nLike prompting, AutoPASTA is applied at inference time and does not require\nchanging any model parameters. Our experiments on open-book QA demonstrate that\nAutoPASTA effectively enables models to grasp essential contextual information,\nleading to substantially improved model faithfulness and performance, e.g., an\naverage improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly\navailable at https://github.com/QingruZhang/AutoPASTA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious real-world tasks. However, they often struggle to fully comprehend and\neffectively utilize their input contexts, resulting in responses that are\nunfaithful or hallucinated. This difficulty increases for contexts that are\nlong or contain distracting information, which can divert LLMs from fully\ncapturing essential evidence. To address this issue, many works use prompting\nto help LLMs utilize contextual information more faithfully. For instance,\niterative prompting highlights key information in two steps that first ask the\nLLM to identify important pieces of context and then derive answers\naccordingly. However, prompting methods are constrained to highlighting key\ninformation implicitly in token space, which is often insufficient to fully\nsteer the model's attention. To improve model faithfulness more reliably, we\npropose AutoPASTA, a method that automatically identifies key contextual\ninformation and explicitly highlights it by steering an LLM's attention scores.\nLike prompting, AutoPASTA is applied at inference time and does not require\nchanging any model parameters. Our experiments on open-book QA demonstrate that\nAutoPASTA effectively enables models to grasp essential contextual information,\nleading to substantially improved model faithfulness and performance, e.g., an\naverage improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly\navailable at https://github.com/QingruZhang/AutoPASTA ."
                },
                "authors": [
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Chandan Singh"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Tuo Zhao"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Hao Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Cheng"
                },
                "author": "Hao Cheng",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10784v1",
                "updated": "2024-09-16T23:36:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    23,
                    36,
                    32,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T23:36:32Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    23,
                    36,
                    32,
                    0,
                    260,
                    0
                ],
                "title": "Benchmarking Sim2Real Gap: High-fidelity Digital Twinning of Agile\n  Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Sim2Real Gap: High-fidelity Digital Twinning of Agile\n  Manufacturing"
                },
                "summary": "As the manufacturing industry shifts from mass production to mass\ncustomization, there is a growing emphasis on adopting agile, resilient, and\nhuman-centric methodologies in line with the directives of Industry 5.0.\nCentral to this transformation is the deployment of digital twins, a technology\nthat digitally replicates manufacturing assets to enable enhanced process\noptimization, predictive maintenance, synthetic data generation, and\naccelerated customization and prototyping. This chapter delves into the\ntechnologies underpinning the creation of digital twins specifically tailored\nto agile manufacturing scenarios within the realm of robotic automation. It\nexplores the transfer of trained policies and process optimizations from\nsimulated settings to real-world applications through advanced techniques such\nas domain randomization, domain adaptation, curriculum learning, and\nmodel-based system identification. The chapter also examines various industrial\nmanufacturing automation scenarios, including bin-picking, part inspection, and\nproduct assembly, under Sim2Real conditions. The performance of digital twin\ntechnologies in these scenarios is evaluated using practical metrics including\ndata latency, adaptation rate, simulation fidelity among others reported,\nproviding a comprehensive assessment of their efficacy and potential impact on\nmodern manufacturing processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the manufacturing industry shifts from mass production to mass\ncustomization, there is a growing emphasis on adopting agile, resilient, and\nhuman-centric methodologies in line with the directives of Industry 5.0.\nCentral to this transformation is the deployment of digital twins, a technology\nthat digitally replicates manufacturing assets to enable enhanced process\noptimization, predictive maintenance, synthetic data generation, and\naccelerated customization and prototyping. This chapter delves into the\ntechnologies underpinning the creation of digital twins specifically tailored\nto agile manufacturing scenarios within the realm of robotic automation. It\nexplores the transfer of trained policies and process optimizations from\nsimulated settings to real-world applications through advanced techniques such\nas domain randomization, domain adaptation, curriculum learning, and\nmodel-based system identification. The chapter also examines various industrial\nmanufacturing automation scenarios, including bin-picking, part inspection, and\nproduct assembly, under Sim2Real conditions. The performance of digital twin\ntechnologies in these scenarios is evaluated using practical metrics including\ndata latency, adaptation rate, simulation fidelity among others reported,\nproviding a comprehensive assessment of their efficacy and potential impact on\nmodern manufacturing processes."
                },
                "authors": [
                    {
                        "name": "Sunny Katyara"
                    },
                    {
                        "name": "Suchita Sharma"
                    },
                    {
                        "name": "Praveen Damacharla"
                    },
                    {
                        "name": "Carlos Garcia Santiago"
                    },
                    {
                        "name": "Lubina Dhirani"
                    },
                    {
                        "name": "Bhawani Shankar Chowdhry"
                    }
                ],
                "author_detail": {
                    "name": "Bhawani Shankar Chowdhry"
                },
                "author": "Bhawani Shankar Chowdhry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10760v1",
                "updated": "2024-09-16T22:27:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    22,
                    27,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T22:27:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    22,
                    27,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Semantics Preserving Emoji Recommendation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantics Preserving Emoji Recommendation with Large Language Models"
                },
                "summary": "Emojis have become an integral part of digital communication, enriching text\nby conveying emotions, tone, and intent. Existing emoji recommendation methods\nare primarily evaluated based on their ability to match the exact emoji a user\nchooses in the original text. However, they ignore the essence of users'\nbehavior on social media in that each text can correspond to multiple\nreasonable emojis. To better assess a model's ability to align with such\nreal-world emoji usage, we propose a new semantics preserving evaluation\nframework for emoji recommendation, which measures a model's ability to\nrecommend emojis that maintain the semantic consistency with the user's text.\nTo evaluate how well a model preserves semantics, we assess whether the\npredicted affective state, demographic profile, and attitudinal stance of the\nuser remain unchanged. If these attributes are preserved, we consider the\nrecommended emojis to have maintained the original semantics. The advanced\nabilities of Large Language Models (LLMs) in understanding and generating\nnuanced, contextually relevant output make them well-suited for handling the\ncomplexities of semantics preserving emoji recommendation. To this end, we\nconstruct a comprehensive benchmark to systematically assess the performance of\nsix proprietary and open-source LLMs using different prompting techniques on\nour task. Our experiments demonstrate that GPT-4o outperforms other LLMs,\nachieving a semantics preservation score of 79.23%. Additionally, we conduct\ncase studies to analyze model biases in downstream classification tasks and\nevaluate the diversity of the recommended emojis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emojis have become an integral part of digital communication, enriching text\nby conveying emotions, tone, and intent. Existing emoji recommendation methods\nare primarily evaluated based on their ability to match the exact emoji a user\nchooses in the original text. However, they ignore the essence of users'\nbehavior on social media in that each text can correspond to multiple\nreasonable emojis. To better assess a model's ability to align with such\nreal-world emoji usage, we propose a new semantics preserving evaluation\nframework for emoji recommendation, which measures a model's ability to\nrecommend emojis that maintain the semantic consistency with the user's text.\nTo evaluate how well a model preserves semantics, we assess whether the\npredicted affective state, demographic profile, and attitudinal stance of the\nuser remain unchanged. If these attributes are preserved, we consider the\nrecommended emojis to have maintained the original semantics. The advanced\nabilities of Large Language Models (LLMs) in understanding and generating\nnuanced, contextually relevant output make them well-suited for handling the\ncomplexities of semantics preserving emoji recommendation. To this end, we\nconstruct a comprehensive benchmark to systematically assess the performance of\nsix proprietary and open-source LLMs using different prompting techniques on\nour task. Our experiments demonstrate that GPT-4o outperforms other LLMs,\nachieving a semantics preservation score of 79.23%. Additionally, we conduct\ncase studies to analyze model biases in downstream classification tasks and\nevaluate the diversity of the recommended emojis."
                },
                "authors": [
                    {
                        "name": "Zhongyi Qiu"
                    },
                    {
                        "name": "Kangyi Qiu"
                    },
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06413v2",
                "updated": "2024-09-16T22:05:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    22,
                    5,
                    56,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-09T16:03:26Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    16,
                    3,
                    26,
                    1,
                    100,
                    0
                ],
                "title": "Foundation Models to the Rescue: Deadlock Resolution in Connected\n  Multi-Robot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models to the Rescue: Deadlock Resolution in Connected\n  Multi-Robot Systems"
                },
                "summary": "Connected multi-agent robotic systems (MRS) are prone to deadlocks in an\nobstacle environment where the robots can get stuck away from their desired\nlocations under a smooth low-level control policy. Without an external\nintervention, often in terms of a high-level command, a low-level control\npolicy cannot resolve such deadlocks. Utilizing the generalizability and low\ndata requirements of foundation models, this paper explores the possibility of\nusing text-based models, i.e., large language models (LLMs), and\ntext-and-image-based models, i.e., vision-language models (VLMs), as high-level\nplanners for deadlock resolution. We propose a hierarchical control framework\nwhere a foundation model-based high-level planner helps to resolve deadlocks by\nassigning a leader to the MRS along with a set of waypoints for the MRS leader.\nThen, a low-level distributed control policy based on graph neural networks is\nexecuted to safely follow these waypoints, thereby evading the deadlock. We\nconduct extensive experiments on various MRS environments using the best\navailable pre-trained LLMs and VLMs. We compare their performance with a\ngraph-based planner in terms of effectiveness in helping the MRS reach their\ntarget locations and computational time. Our results illustrate that, compared\nto grid-based planners, the foundation models perform better in terms of the\ngoal-reaching rate and computational time for complex environments, which helps\nus conclude that foundation models can assist MRS operating in complex\nobstacle-cluttered environments to resolve deadlocks efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected multi-agent robotic systems (MRS) are prone to deadlocks in an\nobstacle environment where the robots can get stuck away from their desired\nlocations under a smooth low-level control policy. Without an external\nintervention, often in terms of a high-level command, a low-level control\npolicy cannot resolve such deadlocks. Utilizing the generalizability and low\ndata requirements of foundation models, this paper explores the possibility of\nusing text-based models, i.e., large language models (LLMs), and\ntext-and-image-based models, i.e., vision-language models (VLMs), as high-level\nplanners for deadlock resolution. We propose a hierarchical control framework\nwhere a foundation model-based high-level planner helps to resolve deadlocks by\nassigning a leader to the MRS along with a set of waypoints for the MRS leader.\nThen, a low-level distributed control policy based on graph neural networks is\nexecuted to safely follow these waypoints, thereby evading the deadlock. We\nconduct extensive experiments on various MRS environments using the best\navailable pre-trained LLMs and VLMs. We compare their performance with a\ngraph-based planner in terms of effectiveness in helping the MRS reach their\ntarget locations and computational time. Our results illustrate that, compared\nto grid-based planners, the foundation models perform better in terms of the\ngoal-reaching rate and computational time for complex environments, which helps\nus conclude that foundation models can assist MRS operating in complex\nobstacle-cluttered environments to resolve deadlocks efficiently."
                },
                "authors": [
                    {
                        "name": "Kunal Garg"
                    },
                    {
                        "name": "Songyuan Zhang"
                    },
                    {
                        "name": "Jacob Arkin"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10756v1",
                "updated": "2024-09-16T22:00:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    22,
                    0,
                    20,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T22:00:20Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    22,
                    0,
                    20,
                    0,
                    260,
                    0
                ],
                "title": "VulnLLMEval: A Framework for Evaluating Large Language Models in\n  Software Vulnerability Detection and Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VulnLLMEval: A Framework for Evaluating Large Language Models in\n  Software Vulnerability Detection and Patching"
                },
                "summary": "Large Language Models (LLMs) have shown promise in tasks like code\ntranslation, prompting interest in their potential for automating software\nvulnerability detection (SVD) and patching (SVP). To further research in this\narea, establishing a benchmark is essential for evaluating the strengths and\nlimitations of LLMs in these tasks. Despite their capabilities, questions\nremain regarding whether LLMs can accurately analyze complex vulnerabilities\nand generate appropriate patches. This paper introduces VulnLLMEval, a\nframework designed to assess the performance of LLMs in identifying and\npatching vulnerabilities in C code. Our study includes 307 real-world\nvulnerabilities extracted from the Linux kernel, creating a well-curated\ndataset that includes both vulnerable and patched code. This dataset, based on\nreal-world code, provides a diverse and representative testbed for evaluating\nLLM performance in SVD and SVP tasks, offering a robust foundation for rigorous\nassessment. Our results reveal that LLMs often struggle with distinguishing\nbetween vulnerable and patched code. Furthermore, in SVP tasks, these models\ntend to oversimplify the code, producing solutions that may not be directly\nusable without further refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in tasks like code\ntranslation, prompting interest in their potential for automating software\nvulnerability detection (SVD) and patching (SVP). To further research in this\narea, establishing a benchmark is essential for evaluating the strengths and\nlimitations of LLMs in these tasks. Despite their capabilities, questions\nremain regarding whether LLMs can accurately analyze complex vulnerabilities\nand generate appropriate patches. This paper introduces VulnLLMEval, a\nframework designed to assess the performance of LLMs in identifying and\npatching vulnerabilities in C code. Our study includes 307 real-world\nvulnerabilities extracted from the Linux kernel, creating a well-curated\ndataset that includes both vulnerable and patched code. This dataset, based on\nreal-world code, provides a diverse and representative testbed for evaluating\nLLM performance in SVD and SVP tasks, offering a robust foundation for rigorous\nassessment. Our results reveal that LLMs often struggle with distinguishing\nbetween vulnerable and patched code. Furthermore, in SVP tasks, these models\ntend to oversimplify the code, producing solutions that may not be directly\nusable without further refinement."
                },
                "authors": [
                    {
                        "name": "Arastoo Zibaeirad"
                    },
                    {
                        "name": "Marco Vieira"
                    }
                ],
                "author_detail": {
                    "name": "Marco Vieira"
                },
                "author": "Marco Vieira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14425v3",
                "updated": "2024-09-16T21:52:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    21,
                    52,
                    55,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-20T15:49:28Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    15,
                    49,
                    28,
                    3,
                    172,
                    0
                ],
                "title": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource\n  Languages"
                },
                "summary": "Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language."
                },
                "authors": [
                    {
                        "name": "Gayane Ghazaryan"
                    },
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10750v1",
                "updated": "2024-09-16T21:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    21,
                    45,
                    41,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T21:45:41Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    21,
                    45,
                    41,
                    0,
                    260,
                    0
                ],
                "title": "GPT takes the SAT: Tracing changes in Test Difficulty and Math\n  Performance of Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT takes the SAT: Tracing changes in Test Difficulty and Math\n  Performance of Students"
                },
                "summary": "Scholastic Aptitude Test (SAT) is crucial for college admissions but its\neffectiveness and relevance are increasingly questioned. This paper enhances\nSynthetic Control methods by introducing \"Transformed Control\", a novel method\nthat employs Large Language Models (LLMs) powered by Artificial Intelligence to\ngenerate control groups. We utilize OpenAI's API to generate a control group\nwhere GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023. This\ncontrol group helps analyze shifts in SAT math difficulty over time, starting\nfrom the baseline year of 2008. Using parallel trends, we calculate the Average\nDifference in Scores (ADS) to assess changes in high school students' math\nperformance. Our results indicate a significant decrease in the difficulty of\nthe SAT math section over time, alongside a decline in students' math\nperformance. The analysis shows a 71-point drop in the rigor of SAT math from\n2008 to 2023, with student performance decreasing by 36 points, resulting in a\n107-point total divergence in average student math performance. We investigate\npossible mechanisms for this decline in math proficiency, such as changing\nuniversity selection criteria, increased screen time, grade inflation, and\nworsening adolescent mental health. Disparities among demographic groups show a\n104-point drop for White students, 84 points for Black students, and 53 points\nfor Asian students. Male students saw a 117-point reduction, while female\nstudents had a 100-point decrease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scholastic Aptitude Test (SAT) is crucial for college admissions but its\neffectiveness and relevance are increasingly questioned. This paper enhances\nSynthetic Control methods by introducing \"Transformed Control\", a novel method\nthat employs Large Language Models (LLMs) powered by Artificial Intelligence to\ngenerate control groups. We utilize OpenAI's API to generate a control group\nwhere GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023. This\ncontrol group helps analyze shifts in SAT math difficulty over time, starting\nfrom the baseline year of 2008. Using parallel trends, we calculate the Average\nDifference in Scores (ADS) to assess changes in high school students' math\nperformance. Our results indicate a significant decrease in the difficulty of\nthe SAT math section over time, alongside a decline in students' math\nperformance. The analysis shows a 71-point drop in the rigor of SAT math from\n2008 to 2023, with student performance decreasing by 36 points, resulting in a\n107-point total divergence in average student math performance. We investigate\npossible mechanisms for this decline in math proficiency, such as changing\nuniversity selection criteria, increased screen time, grade inflation, and\nworsening adolescent mental health. Disparities among demographic groups show a\n104-point drop for White students, 84 points for Black students, and 53 points\nfor Asian students. Male students saw a 117-point reduction, while female\nstudents had a 100-point decrease."
                },
                "authors": [
                    {
                        "name": "Vikram Krishnaveti"
                    },
                    {
                        "name": "Saannidhya Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Saannidhya Rawat"
                },
                "author": "Saannidhya Rawat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]