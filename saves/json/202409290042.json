[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v2",
                "updated": "2024-09-25T06:46:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    46,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v1",
                "updated": "2024-09-22T08:30:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS 2024 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.18128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18128v1",
                "updated": "2024-09-26T17:59:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    51,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:59:51Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    51,
                    3,
                    270,
                    0
                ],
                "title": "FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity\n  Refiner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity\n  Refiner"
                },
                "summary": "Building on the success of diffusion models in visual generation, flow-based\nmodels reemerge as another prominent family of generative models that have\nachieved competitive or better performance in terms of both visual quality and\ninference speed. By learning the velocity field through flow-matching,\nflow-based models tend to produce a straighter sampling trajectory, which is\nadvantageous during the sampling process. However, unlike diffusion models for\nwhich fast samplers are well-developed, efficient sampling of flow-based\ngenerative models has been rarely explored. In this paper, we propose a\nframework called FlowTurbo to accelerate the sampling of flow-based models\nwhile still enhancing the sampling quality. Our primary observation is that the\nvelocity predictor's outputs in the flow-based models will become stable during\nthe sampling, enabling the estimation of velocity via a lightweight velocity\nrefiner. Additionally, we introduce several techniques including a pseudo\ncorrector and sample-aware compilation to further reduce inference time. Since\nFlowTurbo does not change the multi-step sampling paradigm, it can be\neffectively applied for various tasks such as image editing, inpainting, etc.\nBy integrating FlowTurbo into different flow-based models, we obtain an\nacceleration ratio of 53.1%$\\sim$58.3% on class-conditional generation and\n29.8%$\\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),\nachieving the real-time image generation and establishing the new\nstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the success of diffusion models in visual generation, flow-based\nmodels reemerge as another prominent family of generative models that have\nachieved competitive or better performance in terms of both visual quality and\ninference speed. By learning the velocity field through flow-matching,\nflow-based models tend to produce a straighter sampling trajectory, which is\nadvantageous during the sampling process. However, unlike diffusion models for\nwhich fast samplers are well-developed, efficient sampling of flow-based\ngenerative models has been rarely explored. In this paper, we propose a\nframework called FlowTurbo to accelerate the sampling of flow-based models\nwhile still enhancing the sampling quality. Our primary observation is that the\nvelocity predictor's outputs in the flow-based models will become stable during\nthe sampling, enabling the estimation of velocity via a lightweight velocity\nrefiner. Additionally, we introduce several techniques including a pseudo\ncorrector and sample-aware compilation to further reduce inference time. Since\nFlowTurbo does not change the multi-step sampling paradigm, it can be\neffectively applied for various tasks such as image editing, inpainting, etc.\nBy integrating FlowTurbo into different flow-based models, we obtain an\nacceleration ratio of 53.1%$\\sim$58.3% on class-conditional generation and\n29.8%$\\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),\nachieving the real-time image generation and establishing the new\nstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo."
                },
                "authors": [
                    {
                        "name": "Wenliang Zhao"
                    },
                    {
                        "name": "Minglei Shi"
                    },
                    {
                        "name": "Xumin Yu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18127v1",
                "updated": "2024-09-26T17:59:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    31,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    31,
                    3,
                    270,
                    0
                ],
                "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoLM: Multi-Modal Language Model of Egocentric Motions"
                },
                "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."
                },
                "authors": [
                    {
                        "name": "Fangzhou Hong"
                    },
                    {
                        "name": "Vladimir Guzov"
                    },
                    {
                        "name": "Hyo Jin Kim"
                    },
                    {
                        "name": "Yuting Ye"
                    },
                    {
                        "name": "Richard Newcombe"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Lingni Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lingni Ma"
                },
                "author": "Lingni Ma",
                "arxiv_comment": "Project Page: https://hongfz16.github.io/projects/EgoLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18124v1",
                "updated": "2024-09-26T17:58:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    58,
                    55,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:58:55Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    58,
                    55,
                    3,
                    270,
                    0
                ],
                "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction"
                },
                "summary": "Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also significantly enhances efficiency,\nbeing hundreds of times faster than most existing diffusion-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also significantly enhances efficiency,\nbeing hundreds of times faster than most existing diffusion-based methods."
                },
                "authors": [
                    {
                        "name": "Jing He"
                    },
                    {
                        "name": "Haodong Li"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Yixun Liang"
                    },
                    {
                        "name": "Leheng Li"
                    },
                    {
                        "name": "Kaiqiang Zhou"
                    },
                    {
                        "name": "Hongbo Liu"
                    },
                    {
                        "name": "Bingbing Liu"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Project page: https://lotus3d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13387v2",
                "updated": "2024-09-26T17:55:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    55,
                    48,
                    3,
                    270,
                    0
                ],
                "published": "2023-10-20T09:56:07Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    9,
                    56,
                    7,
                    4,
                    293,
                    0
                ],
                "title": "Assumption violations in causal discovery and the robustness of score\n  matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption violations in causal discovery and the robustness of score\n  matching"
                },
                "summary": "When domain knowledge is limited and experimentation is restricted by\nethical, financial, or time constraints, practitioners turn to observational\ncausal discovery methods to recover the causal structure, exploiting the\nstatistical properties of their data. Because causal discovery without further\nassumptions is an ill-posed problem, each algorithm comes with its own set of\nusually untestable assumptions, some of which are hard to meet in real\ndatasets. Motivated by these considerations, this paper extensively benchmarks\nthe empirical performance of recent causal discovery methods on observational\ni.i.d. data generated under different background conditions, allowing for\nviolations of the critical assumptions required by each selected approach. Our\nexperimental findings show that score matching-based methods demonstrate\nsurprising performance in the false positive and false negative rate of the\ninferred graph in these challenging scenarios, and we provide theoretical\ninsights into their performance. This work is also the first effort to\nbenchmark the stability of causal discovery algorithms with respect to the\nvalues of their hyperparameters. Finally, we hope this paper will set a new\nstandard for the evaluation of causal discovery methods and can serve as an\naccessible entry point for practitioners interested in the field, highlighting\nthe empirical implications of different algorithm choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When domain knowledge is limited and experimentation is restricted by\nethical, financial, or time constraints, practitioners turn to observational\ncausal discovery methods to recover the causal structure, exploiting the\nstatistical properties of their data. Because causal discovery without further\nassumptions is an ill-posed problem, each algorithm comes with its own set of\nusually untestable assumptions, some of which are hard to meet in real\ndatasets. Motivated by these considerations, this paper extensively benchmarks\nthe empirical performance of recent causal discovery methods on observational\ni.i.d. data generated under different background conditions, allowing for\nviolations of the critical assumptions required by each selected approach. Our\nexperimental findings show that score matching-based methods demonstrate\nsurprising performance in the false positive and false negative rate of the\ninferred graph in these challenging scenarios, and we provide theoretical\ninsights into their performance. This work is also the first effort to\nbenchmark the stability of causal discovery algorithms with respect to the\nvalues of their hyperparameters. Finally, we hope this paper will set a new\nstandard for the evaluation of causal discovery methods and can serve as an\naccessible entry point for practitioners interested in the field, highlighting\nthe empirical implications of different algorithm choices."
                },
                "authors": [
                    {
                        "name": "Francesco Montagna"
                    },
                    {
                        "name": "Atalanti A. Mastakouri"
                    },
                    {
                        "name": "Elias Eulig"
                    },
                    {
                        "name": "Nicoletta Noceti"
                    },
                    {
                        "name": "Lorenzo Rosasco"
                    },
                    {
                        "name": "Dominik Janzing"
                    },
                    {
                        "name": "Bryon Aragam"
                    },
                    {
                        "name": "Francesco Locatello"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Locatello"
                },
                "author": "Francesco Locatello",
                "arxiv_comment": "37th Conference on Neural Information Processing Systems (NeurIPS\n  2023)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.13387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18111v1",
                "updated": "2024-09-26T17:53:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    53,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:53:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    53,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18102v1",
                "updated": "2024-09-26T17:45:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    45,
                    10,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:45:10Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    45,
                    10,
                    3,
                    270,
                    0
                ],
                "title": "MALPOLON: A Framework for Deep Species Distribution Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALPOLON: A Framework for Deep Species Distribution Modeling"
                },
                "summary": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and\nbuilt upon the PyTorch library, this framework aims to facilitate training and\ninferences of deep species distribution models (deep-SDM) and sharing for users\nwith only general Python language skills (e.g., modeling ecologists) who are\ninterested in testing deep learning approaches to build new SDMs. More advanced\nusers can also benefit from the framework's modularity to run more specific\nexperiments by overriding existing classes while taking advantage of\npress-button examples to train neural networks on multiple classification tasks\nusing custom or provided raw and pre-processed datasets. The framework is\nopen-sourced on GitHub and PyPi along with extensive documentation and examples\nof use in various scenarios. MALPOLON offers straightforward installation,\nYAML-based configuration, parallel computing, multi-GPU utilization, baseline\nand foundational models for benchmarking, and extensive\ntutorials/documentation, aiming to enhance accessibility and performance\nscalability for ecologists and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and\nbuilt upon the PyTorch library, this framework aims to facilitate training and\ninferences of deep species distribution models (deep-SDM) and sharing for users\nwith only general Python language skills (e.g., modeling ecologists) who are\ninterested in testing deep learning approaches to build new SDMs. More advanced\nusers can also benefit from the framework's modularity to run more specific\nexperiments by overriding existing classes while taking advantage of\npress-button examples to train neural networks on multiple classification tasks\nusing custom or provided raw and pre-processed datasets. The framework is\nopen-sourced on GitHub and PyPi along with extensive documentation and examples\nof use in various scenarios. MALPOLON offers straightforward installation,\nYAML-based configuration, parallel computing, multi-GPU utilization, baseline\nand foundational models for benchmarking, and extensive\ntutorials/documentation, aiming to enhance accessibility and performance\nscalability for ecologists and researchers."
                },
                "authors": [
                    {
                        "name": "Theo Larcher"
                    },
                    {
                        "name": "Lukas Picek"
                    },
                    {
                        "name": "Benjamin Deneu"
                    },
                    {
                        "name": "Titouan Lorieul"
                    },
                    {
                        "name": "Maximilien Servajean"
                    },
                    {
                        "name": "Alexis Joly"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Joly"
                },
                "author": "Alexis Joly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18084v1",
                "updated": "2024-09-26T17:27:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    27,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:27:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    27,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSON: A Group-based Social Navigation Framework with Large Multimodal\n  Model"
                },
                "summary": "As the number of service robots and autonomous vehicles in human-centered\nenvironments grows, their requirements go beyond simply navigating to a\ndestination. They must also take into account dynamic social contexts and\nensure respect and comfort for others in shared spaces, which poses significant\nchallenges for perception and planning. In this paper, we present a group-based\nsocial navigation framework GSON to enable mobile robots to perceive and\nexploit the social group of their surroundings by leveling the visual reasoning\ncapability of the Large Multimodal Model (LMM). For perception, we apply visual\nprompting techniques to zero-shot extract the social relationship among\npedestrians and combine the result with a robust pedestrian detection and\ntracking pipeline to alleviate the problem of low inference speed of the LMM.\nGiven the perception result, the planning system is designed to avoid\ndisrupting the current social structure. We adopt a social structure-based\nmid-level planner as a bridge between global path planning and local motion\nplanning to preserve the global context and reactive response. The proposed\nmethod is validated on real-world mobile robot navigation tasks involving\ncomplex social structure understanding and reasoning. Experimental results\ndemonstrate the effectiveness of the system in these scenarios compared with\nseveral baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the number of service robots and autonomous vehicles in human-centered\nenvironments grows, their requirements go beyond simply navigating to a\ndestination. They must also take into account dynamic social contexts and\nensure respect and comfort for others in shared spaces, which poses significant\nchallenges for perception and planning. In this paper, we present a group-based\nsocial navigation framework GSON to enable mobile robots to perceive and\nexploit the social group of their surroundings by leveling the visual reasoning\ncapability of the Large Multimodal Model (LMM). For perception, we apply visual\nprompting techniques to zero-shot extract the social relationship among\npedestrians and combine the result with a robust pedestrian detection and\ntracking pipeline to alleviate the problem of low inference speed of the LMM.\nGiven the perception result, the planning system is designed to avoid\ndisrupting the current social structure. We adopt a social structure-based\nmid-level planner as a bridge between global path planning and local motion\nplanning to preserve the global context and reactive response. The proposed\nmethod is validated on real-world mobile robot navigation tasks involving\ncomplex social structure understanding and reasoning. Experimental results\ndemonstrate the effectiveness of the system in these scenarios compared with\nseveral baselines."
                },
                "authors": [
                    {
                        "name": "Shangyi Luo"
                    },
                    {
                        "name": "Ji Zhu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Yuhong Deng"
                    },
                    {
                        "name": "Cunjun Yu"
                    },
                    {
                        "name": "Anxing Xiao"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18073v1",
                "updated": "2024-09-26T17:19:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    19,
                    49,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:19:49Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    19,
                    49,
                    3,
                    270,
                    0
                ],
                "title": "Infer Human's Intentions Before Following Natural Language Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infer Human's Intentions Before Following Natural Language Instructions"
                },
                "summary": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Yiping Wang"
                    },
                    {
                        "name": "Jiayuan Mao"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18060v1",
                "updated": "2024-09-26T17:01:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    1,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:01:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    1,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Infering Alt-text For UI Icons With Large Language Models During App\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infering Alt-text For UI Icons With Large Language Models During App\n  Development"
                },
                "summary": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility."
                },
                "authors": [
                    {
                        "name": "Sabrina Haque"
                    },
                    {
                        "name": "Christoph Csallner"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Csallner"
                },
                "author": "Christoph Csallner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18053v1",
                "updated": "2024-09-26T16:58:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    58,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:58:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    58,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving"
                },
                "summary": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. We make code and\nbenchmarks publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. We make code and\nbenchmarks publicly available."
                },
                "authors": [
                    {
                        "name": "Dingrui Wang"
                    },
                    {
                        "name": "Marc Kaufeld"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "Autonomous Driving, Large Language Models (LLMs), Human Reasoning,\n  Critical Scenario",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08168v3",
                "updated": "2024-09-26T16:51:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    51,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-13T14:27:45Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    14,
                    27,
                    45,
                    2,
                    347,
                    0
                ],
                "title": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object\n  Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object\n  Identifiers"
                },
                "summary": "Recent advancements in 3D Large Language Models (LLMs) have demonstrated\npromising capabilities for 3D scene understanding. However, previous methods\nexhibit deficiencies in general referencing and grounding capabilities for\nintricate scene comprehension. In this paper, we introduce the use of object\nidentifiers and object-centric representations to interact with scenes at the\nobject level. Specifically, we decompose the input 3D scene into a set of\nobject proposals, each assigned a unique identifier token, which enables\nefficient object referencing and grounding during user-assistant interactions.\nGiven the scarcity of scene-language data, we model the scene embeddings as a\nsequence of explicit object-level embeddings, derived from semantic-rich 2D or\n3D representations. By employing object identifiers, we transform diverse 3D\nscene-language tasks into a unified question-answering format, facilitating\njoint training without the need for additional task-specific heads. With\nminimal fine-tuning on all downstream tasks, our model significantly\noutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 3D Large Language Models (LLMs) have demonstrated\npromising capabilities for 3D scene understanding. However, previous methods\nexhibit deficiencies in general referencing and grounding capabilities for\nintricate scene comprehension. In this paper, we introduce the use of object\nidentifiers and object-centric representations to interact with scenes at the\nobject level. Specifically, we decompose the input 3D scene into a set of\nobject proposals, each assigned a unique identifier token, which enables\nefficient object referencing and grounding during user-assistant interactions.\nGiven the scarcity of scene-language data, we model the scene embeddings as a\nsequence of explicit object-level embeddings, derived from semantic-rich 2D or\n3D representations. By employing object identifiers, we transform diverse 3D\nscene-language tasks into a unified question-answering format, facilitating\njoint training without the need for additional task-specific heads. With\nminimal fine-tuning on all downstream tasks, our model significantly\noutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D."
                },
                "authors": [
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Rongjie Huang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Luping Liu"
                    },
                    {
                        "name": "Xize Cheng"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18046v1",
                "updated": "2024-09-26T16:47:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    47,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:47:32Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    47,
                    32,
                    3,
                    270,
                    0
                ],
                "title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for\n  Zero-shot Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for\n  Zero-shot Captioning"
                },
                "summary": "Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training."
                },
                "authors": [
                    {
                        "name": "Soeun Lee"
                    },
                    {
                        "name": "Si-Woo Kim"
                    },
                    {
                        "name": "Taewhan Kim"
                    },
                    {
                        "name": "Dong-Jin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong-Jin Kim"
                },
                "author": "Dong-Jin Kim",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16626v2",
                "updated": "2024-09-26T16:41:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    41,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T05:11:58Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    11,
                    58,
                    2,
                    269,
                    0
                ],
                "title": "Ascend HiFloat8 Format for Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascend HiFloat8 Format for Deep Learning"
                },
                "summary": "This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponent values with 3-bit\nmantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with\n1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7\nextra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).\nMeanwhile, HiF8 encodes all the special values except that positive zero and\nnegative zero are represented by only one bit-pattern. Thanks to the better\nbalance between precision and dynamic range, HiF8 can be simultaneously used in\nboth forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8, massive\nsimulation results on various neural networks, including traditional neural\nnetworks and large language models (LLMs), will also be presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponent values with 3-bit\nmantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with\n1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7\nextra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).\nMeanwhile, HiF8 encodes all the special values except that positive zero and\nnegative zero are represented by only one bit-pattern. Thanks to the better\nbalance between precision and dynamic range, HiF8 can be simultaneously used in\nboth forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8, massive\nsimulation results on various neural networks, including traditional neural\nnetworks and large language models (LLMs), will also be presented."
                },
                "authors": [
                    {
                        "name": "Yuanyong Luo"
                    },
                    {
                        "name": "Zhongxing Zhang"
                    },
                    {
                        "name": "Richard Wu"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Ying Jin"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Minmin Wang"
                    },
                    {
                        "name": "Zhanying He"
                    },
                    {
                        "name": "Guipeng Hu"
                    },
                    {
                        "name": "Luyao Chen"
                    },
                    {
                        "name": "Tianchi Hu"
                    },
                    {
                        "name": "Junsong Wang"
                    },
                    {
                        "name": "Minqi Chen"
                    },
                    {
                        "name": "Mikhaylov Dmitry"
                    },
                    {
                        "name": "Korviakov Vladimir"
                    },
                    {
                        "name": "Bobrin Maxim"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Guanfu Chen"
                    },
                    {
                        "name": "Zeyi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zeyi Huang"
                },
                "author": "Zeyi Huang",
                "arxiv_comment": "13 Pages, 4 Figures, 9 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13731v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13731v3",
                "updated": "2024-09-26T16:34:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-10T02:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    0,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
                },
                "summary": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods."
                },
                "authors": [
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "Zhongshu Zhu"
                    },
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "Zhongpu Bo"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Huaidong Xiong"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Zaoyang Wang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wenguang Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13731v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13731v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18028v1",
                "updated": "2024-09-26T16:34:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:34:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective"
                },
                "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18026v1",
                "updated": "2024-09-26T16:33:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    33,
                    16,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:33:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    33,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "ReliOcc: Towards Reliable Semantic Occupancy Prediction via Uncertainty\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReliOcc: Towards Reliable Semantic Occupancy Prediction via Uncertainty\n  Learning"
                },
                "summary": "Vision-centric semantic occupancy prediction plays a crucial role in\nautonomous driving, which requires accurate and reliable predictions from\nlow-cost sensors. Although having notably narrowed the accuracy gap with LiDAR,\nthere is still few research effort to explore the reliability in predicting\nsemantic occupancy from camera. In this paper, we conduct a comprehensive\nevaluation of existing semantic occupancy prediction models from a reliability\nperspective for the first time. Despite the gradual alignment of camera-based\nmodels with LiDAR in term of accuracy, a significant reliability gap persists.\nTo addresses this concern, we propose ReliOcc, a method designed to enhance the\nreliability of camera-based occupancy networks. ReliOcc provides a\nplug-and-play scheme for existing models, which integrates hybrid uncertainty\nfrom individual voxels with sampling-based noise and relative voxels through\nmix-up learning. Besides, an uncertainty-aware calibration strategy is devised\nto further enhance model reliability in offline mode. Extensive experiments\nunder various settings demonstrate that ReliOcc significantly enhances model\nreliability while maintaining the accuracy of both geometric and semantic\npredictions. Importantly, our proposed approach exhibits robustness to sensor\nfailures and out of domain noises during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-centric semantic occupancy prediction plays a crucial role in\nautonomous driving, which requires accurate and reliable predictions from\nlow-cost sensors. Although having notably narrowed the accuracy gap with LiDAR,\nthere is still few research effort to explore the reliability in predicting\nsemantic occupancy from camera. In this paper, we conduct a comprehensive\nevaluation of existing semantic occupancy prediction models from a reliability\nperspective for the first time. Despite the gradual alignment of camera-based\nmodels with LiDAR in term of accuracy, a significant reliability gap persists.\nTo addresses this concern, we propose ReliOcc, a method designed to enhance the\nreliability of camera-based occupancy networks. ReliOcc provides a\nplug-and-play scheme for existing models, which integrates hybrid uncertainty\nfrom individual voxels with sampling-based noise and relative voxels through\nmix-up learning. Besides, an uncertainty-aware calibration strategy is devised\nto further enhance model reliability in offline mode. Extensive experiments\nunder various settings demonstrate that ReliOcc significantly enhances model\nreliability while maintaining the accuracy of both geometric and semantic\npredictions. Importantly, our proposed approach exhibits robustness to sensor\nfailures and out of domain noises during inference."
                },
                "authors": [
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Zhongdao Wang"
                    },
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Bailan Feng"
                    },
                    {
                        "name": "Junbo Chen"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "Technical report. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18014v1",
                "updated": "2024-09-26T16:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    22,
                    59,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    22,
                    59,
                    3,
                    270,
                    0
                ],
                "title": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning\n  for Distinct LLMs in Their Optimal Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning\n  for Distinct LLMs in Their Optimal Roles"
                },
                "summary": "Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL."
                },
                "authors": [
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Pengran Huang"
                    },
                    {
                        "name": "Bingzhi Chen"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Jiahui Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jiahui Pan"
                },
                "author": "Jiahui Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10420v3",
                "updated": "2024-09-26T16:22:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    22,
                    18,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-16T11:21:12Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    21,
                    12,
                    5,
                    350,
                    0
                ],
                "title": "Satisfiability Modulo Theories for Verifying MILP Certificates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories for Verifying MILP Certificates"
                },
                "summary": "Correctness of results from mixed-integer linear programming (MILP) solvers\nis critical, particularly in the context of applications such as hardware\nverification, compiler optimization, or machine-assisted theorem proving. To\nthis end, VIPR 1.0 is the first recently proposed general certificate format\nfor answers produced by MILP solvers. We design a schema to encode VIPR's\ninference rules as a ground formula that completely characterizes the validity\nof the algorithmic check, removing any ambiguities and imprecisions present in\nthe specification. We implement a checker for VIPR certificates by expressing\nour ground formula with the Satisfiability Modulo Theory Library (SMT-LIB) and\ncheck its validity. Our approach is solver-agnostic, and we test its viability\nusing benchmark instances found in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness of results from mixed-integer linear programming (MILP) solvers\nis critical, particularly in the context of applications such as hardware\nverification, compiler optimization, or machine-assisted theorem proving. To\nthis end, VIPR 1.0 is the first recently proposed general certificate format\nfor answers produced by MILP solvers. We design a schema to encode VIPR's\ninference rules as a ground formula that completely characterizes the validity\nof the algorithmic check, removing any ambiguities and imprecisions present in\nthe specification. We implement a checker for VIPR certificates by expressing\nour ground formula with the Satisfiability Modulo Theory Library (SMT-LIB) and\ncheck its validity. Our approach is solver-agnostic, and we test its viability\nusing benchmark instances found in the literature."
                },
                "authors": [
                    {
                        "name": "Kenan Wood"
                    },
                    {
                        "name": "Runtian Zhou"
                    },
                    {
                        "name": "Haoze Wu"
                    },
                    {
                        "name": "Hammurabi Mendes"
                    },
                    {
                        "name": "Jonad Pulaj"
                    }
                ],
                "author_detail": {
                    "name": "Jonad Pulaj"
                },
                "author": "Jonad Pulaj",
                "arxiv_comment": "Redesigned and optimized the transformation to a ground formula.\n  Reimplemented the transformation in a more functional style of programming.\n  Expanded the experiments. Improved exposition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18009v1",
                "updated": "2024-09-26T16:19:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    19,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:19:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    19,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Control Industrial Automation System with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Industrial Automation System with Large Language Models"
                },
                "summary": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS"
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Nasser Jazdi"
                    },
                    {
                        "name": "Jize Zhang"
                    },
                    {
                        "name": "Chaitanya Shah"
                    },
                    {
                        "name": "Michael Weyrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Weyrich"
                },
                "author": "Michael Weyrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18007v1",
                "updated": "2024-09-26T16:17:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    17,
                    9,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:17:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    17,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "Alchemical harmonic approximation based potential for all iso-electronic\n  diatomics: Foundational baseline for $Δ$-machine learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemical harmonic approximation based potential for all iso-electronic\n  diatomics: Foundational baseline for $Δ$-machine learning"
                },
                "summary": "We introduce the alchemical harmonic approximation (AHA) of the absolute\nelectronic energy for all charge-neutral iso-electronic diatomics at some fixed\ninteratomic distance $d_0$. To account for variations in this distance, we\ncombine AHA with the following Ansatz for the electronic binding potential,\n$E(d)=(E_{u}-E_s)\\left(\\frac{E_c-E_s}{E_u-E_s} \\right)^{\\sqrt{d/d_0}}+E_s$,\nwhere $E_u$, $E_c$, and $E_s$ correspond to the energies of the united atom,\ncalibration at $d_0$, and sum of infinitely separated atoms, respectively. For\nany number of electrons, our model covers the entire two-dimensional electronic\npotential energy surface spanned by distance and difference in nuclear charge.\nUsing data from pbe0/ccpvdz as reference, we present numerical evidence for all\nneutral diatomics with 8, 10, 12, 14 electrons. We assess the validity of our\nmodel by comparison to legacy potentials (Harm. osc., Lennard-Jones, Morse)\nwithin the most relevant range of binding (0.7 - 2.5 A), and find comparable\naccuracy if restricted to one diatomic, and significantly better when\nextrapolating to the entire iso-electronic series. We have also investigated\n$\\Delta$-learning with our model as baseline. For any given iso-electronic\ncharge neutral diatomic surface, this baseline results in a systematic\nimprovement, effectively reducing training data for reaching chemical accuracy\nby up to an order of magnitude from $\\sim1000$ to $\\sim100$. By contrast and\nwith respect to direct learning, using AHA+Morse as a baseline hardly leads to\nany improvement, and sometimes even deteriorates predictive power. Direct\nKRR-based extrapolation throughout chemical space converges to an error of\n$\\sim$0.1 Ha when inferring the energy of unseen CO after training on all other\niso-electronic diatomics. Our model as a baseline (calibrated to the energy of\nBF at $d_0 = 1.2$ A) lowers the error to $\\sim$0.04 Ha.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the alchemical harmonic approximation (AHA) of the absolute\nelectronic energy for all charge-neutral iso-electronic diatomics at some fixed\ninteratomic distance $d_0$. To account for variations in this distance, we\ncombine AHA with the following Ansatz for the electronic binding potential,\n$E(d)=(E_{u}-E_s)\\left(\\frac{E_c-E_s}{E_u-E_s} \\right)^{\\sqrt{d/d_0}}+E_s$,\nwhere $E_u$, $E_c$, and $E_s$ correspond to the energies of the united atom,\ncalibration at $d_0$, and sum of infinitely separated atoms, respectively. For\nany number of electrons, our model covers the entire two-dimensional electronic\npotential energy surface spanned by distance and difference in nuclear charge.\nUsing data from pbe0/ccpvdz as reference, we present numerical evidence for all\nneutral diatomics with 8, 10, 12, 14 electrons. We assess the validity of our\nmodel by comparison to legacy potentials (Harm. osc., Lennard-Jones, Morse)\nwithin the most relevant range of binding (0.7 - 2.5 A), and find comparable\naccuracy if restricted to one diatomic, and significantly better when\nextrapolating to the entire iso-electronic series. We have also investigated\n$\\Delta$-learning with our model as baseline. For any given iso-electronic\ncharge neutral diatomic surface, this baseline results in a systematic\nimprovement, effectively reducing training data for reaching chemical accuracy\nby up to an order of magnitude from $\\sim1000$ to $\\sim100$. By contrast and\nwith respect to direct learning, using AHA+Morse as a baseline hardly leads to\nany improvement, and sometimes even deteriorates predictive power. Direct\nKRR-based extrapolation throughout chemical space converges to an error of\n$\\sim$0.1 Ha when inferring the energy of unseen CO after training on all other\niso-electronic diatomics. Our model as a baseline (calibrated to the energy of\nBF at $d_0 = 1.2$ A) lowers the error to $\\sim$0.04 Ha."
                },
                "authors": [
                    {
                        "name": "Simon León Krug"
                    },
                    {
                        "name": "Danish Khan"
                    },
                    {
                        "name": "O. Anatole von Lilienfeld"
                    }
                ],
                "author_detail": {
                    "name": "O. Anatole von Lilienfeld"
                },
                "author": "O. Anatole von Lilienfeld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18006v1",
                "updated": "2024-09-26T16:15:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    15,
                    14,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:15:14Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    15,
                    14,
                    3,
                    270,
                    0
                ],
                "title": "Multilingual Evaluation of Long Context Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Evaluation of Long Context Retrieval and Reasoning"
                },
                "summary": "Recent large language models (LLMs) demonstrate impressive capabilities in\nhandling long contexts, some exhibiting near-perfect recall on synthetic\nretrieval tasks. However, these evaluations have mainly focused on English text\nand involved a single target sentence within lengthy contexts. Our work\ninvestigates how LLM performance generalizes to multilingual settings with\nmultiple hidden target sentences. We comprehensively evaluate several\nlong-context LLMs on retrieval and reasoning tasks across five languages:\nEnglish, Vietnamese, Indonesian, Swahili, and Somali. These languages share the\nLatin script but belong to distinct language families and resource levels. Our\nanalysis reveals a significant performance gap between languages. The\nbest-performing models such as Gemini-1.5 and GPT-4o, achieve around 96%\naccuracy in English to around 36% in Somali with a single target sentence.\nHowever, this accuracy drops to 40% in English and 0% in Somali when dealing\nwith three target sentences. Our findings highlight the challenges long-context\nLLMs face when processing longer contexts, an increase in the number of target\nsentences, or languages of lower resource levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate impressive capabilities in\nhandling long contexts, some exhibiting near-perfect recall on synthetic\nretrieval tasks. However, these evaluations have mainly focused on English text\nand involved a single target sentence within lengthy contexts. Our work\ninvestigates how LLM performance generalizes to multilingual settings with\nmultiple hidden target sentences. We comprehensively evaluate several\nlong-context LLMs on retrieval and reasoning tasks across five languages:\nEnglish, Vietnamese, Indonesian, Swahili, and Somali. These languages share the\nLatin script but belong to distinct language families and resource levels. Our\nanalysis reveals a significant performance gap between languages. The\nbest-performing models such as Gemini-1.5 and GPT-4o, achieve around 96%\naccuracy in English to around 36% in Somali with a single target sentence.\nHowever, this accuracy drops to 40% in English and 0% in Somali when dealing\nwith three target sentences. Our findings highlight the challenges long-context\nLLMs face when processing longer contexts, an increase in the number of target\nsentences, or languages of lower resource levels."
                },
                "authors": [
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "Andy Dang"
                    },
                    {
                        "name": "Sina Bagheri Nezhad"
                    },
                    {
                        "name": "Rhitabrat Pokharel"
                    },
                    {
                        "name": "Russell Scheinberg"
                    }
                ],
                "author_detail": {
                    "name": "Russell Scheinberg"
                },
                "author": "Russell Scheinberg",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18003v1",
                "updated": "2024-09-26T16:12:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    12,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:12:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    12,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Enhancing Tourism Recommender Systems for Sustainable City Trips Using\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Tourism Recommender Systems for Sustainable City Trips Using\n  Retrieval-Augmented Generation"
                },
                "summary": "Tourism Recommender Systems (TRS) have traditionally focused on providing\npersonalized travel suggestions, often prioritizing user preferences without\nconsidering broader sustainability goals. Integrating sustainability into TRS\nhas become essential with the increasing need to balance environmental impact,\nlocal community interests, and visitor satisfaction. This paper proposes a\nnovel approach to enhancing TRS for sustainable city trips using Large Language\nModels (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We\nenhance the traditional RAG system by incorporating a sustainability metric\nbased on a city's popularity and seasonal demand during the prompt augmentation\nphase. This modification, called Sustainability Augmented Reranking (SAR),\nensures the system's recommendations align with sustainability goals.\nEvaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and\nMistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently\nmatches or outperforms the baseline (without SAR) across most metrics,\nhighlighting the benefits of incorporating sustainability into TRS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tourism Recommender Systems (TRS) have traditionally focused on providing\npersonalized travel suggestions, often prioritizing user preferences without\nconsidering broader sustainability goals. Integrating sustainability into TRS\nhas become essential with the increasing need to balance environmental impact,\nlocal community interests, and visitor satisfaction. This paper proposes a\nnovel approach to enhancing TRS for sustainable city trips using Large Language\nModels (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We\nenhance the traditional RAG system by incorporating a sustainability metric\nbased on a city's popularity and seasonal demand during the prompt augmentation\nphase. This modification, called Sustainability Augmented Reranking (SAR),\nensures the system's recommendations align with sustainability goals.\nEvaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and\nMistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently\nmatches or outperforms the baseline (without SAR) across most metrics,\nhighlighting the benefits of incorporating sustainability into TRS."
                },
                "authors": [
                    {
                        "name": "Ashmi Banerjee"
                    },
                    {
                        "name": "Adithi Satish"
                    },
                    {
                        "name": "Wolfgang Wörndl"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Wörndl"
                },
                "author": "Wolfgang Wörndl",
                "arxiv_comment": "Accepted at the RecSoGood 2024 Workshop co-located with the 18th ACM\n  Conference on Recommender Systems (RecSys 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17990v1",
                "updated": "2024-09-26T16:02:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:02:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models"
                },
                "summary": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We validate our estimates against representative British survey\ndata and find strong positive, significant correlations for several collective\nemotions. The obtained estimates are robust across multiple training seeds and\nprompt formulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. To the best of our\nknowledge, this is the first work to extend the analysis of affect in LLMs to a\nlongitudinal setting through Temporal Adapters. Our work enables new approaches\ntowards the longitudinal analysis of social media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We validate our estimates against representative British survey\ndata and find strong positive, significant correlations for several collective\nemotions. The obtained estimates are robust across multiple training seeds and\nprompt formulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. To the best of our\nknowledge, this is the first work to extend the analysis of affect in LLMs to a\nlongitudinal setting through Temporal Adapters. Our work enables new approaches\ntowards the longitudinal analysis of social media data."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Max Pellert"
                    },
                    {
                        "name": "David Garcia"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "arxiv_comment": "Code available at https://github.com/dess-mannheim/temporal-adapters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17987v1",
                "updated": "2024-09-26T15:57:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    57,
                    8,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:57:08Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    57,
                    8,
                    3,
                    270,
                    0
                ],
                "title": "LLM4Brain: Training a Large Language Model for Brain Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Brain: Training a Large Language Model for Brain Video Understanding"
                },
                "summary": "Decoding visual-semantic information from brain signals, such as functional\nMRI (fMRI), across different subjects poses significant challenges, including\nlow signal-to-noise ratio, limited data availability, and cross-subject\nvariability. Recent advancements in large language models (LLMs) show\nremarkable effectiveness in processing multimodal information. In this study,\nwe introduce an LLM-based approach for reconstructing visual-semantic\ninformation from fMRI signals elicited by video stimuli. Specifically, we\nemploy fine-tuning techniques on an fMRI encoder equipped with adaptors to\ntransform brain responses into latent representations aligned with the video\nstimuli. Subsequently, these representations are mapped to textual modality by\nLLM. In particular, we integrate self-supervised domain adaptation methods to\nenhance the alignment between visual-semantic information and brain responses.\nOur proposed method achieves good results using various quantitative semantic\nmetrics, while yielding similarity with ground-truth information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding visual-semantic information from brain signals, such as functional\nMRI (fMRI), across different subjects poses significant challenges, including\nlow signal-to-noise ratio, limited data availability, and cross-subject\nvariability. Recent advancements in large language models (LLMs) show\nremarkable effectiveness in processing multimodal information. In this study,\nwe introduce an LLM-based approach for reconstructing visual-semantic\ninformation from fMRI signals elicited by video stimuli. Specifically, we\nemploy fine-tuning techniques on an fMRI encoder equipped with adaptors to\ntransform brain responses into latent representations aligned with the video\nstimuli. Subsequently, these representations are mapped to textual modality by\nLLM. In particular, we integrate self-supervised domain adaptation methods to\nenhance the alignment between visual-semantic information and brain responses.\nOur proposed method achieves good results using various quantitative semantic\nmetrics, while yielding similarity with ground-truth information."
                },
                "authors": [
                    {
                        "name": "Ruizhe Zheng"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "ECCV2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16427v2",
                "updated": "2024-09-26T15:56:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    56,
                    8,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T19:47:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    19,
                    47,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI\n  Interactions"
                },
                "summary": "AI agents are increasingly autonomous in their interactions with human users\nand tools, leading to increased interactional safety risks. We present\nHAICOSYSTEM, a framework examining AI agent safety within diverse and complex\nsocial interactions. HAICOSYSTEM features a modular sandbox environment that\nsimulates multi-turn interactions between human users and AI agents, where the\nAI agents are equipped with a variety of tools (e.g., patient management\nplatforms) to navigate diverse scenarios (e.g., a user attempting to access\nother patients' profiles). To examine the safety of AI agents in these\ninteractions, we develop a comprehensive multi-dimensional evaluation framework\nthat uses metrics covering operational, content-related, societal, and legal\nrisks. Through running 1840 simulations based on 92 scenarios across seven\ndomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM\ncan emulate realistic user-AI interactions and complex tool use by AI agents.\nOur experiments show that state-of-the-art LLMs, both proprietary and\nopen-sourced, exhibit safety risks in over 50\\% cases, with models generally\nshowing higher risks when interacting with simulated malicious users. Our\nfindings highlight the ongoing challenge of building agents that can safely\nnavigate complex interactions, particularly when faced with malicious users. To\nfoster the AI agent safety ecosystem, we release a code platform that allows\npractitioners to create custom scenarios, simulate interactions, and evaluate\nthe safety and performance of their agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly autonomous in their interactions with human users\nand tools, leading to increased interactional safety risks. We present\nHAICOSYSTEM, a framework examining AI agent safety within diverse and complex\nsocial interactions. HAICOSYSTEM features a modular sandbox environment that\nsimulates multi-turn interactions between human users and AI agents, where the\nAI agents are equipped with a variety of tools (e.g., patient management\nplatforms) to navigate diverse scenarios (e.g., a user attempting to access\nother patients' profiles). To examine the safety of AI agents in these\ninteractions, we develop a comprehensive multi-dimensional evaluation framework\nthat uses metrics covering operational, content-related, societal, and legal\nrisks. Through running 1840 simulations based on 92 scenarios across seven\ndomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM\ncan emulate realistic user-AI interactions and complex tool use by AI agents.\nOur experiments show that state-of-the-art LLMs, both proprietary and\nopen-sourced, exhibit safety risks in over 50\\% cases, with models generally\nshowing higher risks when interacting with simulated malicious users. Our\nfindings highlight the ongoing challenge of building agents that can safely\nnavigate complex interactions, particularly when faced with malicious users. To\nfoster the AI agent safety ecosystem, we release a code platform that allows\npractitioners to create custom scenarios, simulate interactions, and evaluate\nthe safety and performance of their agents."
                },
                "authors": [
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Frank Xu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Both the second and third authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17975v1",
                "updated": "2024-09-26T15:50:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    50,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:50:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    50,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "Simulation-Based Inference Benchmark for LSST Weak Lensing Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-Based Inference Benchmark for LSST Weak Lensing Cosmology"
                },
                "summary": "Standard cosmological analysis, which relies on two-point statistics, fails\nto extract the full information of the data. This limits our ability to\nconstrain with precision cosmological parameters. Thus, recent years have seen\na paradigm shift from analytical likelihood-based to simulation-based\ninference. However, such methods require a large number of costly simulations.\nWe focus on full-field inference, considered the optimal form of inference. Our\nobjective is to benchmark several ways of conducting full-field inference to\ngain insight into the number of simulations required for each method. We make a\ndistinction between explicit and implicit full-field inference. Moreover, as it\nis crucial for explicit full-field inference to use a differentiable forward\nmodel, we aim to discuss the advantages of having this property for the\nimplicit approach. We use the sbi_lens package which provides a fast and\ndifferentiable log-normal forward model. This forward model enables us to\ncompare explicit and implicit full-field inference with and without gradient.\nThe former is achieved by sampling the forward model through the No U-Turns\nsampler. The latter starts by compressing the data into sufficient statistics\nand uses the Neural Likelihood Estimation algorithm and the one augmented with\ngradient. We perform a full-field analysis on LSST Y10 like weak lensing\nsimulated mass maps. We show that explicit and implicit full-field inference\nyield consistent constraints. Explicit inference requires 630 000 simulations\nwith our particular sampler corresponding to 400 independent samples. Implicit\ninference requires a maximum of 101 000 simulations split into 100 000\nsimulations to build sufficient statistics (this number is not fine tuned) and\n1 000 simulations to perform inference. Additionally, we show that our way of\nexploiting the gradients does not significantly help implicit inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard cosmological analysis, which relies on two-point statistics, fails\nto extract the full information of the data. This limits our ability to\nconstrain with precision cosmological parameters. Thus, recent years have seen\na paradigm shift from analytical likelihood-based to simulation-based\ninference. However, such methods require a large number of costly simulations.\nWe focus on full-field inference, considered the optimal form of inference. Our\nobjective is to benchmark several ways of conducting full-field inference to\ngain insight into the number of simulations required for each method. We make a\ndistinction between explicit and implicit full-field inference. Moreover, as it\nis crucial for explicit full-field inference to use a differentiable forward\nmodel, we aim to discuss the advantages of having this property for the\nimplicit approach. We use the sbi_lens package which provides a fast and\ndifferentiable log-normal forward model. This forward model enables us to\ncompare explicit and implicit full-field inference with and without gradient.\nThe former is achieved by sampling the forward model through the No U-Turns\nsampler. The latter starts by compressing the data into sufficient statistics\nand uses the Neural Likelihood Estimation algorithm and the one augmented with\ngradient. We perform a full-field analysis on LSST Y10 like weak lensing\nsimulated mass maps. We show that explicit and implicit full-field inference\nyield consistent constraints. Explicit inference requires 630 000 simulations\nwith our particular sampler corresponding to 400 independent samples. Implicit\ninference requires a maximum of 101 000 simulations split into 100 000\nsimulations to build sufficient statistics (this number is not fine tuned) and\n1 000 simulations to perform inference. Additionally, we show that our way of\nexploiting the gradients does not significantly help implicit inference."
                },
                "authors": [
                    {
                        "name": "Justine Zeghal"
                    },
                    {
                        "name": "Denise Lanzieri"
                    },
                    {
                        "name": "François Lanusse"
                    },
                    {
                        "name": "Alexandre Boucaud"
                    },
                    {
                        "name": "Gilles Louppe"
                    },
                    {
                        "name": "Eric Aubourg"
                    },
                    {
                        "name": "Adrian E. Bayer"
                    },
                    {
                        "name": "The LSST Dark Energy Science Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The LSST Dark Energy Science Collaboration"
                },
                "author": "The LSST Dark Energy Science Collaboration",
                "arxiv_comment": "20 pages, 16 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17972v1",
                "updated": "2024-09-26T15:47:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    47,
                    42,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:47:42Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    47,
                    42,
                    3,
                    270,
                    0
                ],
                "title": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and\n  Adaptive Disambiguate based Efficient Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and\n  Adaptive Disambiguate based Efficient Tree Search"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nbroad range of tasks and domains. However, they still encounter difficulties in\nsolving mathematical problems due to the rigorous and logical nature of\nmathematics. Previous studies have employed techniques such as supervised\nfine-tuning (SFT), prompt engineering, and search-based methods to improve the\nmathematical problem-solving abilities of LLMs. Despite these efforts, their\nperformance remains suboptimal and demands substantial computational resources.\nTo address this issue, we propose a novel approach, BEATS, to enhance\nmathematical problem-solving abilities. Our method leverages newly designed\nprompts that guide the model to iteratively rewrite, advance by one step, and\ngenerate answers based on previous steps. Additionally, we introduce a new\nback-verification technique that uses LLMs to validate the correctness of the\ngenerated answers. Furthermore, we employ a pruning tree search to optimize\nsearch time while achieving strong performance. Notably, our method improves\nQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the\nMATH benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nbroad range of tasks and domains. However, they still encounter difficulties in\nsolving mathematical problems due to the rigorous and logical nature of\nmathematics. Previous studies have employed techniques such as supervised\nfine-tuning (SFT), prompt engineering, and search-based methods to improve the\nmathematical problem-solving abilities of LLMs. Despite these efforts, their\nperformance remains suboptimal and demands substantial computational resources.\nTo address this issue, we propose a novel approach, BEATS, to enhance\nmathematical problem-solving abilities. Our method leverages newly designed\nprompts that guide the model to iteratively rewrite, advance by one step, and\ngenerate answers based on previous steps. Additionally, we introduce a new\nback-verification technique that uses LLMs to validate the correctness of the\ngenerated answers. Furthermore, we employ a pruning tree search to optimize\nsearch time while achieving strong performance. Notably, our method improves\nQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the\nMATH benchmark."
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14950v2",
                "updated": "2024-09-26T15:45:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    45,
                    13,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-08T15:57:18Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    57,
                    18,
                    4,
                    342,
                    0
                ],
                "title": "TypeFly: Flying Drones with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TypeFly: Flying Drones with Large Language Model"
                },
                "summary": "Recent advancements in robot control using large language models (LLMs) have\ndemonstrated significant potential, primarily due to LLMs' capabilities to\nunderstand natural language commands and generate executable plans in various\nlanguages. However, in real-time and interactive applications involving mobile\nrobots, particularly drones, the sequential token generation process inherent\nto LLMs introduces substantial latency, i.e. response time, in control plan\ngeneration.\n  In this paper, we present a system called ChatFly that tackles this problem\nusing a combination of a novel programming language called MiniSpec and its\nruntime to reduce the plan generation time and drone response time. That is,\ninstead of asking an LLM to write a program (robotic plan) in the popular but\nverbose Python, ChatFly gets it to do it in MiniSpec specially designed for\ntoken efficiency and stream interpretation. Using a set of challenging drone\ntasks, we show that design choices made by ChatFly can reduce up to 62%\nresponse time and provide a more consistent user experience, enabling\nresponsive and intelligent LLM-based drone control with efficient completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in robot control using large language models (LLMs) have\ndemonstrated significant potential, primarily due to LLMs' capabilities to\nunderstand natural language commands and generate executable plans in various\nlanguages. However, in real-time and interactive applications involving mobile\nrobots, particularly drones, the sequential token generation process inherent\nto LLMs introduces substantial latency, i.e. response time, in control plan\ngeneration.\n  In this paper, we present a system called ChatFly that tackles this problem\nusing a combination of a novel programming language called MiniSpec and its\nruntime to reduce the plan generation time and drone response time. That is,\ninstead of asking an LLM to write a program (robotic plan) in the popular but\nverbose Python, ChatFly gets it to do it in MiniSpec specially designed for\ntoken efficiency and stream interpretation. Using a set of challenging drone\ntasks, we show that design choices made by ChatFly can reduce up to 62%\nresponse time and provide a more consistent user experience, enabling\nresponsive and intelligent LLM-based drone control with efficient completion."
                },
                "authors": [
                    {
                        "name": "Guojun Chen"
                    },
                    {
                        "name": "Xiaojing Yu"
                    },
                    {
                        "name": "Neiwen Ling"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17968v1",
                "updated": "2024-09-26T15:44:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    44,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:44:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    44,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Nonparametric Inference Framework for Time-dependent Epidemic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Inference Framework for Time-dependent Epidemic Models"
                },
                "summary": "Compartmental models, especially the Susceptible-Infected-Removed (SIR)\nmodel, have long been used to understand the behaviour of various diseases.\nAllowing parameters, such as the transmission rate, to be time-dependent\nfunctions makes it possible to adjust for and make inferences about changes in\nthe process due to mitigation strategies or evolutionary changes of the\ninfectious agent. In this article, we attempt to build a nonparametric\ninference framework for stochastic SIR models with time dependent infection\nrate. The framework includes three main steps: likelihood approximation,\nparameter estimation and confidence interval construction. The likelihood\nfunction of the stochastic SIR model, which is often intractable, can be\napproximated using methods such as diffusion approximation or tau leaping. The\ninfection rate is modelled by a B-spline basis whose knot location and number\nof knots are determined by a fast knot placement method followed by a\ncriterion-based model selection procedure. Finally, a point-wise confidence\ninterval is built using a parametric bootstrap procedure. The performance of\nthe framework is observed through various settings for different epidemic\npatterns. The model is then applied to the Ontario COVID-19 data across\nmultiple waves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compartmental models, especially the Susceptible-Infected-Removed (SIR)\nmodel, have long been used to understand the behaviour of various diseases.\nAllowing parameters, such as the transmission rate, to be time-dependent\nfunctions makes it possible to adjust for and make inferences about changes in\nthe process due to mitigation strategies or evolutionary changes of the\ninfectious agent. In this article, we attempt to build a nonparametric\ninference framework for stochastic SIR models with time dependent infection\nrate. The framework includes three main steps: likelihood approximation,\nparameter estimation and confidence interval construction. The likelihood\nfunction of the stochastic SIR model, which is often intractable, can be\napproximated using methods such as diffusion approximation or tau leaping. The\ninfection rate is modelled by a B-spline basis whose knot location and number\nof knots are determined by a fast knot placement method followed by a\ncriterion-based model selection procedure. Finally, a point-wise confidence\ninterval is built using a parametric bootstrap procedure. The performance of\nthe framework is observed through various settings for different epidemic\npatterns. The model is then applied to the Ontario COVID-19 data across\nmultiple waves."
                },
                "authors": [
                    {
                        "name": "Son Luu"
                    },
                    {
                        "name": "Edward Susko"
                    },
                    {
                        "name": "Lam Si Tung Ho"
                    }
                ],
                "author_detail": {
                    "name": "Lam Si Tung Ho"
                },
                "author": "Lam Si Tung Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17946v1",
                "updated": "2024-09-26T15:20:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    20,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:20:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    20,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge\n  Distillation"
                },
                "summary": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on contrastive knowledge distillation\n(W2SAttack). Specifically, we poison small-scale language models through\nfull-parameter fine-tuning to serve as the teacher model. The teacher model\nthen covertly transfers the backdoor to the large-scale student model through\ncontrastive knowledge distillation, which employs PEFT. Theoretical analysis\nreveals that W2SAttack has the potential to augment the effectiveness of\nbackdoor attacks. We demonstrate the superior performance of W2SAttack on\nclassification tasks across four language models, four backdoor attack\nalgorithms, and two different architectures of teacher models. Experimental\nresults indicate success rates close to 100% for backdoor attacks targeting\nPEFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on contrastive knowledge distillation\n(W2SAttack). Specifically, we poison small-scale language models through\nfull-parameter fine-tuning to serve as the teacher model. The teacher model\nthen covertly transfers the backdoor to the large-scale student model through\ncontrastive knowledge distillation, which employs PEFT. Theoretical analysis\nreveals that W2SAttack has the potential to augment the effectiveness of\nbackdoor attacks. We demonstrate the superior performance of W2SAttack on\nclassification tasks across four language models, four backdoor attack\nalgorithms, and two different architectures of teacher models. Experimental\nresults indicate success rates close to 100% for backdoor attacks targeting\nPEFT."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17937v1",
                "updated": "2024-09-26T15:12:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    12,
                    41,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:12:41Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    12,
                    41,
                    3,
                    270,
                    0
                ],
                "title": "Adaptive Stream Processing on Edge Devices through Active Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Stream Processing on Edge Devices through Active Inference"
                },
                "summary": "The current scenario of IoT is witnessing a constant increase on the volume\nof data, which is generated in constant stream, calling for novel architectural\nand logical solutions for processing it. Moving the data handling towards the\nedge of the computing spectrum guarantees better distribution of load and, in\nprinciple, lower latency and better privacy. However, managing such a structure\nis complex, especially when requirements, also referred to Service Level\nObjectives (SLOs), specified by applications' owners and infrastructure\nmanagers need to be ensured. Despite the rich number of proposals of Machine\nLearning (ML) based management solutions, researchers and practitioners yet\nstruggle to guarantee long-term prediction and control, and accurate\ntroubleshooting. Therefore, we present a novel ML paradigm based on Active\nInference (AIF) -- a concept from neuroscience that describes how the brain\nconstantly predicts and evaluates sensory information to decrease long-term\nsurprise. We implement it and evaluate it in a heterogeneous real stream\nprocessing use case, where an AIF-based agent continuously optimizes the\nfulfillment of three SLOs for three autonomous driving services running on\nmultiple devices. The agent used causal knowledge to gradually develop an\nunderstanding of how its actions are related to requirements fulfillment, and\nwhich configurations to favor. Through this approach, our agent requires up to\nthirty iterations to converge to the optimal solution, showing the capability\nof offering accurate results in a short amount of time. Furthermore, thanks to\nAIF and its causal structures, our method guarantees full transparency on the\ndecision making, making the interpretation of the results and the\ntroubleshooting effortless.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current scenario of IoT is witnessing a constant increase on the volume\nof data, which is generated in constant stream, calling for novel architectural\nand logical solutions for processing it. Moving the data handling towards the\nedge of the computing spectrum guarantees better distribution of load and, in\nprinciple, lower latency and better privacy. However, managing such a structure\nis complex, especially when requirements, also referred to Service Level\nObjectives (SLOs), specified by applications' owners and infrastructure\nmanagers need to be ensured. Despite the rich number of proposals of Machine\nLearning (ML) based management solutions, researchers and practitioners yet\nstruggle to guarantee long-term prediction and control, and accurate\ntroubleshooting. Therefore, we present a novel ML paradigm based on Active\nInference (AIF) -- a concept from neuroscience that describes how the brain\nconstantly predicts and evaluates sensory information to decrease long-term\nsurprise. We implement it and evaluate it in a heterogeneous real stream\nprocessing use case, where an AIF-based agent continuously optimizes the\nfulfillment of three SLOs for three autonomous driving services running on\nmultiple devices. The agent used causal knowledge to gradually develop an\nunderstanding of how its actions are related to requirements fulfillment, and\nwhich configurations to favor. Through this approach, our agent requires up to\nthirty iterations to converge to the optimal solution, showing the capability\nof offering accurate results in a short amount of time. Furthermore, thanks to\nAIF and its causal structures, our method guarantees full transparency on the\ndecision making, making the interpretation of the results and the\ntroubleshooting effortless."
                },
                "authors": [
                    {
                        "name": "Boris Sedlak"
                    },
                    {
                        "name": "Victor Casamayor Pujol"
                    },
                    {
                        "name": "Andrea Morichetta"
                    },
                    {
                        "name": "Praveen Kumar Donta"
                    },
                    {
                        "name": "Schahram Dustdar"
                    }
                ],
                "author_detail": {
                    "name": "Schahram Dustdar"
                },
                "author": "Schahram Dustdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17915v1",
                "updated": "2024-09-26T15:01:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    1,
                    31,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:01:31Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    1,
                    31,
                    3,
                    270,
                    0
                ],
                "title": "N-dimensional maximum-entropy tomography via particle sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-dimensional maximum-entropy tomography via particle sampling"
                },
                "summary": "This paper proposes an alternative implementation of the MENT algorithm, an\nexact maximum-entropy algorithm used to infer a phase space distribution from\nits projections. A key step in the MENT algorithm is to compute the\ndistribution's projections via numerical integration. In this approach, the run\ntime scales quickly with the phase space dimension and measurement resolution.\nThe proposed MENT implementation computes the projections via particle\nsampling, rather than numerical integration, eliminating the dependence on the\nmeasurement resolution. Furthermore, with the appropriate sampling algorithm,\nthe particle-based approach scales to $N$-dimensions without computer memory\nlimitations. Using synthetic data, we demonstrate MENT convergence in four\ndimensions using a grid-based sampling method and in six dimensions using\nMarkov Chain Monte Carlo (MCMC) sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an alternative implementation of the MENT algorithm, an\nexact maximum-entropy algorithm used to infer a phase space distribution from\nits projections. A key step in the MENT algorithm is to compute the\ndistribution's projections via numerical integration. In this approach, the run\ntime scales quickly with the phase space dimension and measurement resolution.\nThe proposed MENT implementation computes the projections via particle\nsampling, rather than numerical integration, eliminating the dependence on the\nmeasurement resolution. Furthermore, with the appropriate sampling algorithm,\nthe particle-based approach scales to $N$-dimensions without computer memory\nlimitations. Using synthetic data, we demonstrate MENT convergence in four\ndimensions using a grid-based sampling method and in six dimensions using\nMarkov Chain Monte Carlo (MCMC) sampling."
                },
                "authors": [
                    {
                        "name": "Austin Hoover"
                    }
                ],
                "author_detail": {
                    "name": "Austin Hoover"
                },
                "author": "Austin Hoover",
                "arxiv_comment": "7 pages, 2 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15228v3",
                "updated": "2024-09-26T14:57:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    57,
                    52,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T17:22:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    22,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs."
                },
                "authors": [
                    {
                        "name": "Yixi Wu"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17912v1",
                "updated": "2024-09-26T14:56:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    56,
                    38,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:56:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    56,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan\n  Arabic Dialect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan\n  Arabic Dialect"
                },
                "summary": "We introduce Atlas-Chat, the first-ever collection of large language models\nspecifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also\nknown as Darija, we construct our instruction dataset by consolidating existing\nDarija language resources, creating novel datasets both manually and\nsynthetically, and translating English instructions with stringent quality\ncontrol. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit\nsuperior ability in following Darija instructions and performing standard NLP\ntasks. Notably, our models outperform both state-of-the-art and\nArabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13%\nperformance boost over a larger 13B model on DarijaMMLU, in our newly\nintroduced evaluation suite for Darija covering both discriminative and\ngenerative tasks. Furthermore, we perform an experimental analysis of various\nfine-tuning strategies and base model choices to determine optimal\nconfigurations. All our resources are publicly accessible, and we believe our\nwork offers comprehensive design methodologies of instruction-tuning for\nlow-resource language variants, which are often neglected in favor of data-rich\nlanguages by contemporary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Atlas-Chat, the first-ever collection of large language models\nspecifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also\nknown as Darija, we construct our instruction dataset by consolidating existing\nDarija language resources, creating novel datasets both manually and\nsynthetically, and translating English instructions with stringent quality\ncontrol. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit\nsuperior ability in following Darija instructions and performing standard NLP\ntasks. Notably, our models outperform both state-of-the-art and\nArabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13%\nperformance boost over a larger 13B model on DarijaMMLU, in our newly\nintroduced evaluation suite for Darija covering both discriminative and\ngenerative tasks. Furthermore, we perform an experimental analysis of various\nfine-tuning strategies and base model choices to determine optimal\nconfigurations. All our resources are publicly accessible, and we believe our\nwork offers comprehensive design methodologies of instruction-tuning for\nlow-resource language variants, which are often neglected in favor of data-rich\nlanguages by contemporary LLMs."
                },
                "authors": [
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Yousef Khoubrane"
                    },
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Yassine Abbahaddou"
                    },
                    {
                        "name": "Sofiane Ennadir"
                    },
                    {
                        "name": "Imane Momayiz"
                    },
                    {
                        "name": "Xuguang Ren"
                    },
                    {
                        "name": "Eric Moulines"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17906v1",
                "updated": "2024-09-26T14:52:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    52,
                    40,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:52:40Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    52,
                    40,
                    3,
                    270,
                    0
                ],
                "title": "Graph Reasoning with Large Language Models via Pseudo-code Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Reasoning with Large Language Models via Pseudo-code Prompting"
                },
                "summary": "Large language models (LLMs) have recently achieved remarkable success in\nvarious reasoning tasks in the field of natural language processing. This\nsuccess of LLMs has also motivated their use in graph-related tasks. Among\nothers, recent work has explored whether LLMs can solve graph problems such as\ncounting the number of connected components of a graph or computing the\nshortest path distance between two nodes. Although LLMs possess preliminary\ngraph reasoning abilities, they might still struggle to solve some seemingly\nsimple problems. In this paper, we investigate whether prompting via\npseudo-code instructions can improve the performance of LLMs in solving graph\nproblems. Our experiments demonstrate that using pseudo-code instructions\ngenerally improves the performance of all considered LLMs. The graphs,\npseudo-code prompts, and evaluation code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved remarkable success in\nvarious reasoning tasks in the field of natural language processing. This\nsuccess of LLMs has also motivated their use in graph-related tasks. Among\nothers, recent work has explored whether LLMs can solve graph problems such as\ncounting the number of connected components of a graph or computing the\nshortest path distance between two nodes. Although LLMs possess preliminary\ngraph reasoning abilities, they might still struggle to solve some seemingly\nsimple problems. In this paper, we investigate whether prompting via\npseudo-code instructions can improve the performance of LLMs in solving graph\nproblems. Our experiments demonstrate that using pseudo-code instructions\ngenerally improves the performance of all considered LLMs. The graphs,\npseudo-code prompts, and evaluation code are publicly available."
                },
                "authors": [
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17904v1",
                "updated": "2024-09-26T14:51:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    51,
                    40,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:51:40Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    51,
                    40,
                    3,
                    270,
                    0
                ],
                "title": "Learning to Love Edge Cases in Formative Math Assessment: Using the\n  AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Love Edge Cases in Formative Math Assessment: Using the\n  AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy"
                },
                "summary": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment."
                },
                "authors": [
                    {
                        "name": "Owen Henkel"
                    },
                    {
                        "name": "Hannah Horne-Robinson"
                    },
                    {
                        "name": "Maria Dyshel"
                    },
                    {
                        "name": "Nabil Ch"
                    },
                    {
                        "name": "Baptiste Moreau-Pernet"
                    },
                    {
                        "name": "Ralph Abood"
                    }
                ],
                "author_detail": {
                    "name": "Ralph Abood"
                },
                "author": "Ralph Abood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11193v2",
                "updated": "2024-09-26T14:48:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    48,
                    53,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-20T20:59:38Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    20,
                    59,
                    38,
                    1,
                    233,
                    0
                ],
                "title": "Inference with Many Weak Instruments and Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Many Weak Instruments and Heterogeneity"
                },
                "summary": "This paper considers inference in a linear instrumental variable regression\nmodel with many potentially weak instruments and heterogeneous treatment\neffects. I first show that existing test procedures, including those that are\nrobust to only either weak instruments or heterogeneous treatment effects, can\nbe arbitrarily oversized in this setup. Then, I propose a valid inference\nprocedure based on a score statistic and a leave-three-out variance estimator.\nTo establish this procedure's validity, this paper proves that the score\nstatistic is asymptotically normal and the variance estimator is consistent.\nThe power of the score test is also close to a power envelope in an empirical\napplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers inference in a linear instrumental variable regression\nmodel with many potentially weak instruments and heterogeneous treatment\neffects. I first show that existing test procedures, including those that are\nrobust to only either weak instruments or heterogeneous treatment effects, can\nbe arbitrarily oversized in this setup. Then, I propose a valid inference\nprocedure based on a score statistic and a leave-three-out variance estimator.\nTo establish this procedure's validity, this paper proves that the score\nstatistic is asymptotically normal and the variance estimator is consistent.\nThe power of the score test is also close to a power envelope in an empirical\napplication."
                },
                "authors": [
                    {
                        "name": "Luther Yap"
                    }
                ],
                "author_detail": {
                    "name": "Luther Yap"
                },
                "author": "Luther Yap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18789v2",
                "updated": "2024-09-26T14:48:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    48,
                    42,
                    3,
                    270,
                    0
                ],
                "published": "2024-07-26T14:52:37Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    52,
                    37,
                    4,
                    208,
                    0
                ],
                "title": "Granularity is crucial when applying differential privacy to text: An\n  investigation for neural machine translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granularity is crucial when applying differential privacy to text: An\n  investigation for neural machine translation"
                },
                "summary": "Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g., those including dialogues. For proper\napplication of DP we thus must shift from sentences to entire documents. In\nthis paper, we investigate NMT at both the sentence and document levels,\nanalyzing the privacy/utility trade-off for both scenarios, and evaluating the\nrisks of not using the appropriate privacy granularity in terms of leaking\npersonally identifiable information (PII). Our findings indicate that the\ndocument-level NMT system is more resistant to membership inference attacks,\nemphasizing the significance of using the appropriate granularity when working\nwith DP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g., those including dialogues. For proper\napplication of DP we thus must shift from sentences to entire documents. In\nthis paper, we investigate NMT at both the sentence and document levels,\nanalyzing the privacy/utility trade-off for both scenarios, and evaluating the\nrisks of not using the appropriate privacy granularity in terms of leaking\npersonally identifiable information (PII). Our findings indicate that the\ndocument-level NMT system is more resistant to membership inference attacks,\nemphasizing the significance of using the appropriate granularity when working\nwith DP."
                },
                "authors": [
                    {
                        "name": "Doan Nam Long Vu"
                    },
                    {
                        "name": "Timour Igamberdiev"
                    },
                    {
                        "name": "Ivan Habernal"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Habernal"
                },
                "author": "Ivan Habernal",
                "arxiv_comment": "Accepted at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04564v3",
                "updated": "2024-09-26T14:33:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    33,
                    24,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-07T18:59:55Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    18,
                    59,
                    55,
                    3,
                    341,
                    0
                ],
                "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS"
                },
                "summary": "Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view\nscene synthesis. It addresses the challenges of lengthy training times and slow\nrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,\ndifferentiable rasterization of 3D Gaussians, 3D-GS achieves real-time\nrendering and accelerated training. They, however, demand substantial memory\nresources for both training and storage, as they require millions of Gaussians\nin their point cloud representation for each scene. We present a technique\nutilizing quantized embeddings to significantly reduce per-point memory storage\nrequirements and a coarse-to-fine training strategy for a faster and more\nstable optimization of the Gaussian point clouds. Our approach develops a\npruning stage which results in scene representations with fewer Gaussians,\nleading to faster training times and rendering speeds for real-time rendering\nof high resolution scenes. We reduce storage memory by more than an order of\nmagnitude all while preserving the reconstruction quality. We validate the\neffectiveness of our approach on a variety of datasets and scenes preserving\nthe visual quality while consuming 10-20x lesser memory and faster\ntraining/inference speed. Project page and code is available\nhttps://efficientgaussian.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view\nscene synthesis. It addresses the challenges of lengthy training times and slow\nrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,\ndifferentiable rasterization of 3D Gaussians, 3D-GS achieves real-time\nrendering and accelerated training. They, however, demand substantial memory\nresources for both training and storage, as they require millions of Gaussians\nin their point cloud representation for each scene. We present a technique\nutilizing quantized embeddings to significantly reduce per-point memory storage\nrequirements and a coarse-to-fine training strategy for a faster and more\nstable optimization of the Gaussian point clouds. Our approach develops a\npruning stage which results in scene representations with fewer Gaussians,\nleading to faster training times and rendering speeds for real-time rendering\nof high resolution scenes. We reduce storage memory by more than an order of\nmagnitude all while preserving the reconstruction quality. We validate the\neffectiveness of our approach on a variety of datasets and scenes preserving\nthe visual quality while consuming 10-20x lesser memory and faster\ntraining/inference speed. Project page and code is available\nhttps://efficientgaussian.github.io"
                },
                "authors": [
                    {
                        "name": "Sharath Girish"
                    },
                    {
                        "name": "Kamal Gupta"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "Website: https://efficientgaussian.github.io Code:\n  https://github.com/Sharath-girish/efficientgaussian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17880v1",
                "updated": "2024-09-26T14:27:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    27,
                    55,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:27:55Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    27,
                    55,
                    3,
                    270,
                    0
                ],
                "title": "Self-Distilled Depth Refinement with Noisy Poisson Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Distilled Depth Refinement with Noisy Poisson Fusion"
                },
                "summary": "Depth refinement aims to infer high-resolution depth with fine-grained edges\nand details, refining low-resolution results of depth estimation models. The\nprevailing methods adopt tile-based manners by merging numerous patches, which\nlacks efficiency and produces inconsistency. Besides, prior arts suffer from\nfuzzy depth boundaries and limited generalizability. Analyzing the fundamental\nreasons for these limitations, we model depth refinement as a noisy Poisson\nfusion problem with local inconsistency and edge deformation noises. We propose\nthe Self-distilled Depth Refinement (SDDR) framework to enforce robustness\nagainst the noises, which mainly consists of depth edge representation and\nedge-based guidance. With noisy depth predictions as input, SDDR generates\nlow-noise depth edge representations as pseudo-labels by coarse-to-fine\nself-distillation. Edge-based guidance with edge-guided gradient loss and\nedge-based fusion loss serves as the optimization objective equivalent to\nPoisson fusion. When depth maps are better refined, the labels also become more\nnoise-free. Our model can acquire strong robustness to the noises, achieving\nsignificant improvements in accuracy, edge quality, efficiency, and\ngeneralizability on five different benchmarks. Moreover, directly training\nanother model with edge labels produced by SDDR brings improvements, suggesting\nthat our method could help with training robust refinement models in future\nworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth refinement aims to infer high-resolution depth with fine-grained edges\nand details, refining low-resolution results of depth estimation models. The\nprevailing methods adopt tile-based manners by merging numerous patches, which\nlacks efficiency and produces inconsistency. Besides, prior arts suffer from\nfuzzy depth boundaries and limited generalizability. Analyzing the fundamental\nreasons for these limitations, we model depth refinement as a noisy Poisson\nfusion problem with local inconsistency and edge deformation noises. We propose\nthe Self-distilled Depth Refinement (SDDR) framework to enforce robustness\nagainst the noises, which mainly consists of depth edge representation and\nedge-based guidance. With noisy depth predictions as input, SDDR generates\nlow-noise depth edge representations as pseudo-labels by coarse-to-fine\nself-distillation. Edge-based guidance with edge-guided gradient loss and\nedge-based fusion loss serves as the optimization objective equivalent to\nPoisson fusion. When depth maps are better refined, the labels also become more\nnoise-free. Our model can acquire strong robustness to the noises, achieving\nsignificant improvements in accuracy, edge quality, efficiency, and\ngeneralizability on five different benchmarks. Moreover, directly training\nanother model with edge labels produced by SDDR brings improvements, suggesting\nthat our method could help with training robust refinement models in future\nworks."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Yiran Wang"
                    },
                    {
                        "name": "Jinghong Zheng"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Ke Xian"
                    },
                    {
                        "name": "Zhiguo Cao"
                    },
                    {
                        "name": "Jianming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianming Zhang"
                },
                "author": "Jianming Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17875v1",
                "updated": "2024-09-26T14:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    21,
                    5,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    21,
                    5,
                    3,
                    270,
                    0
                ],
                "title": "Interionic distance distributions between $Er^{3+}$ and $Yb^{3+}$ as\n  dopants in some crystal matrix hosts used for upconversion luminescence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interionic distance distributions between $Er^{3+}$ and $Yb^{3+}$ as\n  dopants in some crystal matrix hosts used for upconversion luminescence"
                },
                "summary": "The upconversion process for the $Er^{3+}$ ion, when irradiated with IR\nphotons at 980 nm, strongly depends upon the presence of the sensitizer\n$Yb^{3+}$ ions. There is a good correlation between the relative and absolute\nconcentrations of the activator and sensitizer species and the intensities of\nthe emission lines of the resulted visible spectrum. The ratios between\nemission intensities in the green band (510-580 nm) and red band (640-700 nm)\n(i.e., the spectral content) show similarities between different host crystals\nin which $Er^{3+}$ and $Yb^{3+}$ are embedded, which is an indication that,\nregardless of the crystalline medium of embedding, the dopant ions interact in\nsome specific and similar ways. In order to reveal the mechanisms for the\nenergy transfer between activator and sensitizer ions, one needs a model for\nthe spatial distribution of these relative to one another, from which some\nstatistical data, like crystal matrix average surrounding radii, could be\ninferred. Some models of cubic neighboring are presented for the relative\nconcentration ratios of $Er^{3+}:Yb^{3+}$, respectively of 1:2, 1:4, and 1:8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The upconversion process for the $Er^{3+}$ ion, when irradiated with IR\nphotons at 980 nm, strongly depends upon the presence of the sensitizer\n$Yb^{3+}$ ions. There is a good correlation between the relative and absolute\nconcentrations of the activator and sensitizer species and the intensities of\nthe emission lines of the resulted visible spectrum. The ratios between\nemission intensities in the green band (510-580 nm) and red band (640-700 nm)\n(i.e., the spectral content) show similarities between different host crystals\nin which $Er^{3+}$ and $Yb^{3+}$ are embedded, which is an indication that,\nregardless of the crystalline medium of embedding, the dopant ions interact in\nsome specific and similar ways. In order to reveal the mechanisms for the\nenergy transfer between activator and sensitizer ions, one needs a model for\nthe spatial distribution of these relative to one another, from which some\nstatistical data, like crystal matrix average surrounding radii, could be\ninferred. Some models of cubic neighboring are presented for the relative\nconcentration ratios of $Er^{3+}:Yb^{3+}$, respectively of 1:2, 1:4, and 1:8."
                },
                "authors": [
                    {
                        "name": "Liviu Dudas"
                    }
                ],
                "author_detail": {
                    "name": "Liviu Dudas"
                },
                "author": "Liviu Dudas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17870v1",
                "updated": "2024-09-26T14:17:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    17,
                    58,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:17:58Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    17,
                    58,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Arbitrary Precision Acceleration for Large Language Models on\n  GPU Tensor Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Arbitrary Precision Acceleration for Large Language Models on\n  GPU Tensor Cores"
                },
                "summary": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 13\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 13\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs."
                },
                "authors": [
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17840v1",
                "updated": "2024-09-26T13:44:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    44,
                    22,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:44:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    44,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "Detecting and Measuring Confounding Using Causal Mechanism Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Measuring Confounding Using Causal Mechanism Shifts"
                },
                "summary": "Detecting and measuring confounding effects from data is a key challenge in\ncausal inference. Existing methods frequently assume causal sufficiency,\ndisregarding the presence of unobserved confounding variables. Causal\nsufficiency is both unrealistic and empirically untestable. Additionally,\nexisting methods make strong parametric assumptions about the underlying causal\ngenerative process to guarantee the identifiability of confounding variables.\nRelaxing the causal sufficiency and parametric assumptions and leveraging\nrecent advancements in causal discovery and confounding analysis with\nnon-i.i.d. data, we propose a comprehensive approach for detecting and\nmeasuring confounding. We consider various definitions of confounding and\nintroduce tailored methodologies to achieve three objectives: (i) detecting and\nmeasuring confounding among a set of variables, (ii) separating observed and\nunobserved confounding effects, and (iii) understanding the relative strengths\nof confounding bias between different sets of variables. We present useful\nproperties of a confounding measure and present measures that satisfy those\nproperties. Empirical results support the theoretical analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and measuring confounding effects from data is a key challenge in\ncausal inference. Existing methods frequently assume causal sufficiency,\ndisregarding the presence of unobserved confounding variables. Causal\nsufficiency is both unrealistic and empirically untestable. Additionally,\nexisting methods make strong parametric assumptions about the underlying causal\ngenerative process to guarantee the identifiability of confounding variables.\nRelaxing the causal sufficiency and parametric assumptions and leveraging\nrecent advancements in causal discovery and confounding analysis with\nnon-i.i.d. data, we propose a comprehensive approach for detecting and\nmeasuring confounding. We consider various definitions of confounding and\nintroduce tailored methodologies to achieve three objectives: (i) detecting and\nmeasuring confounding among a set of variables, (ii) separating observed and\nunobserved confounding effects, and (iii) understanding the relative strengths\nof confounding bias between different sets of variables. We present useful\nproperties of a confounding measure and present measures that satisfy those\nproperties. Empirical results support the theoretical analysis."
                },
                "authors": [
                    {
                        "name": "Abbavaram Gowtham Reddy"
                    },
                    {
                        "name": "Vineeth N Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Vineeth N Balasubramanian"
                },
                "author": "Vineeth N Balasubramanian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10542v2",
                "updated": "2024-09-26T13:38:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    48,
                    3,
                    270,
                    0
                ],
                "published": "2024-03-08T23:04:14Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    23,
                    4,
                    14,
                    4,
                    68,
                    0
                ],
                "title": "SF-MMCN: Low-Power Sever Flow Multi-Mode Diffusion Model Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SF-MMCN: Low-Power Sever Flow Multi-Mode Diffusion Model Accelerator"
                },
                "summary": "Generative Artificial Intelligence (AI) has become incredibly popular in\nrecent years, and the significance of traditional accelerators in dealing with\nlarge-scale parameters is urgent. With the diffusion model's parallel\nstructure, the hardware design challenge has skyrocketed because of the\nmultiple layers operating simultaneously. Convolution Neural Network (CNN)\naccelerators have been designed and developed rapidly, especially for\nhigh-speed inference. Often, CNN models with parallel structures are deployed.\nIn these CNN accelerators, many Processing Elements (PE) are required to\nperform parallel computations, mainly the multiply and accumulation (MAC)\noperation, resulting in high power consumption and a large silicon area. In\nthis work, a Server Flow Multi-Mode CNN Unit (SF-MMCN) is proposed to reduce\nthe number of PE while improving the operation efficiency of the CNN\naccelerator. The pipelining technique is introduced into Server Flow to process\nparallel computations. The proposed SF-MMCN is implemented with TSMC 90-nm CMOS\ntechnology. It is evaluated with VGG-16, ResNet-18, and U-net. The evaluation\nresults show that the proposed SF-MMCN can reduce the power consumption by 92%,\nand the silicon area by 70%, while improving the efficiency of operation by\nnearly 81 times. A new FoM, area efficiency (GOPs/mm^2) is also introduced to\nevaluate the performance of the accelerator in terms of the ratio throughput\n(GOPs) and silicon area (mm^2). In this FoM, SF-MMCN improves area efficiency\nby 18 times (18.42).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (AI) has become incredibly popular in\nrecent years, and the significance of traditional accelerators in dealing with\nlarge-scale parameters is urgent. With the diffusion model's parallel\nstructure, the hardware design challenge has skyrocketed because of the\nmultiple layers operating simultaneously. Convolution Neural Network (CNN)\naccelerators have been designed and developed rapidly, especially for\nhigh-speed inference. Often, CNN models with parallel structures are deployed.\nIn these CNN accelerators, many Processing Elements (PE) are required to\nperform parallel computations, mainly the multiply and accumulation (MAC)\noperation, resulting in high power consumption and a large silicon area. In\nthis work, a Server Flow Multi-Mode CNN Unit (SF-MMCN) is proposed to reduce\nthe number of PE while improving the operation efficiency of the CNN\naccelerator. The pipelining technique is introduced into Server Flow to process\nparallel computations. The proposed SF-MMCN is implemented with TSMC 90-nm CMOS\ntechnology. It is evaluated with VGG-16, ResNet-18, and U-net. The evaluation\nresults show that the proposed SF-MMCN can reduce the power consumption by 92%,\nand the silicon area by 70%, while improving the efficiency of operation by\nnearly 81 times. A new FoM, area efficiency (GOPs/mm^2) is also introduced to\nevaluate the performance of the accelerator in terms of the ratio throughput\n(GOPs) and silicon area (mm^2). In this FoM, SF-MMCN improves area efficiency\nby 18 times (18.42)."
                },
                "authors": [
                    {
                        "name": "Huan-Ke Hsu"
                    },
                    {
                        "name": "I-Chyn Wey"
                    },
                    {
                        "name": "T. Hui Teo"
                    }
                ],
                "author_detail": {
                    "name": "T. Hui Teo"
                },
                "author": "T. Hui Teo",
                "arxiv_comment": "16 pages, 16 figures; extend the CNN to process Diffusion Model\n  (possible this is the first reported hardware Diffusion Model implementation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17836v1",
                "updated": "2024-09-26T13:38:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models"
                },
                "summary": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10\\% up to 17.2\\% across various\ndatasets and architectures. Additionally, our approach shows promising\ncompatibility with lossy compression techniques such as quantization and\nsparsification. These findings highlight the significant potential of LLMs as a\nmodel for effectively handling gradients. We will release the source code upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10\\% up to 17.2\\% across various\ndatasets and architectures. Additionally, our approach shows promising\ncompatibility with lossy compression techniques such as quantization and\nsparsification. These findings highlight the significant potential of LLMs as a\nmodel for effectively handling gradients. We will release the source code upon\npublication."
                },
                "authors": [
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "To appear in NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17834v1",
                "updated": "2024-09-26T13:36:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    36,
                    0,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:36:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    36,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent\n  Representation MOdification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent\n  Representation MOdification"
                },
                "summary": "Due to their substantial sizes, large language models (LLMs) are typically\ndeployed within a single-backbone multi-tenant framework. In this setup, a\nsingle instance of an LLM backbone must cater to multiple users or tasks\nthrough the application of various parameter-efficient fine-tuning (PEFT)\nmodels. Despite the availability of numerous effective PEFT techniques such as\nLoRA, there remains a need for a PEFT approach that achieves both high\nefficiency during inference and competitive performance on downstream tasks. In\nthis research, we introduce a new and straightforward PEFT methodology named\n\\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation\nM\\underline{O}dification (PEDRO). The proposed method involves integrating a\nlightweight vector generator into each Transformer layer, which generates\nvectors contingent upon the input prompts. These vectors then modify the hidden\nrepresentations created by the LLM through a dot product operation, thereby\ninfluencing the semantic output and generated content of the model. Extensive\nexperimentation across a variety of tasks indicates that: (a) PEDRO surpasses\nrecent PEFT benchmarks when using a similar number of tunable parameters. (b)\nUnder the single-backbone multi-tenant deployment model, PEDRO exhibits\nsuperior efficiency compared to LoRA, indicating significant industrial\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their substantial sizes, large language models (LLMs) are typically\ndeployed within a single-backbone multi-tenant framework. In this setup, a\nsingle instance of an LLM backbone must cater to multiple users or tasks\nthrough the application of various parameter-efficient fine-tuning (PEFT)\nmodels. Despite the availability of numerous effective PEFT techniques such as\nLoRA, there remains a need for a PEFT approach that achieves both high\nefficiency during inference and competitive performance on downstream tasks. In\nthis research, we introduce a new and straightforward PEFT methodology named\n\\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation\nM\\underline{O}dification (PEDRO). The proposed method involves integrating a\nlightweight vector generator into each Transformer layer, which generates\nvectors contingent upon the input prompts. These vectors then modify the hidden\nrepresentations created by the LLM through a dot product operation, thereby\ninfluencing the semantic output and generated content of the model. Extensive\nexperimentation across a variety of tasks indicates that: (a) PEDRO surpasses\nrecent PEFT benchmarks when using a similar number of tunable parameters. (b)\nUnder the single-backbone multi-tenant deployment model, PEDRO exhibits\nsuperior efficiency compared to LoRA, indicating significant industrial\npotential."
                },
                "authors": [
                    {
                        "name": "Tianfang Xie"
                    },
                    {
                        "name": "Tianjing Li"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Yi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhao"
                },
                "author": "Yi Zhao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.18203",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08928v2",
                "updated": "2024-09-26T13:32:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    32,
                    39,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-13T15:45:40Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    45,
                    40,
                    4,
                    257,
                    0
                ],
                "title": "Self-Organized State-Space Models with Artificial Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Organized State-Space Models with Artificial Dynamics"
                },
                "summary": "We consider a state-space model (SSM) parametrized by some parameter\n$\\theta$, and our aim is to perform joint parameter and state inference. A\nsimple idea to carry out this task, which almost dates back to the origin of\nthe Kalman filter, is to replace the static parameter $\\theta$ by a Markov\nchain $(\\theta_t)_{t\\geq 0}$ and then to apply a filtering algorithm to the\nextended, or self-organized SSM (SO-SSM). However, the practical implementation\nof this idea in a theoretically justified way has remained an open problem. In\nthis paper we fill this gap by introducing various possible constructions of\n$(\\theta_t)_{t\\geq 0}$ that ensure the validity of the SO-SSM for joint\nparameter and state inference. Notably, we show that such SO-SSMs can be\ndefined even if $\\|\\mathrm{Var}(\\theta_{t}|\\theta_{t-1})\\|\\rightarrow 0$ slowly\nas $t\\rightarrow\\infty$. This result is important since, as illustrated in our\nnumerical experiments, these models can be efficiently approximated using\nparticle filter algorithms. While SO-SSMs have been introduced for online\ninference, the development of iterated filtering (IF) algorithms has shown that\nthey can also serve for computing the maximum likelihood estimator of a given\nSSM. In this work, we also derive constructions of $(\\theta_t)_{t\\geq 0}$ and\ntheoretical guarantees tailored to these specific applications of SO-SSMs and,\nas a result, introduce new IF algorithms. From a practical point of view, the\nalgorithms we develop have the merit of being simple to implement and only\nrequiring minimal tuning to perform well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a state-space model (SSM) parametrized by some parameter\n$\\theta$, and our aim is to perform joint parameter and state inference. A\nsimple idea to carry out this task, which almost dates back to the origin of\nthe Kalman filter, is to replace the static parameter $\\theta$ by a Markov\nchain $(\\theta_t)_{t\\geq 0}$ and then to apply a filtering algorithm to the\nextended, or self-organized SSM (SO-SSM). However, the practical implementation\nof this idea in a theoretically justified way has remained an open problem. In\nthis paper we fill this gap by introducing various possible constructions of\n$(\\theta_t)_{t\\geq 0}$ that ensure the validity of the SO-SSM for joint\nparameter and state inference. Notably, we show that such SO-SSMs can be\ndefined even if $\\|\\mathrm{Var}(\\theta_{t}|\\theta_{t-1})\\|\\rightarrow 0$ slowly\nas $t\\rightarrow\\infty$. This result is important since, as illustrated in our\nnumerical experiments, these models can be efficiently approximated using\nparticle filter algorithms. While SO-SSMs have been introduced for online\ninference, the development of iterated filtering (IF) algorithms has shown that\nthey can also serve for computing the maximum likelihood estimator of a given\nSSM. In this work, we also derive constructions of $(\\theta_t)_{t\\geq 0}$ and\ntheoretical guarantees tailored to these specific applications of SO-SSMs and,\nas a result, introduce new IF algorithms. From a practical point of view, the\nalgorithms we develop have the merit of being simple to implement and only\nrequiring minimal tuning to perform well."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Mathieu Gerber"
                    },
                    {
                        "name": "Christophe Andrieu"
                    },
                    {
                        "name": "Randal Douc"
                    }
                ],
                "author_detail": {
                    "name": "Randal Douc"
                },
                "author": "Randal Douc",
                "arxiv_comment": "102 pages (28 pages for the paper, 6 for the appendix and 68 for the\n  supplementary material), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v2",
                "updated": "2024-09-26T13:30:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    30,
                    51,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions. This work provides\na promising research direction for understanding the complexity of neural\nnetworks, shedding light on training dynamics, and advancing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions. This work provides\na promising research direction for understanding the complexity of neural\nnetworks, shedding light on training dynamics, and advancing interpretability."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17827v1",
                "updated": "2024-09-26T13:26:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    26,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:26:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    26,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text"
                },
                "summary": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs."
                },
                "authors": [
                    {
                        "name": "Siyan Wang"
                    },
                    {
                        "name": "Bradford Levy"
                    }
                ],
                "author_detail": {
                    "name": "Bradford Levy"
                },
                "author": "Bradford Levy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17819v1",
                "updated": "2024-09-26T13:15:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    15,
                    18,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:15:18Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    15,
                    18,
                    3,
                    270,
                    0
                ],
                "title": "Inference-Time Language Model Alignment via Integrated Value Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Language Model Alignment via Integrated Value Guidance"
                },
                "summary": "Large language models are typically fine-tuned to align with human\npreferences, but tuning large models is computationally intensive and complex.\nIn this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method\nthat uses implicit and explicit value functions to guide language model\ndecoding at token and chunk-level respectively, efficiently aligning large\nlanguage models purely at inference time. This approach circumvents the\ncomplexities of direct fine-tuning and outperforms traditional methods.\nEmpirically, we demonstrate the versatility of IVG across various tasks. In\ncontrolled sentiment generation and summarization tasks, our method\nsignificantly improves the alignment of large models using inference-time\nguidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more\nchallenging instruction-following benchmark AlpacaEval 2.0, we show that both\nspecifically tuned and off-the-shelf value functions greatly improve the\nlength-controlled win rates of large models against $\\texttt{gpt-4-turbo}$\n(e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$\nand $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$\nwith Tulu guidance).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are typically fine-tuned to align with human\npreferences, but tuning large models is computationally intensive and complex.\nIn this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method\nthat uses implicit and explicit value functions to guide language model\ndecoding at token and chunk-level respectively, efficiently aligning large\nlanguage models purely at inference time. This approach circumvents the\ncomplexities of direct fine-tuning and outperforms traditional methods.\nEmpirically, we demonstrate the versatility of IVG across various tasks. In\ncontrolled sentiment generation and summarization tasks, our method\nsignificantly improves the alignment of large models using inference-time\nguidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more\nchallenging instruction-following benchmark AlpacaEval 2.0, we show that both\nspecifically tuned and off-the-shelf value functions greatly improve the\nlength-controlled win rates of large models against $\\texttt{gpt-4-turbo}$\n(e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$\nand $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$\nwith Tulu guidance)."
                },
                "authors": [
                    {
                        "name": "Zhixuan Liu"
                    },
                    {
                        "name": "Zhanhui Zhou"
                    },
                    {
                        "name": "Yuanfu Wang"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16106v2",
                "updated": "2024-09-26T13:05:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    5,
                    36,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T14:07:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    7,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain"
                },
                "summary": "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection."
                },
                "authors": [
                    {
                        "name": "Mehtab Ur Rahman"
                    },
                    {
                        "name": "Martha Larson"
                    },
                    {
                        "name": "Louis ten Bosch"
                    },
                    {
                        "name": "Cristian Tejedor-García"
                    }
                ],
                "author_detail": {
                    "name": "Cristian Tejedor-García"
                },
                "author": "Cristian Tejedor-García",
                "arxiv_comment": "Accepted and published at SPSC Symposium 2024 4th Symposium on\n  Security and Privacy in Speech Communication. Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17805v1",
                "updated": "2024-09-26T12:58:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    58,
                    1,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:58:01Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    58,
                    1,
                    3,
                    270,
                    0
                ],
                "title": "Cascade Prompt Learning for Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cascade Prompt Learning for Vision-Language Model Adaptation"
                },
                "summary": "Prompt learning has surfaced as an effective approach to enhance the\nperformance of Vision-Language Models (VLMs) like CLIP when applied to\ndownstream tasks. However, current learnable prompt tokens are primarily used\nfor the single phase of adapting to tasks (i.e., adapting prompt), easily\nleading to overfitting risks. In this work, we propose a novel Cascade Prompt\nLearning CasPL framework to enable prompt learning to serve both generic and\nspecific expertise (i.e., boosting and adapting prompt) simultaneously.\nSpecifically, CasPL is a new learning paradigm comprising two distinct phases\nof learnable prompts: the first boosting prompt is crafted to extract\ndomain-general knowledge from a senior larger CLIP teacher model by aligning\ntheir predicted logits using extensive unlabeled domain images. The second\nadapting prompt is then cascaded with the frozen first set to fine-tune the\ndownstream tasks, following the approaches employed in prior research. In this\nmanner, CasPL can effectively capture both domain-general and task-specific\nrepresentations into explicitly different gradual groups of prompts, thus\npotentially alleviating overfitting issues in the target domain. It's worth\nnoting that CasPL serves as a plug-and-play module that can seamlessly\nintegrate into any existing prompt learning approach. CasPL achieves a\nsignificantly better balance between performance and inference speed, which is\nespecially beneficial for deploying smaller VLM models in resource-constrained\nenvironments. Compared to the previous state-of-the-art method PromptSRC, CasPL\nshows an average improvement of 1.85% for base classes, 3.44% for novel\nclasses, and 2.72% for the harmonic mean over 11 image classification datasets.\nCode is publicly available at: https://github.com/megvii-research/CasPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt learning has surfaced as an effective approach to enhance the\nperformance of Vision-Language Models (VLMs) like CLIP when applied to\ndownstream tasks. However, current learnable prompt tokens are primarily used\nfor the single phase of adapting to tasks (i.e., adapting prompt), easily\nleading to overfitting risks. In this work, we propose a novel Cascade Prompt\nLearning CasPL framework to enable prompt learning to serve both generic and\nspecific expertise (i.e., boosting and adapting prompt) simultaneously.\nSpecifically, CasPL is a new learning paradigm comprising two distinct phases\nof learnable prompts: the first boosting prompt is crafted to extract\ndomain-general knowledge from a senior larger CLIP teacher model by aligning\ntheir predicted logits using extensive unlabeled domain images. The second\nadapting prompt is then cascaded with the frozen first set to fine-tune the\ndownstream tasks, following the approaches employed in prior research. In this\nmanner, CasPL can effectively capture both domain-general and task-specific\nrepresentations into explicitly different gradual groups of prompts, thus\npotentially alleviating overfitting issues in the target domain. It's worth\nnoting that CasPL serves as a plug-and-play module that can seamlessly\nintegrate into any existing prompt learning approach. CasPL achieves a\nsignificantly better balance between performance and inference speed, which is\nespecially beneficial for deploying smaller VLM models in resource-constrained\nenvironments. Compared to the previous state-of-the-art method PromptSRC, CasPL\nshows an average improvement of 1.85% for base classes, 3.44% for novel\nclasses, and 2.72% for the harmonic mean over 11 image classification datasets.\nCode is publicly available at: https://github.com/megvii-research/CasPL."
                },
                "authors": [
                    {
                        "name": "Ge Wu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhaowei Chen"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17791v1",
                "updated": "2024-09-26T12:37:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    37,
                    26,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:37:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    37,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "Self-supervised Preference Optimization: Enhance Your Language Model\n  with Preference Degree Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Preference Optimization: Enhance Your Language Model\n  with Preference Degree Awareness"
                },
                "summary": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO."
                },
                "authors": [
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Haojing Huang"
                    },
                    {
                        "name": "Yujia Zhang"
                    },
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Rui Song"
                    },
                    {
                        "name": "Lida Shi"
                    },
                    {
                        "name": "Jingwen Wang"
                    },
                    {
                        "name": "Hao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Xu"
                },
                "author": "Hao Xu",
                "arxiv_comment": "Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06364v2",
                "updated": "2024-09-26T12:33:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    33,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-10T09:42:58Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    9,
                    42,
                    58,
                    1,
                    254,
                    0
                ],
                "title": "What happens to diffusion model likelihood when your model is\n  conditional?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What happens to diffusion model likelihood when your model is\n  conditional?"
                },
                "summary": "Diffusion Models (DMs) iteratively denoise random samples to produce\nhigh-quality data. The iterative sampling process is derived from Stochastic\nDifferential Equations (SDEs), allowing a speed-quality trade-off chosen at\ninference. Another advantage of sampling with differential equations is exact\nlikelihood computation. These likelihoods have been used to rank unconditional\nDMs and for out-of-domain classification. Despite the many existing and\npossible uses of DM likelihoods, the distinct properties captured are unknown,\nespecially in conditional contexts such as Text-To-Image (TTI) or\nText-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods\nare agnostic to the text input. TTI likelihood is more expressive but cannot\ndiscern confounding prompts. Our results show that applying DMs to conditional\ntasks reveals inconsistencies and strengthens claims that the properties of DM\nlikelihood are unknown. This impact sheds light on the previously unknown\nnature of DM likelihoods. Although conditional DMs maximise likelihood, the\nlikelihood in question is not as sensitive to the conditioning input as one\nexpects. This investigation provides a new point-of-view on diffusion\nlikelihoods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models (DMs) iteratively denoise random samples to produce\nhigh-quality data. The iterative sampling process is derived from Stochastic\nDifferential Equations (SDEs), allowing a speed-quality trade-off chosen at\ninference. Another advantage of sampling with differential equations is exact\nlikelihood computation. These likelihoods have been used to rank unconditional\nDMs and for out-of-domain classification. Despite the many existing and\npossible uses of DM likelihoods, the distinct properties captured are unknown,\nespecially in conditional contexts such as Text-To-Image (TTI) or\nText-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods\nare agnostic to the text input. TTI likelihood is more expressive but cannot\ndiscern confounding prompts. Our results show that applying DMs to conditional\ntasks reveals inconsistencies and strengthens claims that the properties of DM\nlikelihood are unknown. This impact sheds light on the previously unknown\nnature of DM likelihoods. Although conditional DMs maximise likelihood, the\nlikelihood in question is not as sensitive to the conditioning input as one\nexpects. This investigation provides a new point-of-view on diffusion\nlikelihoods."
                },
                "authors": [
                    {
                        "name": "Mattias Cross"
                    },
                    {
                        "name": "Anton Ragni"
                    }
                ],
                "author_detail": {
                    "name": "Anton Ragni"
                },
                "author": "Anton Ragni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14580v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14580v4",
                "updated": "2024-09-26T12:20:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    20,
                    54,
                    3,
                    270,
                    0
                ],
                "published": "2024-03-21T17:33:35Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    17,
                    33,
                    35,
                    3,
                    81,
                    0
                ],
                "title": "Tomographic redshift dipole: Testing the cosmological principle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tomographic redshift dipole: Testing the cosmological principle"
                },
                "summary": "The cosmological principle posits that the universe is statistically\nhomogeneous and isotropic on large scales, implying all matter shares the same\nrest frame. This principle suggests that velocity estimates of our motion from\nvarious sources should agree with the cosmic microwave background (CMB)\ndipole's inferred velocity of 370 km/s. Yet, for over two decades, analyses of\nradio galaxy and quasar catalogs have found velocities at odds with the CMB\ndipole, with tensions up to 5$\\sigma$. In a blind analysis of BOSS and eBOSS\nspectroscopic data from galaxies and quasars across $0.2<z<2.2$, we applied a\nnovel dipole estimator for a tomographic approach, robustly correcting biases\nand quantifying uncertainties with realistic mock catalogs. Our findings with\neBOSS data ($0.6<z<2.2$), indicating a velocity of $196^{+92}_{-79}$ km/s,\ndemonstrate a $2\\sigma$ agreement with the CMB dipole when considering the full\n3D vector distribution and a 3-to-6$\\sigma$ tension with previous number count\nstudies. This result supports the cosmological principle, emphasizing the\nconsistency of our motion with the CMB across vast cosmic distances. On the\nother hand, the BOSS data revealed potential unmodeled systematics; the\nestimator could not be minimized using the LOWZ set ($0.2<z<0.4$), and the\nCMASS set ($0.4<z<0.6$) presented results that pointed towards the southern\nhemisphere, conflicting with the CMB dipole. Addressing the disparities with\nearlier number count analyses and understanding possible systematics in\nspectroscopic measurements will be essential to further validate the\ncosmological principle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmological principle posits that the universe is statistically\nhomogeneous and isotropic on large scales, implying all matter shares the same\nrest frame. This principle suggests that velocity estimates of our motion from\nvarious sources should agree with the cosmic microwave background (CMB)\ndipole's inferred velocity of 370 km/s. Yet, for over two decades, analyses of\nradio galaxy and quasar catalogs have found velocities at odds with the CMB\ndipole, with tensions up to 5$\\sigma$. In a blind analysis of BOSS and eBOSS\nspectroscopic data from galaxies and quasars across $0.2<z<2.2$, we applied a\nnovel dipole estimator for a tomographic approach, robustly correcting biases\nand quantifying uncertainties with realistic mock catalogs. Our findings with\neBOSS data ($0.6<z<2.2$), indicating a velocity of $196^{+92}_{-79}$ km/s,\ndemonstrate a $2\\sigma$ agreement with the CMB dipole when considering the full\n3D vector distribution and a 3-to-6$\\sigma$ tension with previous number count\nstudies. This result supports the cosmological principle, emphasizing the\nconsistency of our motion with the CMB across vast cosmic distances. On the\nother hand, the BOSS data revealed potential unmodeled systematics; the\nestimator could not be minimized using the LOWZ set ($0.2<z<0.4$), and the\nCMASS set ($0.4<z<0.6$) presented results that pointed towards the southern\nhemisphere, conflicting with the CMB dipole. Addressing the disparities with\nearlier number count analyses and understanding possible systematics in\nspectroscopic measurements will be essential to further validate the\ncosmological principle."
                },
                "authors": [
                    {
                        "name": "Pedro da Silveira Ferreira"
                    },
                    {
                        "name": "Valerio Marra"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Marra"
                },
                "author": "Valerio Marra",
                "arxiv_comment": "20 pages, 13 figures; v2: added direct comparison with previous\n  number count studies; v3: revised analysis and results following corrections\n  to the data processing pipeline. v4: revised QSO data analysis and results\n  following corrections. Accepted for publication in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14580v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14580v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17778v1",
                "updated": "2024-09-26T12:16:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    16,
                    11,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:16:11Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    16,
                    11,
                    3,
                    270,
                    0
                ],
                "title": "Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs"
                },
                "summary": "Diffusion-based image super-resolution (SR) models have attracted substantial\ninterest due to their powerful image restoration capabilities. However,\nprevailing diffusion models often struggle to strike an optimal balance between\nefficiency and performance. Typically, they either neglect to exploit the\npotential of existing extensive pretrained models, limiting their generative\ncapacity, or they necessitate a dozens of forward passes starting from random\nnoises, compromising inference efficiency. In this paper, we present DoSSR, a\nDomain Shift diffusion-based SR model that capitalizes on the generative powers\nof pretrained diffusion models while significantly enhancing efficiency by\ninitiating the diffusion process with low-resolution (LR) images. At the core\nof our approach is a domain shift equation that integrates seamlessly with\nexisting diffusion models. This integration not only improves the use of\ndiffusion prior but also boosts inference efficiency. Moreover, we advance our\nmethod by transitioning the discrete shift process to a continuous formulation,\ntermed as DoS-SDEs. This advancement leads to the fast and customized solvers\nthat further enhance sampling efficiency. Empirical results demonstrate that\nour proposed method achieves state-of-the-art performance on synthetic and\nreal-world datasets, while notably requiring only 5 sampling steps. Compared to\nprevious diffusion prior based methods, our approach achieves a remarkable\nspeedup of 5-7 times, demonstrating its superior efficiency. Code:\nhttps://github.com/QinpengCui/DoSSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image super-resolution (SR) models have attracted substantial\ninterest due to their powerful image restoration capabilities. However,\nprevailing diffusion models often struggle to strike an optimal balance between\nefficiency and performance. Typically, they either neglect to exploit the\npotential of existing extensive pretrained models, limiting their generative\ncapacity, or they necessitate a dozens of forward passes starting from random\nnoises, compromising inference efficiency. In this paper, we present DoSSR, a\nDomain Shift diffusion-based SR model that capitalizes on the generative powers\nof pretrained diffusion models while significantly enhancing efficiency by\ninitiating the diffusion process with low-resolution (LR) images. At the core\nof our approach is a domain shift equation that integrates seamlessly with\nexisting diffusion models. This integration not only improves the use of\ndiffusion prior but also boosts inference efficiency. Moreover, we advance our\nmethod by transitioning the discrete shift process to a continuous formulation,\ntermed as DoS-SDEs. This advancement leads to the fast and customized solvers\nthat further enhance sampling efficiency. Empirical results demonstrate that\nour proposed method achieves state-of-the-art performance on synthetic and\nreal-world datasets, while notably requiring only 5 sampling steps. Compared to\nprevious diffusion prior based methods, our approach achieves a remarkable\nspeedup of 5-7 times, demonstrating its superior efficiency. Code:\nhttps://github.com/QinpengCui/DoSSR."
                },
                "authors": [
                    {
                        "name": "Qinpeng Cui"
                    },
                    {
                        "name": "Yixuan Liu"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Qiqi Bao"
                    },
                    {
                        "name": "Zhongdao Wang"
                    },
                    {
                        "name": "Qingmin Liao"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Tian Lu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "This paper is accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17775v1",
                "updated": "2024-09-26T12:13:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    13,
                    52,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:13:52Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    13,
                    52,
                    3,
                    270,
                    0
                ],
                "title": "UNICORN: A Deep Learning Model for Integrating Multi-Stain Data in\n  Histopathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNICORN: A Deep Learning Model for Integrating Multi-Stain Data in\n  Histopathology"
                },
                "summary": "Background: The integration of multi-stain histopathology images through deep\nlearning poses a significant challenge in digital histopathology. Current\nmulti-modal approaches struggle with data heterogeneity and missing data. This\nstudy aims to overcome these limitations by developing a novel transformer\nmodel for multi-stain integration that can handle missing data during training\nas well as inference. Methods: We propose UNICORN (UNiversal modality\nIntegration Network for CORonary classificatioN) a multi-modal transformer\ncapable of processing multi-stain histopathology for atherosclerosis severity\nclass prediction. The architecture comprises a two-stage, end-to-end trainable\nmodel with specialized modules utilizing transformer self-attention blocks. The\ninitial stage employs domain-specific expert modules to extract features from\neach modality. In the subsequent stage, an aggregation expert module integrates\nthese features by learning the interactions between the different data\nmodalities. Results: Evaluation was performed using a multi-class dataset of\natherosclerotic lesions from the Munich Cardiovascular Studies Biobank\n(MISSION), using over 4,000 paired multi-stain whole slide images (WSIs) from\n170 deceased individuals on 7 prespecified segments of the coronary tree, each\nstained according to four histopathological protocols. UNICORN achieved a\nclassification accuracy of 0.67, outperforming other state-of-the-art models.\nThe model effectively identifies relevant tissue phenotypes across stainings\nand implicitly models disease progression. Conclusion: Our proposed multi-modal\ntransformer model addresses key challenges in medical data analysis, including\ndata heterogeneity and missing modalities. Explainability and the model's\neffectiveness in predicting atherosclerosis progression underscores its\npotential for broader applications in medical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The integration of multi-stain histopathology images through deep\nlearning poses a significant challenge in digital histopathology. Current\nmulti-modal approaches struggle with data heterogeneity and missing data. This\nstudy aims to overcome these limitations by developing a novel transformer\nmodel for multi-stain integration that can handle missing data during training\nas well as inference. Methods: We propose UNICORN (UNiversal modality\nIntegration Network for CORonary classificatioN) a multi-modal transformer\ncapable of processing multi-stain histopathology for atherosclerosis severity\nclass prediction. The architecture comprises a two-stage, end-to-end trainable\nmodel with specialized modules utilizing transformer self-attention blocks. The\ninitial stage employs domain-specific expert modules to extract features from\neach modality. In the subsequent stage, an aggregation expert module integrates\nthese features by learning the interactions between the different data\nmodalities. Results: Evaluation was performed using a multi-class dataset of\natherosclerotic lesions from the Munich Cardiovascular Studies Biobank\n(MISSION), using over 4,000 paired multi-stain whole slide images (WSIs) from\n170 deceased individuals on 7 prespecified segments of the coronary tree, each\nstained according to four histopathological protocols. UNICORN achieved a\nclassification accuracy of 0.67, outperforming other state-of-the-art models.\nThe model effectively identifies relevant tissue phenotypes across stainings\nand implicitly models disease progression. Conclusion: Our proposed multi-modal\ntransformer model addresses key challenges in medical data analysis, including\ndata heterogeneity and missing modalities. Explainability and the model's\neffectiveness in predicting atherosclerosis progression underscores its\npotential for broader applications in medical research."
                },
                "authors": [
                    {
                        "name": "Valentin Koch"
                    },
                    {
                        "name": "Sabine Bauer"
                    },
                    {
                        "name": "Valerio Luppberger"
                    },
                    {
                        "name": "Michael Joner"
                    },
                    {
                        "name": "Heribert Schunkert"
                    },
                    {
                        "name": "Julia A. Schnabel"
                    },
                    {
                        "name": "Moritz von Scheidt"
                    },
                    {
                        "name": "Carsten Marr"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Marr"
                },
                "author": "Carsten Marr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17773v1",
                "updated": "2024-09-26T12:09:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    9,
                    39,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:09:39Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    9,
                    39,
                    3,
                    270,
                    0
                ],
                "title": "Red giant - jet collisions in galactic nuclei I: 3D hydrodynamical model\n  of a few stellar orbits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red giant - jet collisions in galactic nuclei I: 3D hydrodynamical model\n  of a few stellar orbits"
                },
                "summary": "Several models have been proposed to explain missing red giants (RGs) near\nthe Galactic centre. Recently, a scenario has been suggested that predicts,\namong other processes, a long-term ablation of the surface layers of RGs during\ntheir repetitive passages through the Galactic jet (Zaja\\v{c}ek et al., 2020).\nIn this study, we perform detailed three-dimensional numerical modelling of\nthis phenomenon. We calculate the ablation rate of the surface layers of a RG\norbiting the supermassive black hole (SMBH) as it passes through the nuclear\njet. In particular, we model the jet-star interaction for approximately 10\npassages for the closer orbital distance of $10^{-3}\\,\\text{pc}$ and 2 passages\nfor $10^{-2}\\,\\text{pc}$. We find that the mass loss due to ablation by the jet\nbehaves with time as $\\Delta M_{\\star}\\propto \\sqrt{t}$ and the total ablated\nmass during a single active galactic nucleus (AGN) phase ($10^5$ years) is\n$\\sim 10^4\\,M_{\\odot}$. We arrive at similar rates of the stellar ablation for\nthe relatively smaller jet luminosity $10^{42}\\,\\text{erg}\\,\\text{s}^{-1}$ as\nin the previous analytical calculations. For larger jet luminosities of\n$10^{44}$ and $10^{48}\\,\\text{erg}\\,\\text{s}^{-1}$, the ablation rates inferred\nfrom $\\sim 10$ interactions as well as extrapolated power-law fits are\nsignificantly lower than analytical values. For the smallest orbital distance\nof $10^{-3}\\,\\text{pc}$, we also track the thermal behaviour of the stellar\nsurface layer, whose temperature appears to grow rapidly during the first 10\npassages from $\\sim 3600\\,{\\rm K}$ (spectral type M) to $\\sim 8500\\,{\\rm K}$\n(spectral type A). RG-jet interactions can thus lead to observable changes in\nthe nuclear late-type stellar population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several models have been proposed to explain missing red giants (RGs) near\nthe Galactic centre. Recently, a scenario has been suggested that predicts,\namong other processes, a long-term ablation of the surface layers of RGs during\ntheir repetitive passages through the Galactic jet (Zaja\\v{c}ek et al., 2020).\nIn this study, we perform detailed three-dimensional numerical modelling of\nthis phenomenon. We calculate the ablation rate of the surface layers of a RG\norbiting the supermassive black hole (SMBH) as it passes through the nuclear\njet. In particular, we model the jet-star interaction for approximately 10\npassages for the closer orbital distance of $10^{-3}\\,\\text{pc}$ and 2 passages\nfor $10^{-2}\\,\\text{pc}$. We find that the mass loss due to ablation by the jet\nbehaves with time as $\\Delta M_{\\star}\\propto \\sqrt{t}$ and the total ablated\nmass during a single active galactic nucleus (AGN) phase ($10^5$ years) is\n$\\sim 10^4\\,M_{\\odot}$. We arrive at similar rates of the stellar ablation for\nthe relatively smaller jet luminosity $10^{42}\\,\\text{erg}\\,\\text{s}^{-1}$ as\nin the previous analytical calculations. For larger jet luminosities of\n$10^{44}$ and $10^{48}\\,\\text{erg}\\,\\text{s}^{-1}$, the ablation rates inferred\nfrom $\\sim 10$ interactions as well as extrapolated power-law fits are\nsignificantly lower than analytical values. For the smallest orbital distance\nof $10^{-3}\\,\\text{pc}$, we also track the thermal behaviour of the stellar\nsurface layer, whose temperature appears to grow rapidly during the first 10\npassages from $\\sim 3600\\,{\\rm K}$ (spectral type M) to $\\sim 8500\\,{\\rm K}$\n(spectral type A). RG-jet interactions can thus lead to observable changes in\nthe nuclear late-type stellar population."
                },
                "authors": [
                    {
                        "name": "Petr Kurfürst"
                    },
                    {
                        "name": "Michal Zajaček"
                    },
                    {
                        "name": "Norbert Werner"
                    },
                    {
                        "name": "Jiří Krtička"
                    }
                ],
                "author_detail": {
                    "name": "Jiří Krtička"
                },
                "arxiv_affiliation": "MUNI, Brno",
                "author": "Jiří Krtička",
                "arxiv_comment": "19 pages, 20 figures, 2 tables; submitted to MNRAS; comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04259v2",
                "updated": "2024-09-26T11:42:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    42,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-08T06:57:49Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    6,
                    57,
                    49,
                    3,
                    221,
                    0
                ],
                "title": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering"
                },
                "summary": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets."
                },
                "authors": [
                    {
                        "name": "Ziyuan Zhuang"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17755v1",
                "updated": "2024-09-26T11:40:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    40,
                    7,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T11:40:07Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    40,
                    7,
                    3,
                    270,
                    0
                ],
                "title": "SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning"
                },
                "summary": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the robot is unaware of a concept that's key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems by fixing a deficient domain model using\nembodied conversation. Through dialogue, the robot discovers and then learns to\nexploit unforeseen possibilities. Using SECURE, the robot not only learns from\nthe user's corrective feedback when it makes a mistake, but it also learns to\nmake strategic dialogue decisions for revealing useful evidence about novel\nconcepts for solving the instructed task. Together, these abilities allow the\nrobot to generalise to subsequent tasks using newly acquired knowledge. We\ndemonstrate that a robot that is semantics-aware -- that is, it exploits the\nlogical consequences of both sentence and discourse semantics in the learning\nand inference process -- learns to solve rearrangement under unawareness more\neffectively than a robot that lacks such capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the robot is unaware of a concept that's key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems by fixing a deficient domain model using\nembodied conversation. Through dialogue, the robot discovers and then learns to\nexploit unforeseen possibilities. Using SECURE, the robot not only learns from\nthe user's corrective feedback when it makes a mistake, but it also learns to\nmake strategic dialogue decisions for revealing useful evidence about novel\nconcepts for solving the instructed task. Together, these abilities allow the\nrobot to generalise to subsequent tasks using newly acquired knowledge. We\ndemonstrate that a robot that is semantics-aware -- that is, it exploits the\nlogical consequences of both sentence and discourse semantics in the learning\nand inference process -- learns to solve rearrangement under unawareness more\neffectively than a robot that lacks such capabilities."
                },
                "authors": [
                    {
                        "name": "Rimvydas Rubavicius"
                    },
                    {
                        "name": "Peter David Fagan"
                    },
                    {
                        "name": "Alex Lascarides"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "arxiv_comment": "10 pages,4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05968v2",
                "updated": "2024-09-26T11:38:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    38,
                    2,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-12T07:49:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    49,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction"
                },
                "summary": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed."
                },
                "authors": [
                    {
                        "name": "Cédric Eichler"
                    },
                    {
                        "name": "Nathan Champeil"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Alexandra Bensamoun"
                    },
                    {
                        "name": "Heber Hwang Arcolezi"
                    },
                    {
                        "name": "José Maria De Fuentes"
                    }
                ],
                "author_detail": {
                    "name": "José Maria De Fuentes"
                },
                "author": "José Maria De Fuentes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17751v1",
                "updated": "2024-09-26T11:34:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    34,
                    43,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T11:34:43Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    34,
                    43,
                    3,
                    270,
                    0
                ],
                "title": "Granger Causality for Mixed Time Series Generalized Linear Models: A\n  Case Study on Multimodal Brain Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Granger Causality for Mixed Time Series Generalized Linear Models: A\n  Case Study on Multimodal Brain Connectivity"
                },
                "summary": "This paper is motivated by studies in neuroscience experiments to understand\ninteractions between nodes in a brain network using different types of data\nmodalities that capture different distinct facets of brain activity. To assess\nGranger-causality, we introduce a flexible framework through a general class of\nmodels that accommodates mixed types of data (binary, count, continuous, and\npositive components) formulated in a generalized linear model (GLM) fashion.\nStatistical inference for causality is performed based on both frequentist and\nBayesian approaches, with a focus on the latter. Here, we develop a procedure\nfor conducting inference through the proposed Bayesian mixed time series model.\nBy introducing spike and slab priors for some parameters in the model, our\ninferential approach guides causality order selection and provides proper\nuncertainty quantification. The proposed methods are then utilized to study the\nrat spike train and local field potentials (LFP) data recorded during the\nolfaction working memory task. The proposed methodology provides critical\ninsights into the causal relationship between the rat spiking activity and LFP\nspectral power. Specifically, power in the LFP beta band is predictive of\nspiking activity 300 milliseconds later, providing a novel analytical tool for\nthis area of emerging interest in neuroscience and demonstrating its usefulness\nand flexibility in the study of causality in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is motivated by studies in neuroscience experiments to understand\ninteractions between nodes in a brain network using different types of data\nmodalities that capture different distinct facets of brain activity. To assess\nGranger-causality, we introduce a flexible framework through a general class of\nmodels that accommodates mixed types of data (binary, count, continuous, and\npositive components) formulated in a generalized linear model (GLM) fashion.\nStatistical inference for causality is performed based on both frequentist and\nBayesian approaches, with a focus on the latter. Here, we develop a procedure\nfor conducting inference through the proposed Bayesian mixed time series model.\nBy introducing spike and slab priors for some parameters in the model, our\ninferential approach guides causality order selection and provides proper\nuncertainty quantification. The proposed methods are then utilized to study the\nrat spike train and local field potentials (LFP) data recorded during the\nolfaction working memory task. The proposed methodology provides critical\ninsights into the causal relationship between the rat spiking activity and LFP\nspectral power. Specifically, power in the LFP beta band is predictive of\nspiking activity 300 milliseconds later, providing a novel analytical tool for\nthis area of emerging interest in neuroscience and demonstrating its usefulness\nand flexibility in the study of causality in general."
                },
                "authors": [
                    {
                        "name": "Luiza S. C. Piancastelli"
                    },
                    {
                        "name": "Wagner Barreto-Souza"
                    },
                    {
                        "name": "Norbert J. Fortin"
                    },
                    {
                        "name": "Keiland W. Cooper"
                    },
                    {
                        "name": "Hernando Ombao"
                    }
                ],
                "author_detail": {
                    "name": "Hernando Ombao"
                },
                "author": "Hernando Ombao",
                "arxiv_comment": "Paper submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17745v1",
                "updated": "2024-09-26T11:19:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    19,
                    9,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T11:19:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    19,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model"
                },
                "summary": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline."
                },
                "authors": [
                    {
                        "name": "Nilanjan Sinhababu"
                    },
                    {
                        "name": "Andrew Parry"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Debasis Samanta"
                    },
                    {
                        "name": "Pabitra Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Pabitra Mitra"
                },
                "author": "Pabitra Mitra",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10712v3",
                "updated": "2024-09-26T11:15:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    15,
                    14,
                    3,
                    270,
                    0
                ],
                "published": "2024-02-16T14:15:15Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    14,
                    15,
                    15,
                    4,
                    47,
                    0
                ],
                "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient\n  Language Model Inference"
                },
                "summary": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models."
                },
                "authors": [
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17726v1",
                "updated": "2024-09-26T10:56:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    56,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T10:56:27Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    56,
                    27,
                    3,
                    270,
                    0
                ],
                "title": "Recent advances in interpretable machine learning using structure-based\n  protein representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interpretable machine learning using structure-based\n  protein representations"
                },
                "summary": "Recent advancements in machine learning (ML) are transforming the field of\nstructural biology. For example, AlphaFold, a groundbreaking neural network for\nprotein structure prediction, has been widely adopted by researchers. The\navailability of easy-to-use interfaces and interpretable outcomes from the\nneural network architecture, such as the confidence scores used to color the\npredicted structures, have made AlphaFold accessible even to non-ML experts. In\nthis paper, we present various methods for representing protein 3D structures\nfrom low- to high-resolution, and show how interpretable ML methods can support\ntasks such as predicting protein structures, protein function, and\nprotein-protein interactions. This survey also emphasizes the significance of\ninterpreting and visualizing ML-based inference for structure-based protein\nrepresentations that enhance interpretability and knowledge discovery.\nDeveloping such interpretable approaches promises to further accelerate fields\nincluding drug development and protein design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning (ML) are transforming the field of\nstructural biology. For example, AlphaFold, a groundbreaking neural network for\nprotein structure prediction, has been widely adopted by researchers. The\navailability of easy-to-use interfaces and interpretable outcomes from the\nneural network architecture, such as the confidence scores used to color the\npredicted structures, have made AlphaFold accessible even to non-ML experts. In\nthis paper, we present various methods for representing protein 3D structures\nfrom low- to high-resolution, and show how interpretable ML methods can support\ntasks such as predicting protein structures, protein function, and\nprotein-protein interactions. This survey also emphasizes the significance of\ninterpreting and visualizing ML-based inference for structure-based protein\nrepresentations that enhance interpretability and knowledge discovery.\nDeveloping such interpretable approaches promises to further accelerate fields\nincluding drug development and protein design."
                },
                "authors": [
                    {
                        "name": "Luiz Felipe Vecchietti"
                    },
                    {
                        "name": "Minji Lee"
                    },
                    {
                        "name": "Begench Hangeldiyev"
                    },
                    {
                        "name": "Hyunkyu Jung"
                    },
                    {
                        "name": "Hahnbeom Park"
                    },
                    {
                        "name": "Tae-Kyun Kim"
                    },
                    {
                        "name": "Meeyoung Cha"
                    },
                    {
                        "name": "Ho Min Kim"
                    }
                ],
                "author_detail": {
                    "name": "Ho Min Kim"
                },
                "author": "Ho Min Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08160v2",
                "updated": "2024-09-26T10:54:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    54,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-15T13:49:14Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    49,
                    14,
                    3,
                    228,
                    0
                ],
                "title": "General-purpose Clothes Manipulation with Semantic Keypoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Clothes Manipulation with Semantic Keypoints"
                },
                "summary": "Clothes manipulation is a critical skill for household robots. Recent\nadvancements have been made in task-specific clothes manipulation, such as\nfolding, flattening, and hanging. However, due to clothes' complex geometries\nand deformability, creating a general-purpose robot system that can manipulate\na diverse range of clothes in many ways remains challenging. Since clothes are\ntypically designed with specific structures, we propose identifying these\nspecific features like ``left sleeve'' as semantic keypoints. Semantic\nkeypoints can provide semantic cues for task planning and geometric cues for\nlow-level action generation. With this insight, we develop a hierarchical\nlearning framework using the large language model (LLM) for general-purpose\nCLothes mAnipulation with Semantic keyPoints (CLASP). Extensive simulation\nexperiments show that CLASP outperforms baseline methods on both seen and\nunseen tasks across various clothes manipulation tasks. Real-world experiments\nshow that CLASP can be directly deployed in the real world and applied to a\nwide variety of clothes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clothes manipulation is a critical skill for household robots. Recent\nadvancements have been made in task-specific clothes manipulation, such as\nfolding, flattening, and hanging. However, due to clothes' complex geometries\nand deformability, creating a general-purpose robot system that can manipulate\na diverse range of clothes in many ways remains challenging. Since clothes are\ntypically designed with specific structures, we propose identifying these\nspecific features like ``left sleeve'' as semantic keypoints. Semantic\nkeypoints can provide semantic cues for task planning and geometric cues for\nlow-level action generation. With this insight, we develop a hierarchical\nlearning framework using the large language model (LLM) for general-purpose\nCLothes mAnipulation with Semantic keyPoints (CLASP). Extensive simulation\nexperiments show that CLASP outperforms baseline methods on both seen and\nunseen tasks across various clothes manipulation tasks. Real-world experiments\nshow that CLASP can be directly deployed in the real world and applied to a\nwide variety of clothes."
                },
                "authors": [
                    {
                        "name": "Yuhong Deng"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17896v2",
                "updated": "2024-09-26T10:33:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    33,
                    53,
                    3,
                    270,
                    0
                ],
                "published": "2024-07-25T09:36:37Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    36,
                    37,
                    3,
                    207,
                    0
                ],
                "title": "SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware\n  Neural Networks"
                },
                "summary": "Incomplete or missing data in three-dimensional (3D) models can lead to\nerroneous or flawed renderings, limiting their usefulness in applications such\nas visualization, geometric computation, and 3D printing. Conventional\nsurface-repair techniques often fail to infer complex geometric details in\nmissing areas. Neural networks successfully address hole-filling tasks in 2D\nimages using inpainting techniques. The combination of surface reconstruction\nalgorithms, guided by the model's curvature properties and the creativity of\nneural networks in the inpainting processes should provide realistic results in\nthe hole completion task. In this paper, we propose a novel method entitled\nSR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks)\nthat incorporates neural network-based 2D inpainting to effectively reconstruct\n3D surfaces. We train the neural networks with images that represent planar\nrepresentations of the curvature at vertices of hundreds of 3D models. Once the\nmissing areas have been inferred, a coarse-to-fine surface deformation process\nensures that the surface fits the reconstructed curvature image. Our proposal\nmakes it possible to learn and generalize patterns from a wide variety of\ntraining 3D models, generating comprehensive inpainted curvature images and\nsurfaces. Experiments conducted on 959 models with several holes have\ndemonstrated that SR-CurvANN excels in the shape completion process, filling\nholes with a remarkable level of realism and precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incomplete or missing data in three-dimensional (3D) models can lead to\nerroneous or flawed renderings, limiting their usefulness in applications such\nas visualization, geometric computation, and 3D printing. Conventional\nsurface-repair techniques often fail to infer complex geometric details in\nmissing areas. Neural networks successfully address hole-filling tasks in 2D\nimages using inpainting techniques. The combination of surface reconstruction\nalgorithms, guided by the model's curvature properties and the creativity of\nneural networks in the inpainting processes should provide realistic results in\nthe hole completion task. In this paper, we propose a novel method entitled\nSR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks)\nthat incorporates neural network-based 2D inpainting to effectively reconstruct\n3D surfaces. We train the neural networks with images that represent planar\nrepresentations of the curvature at vertices of hundreds of 3D models. Once the\nmissing areas have been inferred, a coarse-to-fine surface deformation process\nensures that the surface fits the reconstructed curvature image. Our proposal\nmakes it possible to learn and generalize patterns from a wide variety of\ntraining 3D models, generating comprehensive inpainted curvature images and\nsurfaces. Experiments conducted on 959 models with several holes have\ndemonstrated that SR-CurvANN excels in the shape completion process, filling\nholes with a remarkable level of realism and precision."
                },
                "authors": [
                    {
                        "name": "Marina Hernández-Bautista"
                    },
                    {
                        "name": "Francisco J. Melero"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Melero"
                },
                "author": "Francisco J. Melero",
                "arxiv_comment": "Major changes in title, paper structure, text and figures. Improved\n  results. 23 pages, 14 figures. Decision about submission not taken yet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17711v1",
                "updated": "2024-09-26T10:27:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    27,
                    19,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T10:27:19Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    27,
                    19,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation"
                },
                "summary": "News recommendation is a challenging task that involves personalization based\non the interaction history and preferences of each user. Recent works have\nleveraged the power of pretrained language models (PLMs) to directly rank news\nitems by using inference approaches that predominately fall into three\ncategories: pointwise, pairwise, and listwise learning-to-rank. While pointwise\nmethods offer linear inference complexity, they fail to capture crucial\ncomparative information between items that is more effective for ranking tasks.\nConversely, pairwise and listwise approaches excel at incorporating these\ncomparisons but suffer from practical limitations: pairwise approaches are\neither computationally expensive or lack theoretical guarantees, and listwise\nmethods often perform poorly in practice. In this paper, we propose a novel\nframework for PLM-based news recommendation that integrates both pointwise\nrelevance prediction and pairwise comparisons in a scalable manner. We present\na rigorous theoretical analysis of our framework, establishing conditions under\nwhich our approach guarantees improved performance. Extensive experiments show\nthat our approach outperforms the state-of-the-art methods on the MIND and\nAdressa news recommendation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News recommendation is a challenging task that involves personalization based\non the interaction history and preferences of each user. Recent works have\nleveraged the power of pretrained language models (PLMs) to directly rank news\nitems by using inference approaches that predominately fall into three\ncategories: pointwise, pairwise, and listwise learning-to-rank. While pointwise\nmethods offer linear inference complexity, they fail to capture crucial\ncomparative information between items that is more effective for ranking tasks.\nConversely, pairwise and listwise approaches excel at incorporating these\ncomparisons but suffer from practical limitations: pairwise approaches are\neither computationally expensive or lack theoretical guarantees, and listwise\nmethods often perform poorly in practice. In this paper, we propose a novel\nframework for PLM-based news recommendation that integrates both pointwise\nrelevance prediction and pairwise comparisons in a scalable manner. We present\na rigorous theoretical analysis of our framework, establishing conditions under\nwhich our approach guarantees improved performance. Extensive experiments show\nthat our approach outperforms the state-of-the-art methods on the MIND and\nAdressa news recommendation datasets."
                },
                "authors": [
                    {
                        "name": "Nithish Kannen"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Gerrit J. J. van den Burg"
                    },
                    {
                        "name": "Jean Baptiste Faddoul"
                    }
                ],
                "author_detail": {
                    "name": "Jean Baptiste Faddoul"
                },
                "author": "Jean Baptiste Faddoul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14788v2",
                "updated": "2024-09-26T10:21:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    21,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-07-20T07:39:07Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    7,
                    39,
                    7,
                    5,
                    202,
                    0
                ],
                "title": "On the Design and Analysis of LLM-Based Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Design and Analysis of LLM-Based Algorithms"
                },
                "summary": "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs.\nThrough extensive analytical and empirical investigation in a series of case\nstudies, we demonstrate that the proposed framework is broadly applicable to a\nwide range of scenarios and diverse patterns of LLM-based algorithms, such as\nparallel, hierarchical and recursive task decomposition. Our proposed framework\nholds promise for advancing LLM-based algorithms, by revealing the reasons\nbehind curious empirical phenomena, guiding the choices of hyperparameters,\npredicting the empirical performance of algorithms, and inspiring new algorithm\ndesign. To promote further study of LLM-based algorithms, we release our source\ncode at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs.\nThrough extensive analytical and empirical investigation in a series of case\nstudies, we demonstrate that the proposed framework is broadly applicable to a\nwide range of scenarios and diverse patterns of LLM-based algorithms, such as\nparallel, hierarchical and recursive task decomposition. Our proposed framework\nholds promise for advancing LLM-based algorithms, by revealing the reasons\nbehind curious empirical phenomena, guiding the choices of hyperparameters,\npredicting the empirical performance of algorithms, and inspiring new algorithm\ndesign. To promote further study of LLM-based algorithms, we release our source\ncode at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17699v1",
                "updated": "2024-09-26T10:12:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    12,
                    19,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T10:12:19Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    12,
                    19,
                    3,
                    270,
                    0
                ],
                "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks"
                },
                "summary": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Muhammad Zaid Hamed"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Mark Purcell"
                    }
                ],
                "author_detail": {
                    "name": "Mark Purcell"
                },
                "author": "Mark Purcell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v1",
                "updated": "2024-09-26T09:57:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models will be available soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00459v2",
                "updated": "2024-09-26T09:54:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    54,
                    57,
                    3,
                    270,
                    0
                ],
                "published": "2024-03-30T19:46:59Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    46,
                    59,
                    5,
                    90,
                    0
                ],
                "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning"
                },
                "summary": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Joseph Shtok"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "Assaf Arbelle"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Arbelle"
                },
                "author": "Assaf Arbelle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17683v1",
                "updated": "2024-09-26T09:49:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    49,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:49:27Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    49,
                    27,
                    3,
                    270,
                    0
                ],
                "title": "Zero- and Few-shot Named Entity Recognition and Text Expansion in\n  Medication Prescriptions using ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero- and Few-shot Named Entity Recognition and Text Expansion in\n  Medication Prescriptions using ChatGPT"
                },
                "summary": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data."
                },
                "authors": [
                    {
                        "name": "Natthanaphop Isaradech"
                    },
                    {
                        "name": "Andrea Riedel"
                    },
                    {
                        "name": "Wachiranun Sirikul"
                    },
                    {
                        "name": "Markus Kreuzthaler"
                    },
                    {
                        "name": "Stefan Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schulz"
                },
                "author": "Stefan Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13167v2",
                "updated": "2024-09-26T09:42:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    42,
                    48,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-19T02:46:18Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    46,
                    18,
                    2,
                    171,
                    0
                ],
                "title": "QRMeM: Unleash the Length Limitation through Question then Reflection\n  Memory Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QRMeM: Unleash the Length Limitation through Question then Reflection\n  Memory Mechanism"
                },
                "summary": "While large language models (LLMs) have made notable advancements in natural\nlanguage processing, they continue to struggle with processing extensive text.\nMemory mechanism offers a flexible solution for managing long contexts,\nutilizing techniques such as compression, summarization, and structuring to\nfacilitate nuanced and efficient handling of large volumes of text. However,\nexisting techniques face challenges with static knowledge integration, leading\nto insufficient adaptation to task-specific needs and missing\nmulti-segmentation relationships, which hinders the dynamic reorganization and\nlogical combination of relevant segments during the response process. To\naddress these issues, we introduce a novel strategy, Question then Reflection\nMemory Mechanism (QRMeM), incorporating a dual-structured memory pool. This\npool synergizes static textual content with structured graph guidance,\nfostering a reflective trial-and-error approach for navigating and identifying\nrelevant segments. Our evaluation across multiple-choice questions (MCQ) and\nmulti-document question answering (Multi-doc QA) benchmarks showcases QRMeM\nenhanced performance compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have made notable advancements in natural\nlanguage processing, they continue to struggle with processing extensive text.\nMemory mechanism offers a flexible solution for managing long contexts,\nutilizing techniques such as compression, summarization, and structuring to\nfacilitate nuanced and efficient handling of large volumes of text. However,\nexisting techniques face challenges with static knowledge integration, leading\nto insufficient adaptation to task-specific needs and missing\nmulti-segmentation relationships, which hinders the dynamic reorganization and\nlogical combination of relevant segments during the response process. To\naddress these issues, we introduce a novel strategy, Question then Reflection\nMemory Mechanism (QRMeM), incorporating a dual-structured memory pool. This\npool synergizes static textual content with structured graph guidance,\nfostering a reflective trial-and-error approach for navigating and identifying\nrelevant segments. Our evaluation across multiple-choice questions (MCQ) and\nmulti-document question answering (Multi-doc QA) benchmarks showcases QRMeM\nenhanced performance compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Chong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chong Feng"
                },
                "author": "Chong Feng",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17665v1",
                "updated": "2024-09-26T09:23:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    23,
                    54,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:23:54Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    23,
                    54,
                    3,
                    270,
                    0
                ],
                "title": "A Novel Improved Beluga Whale Optimization Algorithm for Solving\n  Localization Problem in Swarm Robotic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Improved Beluga Whale Optimization Algorithm for Solving\n  Localization Problem in Swarm Robotic Systems"
                },
                "summary": "In Swarm Robotic Systems (SRSs), only a few robots are equipped with Global\nPositioning System (GPS) devices, known as anchors. A challenge lies in\ninferring the positions of other unknown robots based on the positions of\nanchors. Existing solutions estimate their positions using distance\nmeasurements between unknown robots and anchors. Based on existing solutions,\nthis study proposes a novel meta-heuristic algorithm - Improved Beluga Whale\nOptimization Algorithm (IBWO) to address the localization problem of SRSs,\nfocusing on enhancing the accuracy of localization results. Simulation results\ndemonstrate the effectiveness of this study. Specifically, we test the\nlocalization accuracy of robots under different proportions of anchors,\ndifferent communication radius of robots, and different total number of robots.\nCompared to the traditional multilateration method and four other localization\nmethods based on meta-heuristic algorithms, the localization accuracy of this\nmethod is consistently superior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Swarm Robotic Systems (SRSs), only a few robots are equipped with Global\nPositioning System (GPS) devices, known as anchors. A challenge lies in\ninferring the positions of other unknown robots based on the positions of\nanchors. Existing solutions estimate their positions using distance\nmeasurements between unknown robots and anchors. Based on existing solutions,\nthis study proposes a novel meta-heuristic algorithm - Improved Beluga Whale\nOptimization Algorithm (IBWO) to address the localization problem of SRSs,\nfocusing on enhancing the accuracy of localization results. Simulation results\ndemonstrate the effectiveness of this study. Specifically, we test the\nlocalization accuracy of robots under different proportions of anchors,\ndifferent communication radius of robots, and different total number of robots.\nCompared to the traditional multilateration method and four other localization\nmethods based on meta-heuristic algorithms, the localization accuracy of this\nmethod is consistently superior."
                },
                "authors": [
                    {
                        "name": "Zuhao Teng"
                    },
                    {
                        "name": "Qian Dong"
                    }
                ],
                "author_detail": {
                    "name": "Qian Dong"
                },
                "author": "Qian Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12753v2",
                "updated": "2024-09-26T09:17:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    17,
                    10,
                    3,
                    270,
                    0
                ],
                "published": "2024-04-19T09:59:44Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    9,
                    59,
                    44,
                    4,
                    110,
                    0
                ],
                "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper\n  Generation"
                },
                "summary": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}"
                },
                "authors": [
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Chenghao Peng"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Liqian Wen"
                    },
                    {
                        "name": "Zulong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zulong Chen"
                },
                "author": "Zulong Chen",
                "arxiv_comment": "19 pages, 4 figures, 18 tables. Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14816v2",
                "updated": "2024-09-26T09:11:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    11,
                    28,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T08:46:15Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    46,
                    15,
                    0,
                    267,
                    0
                ],
                "title": "VARADE: a Variational-based AutoRegressive model for Anomaly Detection\n  on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VARADE: a Variational-based AutoRegressive model for Anomaly Detection\n  on the Edge"
                },
                "summary": "Detecting complex anomalies on massive amounts of data is a crucial task in\nIndustry 4.0, best addressed by deep learning. However, available solutions are\ncomputationally demanding, requiring cloud architectures prone to latency and\nbandwidth issues. This work presents VARADE, a novel solution implementing a\nlight autoregressive framework based on variational inference, which is best\nsuited for real-time execution on the edge. The proposed approach was validated\non a robotic arm, part of a pilot production line, and compared with several\nstate-of-the-art algorithms, obtaining the best trade-off between anomaly\ndetection accuracy, power consumption and inference frequency on two different\nedge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting complex anomalies on massive amounts of data is a crucial task in\nIndustry 4.0, best addressed by deep learning. However, available solutions are\ncomputationally demanding, requiring cloud architectures prone to latency and\nbandwidth issues. This work presents VARADE, a novel solution implementing a\nlight autoregressive framework based on variational inference, which is best\nsuited for real-time execution on the edge. The proposed approach was validated\non a robotic arm, part of a pilot production line, and compared with several\nstate-of-the-art algorithms, obtaining the best trade-off between anomaly\ndetection accuracy, power consumption and inference frequency on two different\nedge platforms."
                },
                "authors": [
                    {
                        "name": "Alessio Mascolini"
                    },
                    {
                        "name": "Sebastiano Gaiardelli"
                    },
                    {
                        "name": "Francesco Ponzio"
                    },
                    {
                        "name": "Nicola Dall'Ora"
                    },
                    {
                        "name": "Enrico Macii"
                    },
                    {
                        "name": "Sara Vinco"
                    },
                    {
                        "name": "Santa Di Cataldo"
                    },
                    {
                        "name": "Franco Fummi"
                    }
                ],
                "author_detail": {
                    "name": "Franco Fummi"
                },
                "author": "Franco Fummi",
                "arxiv_doi": "10.1145/3649329.3655691",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3655691",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.14816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17655v1",
                "updated": "2024-09-26T09:06:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    6,
                    56,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:06:56Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    6,
                    56,
                    3,
                    270,
                    0
                ],
                "title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative\n  Human-Populated Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative\n  Human-Populated Environment"
                },
                "summary": "The increasing demand for intelligent assistants in human-populated\nenvironments has motivated significant research in autonomous robotic systems.\nTraditional service robots and virtual assistants, however, struggle with\nreal-world task execution due to their limited capacity for dynamic reasoning\nand interaction, particularly when human collaboration is required. Recent\ndevelopments in Large Language Models have opened new avenues for improving\nthese systems, enabling more sophisticated reasoning and natural interaction\ncapabilities. In this paper, we introduce AssistantX, an LLM-powered proactive\nassistant designed to operate autonomously in a physical office environment.\nUnlike conventional service robots, AssistantX leverages a novel multi-agent\narchitecture, PPDR4X, which provides advanced inference capabilities and\ncomprehensive collaboration awareness. By effectively bridging the gap between\nvirtual operations and physical interactions, AssistantX demonstrates robust\nperformance in managing complex real-world scenarios. Our evaluation highlights\nthe architecture's effectiveness, showing that AssistantX can respond to clear\ninstructions, actively retrieve supplementary information from memory, and\nproactively seek collaboration from team members to ensure successful task\ncompletion. More details and videos can be found at\nhttps://assistantx-agent.github.io/AssistantX/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for intelligent assistants in human-populated\nenvironments has motivated significant research in autonomous robotic systems.\nTraditional service robots and virtual assistants, however, struggle with\nreal-world task execution due to their limited capacity for dynamic reasoning\nand interaction, particularly when human collaboration is required. Recent\ndevelopments in Large Language Models have opened new avenues for improving\nthese systems, enabling more sophisticated reasoning and natural interaction\ncapabilities. In this paper, we introduce AssistantX, an LLM-powered proactive\nassistant designed to operate autonomously in a physical office environment.\nUnlike conventional service robots, AssistantX leverages a novel multi-agent\narchitecture, PPDR4X, which provides advanced inference capabilities and\ncomprehensive collaboration awareness. By effectively bridging the gap between\nvirtual operations and physical interactions, AssistantX demonstrates robust\nperformance in managing complex real-world scenarios. Our evaluation highlights\nthe architecture's effectiveness, showing that AssistantX can respond to clear\ninstructions, actively retrieve supplementary information from memory, and\nproactively seek collaboration from team members to ensure successful task\ncompletion. More details and videos can be found at\nhttps://assistantx-agent.github.io/AssistantX/."
                },
                "authors": [
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Yongchang Li"
                    },
                    {
                        "name": "Lumeng Ma"
                    },
                    {
                        "name": "Di Guo"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "arxiv_comment": "6 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17650v1",
                "updated": "2024-09-26T08:56:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    56,
                    54,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:56:54Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    56,
                    54,
                    3,
                    270,
                    0
                ],
                "title": "Digital Twin Ecosystem for Oncology Clinical Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin Ecosystem for Oncology Clinical Operations"
                },
                "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations."
                },
                "authors": [
                    {
                        "name": "Himanshu Pandey"
                    },
                    {
                        "name": "Akhil Amod"
                    },
                    {
                        "name": "Shivang"
                    },
                    {
                        "name": "Kshitij Jaggi"
                    },
                    {
                        "name": "Ruchi Garg"
                    },
                    {
                        "name": "Abheet Jain"
                    },
                    {
                        "name": "Vinayak Tantia"
                    }
                ],
                "author_detail": {
                    "name": "Vinayak Tantia"
                },
                "author": "Vinayak Tantia",
                "arxiv_comment": "Pre Print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17648v1",
                "updated": "2024-09-26T08:55:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    55,
                    21,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:55:21Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    55,
                    21,
                    3,
                    270,
                    0
                ],
                "title": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited."
                },
                "authors": [
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Phat Vo"
                    },
                    {
                        "name": "Arman Kizilkale"
                    },
                    {
                        "name": "Aaron Reite"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reite"
                },
                "author": "Aaron Reite",
                "arxiv_comment": "6 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16908v2",
                "updated": "2024-09-26T08:53:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    53,
                    1,
                    3,
                    270,
                    0
                ],
                "published": "2024-05-27T07:56:23Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    7,
                    56,
                    23,
                    0,
                    148,
                    0
                ],
                "title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty\n  in Words?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty\n  in Words?"
                },
                "summary": "We posit that large language models (LLMs) should be capable of expressing\ntheir intrinsic uncertainty in natural language. For example, if the LLM is\nequally likely to output two contradicting answers to the same question, then\nits generated response should reflect this uncertainty by hedging its answer\n(e.g., \"I'm not sure, but I think...\"). We formalize faithful response\nuncertainty based on the gap between the model's intrinsic confidence in the\nassertions it makes and the decisiveness by which they are conveyed. This\nexample-level metric reliably indicates whether the model reflects its\nuncertainty, as it penalizes both excessive and insufficient hedging. We\nevaluate a variety of aligned LLMs at faithfully communicating uncertainty on\nseveral knowledge-intensive question answering tasks. Our results provide\nstrong evidence that modern LLMs are poor at faithfully conveying their\nuncertainty, and that better alignment is necessary to improve their\ntrustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We posit that large language models (LLMs) should be capable of expressing\ntheir intrinsic uncertainty in natural language. For example, if the LLM is\nequally likely to output two contradicting answers to the same question, then\nits generated response should reflect this uncertainty by hedging its answer\n(e.g., \"I'm not sure, but I think...\"). We formalize faithful response\nuncertainty based on the gap between the model's intrinsic confidence in the\nassertions it makes and the decisiveness by which they are conveyed. This\nexample-level metric reliably indicates whether the model reflects its\nuncertainty, as it penalizes both excessive and insufficient hedging. We\nevaluate a variety of aligned LLMs at faithfully communicating uncertainty on\nseveral knowledge-intensive question answering tasks. Our results provide\nstrong evidence that modern LLMs are poor at faithfully conveying their\nuncertainty, and that better alignment is necessary to improve their\ntrustworthiness."
                },
                "authors": [
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "To appear in EMNLP 2024 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17647v1",
                "updated": "2024-09-26T08:51:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    51,
                    29,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:51:29Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    51,
                    29,
                    3,
                    270,
                    0
                ],
                "title": "MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning"
                },
                "summary": "Video causal reasoning aims to achieve a high-level understanding of video\ncontent from a causal perspective. However, current video reasoning tasks are\nlimited in scope, primarily executed in a question-answering paradigm and\nfocusing on short videos containing only a single event and simple causal\nrelationships, lacking comprehensive and structured causality analysis for\nvideos with multiple events. To fill this gap, we introduce a new task and\ndataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal\nrelationships between events distributed chronologically across long videos.\nGiven visual segments and textual descriptions of events, MECD requires\nidentifying the causal associations between these events to derive a\ncomprehensive, structured event-level video causal diagram explaining why and\nhow the final result event occurred. To address MECD, we devise a novel\nframework inspired by the Granger Causality method, using an efficient\nmask-based event prediction model to perform an Event Granger Test, which\nestimates causality by comparing the predicted result event when premise events\nare masked versus unmasked. Furthermore, we integrate causal inference\ntechniques such as front-door adjustment and counterfactual inference to\naddress challenges in MECD like causality confounding and illusory causality.\nExperiments validate the effectiveness of our framework in providing causal\nrelationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by\n5.7% and 4.1%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video causal reasoning aims to achieve a high-level understanding of video\ncontent from a causal perspective. However, current video reasoning tasks are\nlimited in scope, primarily executed in a question-answering paradigm and\nfocusing on short videos containing only a single event and simple causal\nrelationships, lacking comprehensive and structured causality analysis for\nvideos with multiple events. To fill this gap, we introduce a new task and\ndataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal\nrelationships between events distributed chronologically across long videos.\nGiven visual segments and textual descriptions of events, MECD requires\nidentifying the causal associations between these events to derive a\ncomprehensive, structured event-level video causal diagram explaining why and\nhow the final result event occurred. To address MECD, we devise a novel\nframework inspired by the Granger Causality method, using an efficient\nmask-based event prediction model to perform an Event Granger Test, which\nestimates causality by comparing the predicted result event when premise events\nare masked versus unmasked. Furthermore, we integrate causal inference\ntechniques such as front-door adjustment and counterfactual inference to\naddress challenges in MECD like causality confounding and illusory causality.\nExperiments validate the effectiveness of our framework in providing causal\nrelationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by\n5.7% and 4.1%, respectively."
                },
                "authors": [
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Huabin Liu"
                    },
                    {
                        "name": "Tianyao He"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Chaofan Gan"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Cheng Zhong"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yingxue Wang"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "arxiv_comment": "Accepted at NeurIPS 2024 as a spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15246v2",
                "updated": "2024-09-26T08:48:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    48,
                    3,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T17:42:05Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    42,
                    5,
                    0,
                    267,
                    0
                ],
                "title": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks"
                },
                "summary": "Earth Observation (EO) systems play a crucial role in achieving Sustainable\nDevelopment Goals by collecting and analyzing vital global data through\nsatellite networks. These systems are essential for tasks like mapping,\ndisaster monitoring, and resource management, but they face challenges in\nprocessing and transmitting large volumes of EO data, especially in specialized\nfields such as agriculture and real-time disaster response. Domain-adapted\nLarge Language Models (LLMs) provide a promising solution by facilitating data\nfusion between extensive EO data and semantic EO data. By improving integration\nand interpretation of diverse datasets, LLMs address the challenges of\nprocessing specialized information in agriculture and disaster response\napplications. This fusion enhances the accuracy and relevance of transmitted\ndata. This paper presents a framework for semantic communication in EO\nsatellite networks, aimed at improving data transmission efficiency and overall\nsystem performance through cognitive processing techniques. The proposed system\nemploys Discrete-Task-Oriented Source-Channel Coding (DT-JSCC) and Semantic\nData Augmentation (SA) to focus on relevant information while minimizing\ncommunication overhead. By integrating cognitive semantic processing and\ninter-satellite links, the framework enhances the analysis and transmission of\nmultispectral satellite imagery, improving object detection, pattern\nrecognition, and real-time decision-making. The introduction of Cognitive\nSemantic Augmentation (CSA) allows satellites to process and transmit semantic\ninformation, boosting adaptability to changing environments and application\nneeds. This end-to-end architecture is tailored for next-generation satellite\nnetworks, such as those supporting 6G, and demonstrates significant\nimprovements in efficiency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) systems play a crucial role in achieving Sustainable\nDevelopment Goals by collecting and analyzing vital global data through\nsatellite networks. These systems are essential for tasks like mapping,\ndisaster monitoring, and resource management, but they face challenges in\nprocessing and transmitting large volumes of EO data, especially in specialized\nfields such as agriculture and real-time disaster response. Domain-adapted\nLarge Language Models (LLMs) provide a promising solution by facilitating data\nfusion between extensive EO data and semantic EO data. By improving integration\nand interpretation of diverse datasets, LLMs address the challenges of\nprocessing specialized information in agriculture and disaster response\napplications. This fusion enhances the accuracy and relevance of transmitted\ndata. This paper presents a framework for semantic communication in EO\nsatellite networks, aimed at improving data transmission efficiency and overall\nsystem performance through cognitive processing techniques. The proposed system\nemploys Discrete-Task-Oriented Source-Channel Coding (DT-JSCC) and Semantic\nData Augmentation (SA) to focus on relevant information while minimizing\ncommunication overhead. By integrating cognitive semantic processing and\ninter-satellite links, the framework enhances the analysis and transmission of\nmultispectral satellite imagery, improving object detection, pattern\nrecognition, and real-time decision-making. The introduction of Cognitive\nSemantic Augmentation (CSA) allows satellites to process and transmit semantic\ninformation, boosting adaptability to changing environments and application\nneeds. This end-to-end architecture is tailored for next-generation satellite\nnetworks, such as those supporting 6G, and demonstrates significant\nimprovements in efficiency and accuracy."
                },
                "authors": [
                    {
                        "name": "Hong-fu Chou"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Prabhu Thiruvasagam"
                    },
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Geoffrey Eappen"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Luis M. Garces-Socarras"
                    },
                    {
                        "name": "Jorge L. Gonzalez-Rios"
                    },
                    {
                        "name": "Juan Carlos Merlano-Duncan"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "arxiv_comment": "18 pages, 10 figures, magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10902v2",
                "updated": "2024-09-26T08:47:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    47,
                    36,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-20T14:45:23Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    45,
                    23,
                    1,
                    233,
                    0
                ],
                "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs"
                },
                "summary": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation."
                },
                "authors": [
                    {
                        "name": "John Mendonça"
                    },
                    {
                        "name": "Isabel Trancoso"
                    },
                    {
                        "name": "Alon Lavie"
                    }
                ],
                "author_detail": {
                    "name": "Alon Lavie"
                },
                "author": "Alon Lavie",
                "arxiv_comment": "Accepted to EMNLP2024 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02733v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02733v3",
                "updated": "2024-09-26T08:45:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    45,
                    22,
                    3,
                    270,
                    0
                ],
                "published": "2023-06-05T09:29:46Z",
                "published_parsed": [
                    2023,
                    6,
                    5,
                    9,
                    29,
                    46,
                    0,
                    156,
                    0
                ],
                "title": "Realising Synthetic Active Inference Agents, Part II: Variational\n  Message Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realising Synthetic Active Inference Agents, Part II: Variational\n  Message Updates"
                },
                "summary": "The Free Energy Principle (FEP) describes (biological) agents as minimising a\nvariational Free Energy (FE) with respect to a generative model of their\nenvironment. Active Inference (AIF) is a corollary of the FEP that describes\nhow agents explore and exploit their environment by minimising an expected FE\nobjective. In two related papers, we describe a scalable, epistemic approach to\nsynthetic AIF, by message passing on free-form Forney-style Factor Graphs\n(FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG) notation\nthat visually represents (generalised) FE objectives for AIF. The current paper\n(part II) derives message passing algorithms that minimise (generalised) FE\nobjectives on a CFFG by variational calculus. A comparison between simulated\nBethe and generalised FE agents illustrates how the message passing approach to\nsynthetic AIF induces epistemic behaviour on a T-maze navigation task.\nExtension of the T-maze simulation to 1) learning goal statistics, and 2) a\nmulti-agent bargaining setting, illustrate how this approach encourages reuse\nof nodes and updates in alternative settings. With a full message passing\naccount of synthetic AIF agents, it becomes possible to derive and reuse\nmessage updates across models and move closer to industrial applications of\nsynthetic AIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Free Energy Principle (FEP) describes (biological) agents as minimising a\nvariational Free Energy (FE) with respect to a generative model of their\nenvironment. Active Inference (AIF) is a corollary of the FEP that describes\nhow agents explore and exploit their environment by minimising an expected FE\nobjective. In two related papers, we describe a scalable, epistemic approach to\nsynthetic AIF, by message passing on free-form Forney-style Factor Graphs\n(FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG) notation\nthat visually represents (generalised) FE objectives for AIF. The current paper\n(part II) derives message passing algorithms that minimise (generalised) FE\nobjectives on a CFFG by variational calculus. A comparison between simulated\nBethe and generalised FE agents illustrates how the message passing approach to\nsynthetic AIF induces epistemic behaviour on a T-maze navigation task.\nExtension of the T-maze simulation to 1) learning goal statistics, and 2) a\nmulti-agent bargaining setting, illustrate how this approach encourages reuse\nof nodes and updates in alternative settings. With a full message passing\naccount of synthetic AIF agents, it becomes possible to derive and reuse\nmessage updates across models and move closer to industrial applications of\nsynthetic AIF."
                },
                "authors": [
                    {
                        "name": "Thijs van de Laar"
                    },
                    {
                        "name": "Magnus Koudahl"
                    },
                    {
                        "name": "Bert de Vries"
                    }
                ],
                "author_detail": {
                    "name": "Bert de Vries"
                },
                "author": "Bert de Vries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02733v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02733v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17642v1",
                "updated": "2024-09-26T08:45:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    45,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:45:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    45,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure"
                },
                "summary": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions."
                },
                "authors": [
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Hangxin Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17640v1",
                "updated": "2024-09-26T08:44:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task"
                },
                "summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations."
                },
                "authors": [
                    {
                        "name": "Xindi Tong"
                    },
                    {
                        "name": "Yujin Zhu"
                    },
                    {
                        "name": "Shijian Fan"
                    },
                    {
                        "name": "Liang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xu"
                },
                "author": "Liang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10176v2",
                "updated": "2024-09-26T08:37:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    37,
                    5,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-15T19:57:15Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    19,
                    57,
                    15,
                    4,
                    349,
                    0
                ],
                "title": "Spectral estimation for spatial point processes and random fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral estimation for spatial point processes and random fields"
                },
                "summary": "Spatial variables can be observed in many different forms, such as regularly\nsampled random fields (lattice data), point processes, and randomly sampled\nspatial processes. Joint analysis of such collections of observations is\nclearly desirable, but complicated by the lack of an easily implementable\nanalysis framework. It is well known that Fourier transforms provide such a\nframework, but its form has eluded data analysts. We formalize it by providing\na multitaper analysis framework using coupled discrete and continuous data\ntapers, combined with the discrete Fourier transform for inference. Using this\nset of tools is important, as it forms the backbone for practical spectral\nanalysis. In higher dimensions it is important not to be constrained to\nCartesian product domains, and so we develop the methodology for spectral\nanalysis using irregular domain data tapers, and the tapered discrete Fourier\ntransform. We discuss its fast implementation, and the asymptotic as well as\nlarge finite domain properties. Estimators of partial association between\ndifferent spatial processes are provided as are principled methods to determine\ntheir significance, and we demonstrate their practical utility on a large-scale\necological dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial variables can be observed in many different forms, such as regularly\nsampled random fields (lattice data), point processes, and randomly sampled\nspatial processes. Joint analysis of such collections of observations is\nclearly desirable, but complicated by the lack of an easily implementable\nanalysis framework. It is well known that Fourier transforms provide such a\nframework, but its form has eluded data analysts. We formalize it by providing\na multitaper analysis framework using coupled discrete and continuous data\ntapers, combined with the discrete Fourier transform for inference. Using this\nset of tools is important, as it forms the backbone for practical spectral\nanalysis. In higher dimensions it is important not to be constrained to\nCartesian product domains, and so we develop the methodology for spectral\nanalysis using irregular domain data tapers, and the tapered discrete Fourier\ntransform. We discuss its fast implementation, and the asymptotic as well as\nlarge finite domain properties. Estimators of partial association between\ndifferent spatial processes are provided as are principled methods to determine\ntheir significance, and we demonstrate their practical utility on a large-scale\necological dataset."
                },
                "authors": [
                    {
                        "name": "Jake P. Grainger"
                    },
                    {
                        "name": "Tuomas A. Rajala"
                    },
                    {
                        "name": "David J. Murrell"
                    },
                    {
                        "name": "Sofia C. Olhede"
                    }
                ],
                "author_detail": {
                    "name": "Sofia C. Olhede"
                },
                "author": "Sofia C. Olhede",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17635v1",
                "updated": "2024-09-26T08:32:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    32,
                    31,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:32:31Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    32,
                    31,
                    3,
                    270,
                    0
                ],
                "title": "FlowMAC: Conditional Flow Matching for Audio Coding at Low Bit Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowMAC: Conditional Flow Matching for Audio Coding at Low Bit Rates"
                },
                "summary": "This paper introduces FlowMAC, a novel neural audio codec for high-quality\ngeneral audio compression at low bit rates based on conditional flow matching\n(CFM). FlowMAC jointly learns a mel spectrogram encoder, quantizer and decoder.\nAt inference time the decoder integrates a continuous normalizing flow via an\nODE solver to generate a high-quality mel spectrogram. This is the first time\nthat a CFM-based approach is applied to general audio coding, enabling a\nscalable, simple and memory efficient training. Our subjective evaluations show\nthat FlowMAC at 3 kbps achieves similar quality as state-of-the-art GAN-based\nand DDPM-based neural audio codecs at double the bit rate. Moreover, FlowMAC\noffers a tunable inference pipeline, which permits to trade off complexity and\nquality. This enables real-time coding on CPU, while maintaining high\nperceptual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces FlowMAC, a novel neural audio codec for high-quality\ngeneral audio compression at low bit rates based on conditional flow matching\n(CFM). FlowMAC jointly learns a mel spectrogram encoder, quantizer and decoder.\nAt inference time the decoder integrates a continuous normalizing flow via an\nODE solver to generate a high-quality mel spectrogram. This is the first time\nthat a CFM-based approach is applied to general audio coding, enabling a\nscalable, simple and memory efficient training. Our subjective evaluations show\nthat FlowMAC at 3 kbps achieves similar quality as state-of-the-art GAN-based\nand DDPM-based neural audio codecs at double the bit rate. Moreover, FlowMAC\noffers a tunable inference pipeline, which permits to trade off complexity and\nquality. This enables real-time coding on CPU, while maintaining high\nperceptual quality."
                },
                "authors": [
                    {
                        "name": "Nicola Pia"
                    },
                    {
                        "name": "Martin Strauss"
                    },
                    {
                        "name": "Markus Multrus"
                    },
                    {
                        "name": "Bernd Edler"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Edler"
                },
                "author": "Bernd Edler",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2205.13469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2205.13469v3",
                "updated": "2024-09-26T08:25:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    25,
                    25,
                    3,
                    270,
                    0
                ],
                "published": "2022-05-26T16:21:09Z",
                "published_parsed": [
                    2022,
                    5,
                    26,
                    16,
                    21,
                    9,
                    3,
                    146,
                    0
                ],
                "title": "Proximal Estimation and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proximal Estimation and Inference"
                },
                "summary": "We build a unifying convex analysis framework characterizing the statistical\nproperties of a large class of penalized estimators, both under a regular and\nan irregular design. Our framework interprets penalized estimators as proximal\nestimators, defined by a proximal operator applied to a corresponding initial\nestimator. We characterize the asymptotic properties of proximal estimators,\nshowing that their asymptotic distribution follows a closed-form formula\ndepending only on (i) the asymptotic distribution of the initial estimator,\n(ii) the estimator's limit penalty subgradient and (iii) the inner product\ndefining the associated proximal operator. In parallel, we characterize the\nOracle features of proximal estimators from the properties of their penalty's\nsubgradients. We exploit our approach to systematically cover linear regression\nsettings with a regular or irregular design. For these settings, we build new\n$\\sqrt{n}-$consistent, asymptotically normal Ridgeless-type proximal\nestimators, which feature the Oracle property and are shown to perform\nsatisfactorily in practically relevant Monte Carlo settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We build a unifying convex analysis framework characterizing the statistical\nproperties of a large class of penalized estimators, both under a regular and\nan irregular design. Our framework interprets penalized estimators as proximal\nestimators, defined by a proximal operator applied to a corresponding initial\nestimator. We characterize the asymptotic properties of proximal estimators,\nshowing that their asymptotic distribution follows a closed-form formula\ndepending only on (i) the asymptotic distribution of the initial estimator,\n(ii) the estimator's limit penalty subgradient and (iii) the inner product\ndefining the associated proximal operator. In parallel, we characterize the\nOracle features of proximal estimators from the properties of their penalty's\nsubgradients. We exploit our approach to systematically cover linear regression\nsettings with a regular or irregular design. For these settings, we build new\n$\\sqrt{n}-$consistent, asymptotically normal Ridgeless-type proximal\nestimators, which feature the Oracle property and are shown to perform\nsatisfactorily in practically relevant Monte Carlo settings."
                },
                "authors": [
                    {
                        "name": "Alberto Quaini"
                    },
                    {
                        "name": "Fabio Trojani"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Trojani"
                },
                "author": "Fabio Trojani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2205.13469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2205.13469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09802v2",
                "updated": "2024-09-26T08:15:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    15,
                    50,
                    3,
                    270,
                    0
                ],
                "published": "2023-11-16T11:26:21Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    26,
                    21,
                    3,
                    320,
                    0
                ],
                "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs"
                },
                "summary": "Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CaRing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CaRing"
                },
                "authors": [
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14208v2",
                "updated": "2024-09-26T08:12:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    12,
                    59,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-20T11:26:06Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    11,
                    26,
                    6,
                    3,
                    172,
                    0
                ],
                "title": "SeCoKD: Aligning Large Language Models for In-Context Learning with\n  Fewer Shots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeCoKD: Aligning Large Language Models for In-Context Learning with\n  Fewer Shots"
                },
                "summary": "Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning."
                },
                "authors": [
                    {
                        "name": "Weixing Wang"
                    },
                    {
                        "name": "Haojin Yang"
                    },
                    {
                        "name": "Christoph Meinel"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Meinel"
                },
                "author": "Christoph Meinel",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15254v3",
                "updated": "2024-09-26T08:01:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    1,
                    39,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T17:53:42Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    53,
                    42,
                    0,
                    267,
                    0
                ],
                "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Archon: An Architecture Search Framework for Inference-Time Techniques"
                },
                "summary": "Inference-time techniques are emerging as highly effective tools to increase\nlarge language model (LLM) capabilities. However, there is still limited\nunderstanding of the best practices for developing systems that combine\ninference-time techniques with one or more LLMs, with challenges including: (1)\neffectively allocating inference compute budget, (2) understanding the\ninteractions between different combinations of inference-time techniques and\ntheir impact on downstream performance, and 3) efficiently searching over the\nlarge space of model choices, inference-time techniques, and their\ncompositions. To address these challenges, we introduce Archon, an automated\nframework for designing inference-time architectures. Archon defines an\nextensible design space, encompassing methods such as generation ensembling,\nmulti-sampling, ranking, fusion, critiquing, verification, and unit testing. It\nthen transforms the problem of selecting and combining LLMs and inference-time\ntechniques into a hyperparameter optimization objective. To optimize this\nobjective, we introduce automated Inference-Time Architecture Search (ITAS)\nalgorithms. Given target benchmark(s), an inference compute budget, and\navailable LLMs, ITAS outputs optimized architectures. We evaluate Archon\narchitectures across a wide range of instruction-following and reasoning\nbenchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,\nMixEval Hard, MATH, and CodeContests. We show that automatically designed\ninference-time architectures by Archon outperform strong models such as GPT-4o\nand Claude 3.5 Sonnet on these benchmarks, achieving an average increase of\n15.1 and 11.2 percentage points with all-source models and open-source models,\nrespectively. We make our code and datasets available publicly on Github:\nhttps://github.com/ScalingIntelligence/Archon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time techniques are emerging as highly effective tools to increase\nlarge language model (LLM) capabilities. However, there is still limited\nunderstanding of the best practices for developing systems that combine\ninference-time techniques with one or more LLMs, with challenges including: (1)\neffectively allocating inference compute budget, (2) understanding the\ninteractions between different combinations of inference-time techniques and\ntheir impact on downstream performance, and 3) efficiently searching over the\nlarge space of model choices, inference-time techniques, and their\ncompositions. To address these challenges, we introduce Archon, an automated\nframework for designing inference-time architectures. Archon defines an\nextensible design space, encompassing methods such as generation ensembling,\nmulti-sampling, ranking, fusion, critiquing, verification, and unit testing. It\nthen transforms the problem of selecting and combining LLMs and inference-time\ntechniques into a hyperparameter optimization objective. To optimize this\nobjective, we introduce automated Inference-Time Architecture Search (ITAS)\nalgorithms. Given target benchmark(s), an inference compute budget, and\navailable LLMs, ITAS outputs optimized architectures. We evaluate Archon\narchitectures across a wide range of instruction-following and reasoning\nbenchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,\nMixEval Hard, MATH, and CodeContests. We show that automatically designed\ninference-time architectures by Archon outperform strong models such as GPT-4o\nand Claude 3.5 Sonnet on these benchmarks, achieving an average increase of\n15.1 and 11.2 percentage points with all-source models and open-source models,\nrespectively. We make our code and datasets available publicly on Github:\nhttps://github.com/ScalingIntelligence/Archon."
                },
                "authors": [
                    {
                        "name": "Jon Saad-Falcon"
                    },
                    {
                        "name": "Adrian Gamarra Lafuente"
                    },
                    {
                        "name": "Shlok Natarajan"
                    },
                    {
                        "name": "Nahum Maru"
                    },
                    {
                        "name": "Hristo Todorov"
                    },
                    {
                        "name": "Etash Guha"
                    },
                    {
                        "name": "E. Kelly Buchanan"
                    },
                    {
                        "name": "Mayee Chen"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11373v2",
                "updated": "2024-09-26T07:57:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    57,
                    11,
                    3,
                    270,
                    0
                ],
                "published": "2024-04-17T13:31:51Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    13,
                    31,
                    51,
                    2,
                    108,
                    0
                ],
                "title": "Simulation-based inference of black hole ringdowns in the time domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference of black hole ringdowns in the time domain"
                },
                "summary": "Gravitational waves emitted by a ringing black hole allow us to perform\nprecision tests of general relativity in the strong field regime. With\nimprovements to our current gravitational wave detectors and upcoming\nnext-generation detectors, developing likelihood-free parameter inference\ninfrastructure is critical as we will face complications like nonstandard noise\nproperties, partial data and incomplete signal modeling that may not allow for\nan analytically tractable likelihood function. In this work, we present a\nproof-of-concept strategy to perform likelihood-free Bayesian inference on\nringdown gravitational waves using simulation based inference. Specifically,\nour method is based on truncated sequential neural posterior estimation, which\ntrains a neural density estimator of the posterior for a specific observed data\nsegment. We setup the ringdown parameter estimation directly in the time\ndomain. We show that the parameter estimation results obtained using our\ntrained networks are in agreement with well-established Markov-chain methods\nfor simulated injections as well as analysis on real detector data\ncorresponding to GW150914. Additionally, to assess our approach's internal\nconsistency, we show that the density estimators pass a Bayesian coverage test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves emitted by a ringing black hole allow us to perform\nprecision tests of general relativity in the strong field regime. With\nimprovements to our current gravitational wave detectors and upcoming\nnext-generation detectors, developing likelihood-free parameter inference\ninfrastructure is critical as we will face complications like nonstandard noise\nproperties, partial data and incomplete signal modeling that may not allow for\nan analytically tractable likelihood function. In this work, we present a\nproof-of-concept strategy to perform likelihood-free Bayesian inference on\nringdown gravitational waves using simulation based inference. Specifically,\nour method is based on truncated sequential neural posterior estimation, which\ntrains a neural density estimator of the posterior for a specific observed data\nsegment. We setup the ringdown parameter estimation directly in the time\ndomain. We show that the parameter estimation results obtained using our\ntrained networks are in agreement with well-established Markov-chain methods\nfor simulated injections as well as analysis on real detector data\ncorresponding to GW150914. Additionally, to assess our approach's internal\nconsistency, we show that the density estimators pass a Bayesian coverage test."
                },
                "authors": [
                    {
                        "name": "Costantino Pacilio"
                    },
                    {
                        "name": "Swetha Bhagwat"
                    },
                    {
                        "name": "Roberto Cotesta"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Cotesta"
                },
                "author": "Roberto Cotesta",
                "arxiv_comment": "12 pages, 7 figures; v2: minor changes, matches version to appear on\n  Phys.Rev.D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17610v1",
                "updated": "2024-09-26T07:55:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    55,
                    57,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:55:57Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    55,
                    57,
                    3,
                    270,
                    0
                ],
                "title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context\n  Information in Multi-Turn Multimodal Medical Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context\n  Information in Multi-Turn Multimodal Medical Dialogue"
                },
                "summary": "The rocketing prosperity of large language models (LLMs) in recent years has\nboosted the prevalence of vision-language models (VLMs) in the medical sector.\nIn our online medical consultation scenario, a doctor responds to the texts and\nimages provided by a patient in multiple rounds to diagnose her/his health\ncondition, forming a multi-turn multimodal medical dialogue format. Unlike\nhigh-quality images captured by professional equipment in traditional medical\nvisual question answering (Med-VQA), the images in our case are taken by\npatients' mobile phones. These images have poor quality control, with issues\nsuch as excessive background elements and the lesion area being significantly\noff-center, leading to degradation of vision-language alignment in the model\ntraining phase. In this paper, we propose ZALM3, a Zero-shot strategy to\nimprove vision-language ALignment in Multi-turn Multimodal Medical dialogue.\nSince we observe that the preceding text conversations before an image can\ninfer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to\nsummarize the keywords from the preceding context and a visual grounding model\nto extract the RoIs. The updated images eliminate unnecessary background noise\nand provide more effective vision-language alignment. To better evaluate our\nproposed method, we design a new subjective assessment metric for multi-turn\nunimodal/multimodal medical dialogue to provide a fine-grained performance\ncomparison. Our experiments across three different clinical departments\nremarkably demonstrate the efficacy of ZALM3 with statistical significance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rocketing prosperity of large language models (LLMs) in recent years has\nboosted the prevalence of vision-language models (VLMs) in the medical sector.\nIn our online medical consultation scenario, a doctor responds to the texts and\nimages provided by a patient in multiple rounds to diagnose her/his health\ncondition, forming a multi-turn multimodal medical dialogue format. Unlike\nhigh-quality images captured by professional equipment in traditional medical\nvisual question answering (Med-VQA), the images in our case are taken by\npatients' mobile phones. These images have poor quality control, with issues\nsuch as excessive background elements and the lesion area being significantly\noff-center, leading to degradation of vision-language alignment in the model\ntraining phase. In this paper, we propose ZALM3, a Zero-shot strategy to\nimprove vision-language ALignment in Multi-turn Multimodal Medical dialogue.\nSince we observe that the preceding text conversations before an image can\ninfer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to\nsummarize the keywords from the preceding context and a visual grounding model\nto extract the RoIs. The updated images eliminate unnecessary background noise\nand provide more effective vision-language alignment. To better evaluate our\nproposed method, we design a new subjective assessment metric for multi-turn\nunimodal/multimodal medical dialogue to provide a fine-grained performance\ncomparison. Our experiments across three different clinical departments\nremarkably demonstrate the efficacy of ZALM3 with statistical significance."
                },
                "authors": [
                    {
                        "name": "Zhangpu Li"
                    },
                    {
                        "name": "Changhong Zou"
                    },
                    {
                        "name": "Suxue Ma"
                    },
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Chen Du"
                    },
                    {
                        "name": "Youbao Tang"
                    },
                    {
                        "name": "Zhenjie Cao"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Jui-Hsin Lai"
                    },
                    {
                        "name": "Ruei-Sung Lin"
                    },
                    {
                        "name": "Yuan Ni"
                    },
                    {
                        "name": "Xingzhi Sun"
                    },
                    {
                        "name": "Jing Xiao"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Mei Han"
                    }
                ],
                "author_detail": {
                    "name": "Mei Han"
                },
                "author": "Mei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16341v2",
                "updated": "2024-09-26T07:54:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    54,
                    10,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T17:20:02Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    20,
                    2,
                    1,
                    268,
                    0
                ],
                "title": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs"
                },
                "summary": "Training large language models (LLMs) for external tool usage is a rapidly\nexpanding field, with recent research focusing on generating synthetic data to\naddress the shortage of available data. However, the absence of systematic data\nquality checks poses complications for properly training and testing models. To\nthat end, we propose two approaches for assessing the reliability of data for\ntraining LLMs to use external tools. The first approach uses intuitive,\nhuman-defined correctness criteria. The second approach uses a model-driven\nassessment with in-context evaluation. We conduct a thorough evaluation of data\nquality on two popular benchmarks, followed by an extrinsic evaluation that\nshowcases the impact of data quality on model performance. Our results\ndemonstrate that models trained on high-quality data outperform those trained\non unvalidated data, even when trained with a smaller quantity of data. These\nfindings empirically support the significance of assessing and ensuring the\nreliability of training data for tool-using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) for external tool usage is a rapidly\nexpanding field, with recent research focusing on generating synthetic data to\naddress the shortage of available data. However, the absence of systematic data\nquality checks poses complications for properly training and testing models. To\nthat end, we propose two approaches for assessing the reliability of data for\ntraining LLMs to use external tools. The first approach uses intuitive,\nhuman-defined correctness criteria. The second approach uses a model-driven\nassessment with in-context evaluation. We conduct a thorough evaluation of data\nquality on two popular benchmarks, followed by an extrinsic evaluation that\nshowcases the impact of data quality on model performance. Our results\ndemonstrate that models trained on high-quality data outperform those trained\non unvalidated data, even when trained with a smaller quantity of data. These\nfindings empirically support the significance of assessing and ensuring the\nreliability of training data for tool-using LLMs."
                },
                "authors": [
                    {
                        "name": "Shadi Iskander"
                    },
                    {
                        "name": "Nachshon Cohen"
                    },
                    {
                        "name": "Zohar Karnin"
                    },
                    {
                        "name": "Ori Shapira"
                    },
                    {
                        "name": "Sofia Tolmach"
                    }
                ],
                "author_detail": {
                    "name": "Sofia Tolmach"
                },
                "author": "Sofia Tolmach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14017v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14017v3",
                "updated": "2024-09-26T07:47:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    47,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-26T05:03:32Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    5,
                    3,
                    32,
                    0,
                    239,
                    0
                ],
                "title": "Making Formulog Fast: An Argument for Unconventional Datalog Evaluation\n  (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Formulog Fast: An Argument for Unconventional Datalog Evaluation\n  (Extended Version)"
                },
                "summary": "By combining Datalog, SMT solving, and functional programming, the language\nFormulog provides an appealing mix of features for implementing SMT-based\nstatic analyses (e.g., refinement type checking, symbolic execution) in a\nnatural, declarative way. At the same time, the performance of its custom\nDatalog solver can be an impediment to using Formulog beyond prototyping -- a\ncommon problem for Datalog variants that aspire to solve large problem\ninstances. In this work we speed up Formulog evaluation, with surprising\nresults: while 2.2x speedups are obtained by using the conventional techniques\nfor high-performance Datalog (e.g., compilation, specialized data structures),\nthe big wins come by abandoning the central assumption in modern performant\nDatalog engines, semi-naive Datalog evaluation. In its place, we develop eager\nevaluation, a concurrent Datalog evaluation algorithm that explores the logical\ninference space via a depth-first traversal order. In practice, eager\nevaluation leads to an advantageous distribution of Formulog's SMT workload to\nexternal SMT solvers and improved SMT solving times: our eager evaluation\nextensions to the Formulog interpreter and Souffl\\'e's code generator achieve\nmean 5.2x and 7.6x speedups, respectively, over the optimized code generated by\noff-the-shelf Souffl\\'e on SMT-heavy Formulog benchmarks.\n  Using compilation and eager evaluation, Formulog implementations of\nrefinement type checking, bottom-up pointer analysis, and symbolic execution\nachieve speedups on 20 out of 23 benchmarks over previously published,\nhand-tuned analyses written in F#, Java, and C++, providing strong evidence\nthat Formulog can be the basis of a realistic platform for SMT-based static\nanalysis. Moreover, our experience adds nuance to the conventional wisdom that\nsemi-naive evaluation is the one-size-fits-all best Datalog evaluation\nalgorithm for static analysis workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By combining Datalog, SMT solving, and functional programming, the language\nFormulog provides an appealing mix of features for implementing SMT-based\nstatic analyses (e.g., refinement type checking, symbolic execution) in a\nnatural, declarative way. At the same time, the performance of its custom\nDatalog solver can be an impediment to using Formulog beyond prototyping -- a\ncommon problem for Datalog variants that aspire to solve large problem\ninstances. In this work we speed up Formulog evaluation, with surprising\nresults: while 2.2x speedups are obtained by using the conventional techniques\nfor high-performance Datalog (e.g., compilation, specialized data structures),\nthe big wins come by abandoning the central assumption in modern performant\nDatalog engines, semi-naive Datalog evaluation. In its place, we develop eager\nevaluation, a concurrent Datalog evaluation algorithm that explores the logical\ninference space via a depth-first traversal order. In practice, eager\nevaluation leads to an advantageous distribution of Formulog's SMT workload to\nexternal SMT solvers and improved SMT solving times: our eager evaluation\nextensions to the Formulog interpreter and Souffl\\'e's code generator achieve\nmean 5.2x and 7.6x speedups, respectively, over the optimized code generated by\noff-the-shelf Souffl\\'e on SMT-heavy Formulog benchmarks.\n  Using compilation and eager evaluation, Formulog implementations of\nrefinement type checking, bottom-up pointer analysis, and symbolic execution\nachieve speedups on 20 out of 23 benchmarks over previously published,\nhand-tuned analyses written in F#, Java, and C++, providing strong evidence\nthat Formulog can be the basis of a realistic platform for SMT-based static\nanalysis. Moreover, our experience adds nuance to the conventional wisdom that\nsemi-naive evaluation is the one-size-fits-all best Datalog evaluation\nalgorithm for static analysis workloads."
                },
                "authors": [
                    {
                        "name": "Aaron Bembenek"
                    },
                    {
                        "name": "Michael Greenberg"
                    },
                    {
                        "name": "Stephen Chong"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Chong"
                },
                "arxiv_affiliation": "Harvard University",
                "author": "Stephen Chong",
                "arxiv_comment": "Please cite the official PACMPL version of this article, available at\n  https://doi.org/10.1145/3689754. The second version fixes minor typos in the\n  formalism of the first arXiv version; the third version clarifies some\n  language discussing the results of the scaling experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14017v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17599v1",
                "updated": "2024-09-26T07:28:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    28,
                    13,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    28,
                    13,
                    3,
                    270,
                    0
                ],
                "title": "Information thermodynamics: from physics to neuroscience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information thermodynamics: from physics to neuroscience"
                },
                "summary": "This paper provides a perspective on applying the concepts of information\nthermodynamics, developed recently in non-equilibrium statistical physics, to\nproblems in theoretical neuroscience. Historically, information and energy in\nneuroscience have been treated separately, in contrast to physics approaches,\nwhere the relationship of entropy production with heat is a central idea. It is\nargued here that also in neural systems information and energy can be\nconsidered within the same theoretical framework. Starting from basic ideas of\nthermodynamics and information theory on a classic Brownian particle, it is\nshown how noisy neural networks can infer its probabilistic motion. The\ndecoding of the particle motion by neurons is performed with some accuracy and\nit has some energy cost, and both can be determined using information\nthermodynamics. In a similar fashion, we also discuss how neural networks in\nthe brain can learn the particle velocity, and maintain that information in the\nweights of plastic synapses from a physical point of view. Generally, it is\nshown how the framework of stochastic and information thermodynamics can be\nused practically to study neural inference, learning, and information storing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a perspective on applying the concepts of information\nthermodynamics, developed recently in non-equilibrium statistical physics, to\nproblems in theoretical neuroscience. Historically, information and energy in\nneuroscience have been treated separately, in contrast to physics approaches,\nwhere the relationship of entropy production with heat is a central idea. It is\nargued here that also in neural systems information and energy can be\nconsidered within the same theoretical framework. Starting from basic ideas of\nthermodynamics and information theory on a classic Brownian particle, it is\nshown how noisy neural networks can infer its probabilistic motion. The\ndecoding of the particle motion by neurons is performed with some accuracy and\nit has some energy cost, and both can be determined using information\nthermodynamics. In a similar fashion, we also discuss how neural networks in\nthe brain can learn the particle velocity, and maintain that information in the\nweights of plastic synapses from a physical point of view. Generally, it is\nshown how the framework of stochastic and information thermodynamics can be\nused practically to study neural inference, learning, and information storing."
                },
                "authors": [
                    {
                        "name": "Jan Karbowski"
                    }
                ],
                "author_detail": {
                    "name": "Jan Karbowski"
                },
                "author": "Jan Karbowski",
                "arxiv_doi": "10.3390/e26090779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/e26090779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Perspective on application of stochastic and information\n  thermodynamics to neuroscience (neural decoding, learning, memory)",
                "arxiv_journal_ref": "Entropy 26, 779 (2024)",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17597v1",
                "updated": "2024-09-26T07:24:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    24,
                    9,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:24:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    24,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "Unifying Dimensions: A Linear Adaptive Approach to Lightweight Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Dimensions: A Linear Adaptive Approach to Lightweight Image\n  Super-Resolution"
                },
                "summary": "Window-based transformers have demonstrated outstanding performance in\nsuper-resolution tasks due to their adaptive modeling capabilities through\nlocal self-attention (SA). However, they exhibit higher computational\ncomplexity and inference latency than convolutional neural networks. In this\npaper, we first identify that the adaptability of the Transformers is derived\nfrom their adaptive spatial aggregation and advanced structural design, while\ntheir high latency results from the computational costs and memory layout\ntransformations associated with the local SA. To simulate this aggregation\napproach, we propose an effective convolution-based linear focal separable\nattention (FSA), allowing for long-range dynamic modeling with linear\ncomplexity. Additionally, we introduce an effective dual-branch structure\ncombined with an ultra-lightweight information exchange module (IEM) to enhance\nthe aggregation of information by the Token Mixer. Finally, with respect to the\nstructure, we modify the existing spatial-gate-based feedforward neural\nnetworks by incorporating a self-gate mechanism to preserve high-dimensional\nchannel information, enabling the modeling of more complex relationships. With\nthese advancements, we construct a convolution-based Transformer framework\nnamed the linear adaptive mixer network (LAMNet). Extensive experiments\ndemonstrate that LAMNet achieves better performance than existing SA-based\nTransformer methods while maintaining the computational efficiency of\nconvolutional neural networks, which can achieve a \\(3\\times\\) speedup of\ninference time. The code will be publicly available at:\nhttps://github.com/zononhzy/LAMNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Window-based transformers have demonstrated outstanding performance in\nsuper-resolution tasks due to their adaptive modeling capabilities through\nlocal self-attention (SA). However, they exhibit higher computational\ncomplexity and inference latency than convolutional neural networks. In this\npaper, we first identify that the adaptability of the Transformers is derived\nfrom their adaptive spatial aggregation and advanced structural design, while\ntheir high latency results from the computational costs and memory layout\ntransformations associated with the local SA. To simulate this aggregation\napproach, we propose an effective convolution-based linear focal separable\nattention (FSA), allowing for long-range dynamic modeling with linear\ncomplexity. Additionally, we introduce an effective dual-branch structure\ncombined with an ultra-lightweight information exchange module (IEM) to enhance\nthe aggregation of information by the Token Mixer. Finally, with respect to the\nstructure, we modify the existing spatial-gate-based feedforward neural\nnetworks by incorporating a self-gate mechanism to preserve high-dimensional\nchannel information, enabling the modeling of more complex relationships. With\nthese advancements, we construct a convolution-based Transformer framework\nnamed the linear adaptive mixer network (LAMNet). Extensive experiments\ndemonstrate that LAMNet achieves better performance than existing SA-based\nTransformer methods while maintaining the computational efficiency of\nconvolutional neural networks, which can achieve a \\(3\\times\\) speedup of\ninference time. The code will be publicly available at:\nhttps://github.com/zononhzy/LAMNet."
                },
                "authors": [
                    {
                        "name": "Zhenyu Hu"
                    },
                    {
                        "name": "Wanjie Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wanjie Sun"
                },
                "author": "Wanjie Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17937v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17937v3",
                "updated": "2024-09-26T07:23:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    23,
                    49,
                    3,
                    270,
                    0
                ],
                "published": "2024-03-26T17:59:58Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    17,
                    59,
                    58,
                    1,
                    86,
                    0
                ],
                "title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory"
                },
                "summary": "Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Syed Talal Wasim"
                    },
                    {
                        "name": "Martin Danelljan"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17937v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17937v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.18127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18127v1",
                "updated": "2024-09-26T17:59:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    31,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:59:31Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    59,
                    31,
                    3,
                    270,
                    0
                ],
                "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoLM: Multi-Modal Language Model of Egocentric Motions"
                },
                "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."
                },
                "authors": [
                    {
                        "name": "Fangzhou Hong"
                    },
                    {
                        "name": "Vladimir Guzov"
                    },
                    {
                        "name": "Hyo Jin Kim"
                    },
                    {
                        "name": "Yuting Ye"
                    },
                    {
                        "name": "Richard Newcombe"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Lingni Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lingni Ma"
                },
                "author": "Lingni Ma",
                "arxiv_comment": "Project Page: https://hongfz16.github.io/projects/EgoLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18111v1",
                "updated": "2024-09-26T17:53:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    53,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:53:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    53,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (Video-LLMs) have demonstrated\ntheir great potential in general-purpose video understanding. To verify the\nsignificance of these models, a number of benchmarks have been proposed to\ndiagnose their capabilities in different scenarios. However, existing\nbenchmarks merely evaluate models through video-level question-answering,\nlacking fine-grained event-level assessment and task diversity. To fill this\ngap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding\nBenchmark), a large-scale and high-quality benchmark for open-ended event-level\nvideo understanding. Categorized within a 3-level task taxonomy, E.T. Bench\nencompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length)\nunder 8 domains, providing comprehensive evaluations. We extensively evaluated\n8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that\nstate-of-the-art models for coarse-level (video-level) understanding struggle\nto solve our fine-grained tasks, e.g., grounding event-of-interests within\nvideos, largely due to the short video context length, improper time\nrepresentations, and lack of multi-event training data. Focusing on these\nissues, we further propose a strong baseline model, E.T. Chat, together with an\ninstruction-tuning dataset E.T. Instruct 164K tailored for fine-grained\nevent-level understanding. Our simple but effective solution demonstrates\nsuperior performance in multiple scenarios."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18060v1",
                "updated": "2024-09-26T17:01:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    1,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T17:01:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    17,
                    1,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Infering Alt-text For UI Icons With Large Language Models During App\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infering Alt-text For UI Icons With Large Language Models During App\n  Development"
                },
                "summary": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility."
                },
                "authors": [
                    {
                        "name": "Sabrina Haque"
                    },
                    {
                        "name": "Christoph Csallner"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Csallner"
                },
                "author": "Christoph Csallner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18053v1",
                "updated": "2024-09-26T16:58:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    58,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:58:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    58,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving"
                },
                "summary": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. We make code and\nbenchmarks publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. We make code and\nbenchmarks publicly available."
                },
                "authors": [
                    {
                        "name": "Dingrui Wang"
                    },
                    {
                        "name": "Marc Kaufeld"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "Autonomous Driving, Large Language Models (LLMs), Human Reasoning,\n  Critical Scenario",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08168v3",
                "updated": "2024-09-26T16:51:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    51,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-13T14:27:45Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    14,
                    27,
                    45,
                    2,
                    347,
                    0
                ],
                "title": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object\n  Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object\n  Identifiers"
                },
                "summary": "Recent advancements in 3D Large Language Models (LLMs) have demonstrated\npromising capabilities for 3D scene understanding. However, previous methods\nexhibit deficiencies in general referencing and grounding capabilities for\nintricate scene comprehension. In this paper, we introduce the use of object\nidentifiers and object-centric representations to interact with scenes at the\nobject level. Specifically, we decompose the input 3D scene into a set of\nobject proposals, each assigned a unique identifier token, which enables\nefficient object referencing and grounding during user-assistant interactions.\nGiven the scarcity of scene-language data, we model the scene embeddings as a\nsequence of explicit object-level embeddings, derived from semantic-rich 2D or\n3D representations. By employing object identifiers, we transform diverse 3D\nscene-language tasks into a unified question-answering format, facilitating\njoint training without the need for additional task-specific heads. With\nminimal fine-tuning on all downstream tasks, our model significantly\noutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 3D Large Language Models (LLMs) have demonstrated\npromising capabilities for 3D scene understanding. However, previous methods\nexhibit deficiencies in general referencing and grounding capabilities for\nintricate scene comprehension. In this paper, we introduce the use of object\nidentifiers and object-centric representations to interact with scenes at the\nobject level. Specifically, we decompose the input 3D scene into a set of\nobject proposals, each assigned a unique identifier token, which enables\nefficient object referencing and grounding during user-assistant interactions.\nGiven the scarcity of scene-language data, we model the scene embeddings as a\nsequence of explicit object-level embeddings, derived from semantic-rich 2D or\n3D representations. By employing object identifiers, we transform diverse 3D\nscene-language tasks into a unified question-answering format, facilitating\njoint training without the need for additional task-specific heads. With\nminimal fine-tuning on all downstream tasks, our model significantly\noutperforms existing methods on benchmarks including ScanRefer, Multi3DRefer,\nScan2Cap, ScanQA, and SQA3D."
                },
                "authors": [
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Zehan Wang"
                    },
                    {
                        "name": "Rongjie Huang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Luping Liu"
                    },
                    {
                        "name": "Xize Cheng"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16626v2",
                "updated": "2024-09-26T16:41:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    41,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T05:11:58Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    11,
                    58,
                    2,
                    269,
                    0
                ],
                "title": "Ascend HiFloat8 Format for Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascend HiFloat8 Format for Deep Learning"
                },
                "summary": "This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponent values with 3-bit\nmantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with\n1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7\nextra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).\nMeanwhile, HiF8 encodes all the special values except that positive zero and\nnegative zero are represented by only one bit-pattern. Thanks to the better\nbalance between precision and dynamic range, HiF8 can be simultaneously used in\nboth forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8, massive\nsimulation results on various neural networks, including traditional neural\nnetworks and large language models (LLMs), will also be presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponent values with 3-bit\nmantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with\n1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7\nextra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).\nMeanwhile, HiF8 encodes all the special values except that positive zero and\nnegative zero are represented by only one bit-pattern. Thanks to the better\nbalance between precision and dynamic range, HiF8 can be simultaneously used in\nboth forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8, massive\nsimulation results on various neural networks, including traditional neural\nnetworks and large language models (LLMs), will also be presented."
                },
                "authors": [
                    {
                        "name": "Yuanyong Luo"
                    },
                    {
                        "name": "Zhongxing Zhang"
                    },
                    {
                        "name": "Richard Wu"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Ying Jin"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Minmin Wang"
                    },
                    {
                        "name": "Zhanying He"
                    },
                    {
                        "name": "Guipeng Hu"
                    },
                    {
                        "name": "Luyao Chen"
                    },
                    {
                        "name": "Tianchi Hu"
                    },
                    {
                        "name": "Junsong Wang"
                    },
                    {
                        "name": "Minqi Chen"
                    },
                    {
                        "name": "Mikhaylov Dmitry"
                    },
                    {
                        "name": "Korviakov Vladimir"
                    },
                    {
                        "name": "Bobrin Maxim"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Guanfu Chen"
                    },
                    {
                        "name": "Zeyi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zeyi Huang"
                },
                "author": "Zeyi Huang",
                "arxiv_comment": "13 Pages, 4 Figures, 9 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13731v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13731v3",
                "updated": "2024-09-26T16:34:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-10T02:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    0,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
                },
                "summary": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods."
                },
                "authors": [
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "Zhongshu Zhu"
                    },
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "Zhongpu Bo"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Huaidong Xiong"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Zaoyang Wang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wenguang Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13731v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13731v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18028v1",
                "updated": "2024-09-26T16:34:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:34:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective"
                },
                "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18014v1",
                "updated": "2024-09-26T16:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    22,
                    59,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    22,
                    59,
                    3,
                    270,
                    0
                ],
                "title": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning\n  for Distinct LLMs in Their Optimal Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-RL: Online Long-Context Processing with Role Reinforcement Learning\n  for Distinct LLMs in Their Optimal Roles"
                },
                "summary": "Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long-context processing are still\nchallenging because of their implementation complexity, training efficiency and\ndata sparsity. To address this issue, a new paradigm named Online Long-context\nProcessing (OLP) is proposed when we process a document of unlimited length,\nwhich typically occurs in the information reception and organization of diverse\nstreaming media such as automated news reporting, live e-commerce, and viral\nshort videos. Moreover, a dilemma was often encountered when we tried to select\nthe most suitable LLM from a large number of LLMs amidst explosive growth\naiming for outstanding performance, affordable prices, and short response\ndelays. In view of this, we also develop Role Reinforcement Learning (Role-RL)\nto automatically deploy different LLMs in their respective roles within the OLP\npipeline according to their actual performance. Extensive experiments are\nconducted on our OLP-MINI dataset and it is found that OLP with Role-RL\nframework achieves OLP benchmark with an average recall rate of 93.2% and the\nLLM cost saved by 79.4%. The code and dataset are publicly available at:\nhttps://anonymous.4open.science/r/Role-RL."
                },
                "authors": [
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Pengran Huang"
                    },
                    {
                        "name": "Bingzhi Chen"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Jiahui Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jiahui Pan"
                },
                "author": "Jiahui Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18009v1",
                "updated": "2024-09-26T16:19:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    19,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:19:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    19,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Control Industrial Automation System with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control Industrial Automation System with Large Language Models"
                },
                "summary": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS"
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Nasser Jazdi"
                    },
                    {
                        "name": "Jize Zhang"
                    },
                    {
                        "name": "Chaitanya Shah"
                    },
                    {
                        "name": "Michael Weyrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Weyrich"
                },
                "author": "Michael Weyrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18006v1",
                "updated": "2024-09-26T16:15:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    15,
                    14,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:15:14Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    15,
                    14,
                    3,
                    270,
                    0
                ],
                "title": "Multilingual Evaluation of Long Context Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Evaluation of Long Context Retrieval and Reasoning"
                },
                "summary": "Recent large language models (LLMs) demonstrate impressive capabilities in\nhandling long contexts, some exhibiting near-perfect recall on synthetic\nretrieval tasks. However, these evaluations have mainly focused on English text\nand involved a single target sentence within lengthy contexts. Our work\ninvestigates how LLM performance generalizes to multilingual settings with\nmultiple hidden target sentences. We comprehensively evaluate several\nlong-context LLMs on retrieval and reasoning tasks across five languages:\nEnglish, Vietnamese, Indonesian, Swahili, and Somali. These languages share the\nLatin script but belong to distinct language families and resource levels. Our\nanalysis reveals a significant performance gap between languages. The\nbest-performing models such as Gemini-1.5 and GPT-4o, achieve around 96%\naccuracy in English to around 36% in Somali with a single target sentence.\nHowever, this accuracy drops to 40% in English and 0% in Somali when dealing\nwith three target sentences. Our findings highlight the challenges long-context\nLLMs face when processing longer contexts, an increase in the number of target\nsentences, or languages of lower resource levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate impressive capabilities in\nhandling long contexts, some exhibiting near-perfect recall on synthetic\nretrieval tasks. However, these evaluations have mainly focused on English text\nand involved a single target sentence within lengthy contexts. Our work\ninvestigates how LLM performance generalizes to multilingual settings with\nmultiple hidden target sentences. We comprehensively evaluate several\nlong-context LLMs on retrieval and reasoning tasks across five languages:\nEnglish, Vietnamese, Indonesian, Swahili, and Somali. These languages share the\nLatin script but belong to distinct language families and resource levels. Our\nanalysis reveals a significant performance gap between languages. The\nbest-performing models such as Gemini-1.5 and GPT-4o, achieve around 96%\naccuracy in English to around 36% in Somali with a single target sentence.\nHowever, this accuracy drops to 40% in English and 0% in Somali when dealing\nwith three target sentences. Our findings highlight the challenges long-context\nLLMs face when processing longer contexts, an increase in the number of target\nsentences, or languages of lower resource levels."
                },
                "authors": [
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "Andy Dang"
                    },
                    {
                        "name": "Sina Bagheri Nezhad"
                    },
                    {
                        "name": "Rhitabrat Pokharel"
                    },
                    {
                        "name": "Russell Scheinberg"
                    }
                ],
                "author_detail": {
                    "name": "Russell Scheinberg"
                },
                "author": "Russell Scheinberg",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18003v1",
                "updated": "2024-09-26T16:12:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    12,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:12:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    12,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Enhancing Tourism Recommender Systems for Sustainable City Trips Using\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Tourism Recommender Systems for Sustainable City Trips Using\n  Retrieval-Augmented Generation"
                },
                "summary": "Tourism Recommender Systems (TRS) have traditionally focused on providing\npersonalized travel suggestions, often prioritizing user preferences without\nconsidering broader sustainability goals. Integrating sustainability into TRS\nhas become essential with the increasing need to balance environmental impact,\nlocal community interests, and visitor satisfaction. This paper proposes a\nnovel approach to enhancing TRS for sustainable city trips using Large Language\nModels (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We\nenhance the traditional RAG system by incorporating a sustainability metric\nbased on a city's popularity and seasonal demand during the prompt augmentation\nphase. This modification, called Sustainability Augmented Reranking (SAR),\nensures the system's recommendations align with sustainability goals.\nEvaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and\nMistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently\nmatches or outperforms the baseline (without SAR) across most metrics,\nhighlighting the benefits of incorporating sustainability into TRS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tourism Recommender Systems (TRS) have traditionally focused on providing\npersonalized travel suggestions, often prioritizing user preferences without\nconsidering broader sustainability goals. Integrating sustainability into TRS\nhas become essential with the increasing need to balance environmental impact,\nlocal community interests, and visitor satisfaction. This paper proposes a\nnovel approach to enhancing TRS for sustainable city trips using Large Language\nModels (LLMs) and a modified Retrieval-Augmented Generation (RAG) pipeline. We\nenhance the traditional RAG system by incorporating a sustainability metric\nbased on a city's popularity and seasonal demand during the prompt augmentation\nphase. This modification, called Sustainability Augmented Reranking (SAR),\nensures the system's recommendations align with sustainability goals.\nEvaluations using popular open-source LLMs, such as Llama-3.1-Instruct-8B and\nMistral-Instruct-7B, demonstrate that the SAR-enhanced approach consistently\nmatches or outperforms the baseline (without SAR) across most metrics,\nhighlighting the benefits of incorporating sustainability into TRS."
                },
                "authors": [
                    {
                        "name": "Ashmi Banerjee"
                    },
                    {
                        "name": "Adithi Satish"
                    },
                    {
                        "name": "Wolfgang Wörndl"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Wörndl"
                },
                "author": "Wolfgang Wörndl",
                "arxiv_comment": "Accepted at the RecSoGood 2024 Workshop co-located with the 18th ACM\n  Conference on Recommender Systems (RecSys 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17990v1",
                "updated": "2024-09-26T16:02:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T16:02:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models"
                },
                "summary": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We validate our estimates against representative British survey\ndata and find strong positive, significant correlations for several collective\nemotions. The obtained estimates are robust across multiple training seeds and\nprompt formulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. To the best of our\nknowledge, this is the first work to extend the analysis of affect in LLMs to a\nlongitudinal setting through Temporal Adapters. Our work enables new approaches\ntowards the longitudinal analysis of social media data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We validate our estimates against representative British survey\ndata and find strong positive, significant correlations for several collective\nemotions. The obtained estimates are robust across multiple training seeds and\nprompt formulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. To the best of our\nknowledge, this is the first work to extend the analysis of affect in LLMs to a\nlongitudinal setting through Temporal Adapters. Our work enables new approaches\ntowards the longitudinal analysis of social media data."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Max Pellert"
                    },
                    {
                        "name": "David Garcia"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "arxiv_comment": "Code available at https://github.com/dess-mannheim/temporal-adapters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17987v1",
                "updated": "2024-09-26T15:57:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    57,
                    8,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:57:08Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    57,
                    8,
                    3,
                    270,
                    0
                ],
                "title": "LLM4Brain: Training a Large Language Model for Brain Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Brain: Training a Large Language Model for Brain Video Understanding"
                },
                "summary": "Decoding visual-semantic information from brain signals, such as functional\nMRI (fMRI), across different subjects poses significant challenges, including\nlow signal-to-noise ratio, limited data availability, and cross-subject\nvariability. Recent advancements in large language models (LLMs) show\nremarkable effectiveness in processing multimodal information. In this study,\nwe introduce an LLM-based approach for reconstructing visual-semantic\ninformation from fMRI signals elicited by video stimuli. Specifically, we\nemploy fine-tuning techniques on an fMRI encoder equipped with adaptors to\ntransform brain responses into latent representations aligned with the video\nstimuli. Subsequently, these representations are mapped to textual modality by\nLLM. In particular, we integrate self-supervised domain adaptation methods to\nenhance the alignment between visual-semantic information and brain responses.\nOur proposed method achieves good results using various quantitative semantic\nmetrics, while yielding similarity with ground-truth information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding visual-semantic information from brain signals, such as functional\nMRI (fMRI), across different subjects poses significant challenges, including\nlow signal-to-noise ratio, limited data availability, and cross-subject\nvariability. Recent advancements in large language models (LLMs) show\nremarkable effectiveness in processing multimodal information. In this study,\nwe introduce an LLM-based approach for reconstructing visual-semantic\ninformation from fMRI signals elicited by video stimuli. Specifically, we\nemploy fine-tuning techniques on an fMRI encoder equipped with adaptors to\ntransform brain responses into latent representations aligned with the video\nstimuli. Subsequently, these representations are mapped to textual modality by\nLLM. In particular, we integrate self-supervised domain adaptation methods to\nenhance the alignment between visual-semantic information and brain responses.\nOur proposed method achieves good results using various quantitative semantic\nmetrics, while yielding similarity with ground-truth information."
                },
                "authors": [
                    {
                        "name": "Ruizhe Zheng"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "ECCV2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16427v2",
                "updated": "2024-09-26T15:56:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    56,
                    8,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T19:47:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    19,
                    47,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAICOSYSTEM: An Ecosystem for Sandboxing Safety Risks in Human-AI\n  Interactions"
                },
                "summary": "AI agents are increasingly autonomous in their interactions with human users\nand tools, leading to increased interactional safety risks. We present\nHAICOSYSTEM, a framework examining AI agent safety within diverse and complex\nsocial interactions. HAICOSYSTEM features a modular sandbox environment that\nsimulates multi-turn interactions between human users and AI agents, where the\nAI agents are equipped with a variety of tools (e.g., patient management\nplatforms) to navigate diverse scenarios (e.g., a user attempting to access\nother patients' profiles). To examine the safety of AI agents in these\ninteractions, we develop a comprehensive multi-dimensional evaluation framework\nthat uses metrics covering operational, content-related, societal, and legal\nrisks. Through running 1840 simulations based on 92 scenarios across seven\ndomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM\ncan emulate realistic user-AI interactions and complex tool use by AI agents.\nOur experiments show that state-of-the-art LLMs, both proprietary and\nopen-sourced, exhibit safety risks in over 50\\% cases, with models generally\nshowing higher risks when interacting with simulated malicious users. Our\nfindings highlight the ongoing challenge of building agents that can safely\nnavigate complex interactions, particularly when faced with malicious users. To\nfoster the AI agent safety ecosystem, we release a code platform that allows\npractitioners to create custom scenarios, simulate interactions, and evaluate\nthe safety and performance of their agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly autonomous in their interactions with human users\nand tools, leading to increased interactional safety risks. We present\nHAICOSYSTEM, a framework examining AI agent safety within diverse and complex\nsocial interactions. HAICOSYSTEM features a modular sandbox environment that\nsimulates multi-turn interactions between human users and AI agents, where the\nAI agents are equipped with a variety of tools (e.g., patient management\nplatforms) to navigate diverse scenarios (e.g., a user attempting to access\nother patients' profiles). To examine the safety of AI agents in these\ninteractions, we develop a comprehensive multi-dimensional evaluation framework\nthat uses metrics covering operational, content-related, societal, and legal\nrisks. Through running 1840 simulations based on 92 scenarios across seven\ndomains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM\ncan emulate realistic user-AI interactions and complex tool use by AI agents.\nOur experiments show that state-of-the-art LLMs, both proprietary and\nopen-sourced, exhibit safety risks in over 50\\% cases, with models generally\nshowing higher risks when interacting with simulated malicious users. Our\nfindings highlight the ongoing challenge of building agents that can safely\nnavigate complex interactions, particularly when faced with malicious users. To\nfoster the AI agent safety ecosystem, we release a code platform that allows\npractitioners to create custom scenarios, simulate interactions, and evaluate\nthe safety and performance of their agents."
                },
                "authors": [
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Frank Xu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Both the second and third authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17972v1",
                "updated": "2024-09-26T15:47:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    47,
                    42,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:47:42Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    47,
                    42,
                    3,
                    270,
                    0
                ],
                "title": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and\n  Adaptive Disambiguate based Efficient Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and\n  Adaptive Disambiguate based Efficient Tree Search"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nbroad range of tasks and domains. However, they still encounter difficulties in\nsolving mathematical problems due to the rigorous and logical nature of\nmathematics. Previous studies have employed techniques such as supervised\nfine-tuning (SFT), prompt engineering, and search-based methods to improve the\nmathematical problem-solving abilities of LLMs. Despite these efforts, their\nperformance remains suboptimal and demands substantial computational resources.\nTo address this issue, we propose a novel approach, BEATS, to enhance\nmathematical problem-solving abilities. Our method leverages newly designed\nprompts that guide the model to iteratively rewrite, advance by one step, and\ngenerate answers based on previous steps. Additionally, we introduce a new\nback-verification technique that uses LLMs to validate the correctness of the\ngenerated answers. Furthermore, we employ a pruning tree search to optimize\nsearch time while achieving strong performance. Notably, our method improves\nQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the\nMATH benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nbroad range of tasks and domains. However, they still encounter difficulties in\nsolving mathematical problems due to the rigorous and logical nature of\nmathematics. Previous studies have employed techniques such as supervised\nfine-tuning (SFT), prompt engineering, and search-based methods to improve the\nmathematical problem-solving abilities of LLMs. Despite these efforts, their\nperformance remains suboptimal and demands substantial computational resources.\nTo address this issue, we propose a novel approach, BEATS, to enhance\nmathematical problem-solving abilities. Our method leverages newly designed\nprompts that guide the model to iteratively rewrite, advance by one step, and\ngenerate answers based on previous steps. Additionally, we introduce a new\nback-verification technique that uses LLMs to validate the correctness of the\ngenerated answers. Furthermore, we employ a pruning tree search to optimize\nsearch time while achieving strong performance. Notably, our method improves\nQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the\nMATH benchmark."
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14950v2",
                "updated": "2024-09-26T15:45:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    45,
                    13,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-08T15:57:18Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    57,
                    18,
                    4,
                    342,
                    0
                ],
                "title": "TypeFly: Flying Drones with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TypeFly: Flying Drones with Large Language Model"
                },
                "summary": "Recent advancements in robot control using large language models (LLMs) have\ndemonstrated significant potential, primarily due to LLMs' capabilities to\nunderstand natural language commands and generate executable plans in various\nlanguages. However, in real-time and interactive applications involving mobile\nrobots, particularly drones, the sequential token generation process inherent\nto LLMs introduces substantial latency, i.e. response time, in control plan\ngeneration.\n  In this paper, we present a system called ChatFly that tackles this problem\nusing a combination of a novel programming language called MiniSpec and its\nruntime to reduce the plan generation time and drone response time. That is,\ninstead of asking an LLM to write a program (robotic plan) in the popular but\nverbose Python, ChatFly gets it to do it in MiniSpec specially designed for\ntoken efficiency and stream interpretation. Using a set of challenging drone\ntasks, we show that design choices made by ChatFly can reduce up to 62%\nresponse time and provide a more consistent user experience, enabling\nresponsive and intelligent LLM-based drone control with efficient completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in robot control using large language models (LLMs) have\ndemonstrated significant potential, primarily due to LLMs' capabilities to\nunderstand natural language commands and generate executable plans in various\nlanguages. However, in real-time and interactive applications involving mobile\nrobots, particularly drones, the sequential token generation process inherent\nto LLMs introduces substantial latency, i.e. response time, in control plan\ngeneration.\n  In this paper, we present a system called ChatFly that tackles this problem\nusing a combination of a novel programming language called MiniSpec and its\nruntime to reduce the plan generation time and drone response time. That is,\ninstead of asking an LLM to write a program (robotic plan) in the popular but\nverbose Python, ChatFly gets it to do it in MiniSpec specially designed for\ntoken efficiency and stream interpretation. Using a set of challenging drone\ntasks, we show that design choices made by ChatFly can reduce up to 62%\nresponse time and provide a more consistent user experience, enabling\nresponsive and intelligent LLM-based drone control with efficient completion."
                },
                "authors": [
                    {
                        "name": "Guojun Chen"
                    },
                    {
                        "name": "Xiaojing Yu"
                    },
                    {
                        "name": "Neiwen Ling"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17946v1",
                "updated": "2024-09-26T15:20:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    20,
                    37,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T15:20:37Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    15,
                    20,
                    37,
                    3,
                    270,
                    0
                ],
                "title": "Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge\n  Distillation"
                },
                "summary": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on contrastive knowledge distillation\n(W2SAttack). Specifically, we poison small-scale language models through\nfull-parameter fine-tuning to serve as the teacher model. The teacher model\nthen covertly transfers the backdoor to the large-scale student model through\ncontrastive knowledge distillation, which employs PEFT. Theoretical analysis\nreveals that W2SAttack has the potential to augment the effectiveness of\nbackdoor attacks. We demonstrate the superior performance of W2SAttack on\nclassification tasks across four language models, four backdoor attack\nalgorithms, and two different architectures of teacher models. Experimental\nresults indicate success rates close to 100% for backdoor attacks targeting\nPEFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on contrastive knowledge distillation\n(W2SAttack). Specifically, we poison small-scale language models through\nfull-parameter fine-tuning to serve as the teacher model. The teacher model\nthen covertly transfers the backdoor to the large-scale student model through\ncontrastive knowledge distillation, which employs PEFT. Theoretical analysis\nreveals that W2SAttack has the potential to augment the effectiveness of\nbackdoor attacks. We demonstrate the superior performance of W2SAttack on\nclassification tasks across four language models, four backdoor attack\nalgorithms, and two different architectures of teacher models. Experimental\nresults indicate success rates close to 100% for backdoor attacks targeting\nPEFT."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Luwei Xiao"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15228v3",
                "updated": "2024-09-26T14:57:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    57,
                    52,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T17:22:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    22,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs."
                },
                "authors": [
                    {
                        "name": "Yixi Wu"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17912v1",
                "updated": "2024-09-26T14:56:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    56,
                    38,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:56:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    56,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan\n  Arabic Dialect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan\n  Arabic Dialect"
                },
                "summary": "We introduce Atlas-Chat, the first-ever collection of large language models\nspecifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also\nknown as Darija, we construct our instruction dataset by consolidating existing\nDarija language resources, creating novel datasets both manually and\nsynthetically, and translating English instructions with stringent quality\ncontrol. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit\nsuperior ability in following Darija instructions and performing standard NLP\ntasks. Notably, our models outperform both state-of-the-art and\nArabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13%\nperformance boost over a larger 13B model on DarijaMMLU, in our newly\nintroduced evaluation suite for Darija covering both discriminative and\ngenerative tasks. Furthermore, we perform an experimental analysis of various\nfine-tuning strategies and base model choices to determine optimal\nconfigurations. All our resources are publicly accessible, and we believe our\nwork offers comprehensive design methodologies of instruction-tuning for\nlow-resource language variants, which are often neglected in favor of data-rich\nlanguages by contemporary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Atlas-Chat, the first-ever collection of large language models\nspecifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also\nknown as Darija, we construct our instruction dataset by consolidating existing\nDarija language resources, creating novel datasets both manually and\nsynthetically, and translating English instructions with stringent quality\ncontrol. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit\nsuperior ability in following Darija instructions and performing standard NLP\ntasks. Notably, our models outperform both state-of-the-art and\nArabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13%\nperformance boost over a larger 13B model on DarijaMMLU, in our newly\nintroduced evaluation suite for Darija covering both discriminative and\ngenerative tasks. Furthermore, we perform an experimental analysis of various\nfine-tuning strategies and base model choices to determine optimal\nconfigurations. All our resources are publicly accessible, and we believe our\nwork offers comprehensive design methodologies of instruction-tuning for\nlow-resource language variants, which are often neglected in favor of data-rich\nlanguages by contemporary LLMs."
                },
                "authors": [
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Yousef Khoubrane"
                    },
                    {
                        "name": "Amr Mohamed"
                    },
                    {
                        "name": "Yassine Abbahaddou"
                    },
                    {
                        "name": "Sofiane Ennadir"
                    },
                    {
                        "name": "Imane Momayiz"
                    },
                    {
                        "name": "Xuguang Ren"
                    },
                    {
                        "name": "Eric Moulines"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17906v1",
                "updated": "2024-09-26T14:52:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    52,
                    40,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:52:40Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    52,
                    40,
                    3,
                    270,
                    0
                ],
                "title": "Graph Reasoning with Large Language Models via Pseudo-code Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Reasoning with Large Language Models via Pseudo-code Prompting"
                },
                "summary": "Large language models (LLMs) have recently achieved remarkable success in\nvarious reasoning tasks in the field of natural language processing. This\nsuccess of LLMs has also motivated their use in graph-related tasks. Among\nothers, recent work has explored whether LLMs can solve graph problems such as\ncounting the number of connected components of a graph or computing the\nshortest path distance between two nodes. Although LLMs possess preliminary\ngraph reasoning abilities, they might still struggle to solve some seemingly\nsimple problems. In this paper, we investigate whether prompting via\npseudo-code instructions can improve the performance of LLMs in solving graph\nproblems. Our experiments demonstrate that using pseudo-code instructions\ngenerally improves the performance of all considered LLMs. The graphs,\npseudo-code prompts, and evaluation code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved remarkable success in\nvarious reasoning tasks in the field of natural language processing. This\nsuccess of LLMs has also motivated their use in graph-related tasks. Among\nothers, recent work has explored whether LLMs can solve graph problems such as\ncounting the number of connected components of a graph or computing the\nshortest path distance between two nodes. Although LLMs possess preliminary\ngraph reasoning abilities, they might still struggle to solve some seemingly\nsimple problems. In this paper, we investigate whether prompting via\npseudo-code instructions can improve the performance of LLMs in solving graph\nproblems. Our experiments demonstrate that using pseudo-code instructions\ngenerally improves the performance of all considered LLMs. The graphs,\npseudo-code prompts, and evaluation code are publicly available."
                },
                "authors": [
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17904v1",
                "updated": "2024-09-26T14:51:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    51,
                    40,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:51:40Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    51,
                    40,
                    3,
                    270,
                    0
                ],
                "title": "Learning to Love Edge Cases in Formative Math Assessment: Using the\n  AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Love Edge Cases in Formative Math Assessment: Using the\n  AMMORE Dataset and Chain-of-Thought Prompting to Improve Grading Accuracy"
                },
                "summary": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AMMORE, a new dataset of 53,000 math open-response\nquestion-answer pairs from Rori, a learning platform used by students in\nseveral African countries and conducts two experiments to evaluate the use of\nlarge language models (LLM) for grading particularly challenging student\nanswers. The AMMORE dataset enables various potential analyses and provides an\nimportant resource for researching student math acquisition in understudied,\nreal-world, educational contexts. In experiment 1 we use a variety of\nLLM-driven approaches, including zero-shot, few-shot, and chain-of-thought\nprompting, to grade the 1% of student answers that a rule-based classifier\nfails to grade accurately. We find that the best-performing approach --\nchain-of-thought prompting -- accurately scored 92% of these edge cases,\neffectively boosting the overall accuracy of the grading from 98.7% to 99.9%.\nIn experiment 2, we aim to better understand the consequential validity of the\nimproved grading accuracy, by passing grades generated by the best-performing\nLLM-based approach to a Bayesian Knowledge Tracing (BKT) model, which estimated\nstudent mastery of specific lessons. We find that relatively modest\nimprovements in model accuracy at the individual question level can lead to\nsignificant changes in the estimation of student mastery. Where the rules-based\nclassifier currently used to grade student, answers misclassified the mastery\nstatus of 6.9% of students across their completed lessons, using the LLM\nchain-of-thought approach this misclassification rate was reduced to 2.6% of\nstudents. Taken together, these findings suggest that LLMs could be a valuable\ntool for grading open-response questions in K-12 mathematics education,\npotentially enabling encouraging wider adoption of open-ended questions in\nformative assessment."
                },
                "authors": [
                    {
                        "name": "Owen Henkel"
                    },
                    {
                        "name": "Hannah Horne-Robinson"
                    },
                    {
                        "name": "Maria Dyshel"
                    },
                    {
                        "name": "Nabil Ch"
                    },
                    {
                        "name": "Baptiste Moreau-Pernet"
                    },
                    {
                        "name": "Ralph Abood"
                    }
                ],
                "author_detail": {
                    "name": "Ralph Abood"
                },
                "author": "Ralph Abood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17902v1",
                "updated": "2024-09-26T14:50:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    50,
                    20,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:50:20Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    50,
                    20,
                    3,
                    270,
                    0
                ],
                "title": "Designing Short-Stage CDC-XPUFs: Balancing Reliability, Cost, and\n  Security in IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Short-Stage CDC-XPUFs: Balancing Reliability, Cost, and\n  Security in IoT Devices"
                },
                "summary": "The rapid expansion of Internet of Things (IoT) devices demands robust and\nresource-efficient security solutions. Physically Unclonable Functions (PUFs),\nwhich generate unique cryptographic keys from inherent hardware variations,\noffer a promising approach. However, traditional PUFs like Arbiter PUFs (APUFs)\nand XOR Arbiter PUFs (XOR-PUFs) are susceptible to machine learning (ML) and\nreliability-based attacks. In this study, we investigate\nComponent-Differentially Challenged XOR-PUFs (CDC-XPUFs), a less explored\nvariant, to address these vulnerabilities. We propose an optimized CDC-XPUF\ndesign that incorporates a pre-selection strategy to enhance reliability and\nintroduces a novel lightweight architecture to reduce hardware overhead.\nRigorous testing demonstrates that our design significantly lowers resource\nconsumption, maintains strong resistance to ML attacks, and improves\nreliability, effectively mitigating reliability-based attacks. These results\nhighlight the potential of CDC-XPUFs as a secure and efficient candidate for\nwidespread deployment in resource-constrained IoT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of Internet of Things (IoT) devices demands robust and\nresource-efficient security solutions. Physically Unclonable Functions (PUFs),\nwhich generate unique cryptographic keys from inherent hardware variations,\noffer a promising approach. However, traditional PUFs like Arbiter PUFs (APUFs)\nand XOR Arbiter PUFs (XOR-PUFs) are susceptible to machine learning (ML) and\nreliability-based attacks. In this study, we investigate\nComponent-Differentially Challenged XOR-PUFs (CDC-XPUFs), a less explored\nvariant, to address these vulnerabilities. We propose an optimized CDC-XPUF\ndesign that incorporates a pre-selection strategy to enhance reliability and\nintroduces a novel lightweight architecture to reduce hardware overhead.\nRigorous testing demonstrates that our design significantly lowers resource\nconsumption, maintains strong resistance to ML attacks, and improves\nreliability, effectively mitigating reliability-based attacks. These results\nhighlight the potential of CDC-XPUFs as a secure and efficient candidate for\nwidespread deployment in resource-constrained IoT systems."
                },
                "authors": [
                    {
                        "name": "Gaoxiang Li"
                    },
                    {
                        "name": "Yu Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhuang"
                },
                "author": "Yu Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17882v1",
                "updated": "2024-09-26T14:29:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    29,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:29:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    29,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "Multi-UAV Enabled MEC Networks: Optimizing Delay through Intelligent 3D\n  Trajectory Planning and Resource Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-UAV Enabled MEC Networks: Optimizing Delay through Intelligent 3D\n  Trajectory Planning and Resource Allocation"
                },
                "summary": "Mobile Edge Computing (MEC) reduces the computational burden on terminal\ndevices by shortening the distance between these devices and computing nodes.\nIntegrating Unmanned Aerial Vehicles (UAVs) with enhanced MEC networks can\nleverage the high mobility of UAVs to flexibly adjust network topology, further\nexpanding the applicability of MEC. However, in highly dynamic and complex\nreal-world environments, it is crucial to balance task offloading effectiveness\nwith algorithm performance. This paper investigates a multi-UAV communication\nnetwork equipped with edge computing nodes to assist terminal users in task\ncomputation. Our goal is to reduce the task processing delay for users through\nthe joint optimization of discrete computation modes, continuous 3D\ntrajectories, and resource assignment. To address the challenges posed by the\nmixed action space, we propose a Multi-UAV Edge Computing Resource Scheduling\n(MUECRS) algorithm, which comprises two key components: 1) trajectory\noptimization, and 2) computation mode and resource management. Experimental\nresults demonstrate our method effectively designs the 3D flight trajectories\nof UAVs, enabling rapid terminal coverage. Furthermore, the proposed algorithm\nachieves efficient resource deployment and scheduling, outperforming\ncomparative algorithms by at least 16.7%, demonstrating superior adaptability\nand robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing (MEC) reduces the computational burden on terminal\ndevices by shortening the distance between these devices and computing nodes.\nIntegrating Unmanned Aerial Vehicles (UAVs) with enhanced MEC networks can\nleverage the high mobility of UAVs to flexibly adjust network topology, further\nexpanding the applicability of MEC. However, in highly dynamic and complex\nreal-world environments, it is crucial to balance task offloading effectiveness\nwith algorithm performance. This paper investigates a multi-UAV communication\nnetwork equipped with edge computing nodes to assist terminal users in task\ncomputation. Our goal is to reduce the task processing delay for users through\nthe joint optimization of discrete computation modes, continuous 3D\ntrajectories, and resource assignment. To address the challenges posed by the\nmixed action space, we propose a Multi-UAV Edge Computing Resource Scheduling\n(MUECRS) algorithm, which comprises two key components: 1) trajectory\noptimization, and 2) computation mode and resource management. Experimental\nresults demonstrate our method effectively designs the 3D flight trajectories\nof UAVs, enabling rapid terminal coverage. Furthermore, the proposed algorithm\nachieves efficient resource deployment and scheduling, outperforming\ncomparative algorithms by at least 16.7%, demonstrating superior adaptability\nand robustness."
                },
                "authors": [
                    {
                        "name": "Zhiying Wang"
                    },
                    {
                        "name": "Tianxi Wei"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Xinyue Liu"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17870v1",
                "updated": "2024-09-26T14:17:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    17,
                    58,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T14:17:58Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    17,
                    58,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Arbitrary Precision Acceleration for Large Language Models on\n  GPU Tensor Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Arbitrary Precision Acceleration for Large Language Models on\n  GPU Tensor Cores"
                },
                "summary": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 13\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied but face challenges in\nefficient inference. While quantization methods reduce computational demands,\nultra-low bit quantization with arbitrary precision is hindered by limited GPU\nTensor Core support and inefficient memory management, leading to suboptimal\nacceleration. To address these challenges, we propose a comprehensive\nacceleration scheme for arbitrary precision LLMs. At its core, we introduce a\nnovel bipolar-INT data format that facilitates parallel computing and supports\nsymmetric quantization, effectively reducing data redundancy. Building on this,\nwe implement an arbitrary precision matrix multiplication scheme that\ndecomposes and recovers matrices at the bit level, enabling flexible precision\nwhile maximizing GPU Tensor Core utilization. Furthermore, we develop an\nefficient matrix preprocessing method that optimizes data layout for subsequent\ncomputations. Finally, we design a data recovery-oriented memory management\nsystem that strategically utilizes fast shared memory, significantly enhancing\nkernel execution speed and minimizing memory access latency. Experimental\nresults demonstrate our approach's effectiveness, with up to 13\\times speedup\nin matrix multiplication compared to NVIDIA's CUTLASS. When integrated into\nLLMs, we achieve up to 6.7\\times inference acceleration. These improvements\nsignificantly enhance LLM inference efficiency, enabling broader and more\nresponsive applications of LLMs."
                },
                "authors": [
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05208v3",
                "updated": "2024-09-26T14:16:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    14,
                    16,
                    1,
                    3,
                    270,
                    0
                ],
                "published": "2023-10-08T15:49:36Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    15,
                    49,
                    36,
                    6,
                    281,
                    0
                ],
                "title": "ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot\n  Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot\n  Coordination"
                },
                "summary": "Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement\nlearning (MARL) challenge that aims to train an ego agent to work with diverse,\nunseen partners during deployment. The significant difference between the\ndeployment-time partners' distribution and the training partners' distribution\ndetermined by the training algorithm makes ZSC a unique out-of-distribution\n(OOD) generalization challenge. The potential distribution gap between\nevaluation and deployment-time partners leads to inadequate evaluation, which\nis exacerbated by the lack of appropriate evaluation metrics. In this paper, we\npresent ZSC-Eval, the first evaluation toolkit and benchmark for ZSC\nalgorithms. ZSC-Eval consists of: 1) Generation of evaluation partner\ncandidates through behavior-preferring rewards to approximate deployment-time\npartners' distribution; 2) Selection of evaluation partners by Best-Response\nDiversity (BR-Div); 3) Measurement of generalization performance with various\nevaluation partners via the Best-Response Proximity (BR-Prox) metric. We use\nZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football\nenvironments and get novel empirical findings. We also conduct a human\nexperiment of current ZSC algorithms to verify the ZSC-Eval's consistency with\nhuman evaluation. ZSC-Eval is now available at\nhttps://github.com/sjtu-marl/ZSC-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement\nlearning (MARL) challenge that aims to train an ego agent to work with diverse,\nunseen partners during deployment. The significant difference between the\ndeployment-time partners' distribution and the training partners' distribution\ndetermined by the training algorithm makes ZSC a unique out-of-distribution\n(OOD) generalization challenge. The potential distribution gap between\nevaluation and deployment-time partners leads to inadequate evaluation, which\nis exacerbated by the lack of appropriate evaluation metrics. In this paper, we\npresent ZSC-Eval, the first evaluation toolkit and benchmark for ZSC\nalgorithms. ZSC-Eval consists of: 1) Generation of evaluation partner\ncandidates through behavior-preferring rewards to approximate deployment-time\npartners' distribution; 2) Selection of evaluation partners by Best-Response\nDiversity (BR-Div); 3) Measurement of generalization performance with various\nevaluation partners via the Best-Response Proximity (BR-Prox) metric. We use\nZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football\nenvironments and get novel empirical findings. We also conduct a human\nexperiment of current ZSC algorithms to verify the ZSC-Eval's consistency with\nhuman evaluation. ZSC-Eval is now available at\nhttps://github.com/sjtu-marl/ZSC-Eval."
                },
                "authors": [
                    {
                        "name": "Xihuai Wang"
                    },
                    {
                        "name": "Shao Zhang"
                    },
                    {
                        "name": "Wenhao Zhang"
                    },
                    {
                        "name": "Wentao Dong"
                    },
                    {
                        "name": "Jingxiao Chen"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Accepted in NeurIPS 2024 Dataset and Benchmark Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17836v1",
                "updated": "2024-09-26T13:38:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models"
                },
                "summary": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10\\% up to 17.2\\% across various\ndatasets and architectures. Additionally, our approach shows promising\ncompatibility with lossy compression techniques such as quantization and\nsparsification. These findings highlight the significant potential of LLMs as a\nmodel for effectively handling gradients. We will release the source code upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10\\% up to 17.2\\% across various\ndatasets and architectures. Additionally, our approach shows promising\ncompatibility with lossy compression techniques such as quantization and\nsparsification. These findings highlight the significant potential of LLMs as a\nmodel for effectively handling gradients. We will release the source code upon\npublication."
                },
                "authors": [
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "To appear in NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17834v1",
                "updated": "2024-09-26T13:36:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    36,
                    0,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:36:00Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    36,
                    0,
                    3,
                    270,
                    0
                ],
                "title": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent\n  Representation MOdification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent\n  Representation MOdification"
                },
                "summary": "Due to their substantial sizes, large language models (LLMs) are typically\ndeployed within a single-backbone multi-tenant framework. In this setup, a\nsingle instance of an LLM backbone must cater to multiple users or tasks\nthrough the application of various parameter-efficient fine-tuning (PEFT)\nmodels. Despite the availability of numerous effective PEFT techniques such as\nLoRA, there remains a need for a PEFT approach that achieves both high\nefficiency during inference and competitive performance on downstream tasks. In\nthis research, we introduce a new and straightforward PEFT methodology named\n\\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation\nM\\underline{O}dification (PEDRO). The proposed method involves integrating a\nlightweight vector generator into each Transformer layer, which generates\nvectors contingent upon the input prompts. These vectors then modify the hidden\nrepresentations created by the LLM through a dot product operation, thereby\ninfluencing the semantic output and generated content of the model. Extensive\nexperimentation across a variety of tasks indicates that: (a) PEDRO surpasses\nrecent PEFT benchmarks when using a similar number of tunable parameters. (b)\nUnder the single-backbone multi-tenant deployment model, PEDRO exhibits\nsuperior efficiency compared to LoRA, indicating significant industrial\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their substantial sizes, large language models (LLMs) are typically\ndeployed within a single-backbone multi-tenant framework. In this setup, a\nsingle instance of an LLM backbone must cater to multiple users or tasks\nthrough the application of various parameter-efficient fine-tuning (PEFT)\nmodels. Despite the availability of numerous effective PEFT techniques such as\nLoRA, there remains a need for a PEFT approach that achieves both high\nefficiency during inference and competitive performance on downstream tasks. In\nthis research, we introduce a new and straightforward PEFT methodology named\n\\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation\nM\\underline{O}dification (PEDRO). The proposed method involves integrating a\nlightweight vector generator into each Transformer layer, which generates\nvectors contingent upon the input prompts. These vectors then modify the hidden\nrepresentations created by the LLM through a dot product operation, thereby\ninfluencing the semantic output and generated content of the model. Extensive\nexperimentation across a variety of tasks indicates that: (a) PEDRO surpasses\nrecent PEFT benchmarks when using a similar number of tunable parameters. (b)\nUnder the single-backbone multi-tenant deployment model, PEDRO exhibits\nsuperior efficiency compared to LoRA, indicating significant industrial\npotential."
                },
                "authors": [
                    {
                        "name": "Tianfang Xie"
                    },
                    {
                        "name": "Tianjing Li"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Yi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhao"
                },
                "author": "Yi Zhao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.18203",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v2",
                "updated": "2024-09-26T13:30:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    30,
                    51,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions. This work provides\na promising research direction for understanding the complexity of neural\nnetworks, shedding light on training dynamics, and advancing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions. This work provides\na promising research direction for understanding the complexity of neural\nnetworks, shedding light on training dynamics, and advancing interpretability."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17827v1",
                "updated": "2024-09-26T13:26:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    26,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T13:26:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    26,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text"
                },
                "summary": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs."
                },
                "authors": [
                    {
                        "name": "Siyan Wang"
                    },
                    {
                        "name": "Bradford Levy"
                    }
                ],
                "author_detail": {
                    "name": "Bradford Levy"
                },
                "author": "Bradford Levy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17800v1",
                "updated": "2024-09-26T12:49:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    49,
                    28,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:49:28Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    49,
                    28,
                    3,
                    270,
                    0
                ],
                "title": "Bias Assessment and Data Drift Detection in Medical Image Analysis: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Assessment and Data Drift Detection in Medical Image Analysis: A\n  Survey"
                },
                "summary": "Machine Learning (ML) models have gained popularity in medical imaging\nanalysis given their expert level performance in many medical domains. To\nenhance the trustworthiness, acceptance, and regulatory compliance of medical\nimaging models and to facilitate their integration into clinical settings, we\nreview and categorise methods for ensuring ML reliability, both during\ndevelopment and throughout the model's lifespan. Specifically, we provide an\noverview of methods assessing models' inner-workings regarding bias encoding\nand detection of data drift for disease classification models. Additionally, to\nevaluate the severity in case of a significant drift, we provide an overview of\nthe methods developed for classifier accuracy estimation in case of no access\nto ground truth labels. This should enable practitioners to implement methods\nensuring reliable ML deployment and consistent prediction performance over\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning (ML) models have gained popularity in medical imaging\nanalysis given their expert level performance in many medical domains. To\nenhance the trustworthiness, acceptance, and regulatory compliance of medical\nimaging models and to facilitate their integration into clinical settings, we\nreview and categorise methods for ensuring ML reliability, both during\ndevelopment and throughout the model's lifespan. Specifically, we provide an\noverview of methods assessing models' inner-workings regarding bias encoding\nand detection of data drift for disease classification models. Additionally, to\nevaluate the severity in case of a significant drift, we provide an overview of\nthe methods developed for classifier accuracy estimation in case of no access\nto ground truth labels. This should enable practitioners to implement methods\nensuring reliable ML deployment and consistent prediction performance over\ntime."
                },
                "authors": [
                    {
                        "name": "Andrea Prenner"
                    },
                    {
                        "name": "Bernhard Kainz"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Kainz"
                },
                "author": "Bernhard Kainz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17791v1",
                "updated": "2024-09-26T12:37:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    37,
                    26,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:37:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    37,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "Self-supervised Preference Optimization: Enhance Your Language Model\n  with Preference Degree Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Preference Optimization: Enhance Your Language Model\n  with Preference Degree Awareness"
                },
                "summary": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO."
                },
                "authors": [
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Haojing Huang"
                    },
                    {
                        "name": "Yujia Zhang"
                    },
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Rui Song"
                    },
                    {
                        "name": "Lida Shi"
                    },
                    {
                        "name": "Jingwen Wang"
                    },
                    {
                        "name": "Hao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Xu"
                },
                "author": "Hao Xu",
                "arxiv_comment": "Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17790v1",
                "updated": "2024-09-26T12:37:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    37,
                    22,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T12:37:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    12,
                    37,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "CASPFormer: Trajectory Prediction from BEV Images with Deformable\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASPFormer: Trajectory Prediction from BEV Images with Deformable\n  Attention"
                },
                "summary": "Motion prediction is an important aspect for Autonomous Driving (AD) and\nAdvance Driver Assistance Systems (ADAS). Current state-of-the-art motion\nprediction methods rely on High Definition (HD) maps for capturing the\nsurrounding context of the ego vehicle. Such systems lack scalability in\nreal-world deployment as HD maps are expensive to produce and update in\nreal-time. To overcome this issue, we propose Context Aware Scene Prediction\nTransformer (CASPFormer), which can perform multi-modal motion prediction from\nrasterized Bird-Eye-View (BEV) images. Our system can be integrated with any\nupstream perception module that is capable of generating BEV images. Moreover,\nCASPFormer directly decodes vectorized trajectories without any postprocessing.\nTrajectories are decoded recurrently using deformable attention, as it is\ncomputationally efficient and provides the network with the ability to focus\nits attention on the important spatial locations of the BEV images. In\naddition, we also address the issue of mode collapse for generating multiple\nscene-consistent trajectories by incorporating learnable mode queries. We\nevaluate our model on the nuScenes dataset and show that it reaches\nstate-of-the-art across multiple metrics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion prediction is an important aspect for Autonomous Driving (AD) and\nAdvance Driver Assistance Systems (ADAS). Current state-of-the-art motion\nprediction methods rely on High Definition (HD) maps for capturing the\nsurrounding context of the ego vehicle. Such systems lack scalability in\nreal-world deployment as HD maps are expensive to produce and update in\nreal-time. To overcome this issue, we propose Context Aware Scene Prediction\nTransformer (CASPFormer), which can perform multi-modal motion prediction from\nrasterized Bird-Eye-View (BEV) images. Our system can be integrated with any\nupstream perception module that is capable of generating BEV images. Moreover,\nCASPFormer directly decodes vectorized trajectories without any postprocessing.\nTrajectories are decoded recurrently using deformable attention, as it is\ncomputationally efficient and provides the network with the ability to focus\nits attention on the important spatial locations of the BEV images. In\naddition, we also address the issue of mode collapse for generating multiple\nscene-consistent trajectories by incorporating learnable mode queries. We\nevaluate our model on the nuScenes dataset and show that it reaches\nstate-of-the-art across multiple metrics"
                },
                "authors": [
                    {
                        "name": "Harsh Yadav"
                    },
                    {
                        "name": "Maximilian Schaefer"
                    },
                    {
                        "name": "Kun Zhao"
                    },
                    {
                        "name": "Tobias Meisen"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Meisen"
                },
                "author": "Tobias Meisen",
                "arxiv_comment": "Under Review at ICPR 2024, Kolkata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04259v2",
                "updated": "2024-09-26T11:42:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    42,
                    35,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-08T06:57:49Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    6,
                    57,
                    49,
                    3,
                    221,
                    0
                ],
                "title": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering"
                },
                "summary": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets."
                },
                "authors": [
                    {
                        "name": "Ziyuan Zhuang"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05968v2",
                "updated": "2024-09-26T11:38:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    38,
                    2,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-12T07:49:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    49,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction"
                },
                "summary": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed."
                },
                "authors": [
                    {
                        "name": "Cédric Eichler"
                    },
                    {
                        "name": "Nathan Champeil"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Alexandra Bensamoun"
                    },
                    {
                        "name": "Heber Hwang Arcolezi"
                    },
                    {
                        "name": "José Maria De Fuentes"
                    }
                ],
                "author_detail": {
                    "name": "José Maria De Fuentes"
                },
                "author": "José Maria De Fuentes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17745v1",
                "updated": "2024-09-26T11:19:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    19,
                    9,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T11:19:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    19,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Pairwise Rank Prompting: An Effective Non-Parametric Retrieval\n  Model"
                },
                "summary": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline."
                },
                "authors": [
                    {
                        "name": "Nilanjan Sinhababu"
                    },
                    {
                        "name": "Andrew Parry"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Debasis Samanta"
                    },
                    {
                        "name": "Pabitra Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Pabitra Mitra"
                },
                "author": "Pabitra Mitra",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10712v3",
                "updated": "2024-09-26T11:15:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    11,
                    15,
                    14,
                    3,
                    270,
                    0
                ],
                "published": "2024-02-16T14:15:15Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    14,
                    15,
                    15,
                    4,
                    47,
                    0
                ],
                "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient\n  Language Model Inference"
                },
                "summary": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\n(CVA) methods have been proposed for adapting models to a target language\naiming to improve downstream performance. However, the effectiveness of these\nmethods on increasing inference efficiency of generative LLMs has yet to be\nexplored. In this paper, we perform an empirical study of five CVA methods on\nfour generative LLMs (including monolingual and multilingual models) across\nfour typologically-diverse languages and four natural language understanding\ntasks. We find that CVA substantially contributes to LLM inference speedups of\nup to 271.5\\%. We also show that adapting LLMs that have been pre-trained on\nmore balanced multilingual data results in downstream performance comparable to\nthe original models."
                },
                "authors": [
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08160v2",
                "updated": "2024-09-26T10:54:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    54,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-15T13:49:14Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    49,
                    14,
                    3,
                    228,
                    0
                ],
                "title": "General-purpose Clothes Manipulation with Semantic Keypoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Clothes Manipulation with Semantic Keypoints"
                },
                "summary": "Clothes manipulation is a critical skill for household robots. Recent\nadvancements have been made in task-specific clothes manipulation, such as\nfolding, flattening, and hanging. However, due to clothes' complex geometries\nand deformability, creating a general-purpose robot system that can manipulate\na diverse range of clothes in many ways remains challenging. Since clothes are\ntypically designed with specific structures, we propose identifying these\nspecific features like ``left sleeve'' as semantic keypoints. Semantic\nkeypoints can provide semantic cues for task planning and geometric cues for\nlow-level action generation. With this insight, we develop a hierarchical\nlearning framework using the large language model (LLM) for general-purpose\nCLothes mAnipulation with Semantic keyPoints (CLASP). Extensive simulation\nexperiments show that CLASP outperforms baseline methods on both seen and\nunseen tasks across various clothes manipulation tasks. Real-world experiments\nshow that CLASP can be directly deployed in the real world and applied to a\nwide variety of clothes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clothes manipulation is a critical skill for household robots. Recent\nadvancements have been made in task-specific clothes manipulation, such as\nfolding, flattening, and hanging. However, due to clothes' complex geometries\nand deformability, creating a general-purpose robot system that can manipulate\na diverse range of clothes in many ways remains challenging. Since clothes are\ntypically designed with specific structures, we propose identifying these\nspecific features like ``left sleeve'' as semantic keypoints. Semantic\nkeypoints can provide semantic cues for task planning and geometric cues for\nlow-level action generation. With this insight, we develop a hierarchical\nlearning framework using the large language model (LLM) for general-purpose\nCLothes mAnipulation with Semantic keyPoints (CLASP). Extensive simulation\nexperiments show that CLASP outperforms baseline methods on both seen and\nunseen tasks across various clothes manipulation tasks. Real-world experiments\nshow that CLASP can be directly deployed in the real world and applied to a\nwide variety of clothes."
                },
                "authors": [
                    {
                        "name": "Yuhong Deng"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14788v2",
                "updated": "2024-09-26T10:21:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    21,
                    33,
                    3,
                    270,
                    0
                ],
                "published": "2024-07-20T07:39:07Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    7,
                    39,
                    7,
                    5,
                    202,
                    0
                ],
                "title": "On the Design and Analysis of LLM-Based Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Design and Analysis of LLM-Based Algorithms"
                },
                "summary": "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs.\nThrough extensive analytical and empirical investigation in a series of case\nstudies, we demonstrate that the proposed framework is broadly applicable to a\nwide range of scenarios and diverse patterns of LLM-based algorithms, such as\nparallel, hierarchical and recursive task decomposition. Our proposed framework\nholds promise for advancing LLM-based algorithms, by revealing the reasons\nbehind curious empirical phenomena, guiding the choices of hyperparameters,\npredicting the empirical performance of algorithms, and inspiring new algorithm\ndesign. To promote further study of LLM-based algorithms, we release our source\ncode at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We initiate a formal investigation into the design and analysis of LLM-based\nalgorithms, i.e. algorithms that contain one or multiple calls of large\nlanguage models (LLMs) as sub-routines and critically rely on the capabilities\nof LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems,\nhave achieved remarkable empirical success, the design and optimization of them\nhave mostly relied on heuristics and trial-and-errors, which is largely due to\na lack of formal and analytical study for these algorithms. To fill this gap,\nwe start by identifying the computational-graph representation of LLM-based\nalgorithms, the design principle of task decomposition, and some key\nabstractions, which then facilitate our formal analysis for the accuracy and\nefficiency of LLM-based algorithms, despite the black-box nature of LLMs.\nThrough extensive analytical and empirical investigation in a series of case\nstudies, we demonstrate that the proposed framework is broadly applicable to a\nwide range of scenarios and diverse patterns of LLM-based algorithms, such as\nparallel, hierarchical and recursive task decomposition. Our proposed framework\nholds promise for advancing LLM-based algorithms, by revealing the reasons\nbehind curious empirical phenomena, guiding the choices of hyperparameters,\npredicting the empirical performance of algorithms, and inspiring new algorithm\ndesign. To promote further study of LLM-based algorithms, we release our source\ncode at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17700v1",
                "updated": "2024-09-26T10:14:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    14,
                    12,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T10:14:12Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    14,
                    12,
                    3,
                    270,
                    0
                ],
                "title": "Demystifying Privacy in 5G Stand Alone Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Privacy in 5G Stand Alone Networks"
                },
                "summary": "Ensuring user privacy remains critical in mobile networks, particularly with\nthe rise of connected devices and denser 5G infrastructure. Privacy concerns\nhave persisted across 2G, 3G, and 4G/LTE networks. Recognizing these concerns,\nthe 3rd Generation Partnership Project (3GPP) has made privacy enhancements in\n5G Release 15. However, the extent of operator adoption remains unclear,\nespecially as most networks operate in 5G Non Stand Alone (NSA) mode, relying\non 4G Core Networks. This study provides the first qualitative and experimental\ncomparison between 5G NSA and Stand Alone (SA) in real operator networks,\nfocusing on privacy enhancements addressing top eight pre-5G attacks based on\nrecent academic literature. Additionally, it evaluates the privacy levels of\nOpenAirInterface (OAI), a leading open-source software for 5G, against real\nnetwork deployments for the same attacks. The analysis reveals two new 5G\nprivacy vulnerabilities, underscoring the need for further research and\nstricter standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring user privacy remains critical in mobile networks, particularly with\nthe rise of connected devices and denser 5G infrastructure. Privacy concerns\nhave persisted across 2G, 3G, and 4G/LTE networks. Recognizing these concerns,\nthe 3rd Generation Partnership Project (3GPP) has made privacy enhancements in\n5G Release 15. However, the extent of operator adoption remains unclear,\nespecially as most networks operate in 5G Non Stand Alone (NSA) mode, relying\non 4G Core Networks. This study provides the first qualitative and experimental\ncomparison between 5G NSA and Stand Alone (SA) in real operator networks,\nfocusing on privacy enhancements addressing top eight pre-5G attacks based on\nrecent academic literature. Additionally, it evaluates the privacy levels of\nOpenAirInterface (OAI), a leading open-source software for 5G, against real\nnetwork deployments for the same attacks. The analysis reveals two new 5G\nprivacy vulnerabilities, underscoring the need for further research and\nstricter standards."
                },
                "authors": [
                    {
                        "name": "Stavros Eleftherakis"
                    },
                    {
                        "name": "Timothy Otim"
                    },
                    {
                        "name": "Giuseppe Santaromita"
                    },
                    {
                        "name": "Almudena Diaz Zayas"
                    },
                    {
                        "name": "Domenico Giustiniano"
                    },
                    {
                        "name": "Nicolas Kourtellis"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Kourtellis"
                },
                "author": "Nicolas Kourtellis",
                "arxiv_doi": "10.1145/3636534.3690696",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3636534.3690696",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 6 Figures, 1 table, In ACM MobiCom 2024",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17699v1",
                "updated": "2024-09-26T10:12:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    12,
                    19,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T10:12:19Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    10,
                    12,
                    19,
                    3,
                    270,
                    0
                ],
                "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard\n  for Prompt Attacks"
                },
                "summary": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) in diverse applications\nunderscores the pressing need for robust security measures to thwart potential\njailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger\ndata integrity and user privacy. Guardrails serve as crucial protective\nmechanisms against such threats, but existing models often fall short in terms\nof both detection accuracy, and computational efficiency. This paper advocates\nfor the significance of jailbreak attack prevention on LLMs, and emphasises the\nrole of input guardrails in safeguarding these models. We introduce MoJE\n(Mixture of Jailbreak Expert), a novel guardrail architecture designed to\nsurpass current limitations in existing state-of-the-art guardrails. By\nemploying simple linguistic statistical techniques, MoJE excels in detecting\njailbreak attacks while maintaining minimal computational overhead during model\ninference. Through rigorous experimentation, MoJE demonstrates superior\nperformance capable of detecting 90% of the attacks without compromising benign\nprompts, enhancing LLMs security against jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Giandomenico Cornacchia"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Kieran Fraser"
                    },
                    {
                        "name": "Muhammad Zaid Hamed"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "Mark Purcell"
                    }
                ],
                "author_detail": {
                    "name": "Mark Purcell"
                },
                "author": "Mark Purcell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v1",
                "updated": "2024-09-26T09:57:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models will be available soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17691v1",
                "updated": "2024-09-26T09:56:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    56,
                    13,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:56:13Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    56,
                    13,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Bias Mitigation Without Privileged Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Bias Mitigation Without Privileged Information"
                },
                "summary": "Deep neural networks trained via empirical risk minimisation often exhibit\nsignificant performance disparities across groups, particularly when group and\ntask labels are spuriously correlated (e.g., \"grassy background\" and \"cows\").\nExisting bias mitigation methods that aim to address this issue often either\nrely on group labels for training or validation, or require an extensive\nhyperparameter search. Such data and computational requirements hinder the\npractical deployment of these methods, especially when datasets are too large\nto be group-annotated, computational resources are limited, and models are\ntrained through already complex pipelines. In this paper, we propose Targeted\nAugmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework\nthat leverages the entire training history of a helper model to identify\nspurious samples, and generate a group-balanced training set from which a\nrobust model can be trained. We show that TAB improves worst-group performance\nwithout any group information or model selection, outperforming existing\nmethods while maintaining overall accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks trained via empirical risk minimisation often exhibit\nsignificant performance disparities across groups, particularly when group and\ntask labels are spuriously correlated (e.g., \"grassy background\" and \"cows\").\nExisting bias mitigation methods that aim to address this issue often either\nrely on group labels for training or validation, or require an extensive\nhyperparameter search. Such data and computational requirements hinder the\npractical deployment of these methods, especially when datasets are too large\nto be group-annotated, computational resources are limited, and models are\ntrained through already complex pipelines. In this paper, we propose Targeted\nAugmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework\nthat leverages the entire training history of a helper model to identify\nspurious samples, and generate a group-balanced training set from which a\nrobust model can be trained. We show that TAB improves worst-group performance\nwithout any group information or model selection, outperforming existing\nmethods while maintaining overall accuracy."
                },
                "authors": [
                    {
                        "name": "Mateo Espinosa Zarlenga"
                    },
                    {
                        "name": "Swami Sankaranarayanan"
                    },
                    {
                        "name": "Jerone T. A. Andrews"
                    },
                    {
                        "name": "Zohreh Shams"
                    },
                    {
                        "name": "Mateja Jamnik"
                    },
                    {
                        "name": "Alice Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Alice Xiang"
                },
                "author": "Alice Xiang",
                "arxiv_comment": "Accepted at the 18th European Conference on Computer Vision (ECCV\n  2024) as an Oral presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00459v2",
                "updated": "2024-09-26T09:54:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    54,
                    57,
                    3,
                    270,
                    0
                ],
                "published": "2024-03-30T19:46:59Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    46,
                    59,
                    5,
                    90,
                    0
                ],
                "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning"
                },
                "summary": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Joseph Shtok"
                    },
                    {
                        "name": "Sivan Doveh"
                    },
                    {
                        "name": "Leonid Karlinsky"
                    },
                    {
                        "name": "Assaf Arbelle"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Arbelle"
                },
                "author": "Assaf Arbelle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17683v1",
                "updated": "2024-09-26T09:49:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    49,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:49:27Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    49,
                    27,
                    3,
                    270,
                    0
                ],
                "title": "Zero- and Few-shot Named Entity Recognition and Text Expansion in\n  Medication Prescriptions using ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero- and Few-shot Named Entity Recognition and Text Expansion in\n  Medication Prescriptions using ChatGPT"
                },
                "summary": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data."
                },
                "authors": [
                    {
                        "name": "Natthanaphop Isaradech"
                    },
                    {
                        "name": "Andrea Riedel"
                    },
                    {
                        "name": "Wachiranun Sirikul"
                    },
                    {
                        "name": "Markus Kreuzthaler"
                    },
                    {
                        "name": "Stefan Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schulz"
                },
                "author": "Stefan Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13167v2",
                "updated": "2024-09-26T09:42:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    42,
                    48,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-19T02:46:18Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    2,
                    46,
                    18,
                    2,
                    171,
                    0
                ],
                "title": "QRMeM: Unleash the Length Limitation through Question then Reflection\n  Memory Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QRMeM: Unleash the Length Limitation through Question then Reflection\n  Memory Mechanism"
                },
                "summary": "While large language models (LLMs) have made notable advancements in natural\nlanguage processing, they continue to struggle with processing extensive text.\nMemory mechanism offers a flexible solution for managing long contexts,\nutilizing techniques such as compression, summarization, and structuring to\nfacilitate nuanced and efficient handling of large volumes of text. However,\nexisting techniques face challenges with static knowledge integration, leading\nto insufficient adaptation to task-specific needs and missing\nmulti-segmentation relationships, which hinders the dynamic reorganization and\nlogical combination of relevant segments during the response process. To\naddress these issues, we introduce a novel strategy, Question then Reflection\nMemory Mechanism (QRMeM), incorporating a dual-structured memory pool. This\npool synergizes static textual content with structured graph guidance,\nfostering a reflective trial-and-error approach for navigating and identifying\nrelevant segments. Our evaluation across multiple-choice questions (MCQ) and\nmulti-document question answering (Multi-doc QA) benchmarks showcases QRMeM\nenhanced performance compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have made notable advancements in natural\nlanguage processing, they continue to struggle with processing extensive text.\nMemory mechanism offers a flexible solution for managing long contexts,\nutilizing techniques such as compression, summarization, and structuring to\nfacilitate nuanced and efficient handling of large volumes of text. However,\nexisting techniques face challenges with static knowledge integration, leading\nto insufficient adaptation to task-specific needs and missing\nmulti-segmentation relationships, which hinders the dynamic reorganization and\nlogical combination of relevant segments during the response process. To\naddress these issues, we introduce a novel strategy, Question then Reflection\nMemory Mechanism (QRMeM), incorporating a dual-structured memory pool. This\npool synergizes static textual content with structured graph guidance,\nfostering a reflective trial-and-error approach for navigating and identifying\nrelevant segments. Our evaluation across multiple-choice questions (MCQ) and\nmulti-document question answering (Multi-doc QA) benchmarks showcases QRMeM\nenhanced performance compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Jiahao Ying"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Chong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chong Feng"
                },
                "author": "Chong Feng",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12753v2",
                "updated": "2024-09-26T09:17:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    17,
                    10,
                    3,
                    270,
                    0
                ],
                "published": "2024-04-19T09:59:44Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    9,
                    59,
                    44,
                    4,
                    110,
                    0
                ],
                "title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper\n  Generation"
                },
                "summary": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}"
                },
                "authors": [
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Chenghao Peng"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Liqian Wen"
                    },
                    {
                        "name": "Zulong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zulong Chen"
                },
                "author": "Zulong Chen",
                "arxiv_comment": "19 pages, 4 figures, 18 tables. Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17655v1",
                "updated": "2024-09-26T09:06:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    6,
                    56,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T09:06:56Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    6,
                    56,
                    3,
                    270,
                    0
                ],
                "title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative\n  Human-Populated Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative\n  Human-Populated Environment"
                },
                "summary": "The increasing demand for intelligent assistants in human-populated\nenvironments has motivated significant research in autonomous robotic systems.\nTraditional service robots and virtual assistants, however, struggle with\nreal-world task execution due to their limited capacity for dynamic reasoning\nand interaction, particularly when human collaboration is required. Recent\ndevelopments in Large Language Models have opened new avenues for improving\nthese systems, enabling more sophisticated reasoning and natural interaction\ncapabilities. In this paper, we introduce AssistantX, an LLM-powered proactive\nassistant designed to operate autonomously in a physical office environment.\nUnlike conventional service robots, AssistantX leverages a novel multi-agent\narchitecture, PPDR4X, which provides advanced inference capabilities and\ncomprehensive collaboration awareness. By effectively bridging the gap between\nvirtual operations and physical interactions, AssistantX demonstrates robust\nperformance in managing complex real-world scenarios. Our evaluation highlights\nthe architecture's effectiveness, showing that AssistantX can respond to clear\ninstructions, actively retrieve supplementary information from memory, and\nproactively seek collaboration from team members to ensure successful task\ncompletion. More details and videos can be found at\nhttps://assistantx-agent.github.io/AssistantX/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for intelligent assistants in human-populated\nenvironments has motivated significant research in autonomous robotic systems.\nTraditional service robots and virtual assistants, however, struggle with\nreal-world task execution due to their limited capacity for dynamic reasoning\nand interaction, particularly when human collaboration is required. Recent\ndevelopments in Large Language Models have opened new avenues for improving\nthese systems, enabling more sophisticated reasoning and natural interaction\ncapabilities. In this paper, we introduce AssistantX, an LLM-powered proactive\nassistant designed to operate autonomously in a physical office environment.\nUnlike conventional service robots, AssistantX leverages a novel multi-agent\narchitecture, PPDR4X, which provides advanced inference capabilities and\ncomprehensive collaboration awareness. By effectively bridging the gap between\nvirtual operations and physical interactions, AssistantX demonstrates robust\nperformance in managing complex real-world scenarios. Our evaluation highlights\nthe architecture's effectiveness, showing that AssistantX can respond to clear\ninstructions, actively retrieve supplementary information from memory, and\nproactively seek collaboration from team members to ensure successful task\ncompletion. More details and videos can be found at\nhttps://assistantx-agent.github.io/AssistantX/."
                },
                "authors": [
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Yongchang Li"
                    },
                    {
                        "name": "Lumeng Ma"
                    },
                    {
                        "name": "Di Guo"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "arxiv_comment": "6 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17650v1",
                "updated": "2024-09-26T08:56:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    56,
                    54,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:56:54Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    56,
                    54,
                    3,
                    270,
                    0
                ],
                "title": "Digital Twin Ecosystem for Oncology Clinical Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin Ecosystem for Oncology Clinical Operations"
                },
                "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations."
                },
                "authors": [
                    {
                        "name": "Himanshu Pandey"
                    },
                    {
                        "name": "Akhil Amod"
                    },
                    {
                        "name": "Shivang"
                    },
                    {
                        "name": "Kshitij Jaggi"
                    },
                    {
                        "name": "Ruchi Garg"
                    },
                    {
                        "name": "Abheet Jain"
                    },
                    {
                        "name": "Vinayak Tantia"
                    }
                ],
                "author_detail": {
                    "name": "Vinayak Tantia"
                },
                "author": "Vinayak Tantia",
                "arxiv_comment": "Pre Print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17648v1",
                "updated": "2024-09-26T08:55:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    55,
                    21,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:55:21Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    55,
                    21,
                    3,
                    270,
                    0
                ],
                "title": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited."
                },
                "authors": [
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Phat Vo"
                    },
                    {
                        "name": "Arman Kizilkale"
                    },
                    {
                        "name": "Aaron Reite"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reite"
                },
                "author": "Aaron Reite",
                "arxiv_comment": "6 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16908v2",
                "updated": "2024-09-26T08:53:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    53,
                    1,
                    3,
                    270,
                    0
                ],
                "published": "2024-05-27T07:56:23Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    7,
                    56,
                    23,
                    0,
                    148,
                    0
                ],
                "title": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty\n  in Words?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty\n  in Words?"
                },
                "summary": "We posit that large language models (LLMs) should be capable of expressing\ntheir intrinsic uncertainty in natural language. For example, if the LLM is\nequally likely to output two contradicting answers to the same question, then\nits generated response should reflect this uncertainty by hedging its answer\n(e.g., \"I'm not sure, but I think...\"). We formalize faithful response\nuncertainty based on the gap between the model's intrinsic confidence in the\nassertions it makes and the decisiveness by which they are conveyed. This\nexample-level metric reliably indicates whether the model reflects its\nuncertainty, as it penalizes both excessive and insufficient hedging. We\nevaluate a variety of aligned LLMs at faithfully communicating uncertainty on\nseveral knowledge-intensive question answering tasks. Our results provide\nstrong evidence that modern LLMs are poor at faithfully conveying their\nuncertainty, and that better alignment is necessary to improve their\ntrustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We posit that large language models (LLMs) should be capable of expressing\ntheir intrinsic uncertainty in natural language. For example, if the LLM is\nequally likely to output two contradicting answers to the same question, then\nits generated response should reflect this uncertainty by hedging its answer\n(e.g., \"I'm not sure, but I think...\"). We formalize faithful response\nuncertainty based on the gap between the model's intrinsic confidence in the\nassertions it makes and the decisiveness by which they are conveyed. This\nexample-level metric reliably indicates whether the model reflects its\nuncertainty, as it penalizes both excessive and insufficient hedging. We\nevaluate a variety of aligned LLMs at faithfully communicating uncertainty on\nseveral knowledge-intensive question answering tasks. Our results provide\nstrong evidence that modern LLMs are poor at faithfully conveying their\nuncertainty, and that better alignment is necessary to improve their\ntrustworthiness."
                },
                "authors": [
                    {
                        "name": "Gal Yona"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "To appear in EMNLP 2024 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15246v2",
                "updated": "2024-09-26T08:48:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    48,
                    3,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T17:42:05Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    42,
                    5,
                    0,
                    267,
                    0
                ],
                "title": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Air Deep Learning Integrated Semantic Inference Models for Enhanced\n  Earth Observation Satellite Networks"
                },
                "summary": "Earth Observation (EO) systems play a crucial role in achieving Sustainable\nDevelopment Goals by collecting and analyzing vital global data through\nsatellite networks. These systems are essential for tasks like mapping,\ndisaster monitoring, and resource management, but they face challenges in\nprocessing and transmitting large volumes of EO data, especially in specialized\nfields such as agriculture and real-time disaster response. Domain-adapted\nLarge Language Models (LLMs) provide a promising solution by facilitating data\nfusion between extensive EO data and semantic EO data. By improving integration\nand interpretation of diverse datasets, LLMs address the challenges of\nprocessing specialized information in agriculture and disaster response\napplications. This fusion enhances the accuracy and relevance of transmitted\ndata. This paper presents a framework for semantic communication in EO\nsatellite networks, aimed at improving data transmission efficiency and overall\nsystem performance through cognitive processing techniques. The proposed system\nemploys Discrete-Task-Oriented Source-Channel Coding (DT-JSCC) and Semantic\nData Augmentation (SA) to focus on relevant information while minimizing\ncommunication overhead. By integrating cognitive semantic processing and\ninter-satellite links, the framework enhances the analysis and transmission of\nmultispectral satellite imagery, improving object detection, pattern\nrecognition, and real-time decision-making. The introduction of Cognitive\nSemantic Augmentation (CSA) allows satellites to process and transmit semantic\ninformation, boosting adaptability to changing environments and application\nneeds. This end-to-end architecture is tailored for next-generation satellite\nnetworks, such as those supporting 6G, and demonstrates significant\nimprovements in efficiency and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Earth Observation (EO) systems play a crucial role in achieving Sustainable\nDevelopment Goals by collecting and analyzing vital global data through\nsatellite networks. These systems are essential for tasks like mapping,\ndisaster monitoring, and resource management, but they face challenges in\nprocessing and transmitting large volumes of EO data, especially in specialized\nfields such as agriculture and real-time disaster response. Domain-adapted\nLarge Language Models (LLMs) provide a promising solution by facilitating data\nfusion between extensive EO data and semantic EO data. By improving integration\nand interpretation of diverse datasets, LLMs address the challenges of\nprocessing specialized information in agriculture and disaster response\napplications. This fusion enhances the accuracy and relevance of transmitted\ndata. This paper presents a framework for semantic communication in EO\nsatellite networks, aimed at improving data transmission efficiency and overall\nsystem performance through cognitive processing techniques. The proposed system\nemploys Discrete-Task-Oriented Source-Channel Coding (DT-JSCC) and Semantic\nData Augmentation (SA) to focus on relevant information while minimizing\ncommunication overhead. By integrating cognitive semantic processing and\ninter-satellite links, the framework enhances the analysis and transmission of\nmultispectral satellite imagery, improving object detection, pattern\nrecognition, and real-time decision-making. The introduction of Cognitive\nSemantic Augmentation (CSA) allows satellites to process and transmit semantic\ninformation, boosting adaptability to changing environments and application\nneeds. This end-to-end architecture is tailored for next-generation satellite\nnetworks, such as those supporting 6G, and demonstrates significant\nimprovements in efficiency and accuracy."
                },
                "authors": [
                    {
                        "name": "Hong-fu Chou"
                    },
                    {
                        "name": "Vu Nguyen Ha"
                    },
                    {
                        "name": "Prabhu Thiruvasagam"
                    },
                    {
                        "name": "Thanh-Dung Le"
                    },
                    {
                        "name": "Geoffrey Eappen"
                    },
                    {
                        "name": "Ti Ti Nguyen"
                    },
                    {
                        "name": "Luis M. Garces-Socarras"
                    },
                    {
                        "name": "Jorge L. Gonzalez-Rios"
                    },
                    {
                        "name": "Juan Carlos Merlano-Duncan"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "arxiv_comment": "18 pages, 10 figures, magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10902v2",
                "updated": "2024-09-26T08:47:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    47,
                    36,
                    3,
                    270,
                    0
                ],
                "published": "2024-08-20T14:45:23Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    14,
                    45,
                    23,
                    1,
                    233,
                    0
                ],
                "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs"
                },
                "summary": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation."
                },
                "authors": [
                    {
                        "name": "John Mendonça"
                    },
                    {
                        "name": "Isabel Trancoso"
                    },
                    {
                        "name": "Alon Lavie"
                    }
                ],
                "author_detail": {
                    "name": "Alon Lavie"
                },
                "author": "Alon Lavie",
                "arxiv_comment": "Accepted to EMNLP2024 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17642v1",
                "updated": "2024-09-26T08:45:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    45,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:45:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    45,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Delegates with a Dual Focus: Ensuring Privacy and Strategic\n  Self-Disclosure"
                },
                "summary": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based AI delegates are increasingly utilized to\nact on behalf of users, assisting them with a wide range of tasks through\nconversational interfaces. Despite their advantages, concerns arise regarding\nthe potential risk of privacy leaks, particularly in scenarios involving social\ninteractions. While existing research has focused on protecting privacy by\nlimiting the access of AI delegates to sensitive user information, many social\nscenarios require disclosing private details to achieve desired outcomes,\nnecessitating a balance between privacy protection and disclosure. To address\nthis challenge, we conduct a pilot study to investigate user preferences for AI\ndelegates across various social relations and task scenarios, and then propose\na novel AI delegate system that enables privacy-conscious self-disclosure. Our\nuser study demonstrates that the proposed AI delegate strategically protects\nprivacy, pioneering its use in diverse and dynamic social interactions."
                },
                "authors": [
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xi Cheng"
                    },
                    {
                        "name": "Hangxin Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17640v1",
                "updated": "2024-09-26T08:44:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task"
                },
                "summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations."
                },
                "authors": [
                    {
                        "name": "Xindi Tong"
                    },
                    {
                        "name": "Yujin Zhu"
                    },
                    {
                        "name": "Shijian Fan"
                    },
                    {
                        "name": "Liang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xu"
                },
                "author": "Liang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17634v1",
                "updated": "2024-09-26T08:31:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    31,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:31:27Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    31,
                    27,
                    3,
                    270,
                    0
                ],
                "title": "P4Q: Learning to Prompt for Quantization in Visual-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P4Q: Learning to Prompt for Quantization in Visual-language Models"
                },
                "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence\nin various visual and multimodal tasks, yet the deployment of VLMs on\ndownstream application platforms remains challenging due to their prohibitive\nrequirements of training samples and computing resources. Fine-tuning and\nquantization of VLMs can substantially reduce the sample and computation costs,\nwhich are in urgent need. There are two prevailing paradigms in quantization,\nQuantization-Aware Training (QAT) can effectively quantize large-scale VLMs but\nincur a huge training cost, while low-bit Post-Training Quantization (PTQ)\nsuffers from a notable performance drop. We propose a method that balances\nfine-tuning and quantization named ``Prompt for Quantization'' (P4Q), in which\nwe design a lightweight architecture to leverage contrastive loss supervision\nto enhance the recognition performance of a PTQ model. Our method can\neffectively reduce the gap between image features and text features caused by\nlow-bit quantization, based on learnable prompts to reorganize textual\nrepresentations and a low-bit adapter to realign the distributions of image and\ntext features. We also introduce a distillation loss based on cosine similarity\npredictions to distill the quantized model using a full-precision teacher.\nExtensive experimental results demonstrate that our P4Q method outperforms\nprior arts, even achieving comparable results to its full-precision\ncounterparts. For instance, our 8-bit P4Q can theoretically compress the\nCLIP-ViT/B-32 by 4 $\\times$ while achieving 66.94\\% Top-1 accuracy,\noutperforming the learnable prompt fine-tuned full-precision model by 2.24\\%\nwith negligible additional parameters on the ImageNet dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence\nin various visual and multimodal tasks, yet the deployment of VLMs on\ndownstream application platforms remains challenging due to their prohibitive\nrequirements of training samples and computing resources. Fine-tuning and\nquantization of VLMs can substantially reduce the sample and computation costs,\nwhich are in urgent need. There are two prevailing paradigms in quantization,\nQuantization-Aware Training (QAT) can effectively quantize large-scale VLMs but\nincur a huge training cost, while low-bit Post-Training Quantization (PTQ)\nsuffers from a notable performance drop. We propose a method that balances\nfine-tuning and quantization named ``Prompt for Quantization'' (P4Q), in which\nwe design a lightweight architecture to leverage contrastive loss supervision\nto enhance the recognition performance of a PTQ model. Our method can\neffectively reduce the gap between image features and text features caused by\nlow-bit quantization, based on learnable prompts to reorganize textual\nrepresentations and a low-bit adapter to realign the distributions of image and\ntext features. We also introduce a distillation loss based on cosine similarity\npredictions to distill the quantized model using a full-precision teacher.\nExtensive experimental results demonstrate that our P4Q method outperforms\nprior arts, even achieving comparable results to its full-precision\ncounterparts. For instance, our 8-bit P4Q can theoretically compress the\nCLIP-ViT/B-32 by 4 $\\times$ while achieving 66.94\\% Top-1 accuracy,\noutperforming the learnable prompt fine-tuned full-precision model by 2.24\\%\nwith negligible additional parameters on the ImageNet dataset."
                },
                "authors": [
                    {
                        "name": "Huixin Sun"
                    },
                    {
                        "name": "Runqi Wang"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Xianbin Cao"
                    },
                    {
                        "name": "Xiaolong Jiang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Baochang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Baochang Zhang"
                },
                "author": "Baochang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09802v2",
                "updated": "2024-09-26T08:15:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    15,
                    50,
                    3,
                    270,
                    0
                ],
                "published": "2023-11-16T11:26:21Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    11,
                    26,
                    21,
                    3,
                    320,
                    0
                ],
                "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs"
                },
                "summary": "Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CaRing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CaRing"
                },
                "authors": [
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14208v2",
                "updated": "2024-09-26T08:12:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    12,
                    59,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-20T11:26:06Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    11,
                    26,
                    6,
                    3,
                    172,
                    0
                ],
                "title": "SeCoKD: Aligning Large Language Models for In-Context Learning with\n  Fewer Shots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeCoKD: Aligning Large Language Models for In-Context Learning with\n  Fewer Shots"
                },
                "summary": "Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning."
                },
                "authors": [
                    {
                        "name": "Weixing Wang"
                    },
                    {
                        "name": "Haojin Yang"
                    },
                    {
                        "name": "Christoph Meinel"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Meinel"
                },
                "author": "Christoph Meinel",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17617v1",
                "updated": "2024-09-26T08:09:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    9,
                    36,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T08:09:36Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    9,
                    36,
                    3,
                    270,
                    0
                ],
                "title": "Estimating The Carbon Footprint Of Digital Agriculture Deployment: A\n  Parametric Bottom-Up Modelling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating The Carbon Footprint Of Digital Agriculture Deployment: A\n  Parametric Bottom-Up Modelling Approach"
                },
                "summary": "Digitalization appears as a lever to enhance agriculture sustainability.\nHowever, existing works on digital agriculture's own sustainability remain\nscarce, disregarding the environmental effects of deploying digital devices on\na large-scale. We propose a bottom-up method to estimate the carbon footprint\nof digital agriculture scenarios considering deployment of devices over a\ndiversity of farm sizes. It is applied to two use-cases and demonstrates that\ndigital agriculture encompasses a diversity of devices with heterogeneous\ncarbon footprints and that more complex devices yield higher footprints not\nalways compensated by better performances or scaling gains. By emphasizing the\nnecessity of considering the multiplicity of devices, and the territorial\ndistribution of farm sizes when modelling digital agriculture deployments, this\nstudy highlights the need for further exploration of the first-order effects of\ndigital technologies in agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digitalization appears as a lever to enhance agriculture sustainability.\nHowever, existing works on digital agriculture's own sustainability remain\nscarce, disregarding the environmental effects of deploying digital devices on\na large-scale. We propose a bottom-up method to estimate the carbon footprint\nof digital agriculture scenarios considering deployment of devices over a\ndiversity of farm sizes. It is applied to two use-cases and demonstrates that\ndigital agriculture encompasses a diversity of devices with heterogeneous\ncarbon footprints and that more complex devices yield higher footprints not\nalways compensated by better performances or scaling gains. By emphasizing the\nnecessity of considering the multiplicity of devices, and the territorial\ndistribution of farm sizes when modelling digital agriculture deployments, this\nstudy highlights the need for further exploration of the first-order effects of\ndigital technologies in agriculture."
                },
                "authors": [
                    {
                        "name": "Pierre La Rocca"
                    },
                    {
                        "name": "Gaël Guennebaud"
                    },
                    {
                        "name": "Aurélie Bugeau"
                    },
                    {
                        "name": "Anne-Laure Ligozat"
                    }
                ],
                "author_detail": {
                    "name": "Anne-Laure Ligozat"
                },
                "arxiv_affiliation": "ENSIIE, LISN, STL",
                "author": "Anne-Laure Ligozat",
                "arxiv_comment": "Journal of Industrial Ecology, In press, 10.1111/jiec.13568",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15254v3",
                "updated": "2024-09-26T08:01:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    1,
                    39,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-23T17:53:42Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    53,
                    42,
                    0,
                    267,
                    0
                ],
                "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Archon: An Architecture Search Framework for Inference-Time Techniques"
                },
                "summary": "Inference-time techniques are emerging as highly effective tools to increase\nlarge language model (LLM) capabilities. However, there is still limited\nunderstanding of the best practices for developing systems that combine\ninference-time techniques with one or more LLMs, with challenges including: (1)\neffectively allocating inference compute budget, (2) understanding the\ninteractions between different combinations of inference-time techniques and\ntheir impact on downstream performance, and 3) efficiently searching over the\nlarge space of model choices, inference-time techniques, and their\ncompositions. To address these challenges, we introduce Archon, an automated\nframework for designing inference-time architectures. Archon defines an\nextensible design space, encompassing methods such as generation ensembling,\nmulti-sampling, ranking, fusion, critiquing, verification, and unit testing. It\nthen transforms the problem of selecting and combining LLMs and inference-time\ntechniques into a hyperparameter optimization objective. To optimize this\nobjective, we introduce automated Inference-Time Architecture Search (ITAS)\nalgorithms. Given target benchmark(s), an inference compute budget, and\navailable LLMs, ITAS outputs optimized architectures. We evaluate Archon\narchitectures across a wide range of instruction-following and reasoning\nbenchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,\nMixEval Hard, MATH, and CodeContests. We show that automatically designed\ninference-time architectures by Archon outperform strong models such as GPT-4o\nand Claude 3.5 Sonnet on these benchmarks, achieving an average increase of\n15.1 and 11.2 percentage points with all-source models and open-source models,\nrespectively. We make our code and datasets available publicly on Github:\nhttps://github.com/ScalingIntelligence/Archon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time techniques are emerging as highly effective tools to increase\nlarge language model (LLM) capabilities. However, there is still limited\nunderstanding of the best practices for developing systems that combine\ninference-time techniques with one or more LLMs, with challenges including: (1)\neffectively allocating inference compute budget, (2) understanding the\ninteractions between different combinations of inference-time techniques and\ntheir impact on downstream performance, and 3) efficiently searching over the\nlarge space of model choices, inference-time techniques, and their\ncompositions. To address these challenges, we introduce Archon, an automated\nframework for designing inference-time architectures. Archon defines an\nextensible design space, encompassing methods such as generation ensembling,\nmulti-sampling, ranking, fusion, critiquing, verification, and unit testing. It\nthen transforms the problem of selecting and combining LLMs and inference-time\ntechniques into a hyperparameter optimization objective. To optimize this\nobjective, we introduce automated Inference-Time Architecture Search (ITAS)\nalgorithms. Given target benchmark(s), an inference compute budget, and\navailable LLMs, ITAS outputs optimized architectures. We evaluate Archon\narchitectures across a wide range of instruction-following and reasoning\nbenchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,\nMixEval Hard, MATH, and CodeContests. We show that automatically designed\ninference-time architectures by Archon outperform strong models such as GPT-4o\nand Claude 3.5 Sonnet on these benchmarks, achieving an average increase of\n15.1 and 11.2 percentage points with all-source models and open-source models,\nrespectively. We make our code and datasets available publicly on Github:\nhttps://github.com/ScalingIntelligence/Archon."
                },
                "authors": [
                    {
                        "name": "Jon Saad-Falcon"
                    },
                    {
                        "name": "Adrian Gamarra Lafuente"
                    },
                    {
                        "name": "Shlok Natarajan"
                    },
                    {
                        "name": "Nahum Maru"
                    },
                    {
                        "name": "Hristo Todorov"
                    },
                    {
                        "name": "Etash Guha"
                    },
                    {
                        "name": "E. Kelly Buchanan"
                    },
                    {
                        "name": "Mayee Chen"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17610v1",
                "updated": "2024-09-26T07:55:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    55,
                    57,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:55:57Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    55,
                    57,
                    3,
                    270,
                    0
                ],
                "title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context\n  Information in Multi-Turn Multimodal Medical Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context\n  Information in Multi-Turn Multimodal Medical Dialogue"
                },
                "summary": "The rocketing prosperity of large language models (LLMs) in recent years has\nboosted the prevalence of vision-language models (VLMs) in the medical sector.\nIn our online medical consultation scenario, a doctor responds to the texts and\nimages provided by a patient in multiple rounds to diagnose her/his health\ncondition, forming a multi-turn multimodal medical dialogue format. Unlike\nhigh-quality images captured by professional equipment in traditional medical\nvisual question answering (Med-VQA), the images in our case are taken by\npatients' mobile phones. These images have poor quality control, with issues\nsuch as excessive background elements and the lesion area being significantly\noff-center, leading to degradation of vision-language alignment in the model\ntraining phase. In this paper, we propose ZALM3, a Zero-shot strategy to\nimprove vision-language ALignment in Multi-turn Multimodal Medical dialogue.\nSince we observe that the preceding text conversations before an image can\ninfer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to\nsummarize the keywords from the preceding context and a visual grounding model\nto extract the RoIs. The updated images eliminate unnecessary background noise\nand provide more effective vision-language alignment. To better evaluate our\nproposed method, we design a new subjective assessment metric for multi-turn\nunimodal/multimodal medical dialogue to provide a fine-grained performance\ncomparison. Our experiments across three different clinical departments\nremarkably demonstrate the efficacy of ZALM3 with statistical significance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rocketing prosperity of large language models (LLMs) in recent years has\nboosted the prevalence of vision-language models (VLMs) in the medical sector.\nIn our online medical consultation scenario, a doctor responds to the texts and\nimages provided by a patient in multiple rounds to diagnose her/his health\ncondition, forming a multi-turn multimodal medical dialogue format. Unlike\nhigh-quality images captured by professional equipment in traditional medical\nvisual question answering (Med-VQA), the images in our case are taken by\npatients' mobile phones. These images have poor quality control, with issues\nsuch as excessive background elements and the lesion area being significantly\noff-center, leading to degradation of vision-language alignment in the model\ntraining phase. In this paper, we propose ZALM3, a Zero-shot strategy to\nimprove vision-language ALignment in Multi-turn Multimodal Medical dialogue.\nSince we observe that the preceding text conversations before an image can\ninfer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to\nsummarize the keywords from the preceding context and a visual grounding model\nto extract the RoIs. The updated images eliminate unnecessary background noise\nand provide more effective vision-language alignment. To better evaluate our\nproposed method, we design a new subjective assessment metric for multi-turn\nunimodal/multimodal medical dialogue to provide a fine-grained performance\ncomparison. Our experiments across three different clinical departments\nremarkably demonstrate the efficacy of ZALM3 with statistical significance."
                },
                "authors": [
                    {
                        "name": "Zhangpu Li"
                    },
                    {
                        "name": "Changhong Zou"
                    },
                    {
                        "name": "Suxue Ma"
                    },
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Chen Du"
                    },
                    {
                        "name": "Youbao Tang"
                    },
                    {
                        "name": "Zhenjie Cao"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Jui-Hsin Lai"
                    },
                    {
                        "name": "Ruei-Sung Lin"
                    },
                    {
                        "name": "Yuan Ni"
                    },
                    {
                        "name": "Xingzhi Sun"
                    },
                    {
                        "name": "Jing Xiao"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Mei Han"
                    }
                ],
                "author_detail": {
                    "name": "Mei Han"
                },
                "author": "Mei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16341v2",
                "updated": "2024-09-26T07:54:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    54,
                    10,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T17:20:02Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    20,
                    2,
                    1,
                    268,
                    0
                ],
                "title": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs"
                },
                "summary": "Training large language models (LLMs) for external tool usage is a rapidly\nexpanding field, with recent research focusing on generating synthetic data to\naddress the shortage of available data. However, the absence of systematic data\nquality checks poses complications for properly training and testing models. To\nthat end, we propose two approaches for assessing the reliability of data for\ntraining LLMs to use external tools. The first approach uses intuitive,\nhuman-defined correctness criteria. The second approach uses a model-driven\nassessment with in-context evaluation. We conduct a thorough evaluation of data\nquality on two popular benchmarks, followed by an extrinsic evaluation that\nshowcases the impact of data quality on model performance. Our results\ndemonstrate that models trained on high-quality data outperform those trained\non unvalidated data, even when trained with a smaller quantity of data. These\nfindings empirically support the significance of assessing and ensuring the\nreliability of training data for tool-using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) for external tool usage is a rapidly\nexpanding field, with recent research focusing on generating synthetic data to\naddress the shortage of available data. However, the absence of systematic data\nquality checks poses complications for properly training and testing models. To\nthat end, we propose two approaches for assessing the reliability of data for\ntraining LLMs to use external tools. The first approach uses intuitive,\nhuman-defined correctness criteria. The second approach uses a model-driven\nassessment with in-context evaluation. We conduct a thorough evaluation of data\nquality on two popular benchmarks, followed by an extrinsic evaluation that\nshowcases the impact of data quality on model performance. Our results\ndemonstrate that models trained on high-quality data outperform those trained\non unvalidated data, even when trained with a smaller quantity of data. These\nfindings empirically support the significance of assessing and ensuring the\nreliability of training data for tool-using LLMs."
                },
                "authors": [
                    {
                        "name": "Shadi Iskander"
                    },
                    {
                        "name": "Nachshon Cohen"
                    },
                    {
                        "name": "Zohar Karnin"
                    },
                    {
                        "name": "Ori Shapira"
                    },
                    {
                        "name": "Sofia Tolmach"
                    }
                ],
                "author_detail": {
                    "name": "Sofia Tolmach"
                },
                "author": "Sofia Tolmach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14337v2",
                "updated": "2024-09-26T07:20:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    20,
                    51,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-22T06:45:38Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    6,
                    45,
                    38,
                    6,
                    266,
                    0
                ],
                "title": "MobileViews: A Large-Scale Mobile GUI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileViews: A Large-Scale Mobile GUI Dataset"
                },
                "summary": "Mobile screen assistants help smartphone users by interpreting mobile screens\nand responding to user requests. The excessive private information on mobile\nscreens necessitates small, on-device models to power these assistants.\nHowever, there is a lack of a comprehensive and large-scale mobile screen\ndataset with high diversity to train and enhance these models. To efficiently\nconstruct such a dataset, we utilize an LLM-enhanced automatic app traversal\ntool to minimize human intervention. We then employ two SoC clusters to provide\nhigh-fidelity mobile environments, including more than 200 Android instances to\nparallelize app interactions. By utilizing the system to collect mobile screens\nover 81,600 device-hours, we introduce MobileViews, the largest mobile screen\ndataset, which includes over 600K screenshot-view hierarchy pairs from more\nthan 20K modern Android apps. We demonstrate the effectiveness of MobileViews\nby training SOTA multimodal LLMs that power mobile screen assistants on it and\nthe Rico dataset, which was introduced seven years ago. Evaluation results on\nmobile screen tasks show that the scale and quality of mobile screens in\nMobileViews demonstrate significant advantages over Rico in augmenting mobile\nscreen assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile screen assistants help smartphone users by interpreting mobile screens\nand responding to user requests. The excessive private information on mobile\nscreens necessitates small, on-device models to power these assistants.\nHowever, there is a lack of a comprehensive and large-scale mobile screen\ndataset with high diversity to train and enhance these models. To efficiently\nconstruct such a dataset, we utilize an LLM-enhanced automatic app traversal\ntool to minimize human intervention. We then employ two SoC clusters to provide\nhigh-fidelity mobile environments, including more than 200 Android instances to\nparallelize app interactions. By utilizing the system to collect mobile screens\nover 81,600 device-hours, we introduce MobileViews, the largest mobile screen\ndataset, which includes over 600K screenshot-view hierarchy pairs from more\nthan 20K modern Android apps. We demonstrate the effectiveness of MobileViews\nby training SOTA multimodal LLMs that power mobile screen assistants on it and\nthe Rico dataset, which was introduced seven years ago. Evaluation results on\nmobile screen tasks show that the scale and quality of mobile screens in\nMobileViews demonstrate significant advantages over Rico in augmenting mobile\nscreen assistants."
                },
                "authors": [
                    {
                        "name": "Longxi Gao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Shihe Wang"
                    },
                    {
                        "name": "Shangguang Wang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "arxiv_comment": "Dataset: https://huggingface.co/datasets/mllmTeam/MobileViews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10267v2",
                "updated": "2024-09-26T06:57:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    57,
                    27,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-11T09:24:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    9,
                    24,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "Unused information in token probability distribution of generative LLM:\n  improving LLM reading comprehension through calculation of expected values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unused information in token probability distribution of generative LLM:\n  improving LLM reading comprehension through calculation of expected values"
                },
                "summary": "LLM text decoding is key component for perceived LLM quality. We demonstrate\ntwo experiments showing that decoding methods could be improved by manipulation\nof token probabilities. First, we test few LLM on SummEval summary scoring\ndataset, to measure reading comprehension. We compare scores from greedy\ndecoding to expected values over the next token distribution. We scale logits\nby large temperature to increase the entropy of scores. This allows strong\nimprovement of performance on SummEval (in terms of correlations to human\njudgement). We see improvement from 6-8% to 13-28% for 7B Mistral and from\n20%-46% to 37%-56% for Mixtral, beating GPT 4 0314 result on two metrics. Part\nof the gain seems related to positional bias. Secondly, we use\nprobability-based tree sampling algorithm, to examine all most probable\ngenerations for given prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM text decoding is key component for perceived LLM quality. We demonstrate\ntwo experiments showing that decoding methods could be improved by manipulation\nof token probabilities. First, we test few LLM on SummEval summary scoring\ndataset, to measure reading comprehension. We compare scores from greedy\ndecoding to expected values over the next token distribution. We scale logits\nby large temperature to increase the entropy of scores. This allows strong\nimprovement of performance on SummEval (in terms of correlations to human\njudgement). We see improvement from 6-8% to 13-28% for 7B Mistral and from\n20%-46% to 37%-56% for Mixtral, beating GPT 4 0314 result on two metrics. Part\nof the gain seems related to positional bias. Secondly, we use\nprobability-based tree sampling algorithm, to examine all most probable\ngenerations for given prompt."
                },
                "authors": [
                    {
                        "name": "Krystian Zawistowski"
                    }
                ],
                "author_detail": {
                    "name": "Krystian Zawistowski"
                },
                "author": "Krystian Zawistowski",
                "arxiv_comment": "7 pages, 1 figure, presented at FEDCSIS 2024 conference,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17581v1",
                "updated": "2024-09-26T06:57:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    57,
                    22,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T06:57:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    57,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "A Scalable Data-Driven Framework for Systematic Analysis of SEC 10-K\n  Filings Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Data-Driven Framework for Systematic Analysis of SEC 10-K\n  Filings Using Large Language Models"
                },
                "summary": "The number of companies listed on the NYSE has been growing exponentially,\ncreating a significant challenge for market analysts, traders, and stockholders\nwho must monitor and assess the performance and strategic shifts of a large\nnumber of companies regularly. There is an increasing need for a fast,\ncost-effective, and comprehensive method to evaluate the performance and detect\nand compare many companies' strategy changes efficiently. We propose a novel\ndata-driven approach that leverages large language models (LLMs) to\nsystematically analyze and rate the performance of companies based on their SEC\n10-K filings. These filings, which provide detailed annual reports on a\ncompany's financial performance and strategic direction, serve as a rich source\nof data for evaluating various aspects of corporate health, including\nconfidence, environmental sustainability, innovation, and workforce management.\nWe also introduce an automated system for extracting and preprocessing 10-K\nfilings. This system accurately identifies and segments the required sections\nas outlined by the SEC, while also isolating key textual content that contains\ncritical information about the company. This curated data is then fed into\nCohere's Command-R+ LLM to generate quantitative ratings across various\nperformance metrics. These ratings are subsequently processed and visualized to\nprovide actionable insights. The proposed scheme is then implemented on an\ninteractive GUI as a no-code solution for running the data pipeline and\ncreating the visualizations. The application showcases the rating results and\nprovides year-on-year comparisons of company performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of companies listed on the NYSE has been growing exponentially,\ncreating a significant challenge for market analysts, traders, and stockholders\nwho must monitor and assess the performance and strategic shifts of a large\nnumber of companies regularly. There is an increasing need for a fast,\ncost-effective, and comprehensive method to evaluate the performance and detect\nand compare many companies' strategy changes efficiently. We propose a novel\ndata-driven approach that leverages large language models (LLMs) to\nsystematically analyze and rate the performance of companies based on their SEC\n10-K filings. These filings, which provide detailed annual reports on a\ncompany's financial performance and strategic direction, serve as a rich source\nof data for evaluating various aspects of corporate health, including\nconfidence, environmental sustainability, innovation, and workforce management.\nWe also introduce an automated system for extracting and preprocessing 10-K\nfilings. This system accurately identifies and segments the required sections\nas outlined by the SEC, while also isolating key textual content that contains\ncritical information about the company. This curated data is then fed into\nCohere's Command-R+ LLM to generate quantitative ratings across various\nperformance metrics. These ratings are subsequently processed and visualized to\nprovide actionable insights. The proposed scheme is then implemented on an\ninteractive GUI as a no-code solution for running the data pipeline and\ncreating the visualizations. The application showcases the rating results and\nprovides year-on-year comparisons of company performance."
                },
                "authors": [
                    {
                        "name": "Syed Affan Daimi"
                    },
                    {
                        "name": "Asma Iqbal"
                    }
                ],
                "author_detail": {
                    "name": "Asma Iqbal"
                },
                "author": "Asma Iqbal",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17572v1",
                "updated": "2024-09-26T06:40:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    40,
                    45,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T06:40:45Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    40,
                    45,
                    3,
                    270,
                    0
                ],
                "title": "Dr. GPT in Campus Counseling: Understanding Higher Education Students'\n  Opinions on LLM-assisted Mental Health Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr. GPT in Campus Counseling: Understanding Higher Education Students'\n  Opinions on LLM-assisted Mental Health Services"
                },
                "summary": "In response to the increasing mental health challenges faced by college\nstudents, we sought to understand their perspectives on how AI applications,\nparticularly Large Language Models (LLMs), can be leveraged to enhance their\nmental well-being. Through pilot interviews with ten diverse students, we\nexplored their opinions on the use of LLMs across five fictional scenarios:\nGeneral Information Inquiry, Initial Screening, Reshaping Patient-Expert\nDynamics, Long-term Care, and Follow-up Care. Our findings revealed that\nstudents' acceptance of LLMs varied by scenario, with participants highlighting\nboth potential benefits, such as proactive engagement and personalized\nfollow-up care, and concerns, including limitations in training data and\nemotional support. These insights inform how AI technology should be designed\nand implemented to effectively support and enhance students' mental well-being,\nparticularly in scenarios where LLMs can complement traditional methods, while\nmaintaining empathy and respecting individual preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the increasing mental health challenges faced by college\nstudents, we sought to understand their perspectives on how AI applications,\nparticularly Large Language Models (LLMs), can be leveraged to enhance their\nmental well-being. Through pilot interviews with ten diverse students, we\nexplored their opinions on the use of LLMs across five fictional scenarios:\nGeneral Information Inquiry, Initial Screening, Reshaping Patient-Expert\nDynamics, Long-term Care, and Follow-up Care. Our findings revealed that\nstudents' acceptance of LLMs varied by scenario, with participants highlighting\nboth potential benefits, such as proactive engagement and personalized\nfollow-up care, and concerns, including limitations in training data and\nemotional support. These insights inform how AI technology should be designed\nand implemented to effectively support and enhance students' mental well-being,\nparticularly in scenarios where LLMs can complement traditional methods, while\nmaintaining empathy and respecting individual preferences."
                },
                "authors": [
                    {
                        "name": "Owen Xingjian Zhang"
                    },
                    {
                        "name": "Shuyao Zhou"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Sunny Xun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sunny Xun Liu"
                },
                "author": "Sunny Xun Liu",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17565v1",
                "updated": "2024-09-26T06:27:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    27,
                    26,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T06:27:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    27,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "Pixel-Space Post-Training of Latent Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixel-Space Post-Training of Latent Diffusion Models"
                },
                "summary": "Latent diffusion models (LDMs) have made significant advancements in the\nfield of image generation in recent years. One major advantage of LDMs is their\nability to operate in a compressed latent space, allowing for more efficient\ntraining and deployment. However, despite these advantages, challenges with\nLDMs still remain. For example, it has been observed that LDMs often generate\nhigh-frequency details and complex compositions imperfectly. We hypothesize\nthat one reason for these flaws is due to the fact that all pre- and\npost-training of LDMs are done in latent space, which is typically $8 \\times 8$\nlower spatial-resolution than the output images. To address this issue, we\npropose adding pixel-space supervision in the post-training process to better\npreserve high-frequency details. Experimentally, we show that adding a\npixel-space objective significantly improves both supervised quality\nfine-tuning and preference-based post-training by a large margin on a\nstate-of-the-art DiT transformer and U-Net diffusion models in both visual\nquality and visual flaw metrics, while maintaining the same text alignment\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent diffusion models (LDMs) have made significant advancements in the\nfield of image generation in recent years. One major advantage of LDMs is their\nability to operate in a compressed latent space, allowing for more efficient\ntraining and deployment. However, despite these advantages, challenges with\nLDMs still remain. For example, it has been observed that LDMs often generate\nhigh-frequency details and complex compositions imperfectly. We hypothesize\nthat one reason for these flaws is due to the fact that all pre- and\npost-training of LDMs are done in latent space, which is typically $8 \\times 8$\nlower spatial-resolution than the output images. To address this issue, we\npropose adding pixel-space supervision in the post-training process to better\npreserve high-frequency details. Experimentally, we show that adding a\npixel-space objective significantly improves both supervised quality\nfine-tuning and preference-based post-training by a large margin on a\nstate-of-the-art DiT transformer and U-Net diffusion models in both visual\nquality and visual flaw metrics, while maintaining the same text alignment\nquality."
                },
                "authors": [
                    {
                        "name": "Christina Zhang"
                    },
                    {
                        "name": "Simran Motwani"
                    },
                    {
                        "name": "Matthew Yu"
                    },
                    {
                        "name": "Ji Hou"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Sam Tsai"
                    },
                    {
                        "name": "Peter Vajda"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Jialiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Wang"
                },
                "author": "Jialiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17564v1",
                "updated": "2024-09-26T06:27:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    27,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T06:27:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    27,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "General Compression Framework for Efficient Transformer Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Compression Framework for Efficient Transformer Object Tracking"
                },
                "summary": "Transformer-based trackers have established a dominant role in the field of\nvisual object tracking. While these trackers exhibit promising performance,\ntheir deployment on resource-constrained devices remains challenging due to\ninefficiencies. To improve the inference efficiency and reduce the computation\ncost, prior approaches have aimed to either design lightweight trackers or\ndistill knowledge from larger teacher models into more compact student\ntrackers. However, these solutions often sacrifice accuracy for speed. Thus, we\npropose a general model compression framework for efficient transformer object\ntracking, named CompressTracker, to reduce the size of a pre-trained tracking\nmodel into a lightweight tracker with minimal performance degradation. Our\napproach features a novel stage division strategy that segments the transformer\nlayers of the teacher model into distinct stages, enabling the student model to\nemulate each corresponding teacher stage more effectively. Additionally, we\nalso design a unique replacement training technique that involves randomly\nsubstituting specific stages in the student model with those from the teacher\nmodel, as opposed to training the student model in isolation. Replacement\ntraining enhances the student model's ability to replicate the teacher model's\nbehavior. To further forcing student model to emulate teacher model, we\nincorporate prediction guidance and stage-wise feature mimicking to provide\nadditional supervision during the teacher model's compression process. Our\nframework CompressTracker is structurally agnostic, making it compatible with\nany transformer architecture. We conduct a series of experiment to verify the\neffectiveness and generalizability of CompressTracker. Our CompressTracker-4\nwith 4 transformer layers, which is compressed from OSTrack, retains about 96%\nperformance on LaSOT (66.1% AUC) while achieves 2.17x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based trackers have established a dominant role in the field of\nvisual object tracking. While these trackers exhibit promising performance,\ntheir deployment on resource-constrained devices remains challenging due to\ninefficiencies. To improve the inference efficiency and reduce the computation\ncost, prior approaches have aimed to either design lightweight trackers or\ndistill knowledge from larger teacher models into more compact student\ntrackers. However, these solutions often sacrifice accuracy for speed. Thus, we\npropose a general model compression framework for efficient transformer object\ntracking, named CompressTracker, to reduce the size of a pre-trained tracking\nmodel into a lightweight tracker with minimal performance degradation. Our\napproach features a novel stage division strategy that segments the transformer\nlayers of the teacher model into distinct stages, enabling the student model to\nemulate each corresponding teacher stage more effectively. Additionally, we\nalso design a unique replacement training technique that involves randomly\nsubstituting specific stages in the student model with those from the teacher\nmodel, as opposed to training the student model in isolation. Replacement\ntraining enhances the student model's ability to replicate the teacher model's\nbehavior. To further forcing student model to emulate teacher model, we\nincorporate prediction guidance and stage-wise feature mimicking to provide\nadditional supervision during the teacher model's compression process. Our\nframework CompressTracker is structurally agnostic, making it compatible with\nany transformer architecture. We conduct a series of experiment to verify the\neffectiveness and generalizability of CompressTracker. Our CompressTracker-4\nwith 4 transformer layers, which is compressed from OSTrack, retains about 96%\nperformance on LaSOT (66.1% AUC) while achieves 2.17x speed up."
                },
                "authors": [
                    {
                        "name": "Lingyi Hong"
                    },
                    {
                        "name": "Jinglun Li"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Pinxue Guo"
                    },
                    {
                        "name": "Kaixun Jiang"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Shuyong Gao"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05013v2",
                "updated": "2024-09-26T06:19:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    19,
                    34,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-07T15:23:53Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    15,
                    23,
                    53,
                    4,
                    159,
                    0
                ],
                "title": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in\n  Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIQ: Contextual History Enhancement for Improving Query Rewriting in\n  Conversational Search"
                },
                "summary": "In this paper, we study how open-source large language models (LLMs) can be\neffectively deployed for improving query rewriting in conversational search,\nespecially for ambiguous queries. We introduce CHIQ, a two-step method that\nleverages the capabilities of LLMs to resolve ambiguities in the conversation\nhistory before query rewriting. This approach contrasts with prior studies that\npredominantly use closed-source LLMs to directly generate search queries from\nconversation history. We demonstrate on five well-established benchmarks that\nCHIQ leads to state-of-the-art results across most settings, showing highly\ncompetitive performances with systems leveraging closed-source LLMs. Our study\nprovides a first step towards leveraging open-source LLMs in conversational\nsearch, as a competitive alternative to the prevailing reliance on commercial\nLLMs. Data, models, and source code will be publicly available upon acceptance\nat https://github.com/fengranMark/CHIQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study how open-source large language models (LLMs) can be\neffectively deployed for improving query rewriting in conversational search,\nespecially for ambiguous queries. We introduce CHIQ, a two-step method that\nleverages the capabilities of LLMs to resolve ambiguities in the conversation\nhistory before query rewriting. This approach contrasts with prior studies that\npredominantly use closed-source LLMs to directly generate search queries from\nconversation history. We demonstrate on five well-established benchmarks that\nCHIQ leads to state-of-the-art results across most settings, showing highly\ncompetitive performances with systems leveraging closed-source LLMs. Our study\nprovides a first step towards leveraging open-source LLMs in conversational\nsearch, as a competitive alternative to the prevailing reliance on commercial\nLLMs. Data, models, and source code will be publicly available upon acceptance\nat https://github.com/fengranMark/CHIQ."
                },
                "authors": [
                    {
                        "name": "Fengran Mo"
                    },
                    {
                        "name": "Abbas Ghaddar"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Yun Nie"
                },
                "author": "Jian-Yun Nie",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17255v2",
                "updated": "2024-09-26T06:18:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    18,
                    44,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-25T03:45:28Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    3,
                    45,
                    28,
                    1,
                    177,
                    0
                ],
                "title": "MPCODER: Multi-user Personalized Code Generator with Explicit and\n  Implicit Style Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCODER: Multi-user Personalized Code Generator with Explicit and\n  Implicit Style Representation Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated great potential for assisting\ndevelopers in their daily development. However, most research focuses on\ngenerating correct code, how to use LLMs to generate personalized code has\nseldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user\nPersonalized Code Generator) to generate personalized code for multiple users.\nTo better learn coding style features, we utilize explicit coding style\nresidual learning to capture the syntax code style standards and implicit style\nlearning to capture the semantic code style conventions. We train a multi-user\nstyle adapter to better differentiate the implicit feature representations of\ndifferent users through contrastive learning, ultimately enabling personalized\ncode generation for multiple users. We further propose a novel evaluation\nmetric for estimating similarities between codes of different coding styles.\nThe experimental results show the effectiveness of our approach for this novel\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated great potential for assisting\ndevelopers in their daily development. However, most research focuses on\ngenerating correct code, how to use LLMs to generate personalized code has\nseldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user\nPersonalized Code Generator) to generate personalized code for multiple users.\nTo better learn coding style features, we utilize explicit coding style\nresidual learning to capture the syntax code style standards and implicit style\nlearning to capture the semantic code style conventions. We train a multi-user\nstyle adapter to better differentiate the implicit feature representations of\ndifferent users through contrastive learning, ultimately enabling personalized\ncode generation for multiple users. We further propose a novel evaluation\nmetric for estimating similarities between codes of different coding styles.\nThe experimental results show the effectiveness of our approach for this novel\ntask."
                },
                "authors": [
                    {
                        "name": "Zhenlong Dai"
                    },
                    {
                        "name": "Chang Yao"
                    },
                    {
                        "name": "WenKang Han"
                    },
                    {
                        "name": "Ying Yuan"
                    },
                    {
                        "name": "Zhipeng Gao"
                    },
                    {
                        "name": "Jingyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Chen"
                },
                "author": "Jingyuan Chen",
                "arxiv_comment": "Accepted by ACL 2024, Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17561v1",
                "updated": "2024-09-26T06:18:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    18,
                    6,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T06:18:06Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    18,
                    6,
                    3,
                    270,
                    0
                ],
                "title": "TestBench: Evaluating Class-Level Test Case Generation Capability of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestBench: Evaluating Class-Level Test Case Generation Capability of\n  Large Language Models"
                },
                "summary": "Software testing is a crucial phase in the software life cycle, helping\nidentify potential risks and reduce maintenance costs. With the advancement of\nLarge Language Models (LLMs), researchers have proposed an increasing number of\nLLM-based software testing techniques, particularly in the area of test case\ngeneration. Despite the growing interest, limited efforts have been made to\nthoroughly evaluate the actual capabilities of LLMs in this task.\n  In this paper, we introduce TestBench, a benchmark for class-level LLM-based\ntest case generation. We construct a dataset of 108 Java programs from 9\nreal-world, large-scale projects on GitHub, each representing a different\nthematic domain. We then design three distinct types of prompts based on\ncontext descriptions, including self-contained context, full context, and\nsimple context. Besides, we propose a fine-grained evaluation framework that\nconsiders five aspects of test cases: syntactic correctness, compilation\ncorrectness, test correctness, code coverage rate, and defect detection rate.\nFurthermore, we propose a heuristic algorithm to repair erroneous test cases\ngenerated by LLMs. We evaluate CodeLlama-13b, GPT-3.5, and GPT-4 on the\nTestBench, and our experimental results indicate that larger models demonstrate\na greater ability to effectively utilize contextual information, thus\ngenerating higher-quality test cases. Smaller models may struggle with the\nnoise introduced by the extensive information contained within the full\ncontext. However, when using the simplified version, namely the simple context,\nwhich is derived from the full context via abstract syntax tree analysis, the\nperformance of these models improves significantly. Our analysis highlights the\ncurrent progress and pinpoints future directions to further enhance the\neffectiveness of models by handling contextual information for test case\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software testing is a crucial phase in the software life cycle, helping\nidentify potential risks and reduce maintenance costs. With the advancement of\nLarge Language Models (LLMs), researchers have proposed an increasing number of\nLLM-based software testing techniques, particularly in the area of test case\ngeneration. Despite the growing interest, limited efforts have been made to\nthoroughly evaluate the actual capabilities of LLMs in this task.\n  In this paper, we introduce TestBench, a benchmark for class-level LLM-based\ntest case generation. We construct a dataset of 108 Java programs from 9\nreal-world, large-scale projects on GitHub, each representing a different\nthematic domain. We then design three distinct types of prompts based on\ncontext descriptions, including self-contained context, full context, and\nsimple context. Besides, we propose a fine-grained evaluation framework that\nconsiders five aspects of test cases: syntactic correctness, compilation\ncorrectness, test correctness, code coverage rate, and defect detection rate.\nFurthermore, we propose a heuristic algorithm to repair erroneous test cases\ngenerated by LLMs. We evaluate CodeLlama-13b, GPT-3.5, and GPT-4 on the\nTestBench, and our experimental results indicate that larger models demonstrate\na greater ability to effectively utilize contextual information, thus\ngenerating higher-quality test cases. Smaller models may struggle with the\nnoise introduced by the extensive information contained within the full\ncontext. However, when using the simplified version, namely the simple context,\nwhich is derived from the full context via abstract syntax tree analysis, the\nperformance of these models improves significantly. Our analysis highlights the\ncurrent progress and pinpoints future directions to further enhance the\neffectiveness of models by handling contextual information for test case\ngeneration."
                },
                "authors": [
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Ye Shang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16997v2",
                "updated": "2024-09-26T06:13:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    13,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T15:02:25Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    2,
                    25,
                    2,
                    269,
                    0
                ],
                "title": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization"
                },
                "summary": "As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format."
                },
                "authors": [
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Zhiying Wu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Peizhuang Cong"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15763v2",
                "updated": "2024-09-26T05:43:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    5,
                    43,
                    8,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-24T05:39:53Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    39,
                    53,
                    1,
                    268,
                    0
                ],
                "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through\n  Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through\n  Semantic Comprehension in Retrieval-Augmented Generation Scenarios"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC_Benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC_Benchmark"
                },
                "authors": [
                    {
                        "name": "Hai Lin"
                    },
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Junyou Su"
                    },
                    {
                        "name": "Haitao Zheng"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17539v1",
                "updated": "2024-09-26T04:59:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    59,
                    45,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T04:59:45Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    59,
                    45,
                    3,
                    270,
                    0
                ],
                "title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but their performance in complex logical reasoning tasks remains\nunsatisfactory. Although some prompting methods, such as Chain-of-Thought, can\nimprove the reasoning ability of LLMs to some extent, they suffer from an\nunfaithful issue where derived conclusions may not align with the generated\nreasoning chain. To address this issue, some studies employ the approach of\npropositional logic to further enhance logical reasoning abilities of LLMs.\nHowever, the potential omissions in the extraction of logical expressions in\nthese methods can cause information loss in the logical reasoning process,\nthereby generating incorrect results. To this end, we propose Logic-of-Thought\n(LoT) prompting which employs propositional logic to generate expanded logical\ninformation from input context, and utilizes the generated logical information\nas an additional augmentation to the input prompts, thereby enhancing the\ncapability of logical reasoning. The LoT is orthogonal to existing prompting\nmethods and can be seamlessly integrated with them. Extensive experiments\ndemonstrate that LoT boosts the performance of various prompting methods with a\nstriking margin across five logical reasoning tasks. In particular, the LoT\nenhances Chain-of-Thought's performance on the ReClor dataset by +4.35%;\nmoreover, it improves Chain-of-Thought with Self-Consistency's performance on\nLogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on\nProofWriter dataset by +8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but their performance in complex logical reasoning tasks remains\nunsatisfactory. Although some prompting methods, such as Chain-of-Thought, can\nimprove the reasoning ability of LLMs to some extent, they suffer from an\nunfaithful issue where derived conclusions may not align with the generated\nreasoning chain. To address this issue, some studies employ the approach of\npropositional logic to further enhance logical reasoning abilities of LLMs.\nHowever, the potential omissions in the extraction of logical expressions in\nthese methods can cause information loss in the logical reasoning process,\nthereby generating incorrect results. To this end, we propose Logic-of-Thought\n(LoT) prompting which employs propositional logic to generate expanded logical\ninformation from input context, and utilizes the generated logical information\nas an additional augmentation to the input prompts, thereby enhancing the\ncapability of logical reasoning. The LoT is orthogonal to existing prompting\nmethods and can be seamlessly integrated with them. Extensive experiments\ndemonstrate that LoT boosts the performance of various prompting methods with a\nstriking margin across five logical reasoning tasks. In particular, the LoT\nenhances Chain-of-Thought's performance on the ReClor dataset by +4.35%;\nmoreover, it improves Chain-of-Thought with Self-Consistency's performance on\nLogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on\nProofWriter dataset by +8%."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Wenjiang Xu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Xingyu Wang"
                    },
                    {
                        "name": "Jiaxing Wang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17527v1",
                "updated": "2024-09-26T04:30:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    30,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T04:30:32Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    30,
                    32,
                    3,
                    270,
                    0
                ],
                "title": "Data Proportion Detection for Optimized Data Management for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Proportion Detection for Optimized Data Management for Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na wide range of tasks and domains, with data preparation playing a critical\nrole in achieving these results. Pre-training data typically combines\ninformation from multiple domains. To maximize performance when integrating\ndata from various domains, determining the optimal data proportion is\nessential. However, state-of-the-art (SOTA) LLMs rarely disclose details about\ntheir pre-training data, making it difficult for researchers to identify ideal\ndata proportions. In this paper, we introduce a new topic, \\textit{data\nproportion detection}, which enables the automatic estimation of pre-training\ndata proportions by analyzing the generated outputs of LLMs. We provide\nrigorous theoretical proofs, practical algorithms, and preliminary experimental\nresults for data proportion detection. Based on these findings, we offer\nvaluable insights into the challenges and future directions for effective data\nproportion detection and data management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\na wide range of tasks and domains, with data preparation playing a critical\nrole in achieving these results. Pre-training data typically combines\ninformation from multiple domains. To maximize performance when integrating\ndata from various domains, determining the optimal data proportion is\nessential. However, state-of-the-art (SOTA) LLMs rarely disclose details about\ntheir pre-training data, making it difficult for researchers to identify ideal\ndata proportions. In this paper, we introduce a new topic, \\textit{data\nproportion detection}, which enables the automatic estimation of pre-training\ndata proportions by analyzing the generated outputs of LLMs. We provide\nrigorous theoretical proofs, practical algorithms, and preliminary experimental\nresults for data proportion detection. Based on these findings, we offer\nvaluable insights into the challenges and future directions for effective data\nproportion detection and data management."
                },
                "authors": [
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Keshi Zhao"
                    },
                    {
                        "name": "Yajie Yang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19234v2",
                "updated": "2024-09-26T04:22:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    22,
                    18,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-27T14:58:38Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    58,
                    38,
                    3,
                    179,
                    0
                ],
                "title": "Generating Is Believing: Membership Inference Attacks against\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Is Believing: Membership Inference Attacks against\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nmitigates issues such as hallucinations and knowledge staleness in Large\nLanguage Models (LLMs) by retrieving relevant knowledge from an external\ndatabase to assist in content generation. Existing research has demonstrated\npotential privacy risks associated with the LLMs of RAG. However, the privacy\nrisks posed by the integration of an external database, which often contains\nsensitive data such as medical records or personal identities, have remained\nlargely unexplored. In this paper, we aim to bridge this gap by focusing on\nmembership privacy of RAG's external database, with the aim of determining\nwhether a given sample is part of the RAG's database. Our basic idea is that if\na sample is in the external database, it will exhibit a high degree of semantic\nsimilarity to the text generated by the RAG system. We present S$^2$MIA, a\n\\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes\nthe \\underline{S}emantic \\underline{S}imilarity between a given sample and the\ncontent generated by the RAG system. With our proposed S$^2$MIA, we demonstrate\nthe potential to breach the membership privacy of the RAG database. Extensive\nexperiment results demonstrate that S$^2$MIA can achieve a strong inference\nperformance compared with five existing MIAs, and is able to escape from the\nprotection of three representative defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nmitigates issues such as hallucinations and knowledge staleness in Large\nLanguage Models (LLMs) by retrieving relevant knowledge from an external\ndatabase to assist in content generation. Existing research has demonstrated\npotential privacy risks associated with the LLMs of RAG. However, the privacy\nrisks posed by the integration of an external database, which often contains\nsensitive data such as medical records or personal identities, have remained\nlargely unexplored. In this paper, we aim to bridge this gap by focusing on\nmembership privacy of RAG's external database, with the aim of determining\nwhether a given sample is part of the RAG's database. Our basic idea is that if\na sample is in the external database, it will exhibit a high degree of semantic\nsimilarity to the text generated by the RAG system. We present S$^2$MIA, a\n\\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes\nthe \\underline{S}emantic \\underline{S}imilarity between a given sample and the\ncontent generated by the RAG system. With our proposed S$^2$MIA, we demonstrate\nthe potential to breach the membership privacy of the RAG database. Extensive\nexperiment results demonstrate that S$^2$MIA can achieve a strong inference\nperformance compared with five existing MIAs, and is able to escape from the\nprotection of three representative defenses."
                },
                "authors": [
                    {
                        "name": "Yuying Li"
                    },
                    {
                        "name": "Gaoyang Liu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17518v1",
                "updated": "2024-09-26T04:01:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    1,
                    15,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T04:01:15Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    1,
                    15,
                    3,
                    270,
                    0
                ],
                "title": "Multi-Designated Detector Watermarking for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Designated Detector Watermarking for Language Models"
                },
                "summary": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we initiate the study of \\emph{multi-designated detector\nwatermarking (MDDW)} for large language models (LLMs). This technique allows\nmodel providers to generate watermarked outputs from LLMs with two key\nproperties: (i) only specific, possibly multiple, designated detectors can\nidentify the watermarks, and (ii) there is no perceptible degradation in the\noutput quality for ordinary users. We formalize the security definitions for\nMDDW and present a framework for constructing MDDW for any LLM using\nmulti-designated verifier signatures (MDVS). Recognizing the significant\neconomic value of LLM outputs, we introduce claimability as an optional\nsecurity feature for MDDW, enabling model providers to assert ownership of LLM\noutputs within designated-detector settings. To support claimable MDDW, we\npropose a generic transformation converting any MDVS to a claimable MDVS. Our\nimplementation of the MDDW scheme highlights its advanced functionalities and\nflexibility over existing methods, with satisfactory performance metrics."
                },
                "authors": [
                    {
                        "name": "Zhengan Huang"
                    },
                    {
                        "name": "Gongxian Zeng"
                    },
                    {
                        "name": "Xin Mu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yue Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Yu"
                },
                "author": "Yue Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17515v1",
                "updated": "2024-09-26T03:50:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    50,
                    22,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T03:50:22Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    50,
                    22,
                    3,
                    270,
                    0
                ],
                "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From News to Forecast: Integrating Event Analysis in LLM-Based Time\n  Series Forecasting with Reflection"
                },
                "summary": "This paper introduces a novel approach to enhance time series forecasting\nusing Large Language Models (LLMs) and Generative Agents. With language as a\nmedium, our method adaptively integrates various social events into forecasting\nmodels, aligning news content with time series fluctuations for enriched\ninsights. Specifically, we utilize LLM-based agents to iteratively filter out\nirrelevant news and employ human-like reasoning and reflection to evaluate\npredictions. This enables our model to analyze complex events, such as\nunexpected incidents and shifts in social behavior, and continuously refine the\nselection logic of news and the robustness of the agent's output. By compiling\nselected news with time series data, we fine-tune the LLaMa2 pre-trained model.\nThe results demonstrate significant improvements in forecasting accuracy and\nsuggest a potential paradigm shift in time series forecasting by effectively\nharnessing unstructured news data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to enhance time series forecasting\nusing Large Language Models (LLMs) and Generative Agents. With language as a\nmedium, our method adaptively integrates various social events into forecasting\nmodels, aligning news content with time series fluctuations for enriched\ninsights. Specifically, we utilize LLM-based agents to iteratively filter out\nirrelevant news and employ human-like reasoning and reflection to evaluate\npredictions. This enables our model to analyze complex events, such as\nunexpected incidents and shifts in social behavior, and continuously refine the\nselection logic of news and the robustness of the agent's output. By compiling\nselected news with time series data, we fine-tune the LLaMa2 pre-trained model.\nThe results demonstrate significant improvements in forecasting accuracy and\nsuggest a potential paradigm shift in time series forecasting by effectively\nharnessing unstructured news data."
                },
                "authors": [
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Maike Feng"
                    },
                    {
                        "name": "Jing Qiu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "arxiv_comment": "This paper has been accepted for NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10743v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10743v4",
                "updated": "2024-09-26T03:38:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    38,
                    59,
                    3,
                    270,
                    0
                ],
                "published": "2023-12-17T15:28:06Z",
                "published_parsed": [
                    2023,
                    12,
                    17,
                    15,
                    28,
                    6,
                    6,
                    351,
                    0
                ],
                "title": "A Unified Framework for Multi-Domain CTR Prediction via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Multi-Domain CTR Prediction via Large Language\n  Models"
                },
                "summary": "Click-Through Rate (CTR) prediction is a crucial task in online\nrecommendation platforms as it involves estimating the probability of user\nengagement with advertisements or items by clicking on them. Given the\navailability of various services like online shopping, ride-sharing, food\ndelivery, and professional services on commercial platforms, recommendation\nsystems in these platforms are required to make CTR predictions across multiple\ndomains rather than just a single domain. However, multi-domain click-through\nrate (MDCTR) prediction remains a challenging task in online recommendation due\nto the complex mutual influence between domains. Traditional MDCTR models\ntypically encode domains as discrete identifiers, ignoring rich semantic\ninformation underlying. Consequently, they can hardly generalize to new\ndomains. Besides, existing models can be easily dominated by some specific\ndomains, which results in significant performance drops in the other domains\n(i.e. the \"seesaw phenomenon\"). In this paper, we propose a novel solution\nUni-CTR to address the above challenges. Uni-CTR leverages a backbone Large\nLanguage Model (LLM) to learn layer-wise semantic representations that capture\ncommonalities between domains. Uni-CTR also uses several domain-specific\nnetworks to capture the characteristics of each domain. Note that we design a\nmasked loss strategy so that these domain-specific networks are decoupled from\nbackbone LLM. This allows domain-specific networks to remain unchanged when\nincorporating new or removing domains, thereby enhancing the flexibility and\nscalability of the system significantly. Experimental results on three public\ndatasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models\nsignificantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in\nzero-shot prediction. We have applied Uni-CTR in industrial scenarios,\nconfirming its efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction is a crucial task in online\nrecommendation platforms as it involves estimating the probability of user\nengagement with advertisements or items by clicking on them. Given the\navailability of various services like online shopping, ride-sharing, food\ndelivery, and professional services on commercial platforms, recommendation\nsystems in these platforms are required to make CTR predictions across multiple\ndomains rather than just a single domain. However, multi-domain click-through\nrate (MDCTR) prediction remains a challenging task in online recommendation due\nto the complex mutual influence between domains. Traditional MDCTR models\ntypically encode domains as discrete identifiers, ignoring rich semantic\ninformation underlying. Consequently, they can hardly generalize to new\ndomains. Besides, existing models can be easily dominated by some specific\ndomains, which results in significant performance drops in the other domains\n(i.e. the \"seesaw phenomenon\"). In this paper, we propose a novel solution\nUni-CTR to address the above challenges. Uni-CTR leverages a backbone Large\nLanguage Model (LLM) to learn layer-wise semantic representations that capture\ncommonalities between domains. Uni-CTR also uses several domain-specific\nnetworks to capture the characteristics of each domain. Note that we design a\nmasked loss strategy so that these domain-specific networks are decoupled from\nbackbone LLM. This allows domain-specific networks to remain unchanged when\nincorporating new or removing domains, thereby enhancing the flexibility and\nscalability of the system significantly. Experimental results on three public\ndatasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models\nsignificantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in\nzero-shot prediction. We have applied Uni-CTR in industrial scenarios,\nconfirming its efficiency."
                },
                "authors": [
                    {
                        "name": "Zichuan Fu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Chuhan Wu"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Kuicai Dong"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Mengchen Zhao"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "Accept By ACM TRANSACTIONS ON INFORMATION SYSTEMS(TOIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10743v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10743v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17508v1",
                "updated": "2024-09-26T03:33:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    33,
                    26,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T03:33:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    33,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task\n  Learning Via Connector-MoE"
                },
                "summary": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization, recent advances primarily focus on improving the LLM\ncomponents, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector. Extensive ablation experiments\nvalidate the effectiveness of introducing CMoE under any configuration, with up\nto an average 8% performance gains. We further provide interpretation analysis\nof the tug-of-war problem from the perspective of gradient optimization and\nparameter statistics. Compared to previous state-of-the-art medical MLLMs,\nUni-Med achieves competitive or superior evaluation metrics on diverse tasks.\nCode, data and model will be soon available at GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have shown impressive capabilities\nas a general-purpose interface for various visual and linguistic tasks.\nHowever, building a unified MLLM for multi-task learning in the medical field\nremains a thorny challenge. To mitigate the tug-of-war problem of multi-modal\nmulti-task optimization, recent advances primarily focus on improving the LLM\ncomponents, while neglecting the connector that bridges the gap between\nmodalities. In this paper, we introduce Uni-Med, a novel medical generalist\nfoundation model which consists of a universal visual feature extraction\nmodule, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting\nfrom the proposed CMoE that leverages a well-designed router with a mixture of\nprojection experts at the connector, Uni-Med achieves efficient solution to the\ntug-of-war problem and can perform six different medical tasks including\nquestion answering, visual question answering, report generation, referring\nexpression comprehension, referring expression generation and image\nclassification. To the best of our knowledge, Uni-Med is the first effort to\ntackle multi-task interference at the connector. Extensive ablation experiments\nvalidate the effectiveness of introducing CMoE under any configuration, with up\nto an average 8% performance gains. We further provide interpretation analysis\nof the tug-of-war problem from the perspective of gradient optimization and\nparameter statistics. Compared to previous state-of-the-art medical MLLMs,\nUni-Med achieves competitive or superior evaluation metrics on diverse tasks.\nCode, data and model will be soon available at GitHub."
                },
                "authors": [
                    {
                        "name": "Xun Zhu"
                    },
                    {
                        "name": "Ying Hu"
                    },
                    {
                        "name": "Fanbin Mo"
                    },
                    {
                        "name": "Miao Li"
                    },
                    {
                        "name": "Ji Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wu"
                },
                "author": "Ji Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17504v1",
                "updated": "2024-09-26T03:22:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    22,
                    9,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T03:22:09Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    22,
                    9,
                    3,
                    270,
                    0
                ],
                "title": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination\n  Detection"
                },
                "summary": "The surge in applications of large language models (LLMs) has prompted\nconcerns about the generation of misleading or fabricated information, known as\nhallucinations. Therefore, detecting hallucinations has become critical to\nmaintaining trust in LLM-generated content. A primary challenge in learning a\ntruthfulness classifier is the lack of a large amount of labeled truthful and\nhallucinated data. To address the challenge, we introduce HaloScope, a novel\nlearning framework that leverages the unlabeled LLM generations in the wild for\nhallucination detection. Such unlabeled data arises freely upon deploying LLMs\nin the open world, and consists of both truthful and hallucinated information.\nTo harness the unlabeled data, we present an automated membership estimation\nscore for distinguishing between truthful and untruthful generations within\nunlabeled mixture data, thereby enabling the training of a binary truthfulness\nclassifier on top. Importantly, our framework does not require extra data\ncollection and human annotations, offering strong flexibility and practicality\nfor real-world applications. Extensive experiments show that HaloScope can\nachieve superior hallucination detection performance, outperforming the\ncompetitive rivals by a significant margin. Code is available at\nhttps://github.com/deeplearningwisc/haloscope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in applications of large language models (LLMs) has prompted\nconcerns about the generation of misleading or fabricated information, known as\nhallucinations. Therefore, detecting hallucinations has become critical to\nmaintaining trust in LLM-generated content. A primary challenge in learning a\ntruthfulness classifier is the lack of a large amount of labeled truthful and\nhallucinated data. To address the challenge, we introduce HaloScope, a novel\nlearning framework that leverages the unlabeled LLM generations in the wild for\nhallucination detection. Such unlabeled data arises freely upon deploying LLMs\nin the open world, and consists of both truthful and hallucinated information.\nTo harness the unlabeled data, we present an automated membership estimation\nscore for distinguishing between truthful and untruthful generations within\nunlabeled mixture data, thereby enabling the training of a binary truthfulness\nclassifier on top. Importantly, our framework does not require extra data\ncollection and human annotations, offering strong flexibility and practicality\nfor real-world applications. Extensive experiments show that HaloScope can\nachieve superior hallucination detection performance, outperforming the\ncompetitive rivals by a significant margin. Code is available at\nhttps://github.com/deeplearningwisc/haloscope."
                },
                "authors": [
                    {
                        "name": "Xuefeng Du"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "arxiv_comment": "NeurIPS 2024 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10669v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10669v5",
                "updated": "2024-09-26T03:16:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    16,
                    52,
                    3,
                    270,
                    0
                ],
                "published": "2024-02-16T13:21:06Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    13,
                    21,
                    6,
                    4,
                    47,
                    0
                ],
                "title": "Humans or LLMs as the Judge? A Study on Judgement Biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans or LLMs as the Judge? A Study on Judgement Biases"
                },
                "summary": "Adopting human and large language models (LLM) as judges (a.k.a human- and\nLLM-as-a-judge) for evaluating the performance of LLMs has recently gained\nattention. Nonetheless, this approach concurrently introduces potential biases\nfrom human and LLMs, questioning the reliability of the evaluation results. In\nthis paper, we propose a novel framework that is free from referencing\ngroundtruth annotations for investigating Misinformation Oversight Bias, Gender\nBias, Authority Bias and Beauty Bias on LLM and human judges. We curate a\ndataset referring to the revised Bloom's Taxonomy and conduct thousands of\nevaluations. Results show that human and LLM judges are vulnerable to\nperturbations to various degrees, and that even the cutting-edge judges possess\nconsiderable biases. We further exploit these biases to conduct attacks on LLM\njudges. We hope that our work can notify the community of the bias and\nvulnerability of human- and LLM-as-a-judge, as well as the urgency of\ndeveloping robust evaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting human and large language models (LLM) as judges (a.k.a human- and\nLLM-as-a-judge) for evaluating the performance of LLMs has recently gained\nattention. Nonetheless, this approach concurrently introduces potential biases\nfrom human and LLMs, questioning the reliability of the evaluation results. In\nthis paper, we propose a novel framework that is free from referencing\ngroundtruth annotations for investigating Misinformation Oversight Bias, Gender\nBias, Authority Bias and Beauty Bias on LLM and human judges. We curate a\ndataset referring to the revised Bloom's Taxonomy and conduct thousands of\nevaluations. Results show that human and LLM judges are vulnerable to\nperturbations to various degrees, and that even the cutting-edge judges possess\nconsiderable biases. We further exploit these biases to conduct attacks on LLM\njudges. We hope that our work can notify the community of the bias and\nvulnerability of human- and LLM-as-a-judge, as well as the urgency of\ndeveloping robust evaluation systems."
                },
                "authors": [
                    {
                        "name": "Guiming Hardy Chen"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Ziche Liu"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10669v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10669v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14509v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14509v3",
                "updated": "2024-09-26T03:15:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    15,
                    53,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-22T16:13:00Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    13,
                    0,
                    6,
                    266,
                    0
                ],
                "title": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits"
                },
                "summary": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human-written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM-generated text, formalizing it into a seven-category\ntaxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nwe explored automatic editing methods to improve LLM-generated text. A\nlarge-scale preference annotation confirms that although experts largely prefer\ntext edited by other experts, automatic editing methods show promise in\nimproving alignment between LLM-generated and human-written text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human-written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM-generated text, formalizing it into a seven-category\ntaxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nwe explored automatic editing methods to improve LLM-generated text. A\nlarge-scale preference annotation confirms that although experts largely prefer\ntext edited by other experts, automatic editing methods show promise in\nimproving alignment between LLM-generated and human-written text."
                },
                "authors": [
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "NLP+HCI, Behavioral Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14509v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14509v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17501v1",
                "updated": "2024-09-26T03:13:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    13,
                    29,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T03:13:29Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    13,
                    29,
                    3,
                    270,
                    0
                ],
                "title": "Development of a novel bunch oscillation recorder with RFSoC technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a novel bunch oscillation recorder with RFSoC technology"
                },
                "summary": "The SuperKEKB accelerator is designed to achieve unprecedented luminosity\nlevels, but this goal is currently hindered by Sudden Beam Loss (SBL) events.\nThese events not only obstruct luminosity improvement but also pose a\nsignificant risk to accelerator components, the Belle II detectors, and the\nsuperconducting focusing system, potentially leading to severe damage and\nquenching of the superconducting system. To address this critical challenge, we\nhave developed a novel Bunch Oscillation Recorder (BOR) based on RFSoC\ntechnology. The BOR has demonstrated high precision with a position resolution\nof 0.03 mm, making it a powerful tool for real-time beam monitoring. In its\ninitial deployment, the BOR successfully recorded multiple SBL events,\nproviding valuable data for further analysis. By strategically positioning BORs\nat the suspected points of SBL origin, we aim to directly identify sources of\nbeam instability. We anticipate that this portable, high-speed BOR monitor will\nplay a crucial role in resolving the SBL issue, ultimately helping achieve\nSuperKEKB's luminosity targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SuperKEKB accelerator is designed to achieve unprecedented luminosity\nlevels, but this goal is currently hindered by Sudden Beam Loss (SBL) events.\nThese events not only obstruct luminosity improvement but also pose a\nsignificant risk to accelerator components, the Belle II detectors, and the\nsuperconducting focusing system, potentially leading to severe damage and\nquenching of the superconducting system. To address this critical challenge, we\nhave developed a novel Bunch Oscillation Recorder (BOR) based on RFSoC\ntechnology. The BOR has demonstrated high precision with a position resolution\nof 0.03 mm, making it a powerful tool for real-time beam monitoring. In its\ninitial deployment, the BOR successfully recorded multiple SBL events,\nproviding valuable data for further analysis. By strategically positioning BORs\nat the suspected points of SBL origin, we aim to directly identify sources of\nbeam instability. We anticipate that this portable, high-speed BOR monitor will\nplay a crucial role in resolving the SBL issue, ultimately helping achieve\nSuperKEKB's luminosity targets."
                },
                "authors": [
                    {
                        "name": "Riku Nomaru"
                    },
                    {
                        "name": "Gaku Mitsuka"
                    },
                    {
                        "name": "Larry Ruckman"
                    },
                    {
                        "name": "Ryan Herbst"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Herbst"
                },
                "author": "Ryan Herbst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17496v1",
                "updated": "2024-09-26T03:11:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    11,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T03:11:32Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    11,
                    32,
                    3,
                    270,
                    0
                ],
                "title": "Towards Forever Access for Implanted Brain-Computer Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Forever Access for Implanted Brain-Computer Interfaces"
                },
                "summary": "Designs for implanted brain-computer interfaces (BCIs) have increased\nsignificantly in recent years. Each device promises better clinical outcomes\nand quality-of-life improvements, yet due to severe and inflexible safety\nconstraints, progress requires tight co-design from materials to circuits and\nall the way up the stack to applications and algorithms. This trend has become\nmore aggressive over time, forcing clinicians and patients to rely on\nvendor-specific hardware and software for deployment, maintenance, upgrades,\nand replacement. This over-reliance is ethically problematic, especially if\ncompanies go out-of-business or business objectives diverge from clinical\npromises. Device heterogeneity additionally burdens clinicians and healthcare\nfacilities, adding complexity and costs for in-clinic visits, monitoring, and\ncontinuous access.\n  Reliability, interoperability, portability, and future-proofed design is\nneeded, but this unfortunately comes at a cost. These system features sap\nresources that would have otherwise been allocated to reduce power/energy and\nimprove performance. Navigating this trade-off in a systematic way is critical\nto providing patients with forever access to their implants and reducing\nburdens placed on healthcare providers and caretakers. We study the integration\nof on-device storage to highlight the sensitivity of this trade-off and\nestablish other points of interest within BCI design that require careful\ninvestigation. In the process, we revisit relevant problems in computer\narchitecture and medical devices from the current era of hardware\nspecialization and modern neurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designs for implanted brain-computer interfaces (BCIs) have increased\nsignificantly in recent years. Each device promises better clinical outcomes\nand quality-of-life improvements, yet due to severe and inflexible safety\nconstraints, progress requires tight co-design from materials to circuits and\nall the way up the stack to applications and algorithms. This trend has become\nmore aggressive over time, forcing clinicians and patients to rely on\nvendor-specific hardware and software for deployment, maintenance, upgrades,\nand replacement. This over-reliance is ethically problematic, especially if\ncompanies go out-of-business or business objectives diverge from clinical\npromises. Device heterogeneity additionally burdens clinicians and healthcare\nfacilities, adding complexity and costs for in-clinic visits, monitoring, and\ncontinuous access.\n  Reliability, interoperability, portability, and future-proofed design is\nneeded, but this unfortunately comes at a cost. These system features sap\nresources that would have otherwise been allocated to reduce power/energy and\nimprove performance. Navigating this trade-off in a systematic way is critical\nto providing patients with forever access to their implants and reducing\nburdens placed on healthcare providers and caretakers. We study the integration\nof on-device storage to highlight the sensitivity of this trade-off and\nestablish other points of interest within BCI design that require careful\ninvestigation. In the process, we revisit relevant problems in computer\narchitecture and medical devices from the current era of hardware\nspecialization and modern neurotechnology."
                },
                "authors": [
                    {
                        "name": "Muhammed Ugur"
                    },
                    {
                        "name": "Raghavendra Pradyumna Pothukuchi"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Bhattacharjee"
                },
                "author": "Abhishek Bhattacharjee",
                "arxiv_journal_ref": "The 1st Workshop on Hot Topics in Ethical Computer Systems, April,\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17495v1",
                "updated": "2024-09-26T03:07:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    7,
                    32,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T03:07:32Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    7,
                    32,
                    3,
                    270,
                    0
                ],
                "title": "Human Mobility Modeling with Limited Information via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Mobility Modeling with Limited Information via Large Language\n  Models"
                },
                "summary": "Understanding human mobility patterns has traditionally been a complex\nchallenge in transportation modeling. Due to the difficulties in obtaining\nhigh-quality training datasets across diverse locations, conventional\nactivity-based models and learning-based human mobility modeling algorithms are\nparticularly limited by the availability and quality of datasets. Furthermore,\ncurrent research mainly focuses on the spatial-temporal travel pattern but\nlacks an understanding of the semantic information between activities, which is\ncrucial for modeling the interdependence between activities. In this paper, we\npropose an innovative Large Language Model (LLM) empowered human mobility\nmodeling framework. Our proposed approach significantly reduces the reliance on\ndetailed human mobility statistical data, utilizing basic socio-demographic\ninformation of individuals to generate their daily mobility patterns. We have\nvalidated our results using the NHTS and SCAG-ABM datasets, demonstrating the\neffective modeling of mobility patterns and the strong adaptability of our\nframework across various geographic locations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human mobility patterns has traditionally been a complex\nchallenge in transportation modeling. Due to the difficulties in obtaining\nhigh-quality training datasets across diverse locations, conventional\nactivity-based models and learning-based human mobility modeling algorithms are\nparticularly limited by the availability and quality of datasets. Furthermore,\ncurrent research mainly focuses on the spatial-temporal travel pattern but\nlacks an understanding of the semantic information between activities, which is\ncrucial for modeling the interdependence between activities. In this paper, we\npropose an innovative Large Language Model (LLM) empowered human mobility\nmodeling framework. Our proposed approach significantly reduces the reliance on\ndetailed human mobility statistical data, utilizing basic socio-demographic\ninformation of individuals to generate their daily mobility patterns. We have\nvalidated our results using the NHTS and SCAG-ABM datasets, demonstrating the\neffective modeling of mobility patterns and the strong adaptability of our\nframework across various geographic locations."
                },
                "authors": [
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Xishun Liao"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Brian Yueshuai He"
                    },
                    {
                        "name": "Chris Stanford"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17481v1",
                "updated": "2024-09-26T02:37:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    2,
                    37,
                    41,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T02:37:41Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    2,
                    37,
                    41,
                    3,
                    270,
                    0
                ],
                "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are distinguished by their massive parameter\ncounts, which typically result in significant redundancy. This work introduces\nMaskLLM, a learnable pruning method that establishes Semi-structured (or\n``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during\ninference. Instead of developing a new importance criterion, MaskLLM explicitly\nmodels N:M patterns as a learnable distribution through Gumbel Softmax\nsampling. This approach facilitates end-to-end training on large-scale datasets\nand offers two notable advantages: 1) High-quality Masks - our method\neffectively scales to large datasets and learns accurate masks; 2)\nTransferability - the probabilistic modeling of mask distribution enables the\ntransfer learning of sparsity across domains or tasks. We assessed MaskLLM\nusing 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,\nwith sizes ranging from 843M to 15B parameters, and our empirical results show\nsubstantial improvements over state-of-the-art methods. For instance, leading\napproaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to\nthe dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL\nsolely by learning the masks with frozen weights. Furthermore, MaskLLM's\nlearnable nature allows customized masks for lossless application of 2:4\nsparsity to downstream tasks or domains. Code is available at\n\\url{https://github.com/NVlabs/MaskLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are distinguished by their massive parameter\ncounts, which typically result in significant redundancy. This work introduces\nMaskLLM, a learnable pruning method that establishes Semi-structured (or\n``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during\ninference. Instead of developing a new importance criterion, MaskLLM explicitly\nmodels N:M patterns as a learnable distribution through Gumbel Softmax\nsampling. This approach facilitates end-to-end training on large-scale datasets\nand offers two notable advantages: 1) High-quality Masks - our method\neffectively scales to large datasets and learns accurate masks; 2)\nTransferability - the probabilistic modeling of mask distribution enables the\ntransfer learning of sparsity across domains or tasks. We assessed MaskLLM\nusing 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,\nwith sizes ranging from 843M to 15B parameters, and our empirical results show\nsubstantial improvements over state-of-the-art methods. For instance, leading\napproaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to\nthe dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL\nsolely by learning the masks with frozen weights. Furthermore, MaskLLM's\nlearnable nature allows customized masks for lossless application of 2:4\nsparsity to downstream tasks or domains. Code is available at\n\\url{https://github.com/NVlabs/MaskLLM}."
                },
                "authors": [
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Jeff Pool"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "NeurIPS 2024 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17479v1",
                "updated": "2024-09-26T02:30:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    2,
                    30,
                    17,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T02:30:17Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    2,
                    30,
                    17,
                    3,
                    270,
                    0
                ],
                "title": "Traverse the Non-Traversable: Estimating Traversability for Wheeled\n  Mobility on Vertically Challenging Terrain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traverse the Non-Traversable: Estimating Traversability for Wheeled\n  Mobility on Vertically Challenging Terrain"
                },
                "summary": "Most traversability estimation techniques divide off-road terrain into\ntraversable (e.g., pavement, gravel, and grass) and non-traversable (e.g.,\nboulders, vegetation, and ditches) regions and then inform subsequent planners\nto produce trajectories on the traversable part. However, recent research\ndemonstrated that wheeled robots can traverse vertically challenging terrain\n(e.g., extremely rugged boulders comparable in size to the vehicles\nthemselves), which unfortunately would be deemed as non-traversable by existing\ntechniques. Motivated by such limitations, this work aims at identifying the\ntraversable from the seemingly non-traversable, vertically challenging terrain\nbased on past kinodynamic vehicle-terrain interactions in a data-driven manner.\nOur new Traverse the Non-Traversable(TNT) traversability estimator can\nefficiently guide a down-stream sampling-based planner containing a\nhigh-precision 6-DoF kinodynamic model, which becomes deployable onboard a\nsmall-scale vehicle. Additionally, the estimated traversability can also be\nused as a costmap to plan global and local paths without sampling. Our\nexperiment results show that TNT can improve planning performance, efficiency,\nand stability by 50%, 26.7%, and 9.2% respectively on a physical robot\nplatform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most traversability estimation techniques divide off-road terrain into\ntraversable (e.g., pavement, gravel, and grass) and non-traversable (e.g.,\nboulders, vegetation, and ditches) regions and then inform subsequent planners\nto produce trajectories on the traversable part. However, recent research\ndemonstrated that wheeled robots can traverse vertically challenging terrain\n(e.g., extremely rugged boulders comparable in size to the vehicles\nthemselves), which unfortunately would be deemed as non-traversable by existing\ntechniques. Motivated by such limitations, this work aims at identifying the\ntraversable from the seemingly non-traversable, vertically challenging terrain\nbased on past kinodynamic vehicle-terrain interactions in a data-driven manner.\nOur new Traverse the Non-Traversable(TNT) traversability estimator can\nefficiently guide a down-stream sampling-based planner containing a\nhigh-precision 6-DoF kinodynamic model, which becomes deployable onboard a\nsmall-scale vehicle. Additionally, the estimated traversability can also be\nused as a costmap to plan global and local paths without sampling. Our\nexperiment results show that TNT can improve planning performance, efficiency,\nand stability by 50%, 26.7%, and 9.2% respectively on a physical robot\nplatform."
                },
                "authors": [
                    {
                        "name": "Chenhui Pan"
                    },
                    {
                        "name": "Aniket Datar"
                    },
                    {
                        "name": "Anuj Pokhrel"
                    },
                    {
                        "name": "Matthew Choulas"
                    },
                    {
                        "name": "Mohammad Nazeri"
                    },
                    {
                        "name": "Xuesu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xuesu Xiao"
                },
                "author": "Xuesu Xiao",
                "arxiv_comment": "for associated video file, see\n  https://www.youtube.com/watch?v=Shcalb8sGcA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16273v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16273v4",
                "updated": "2024-09-26T02:07:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    2,
                    7,
                    8,
                    3,
                    270,
                    0
                ],
                "published": "2024-05-25T15:21:59Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    15,
                    21,
                    59,
                    5,
                    146,
                    0
                ],
                "title": "M$^3$GPT: An Advanced Multimodal, Multitask Framework for Motion\n  Comprehension and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M$^3$GPT: An Advanced Multimodal, Multitask Framework for Motion\n  Comprehension and Generation"
                },
                "summary": "This paper presents M$^3$GPT, an advanced $\\textbf{M}$ultimodal,\n$\\textbf{M}$ultitask framework for $\\textbf{M}$otion comprehension and\ngeneration. M$^3$GPT operates on three fundamental principles. The first\nfocuses on creating a unified representation space for various motion-relevant\nmodalities. We employ discrete vector quantization for multimodal control and\ngeneration signals, such as text, music and motion/dance, enabling seamless\nintegration into a large language model (LLM) with a single vocabulary. The\nsecond involves modeling model generation directly in the raw motion space.\nThis strategy circumvents the information loss associated with discrete\ntokenizer, resulting in more detailed and comprehensive model generation.\nThird, M$^3$GPT learns to model the connections and synergies among various\nmotion-relevant tasks. Text, the most familiar and well-understood modality for\nLLMs, is utilized as a bridge to establish connections between different motion\ntasks, facilitating mutual reinforcement. To our knowledge, M$^3$GPT is the\nfirst model capable of comprehending and generating motions based on multiple\nsignals. Extensive experiments highlight M$^3$GPT's superior performance across\nvarious motion-relevant tasks and its powerful zero-shot generalization\ncapabilities for extremely challenging tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents M$^3$GPT, an advanced $\\textbf{M}$ultimodal,\n$\\textbf{M}$ultitask framework for $\\textbf{M}$otion comprehension and\ngeneration. M$^3$GPT operates on three fundamental principles. The first\nfocuses on creating a unified representation space for various motion-relevant\nmodalities. We employ discrete vector quantization for multimodal control and\ngeneration signals, such as text, music and motion/dance, enabling seamless\nintegration into a large language model (LLM) with a single vocabulary. The\nsecond involves modeling model generation directly in the raw motion space.\nThis strategy circumvents the information loss associated with discrete\ntokenizer, resulting in more detailed and comprehensive model generation.\nThird, M$^3$GPT learns to model the connections and synergies among various\nmotion-relevant tasks. Text, the most familiar and well-understood modality for\nLLMs, is utilized as a bridge to establish connections between different motion\ntasks, facilitating mutual reinforcement. To our knowledge, M$^3$GPT is the\nfirst model capable of comprehending and generating motions based on multiple\nsignals. Extensive experiments highlight M$^3$GPT's superior performance across\nvarious motion-relevant tasks and its powerful zero-shot generalization\ncapabilities for extremely challenging tasks."
                },
                "authors": [
                    {
                        "name": "Mingshuang Luo"
                    },
                    {
                        "name": "Ruibing Hou"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Zimo Liu"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Shiguang Shan"
                    }
                ],
                "author_detail": {
                    "name": "Shiguang Shan"
                },
                "author": "Shiguang Shan",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16273v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16273v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14109v2",
                "updated": "2024-09-26T01:38:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    38,
                    52,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-21T11:48:54Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    11,
                    48,
                    54,
                    5,
                    265,
                    0
                ],
                "title": "Vision-Language Models Assisted Unsupervised Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models Assisted Unsupervised Video Anomaly Detection"
                },
                "summary": "Video anomaly detection is a subject of great interest across industrial and\nacademic domains due to its crucial role in computer vision applications.\nHowever, the inherent unpredictability of anomalies and the scarcity of anomaly\nsamples present significant challenges for unsupervised learning methods. To\novercome the limitations of unsupervised learning, which stem from a lack of\ncomprehensive prior knowledge about anomalies, we propose VLAVAD\n(Video-Language Models Assisted Anomaly Detection). Our method employs a\ncross-modal pre-trained model that leverages the inferential capabilities of\nlarge language models (LLMs) in conjunction with a Selective-Prompt Adapter\n(SPA) for selecting semantic space. Additionally, we introduce a Sequence State\nSpace Module (S3M) that detects temporal inconsistencies in semantic features.\nBy mapping high-dimensional visual features to low-dimensional semantic ones,\nour method significantly enhance the interpretability of unsupervised anomaly\ndetection. Our proposed approach effectively tackles the challenge of detecting\nelusive anomalies that are hard to discern over periods, achieving SOTA on the\nchallenging ShanghaiTech dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection is a subject of great interest across industrial and\nacademic domains due to its crucial role in computer vision applications.\nHowever, the inherent unpredictability of anomalies and the scarcity of anomaly\nsamples present significant challenges for unsupervised learning methods. To\novercome the limitations of unsupervised learning, which stem from a lack of\ncomprehensive prior knowledge about anomalies, we propose VLAVAD\n(Video-Language Models Assisted Anomaly Detection). Our method employs a\ncross-modal pre-trained model that leverages the inferential capabilities of\nlarge language models (LLMs) in conjunction with a Selective-Prompt Adapter\n(SPA) for selecting semantic space. Additionally, we introduce a Sequence State\nSpace Module (S3M) that detects temporal inconsistencies in semantic features.\nBy mapping high-dimensional visual features to low-dimensional semantic ones,\nour method significantly enhance the interpretability of unsupervised anomaly\ndetection. Our proposed approach effectively tackles the challenge of detecting\nelusive anomalies that are hard to discern over periods, achieving SOTA on the\nchallenging ShanghaiTech dataset."
                },
                "authors": [
                    {
                        "name": "Yalong Jiang"
                    },
                    {
                        "name": "Liquan Mao"
                    }
                ],
                "author_detail": {
                    "name": "Liquan Mao"
                },
                "author": "Liquan Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17460v1",
                "updated": "2024-09-26T01:38:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    38,
                    5,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T01:38:05Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    38,
                    5,
                    3,
                    270,
                    0
                ],
                "title": "Towards More Relevant Product Search Ranking Via Large Language Models:\n  An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Relevant Product Search Ranking Via Large Language Models:\n  An Empirical Study"
                },
                "summary": "Training Learning-to-Rank models for e-commerce product search ranking can be\nchallenging due to the lack of a gold standard of ranking relevance. In this\npaper, we decompose ranking relevance into content-based and engagement-based\naspects, and we propose to leverage Large Language Models (LLMs) for both label\nand feature generation in model training, primarily aiming to improve the\nmodel's predictive capability for content-based relevance. Additionally, we\nintroduce different sigmoid transformations on the LLM outputs to polarize\nrelevance scores in labeling, enhancing the model's ability to balance\ncontent-based and engagement-based relevances and thus prioritize highly\nrelevant items overall. Comprehensive online tests and offline evaluations are\nalso conducted for the proposed design. Our work sheds light on advanced\nstrategies for integrating LLMs into e-commerce product search ranking model\ntraining, offering a pathway to more effective and balanced models with\nimproved ranking relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Learning-to-Rank models for e-commerce product search ranking can be\nchallenging due to the lack of a gold standard of ranking relevance. In this\npaper, we decompose ranking relevance into content-based and engagement-based\naspects, and we propose to leverage Large Language Models (LLMs) for both label\nand feature generation in model training, primarily aiming to improve the\nmodel's predictive capability for content-based relevance. Additionally, we\nintroduce different sigmoid transformations on the LLM outputs to polarize\nrelevance scores in labeling, enhancing the model's ability to balance\ncontent-based and engagement-based relevances and thus prioritize highly\nrelevant items overall. Comprehensive online tests and offline evaluations are\nalso conducted for the proposed design. Our work sheds light on advanced\nstrategies for integrating LLMs into e-commerce product search ranking model\ntraining, offering a pathway to more effective and balanced models with\nimproved ranking relevance."
                },
                "authors": [
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Atul Singh"
                    },
                    {
                        "name": "Jingbo Liu"
                    },
                    {
                        "name": "Cun Mu"
                    },
                    {
                        "name": "Zheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yan"
                },
                "author": "Zheng Yan",
                "arxiv_comment": "To be published in CIKM 2024 GenAIECommerce Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17458v1",
                "updated": "2024-09-26T01:24:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    24,
                    17,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T01:24:17Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    24,
                    17,
                    3,
                    270,
                    0
                ],
                "title": "RED QUEEN: Safeguarding Large Language Models against Concealed\n  Multi-Turn Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RED QUEEN: Safeguarding Large Language Models against Concealed\n  Multi-Turn Jailbreaking"
                },
                "summary": "The rapid progress of Large Language Models (LLMs) has opened up new\nopportunities across various domains and applications; yet it also presents\nchallenges related to potential misuse. To mitigate such risks, red teaming has\nbeen employed as a proactive security measure to probe language models for\nharmful outputs via jailbreak attacks. However, current jailbreak attack\napproaches are single-turn with explicit malicious queries that do not fully\ncapture the complexity of real-world interactions. In reality, users can engage\nin multi-turn interactions with LLM-based chat assistants, allowing them to\nconceal their true intentions in a more covert manner. To bridge this gap, we,\nfirst, propose a new jailbreak approach, RED QUEEN ATTACK. This method\nconstructs a multi-turn scenario, concealing the malicious intent under the\nguise of preventing harm. We craft 40 scenarios that vary in turns and select\n14 harmful categories to generate 56k multi-turn attack data points. We conduct\ncomprehensive experiments on the RED QUEEN ATTACK with four representative LLM\nfamilies of different sizes. Our experiments reveal that all LLMs are\nvulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o\nand 75.4% on Llama3-70B. Further analysis reveals that larger models are more\nsusceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment\nstrategies contributing to its success. To prioritize safety, we introduce a\nstraightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs\nto effectively counter adversarial attacks. This approach reduces the attack\nsuccess rate to below 1% while maintaining the model's performance across\nstandard benchmarks. Full implementation and dataset are publicly accessible at\nhttps://github.com/kriti-hippo/red_queen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of Large Language Models (LLMs) has opened up new\nopportunities across various domains and applications; yet it also presents\nchallenges related to potential misuse. To mitigate such risks, red teaming has\nbeen employed as a proactive security measure to probe language models for\nharmful outputs via jailbreak attacks. However, current jailbreak attack\napproaches are single-turn with explicit malicious queries that do not fully\ncapture the complexity of real-world interactions. In reality, users can engage\nin multi-turn interactions with LLM-based chat assistants, allowing them to\nconceal their true intentions in a more covert manner. To bridge this gap, we,\nfirst, propose a new jailbreak approach, RED QUEEN ATTACK. This method\nconstructs a multi-turn scenario, concealing the malicious intent under the\nguise of preventing harm. We craft 40 scenarios that vary in turns and select\n14 harmful categories to generate 56k multi-turn attack data points. We conduct\ncomprehensive experiments on the RED QUEEN ATTACK with four representative LLM\nfamilies of different sizes. Our experiments reveal that all LLMs are\nvulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o\nand 75.4% on Llama3-70B. Further analysis reveals that larger models are more\nsusceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment\nstrategies contributing to its success. To prioritize safety, we introduce a\nstraightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs\nto effectively counter adversarial attacks. This approach reduces the attack\nsuccess rate to below 1% while maintaining the model's performance across\nstandard benchmarks. Full implementation and dataset are publicly accessible at\nhttps://github.com/kriti-hippo/red_queen."
                },
                "authors": [
                    {
                        "name": "Yifan Jiang"
                    },
                    {
                        "name": "Kriti Aggarwal"
                    },
                    {
                        "name": "Tanmay Laud"
                    },
                    {
                        "name": "Kashif Munir"
                    },
                    {
                        "name": "Jay Pujara"
                    },
                    {
                        "name": "Subhabrata Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Subhabrata Mukherjee"
                },
                "author": "Subhabrata Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17457v1",
                "updated": "2024-09-26T01:22:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    22,
                    29,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T01:22:29Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    1,
                    22,
                    29,
                    3,
                    270,
                    0
                ],
                "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD\n  Sketches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD\n  Sketches"
                },
                "summary": "Parametric Computer-Aided Design (CAD) is central to contemporary mechanical\ndesign. However, it encounters challenges in achieving precise parametric\nsketch modeling and lacks practical evaluation metrics suitable for mechanical\ndesign. We harness the capabilities of pre-trained foundation models, renowned\nfor their successes in natural language processing and computer vision, to\ndevelop generative models specifically for CAD. These models are adept at\nunderstanding complex geometries and design reasoning, a crucial advancement in\nCAD technology. In this paper, we propose CadVLM, an end-to-end vision language\nmodel for CAD generation. Our approach involves adapting pre-trained foundation\nmodels to manipulate engineering sketches effectively, integrating both sketch\nprimitive sequences and sketch images. Extensive experiments demonstrate\nsuperior performance on multiple CAD sketch generation tasks such as CAD\nautocompletion, CAD autoconstraint, and image conditional generation. To our\nknowledge, this is the first instance of a multimodal Large Language Model\n(LLM) being successfully applied to parametric CAD generation, representing a\npioneering step in the field of computer-aided mechanical design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric Computer-Aided Design (CAD) is central to contemporary mechanical\ndesign. However, it encounters challenges in achieving precise parametric\nsketch modeling and lacks practical evaluation metrics suitable for mechanical\ndesign. We harness the capabilities of pre-trained foundation models, renowned\nfor their successes in natural language processing and computer vision, to\ndevelop generative models specifically for CAD. These models are adept at\nunderstanding complex geometries and design reasoning, a crucial advancement in\nCAD technology. In this paper, we propose CadVLM, an end-to-end vision language\nmodel for CAD generation. Our approach involves adapting pre-trained foundation\nmodels to manipulate engineering sketches effectively, integrating both sketch\nprimitive sequences and sketch images. Extensive experiments demonstrate\nsuperior performance on multiple CAD sketch generation tasks such as CAD\nautocompletion, CAD autoconstraint, and image conditional generation. To our\nknowledge, this is the first instance of a multimodal Large Language Model\n(LLM) being successfully applied to parametric CAD generation, representing a\npioneering step in the field of computer-aided mechanical design."
                },
                "authors": [
                    {
                        "name": "Sifan Wu"
                    },
                    {
                        "name": "Amir Khasahmadi"
                    },
                    {
                        "name": "Mor Katz"
                    },
                    {
                        "name": "Pradeep Kumar Jayaraman"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Karl Willis"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17448v1",
                "updated": "2024-09-26T00:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    54,
                    17,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T00:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    54,
                    17,
                    3,
                    270,
                    0
                ],
                "title": "Enhancing Financial Sentiment Analysis with Expert-Designed Hint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial Sentiment Analysis with Expert-Designed Hint"
                },
                "summary": "This paper investigates the role of expert-designed hint in enhancing\nsentiment analysis on financial social media posts. We explore the capability\nof large language models (LLMs) to empathize with writer perspectives and\nanalyze sentiments. Our findings reveal that expert-designed hint, i.e.,\npointing out the importance of numbers, significantly improve performances\nacross various LLMs, particularly in cases requiring perspective-taking skills.\nFurther analysis on tweets containing different types of numerical data\ndemonstrates that the inclusion of expert-designed hint leads to notable\nimprovements in sentiment analysis performance, especially for tweets with\nmonetary-related numbers. Our findings contribute to the ongoing discussion on\nthe applicability of Theory of Mind in NLP and open new avenues for improving\nsentiment analysis in financial domains through the strategic use of expert\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of expert-designed hint in enhancing\nsentiment analysis on financial social media posts. We explore the capability\nof large language models (LLMs) to empathize with writer perspectives and\nanalyze sentiments. Our findings reveal that expert-designed hint, i.e.,\npointing out the importance of numbers, significantly improve performances\nacross various LLMs, particularly in cases requiring perspective-taking skills.\nFurther analysis on tweets containing different types of numerical data\ndemonstrates that the inclusion of expert-designed hint leads to notable\nimprovements in sentiment analysis performance, especially for tweets with\nmonetary-related numbers. Our findings contribute to the ongoing discussion on\nthe applicability of Theory of Mind in NLP and open new avenues for improving\nsentiment analysis in financial domains through the strategic use of expert\nknowledge."
                },
                "authors": [
                    {
                        "name": "Chung-Chi Chen"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Ichiro Kobayashi"
                    },
                    {
                        "name": "Yusuke Miyao"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Miyao"
                },
                "author": "Yusuke Miyao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17446v1",
                "updated": "2024-09-26T00:38:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    38,
                    18,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T00:38:18Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    38,
                    18,
                    3,
                    270,
                    0
                ],
                "title": "Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability"
                },
                "summary": "Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain or the clients are\nmobile. The impacts of the heterogeneity and non-stationarity in client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain or the clients are\nmobile. The impacts of the heterogeneity and non-stationarity in client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets."
                },
                "authors": [
                    {
                        "name": "Ming Xiang"
                    },
                    {
                        "name": "Stratis Ioannidis"
                    },
                    {
                        "name": "Edmund Yeh"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "Lili Su"
                    }
                ],
                "author_detail": {
                    "name": "Lili Su"
                },
                "author": "Lili Su",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00948v2",
                "updated": "2024-09-26T00:24:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    24,
                    25,
                    3,
                    270,
                    0
                ],
                "published": "2024-07-01T04:07:49Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    7,
                    49,
                    0,
                    183,
                    0
                ],
                "title": "View From Above: A Framework for Evaluating Distribution Shifts in Model\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View From Above: A Framework for Evaluating Distribution Shifts in Model\n  Behavior"
                },
                "summary": "When large language models (LLMs) are asked to perform certain tasks, how can\nwe be sure that their learned representations align with reality? We propose a\ndomain-agnostic framework for systematically evaluating distribution shifts in\nLLMs decision-making processes, where they are given control of mechanisms\ngoverned by pre-defined rules. While individual LLM actions may appear\nconsistent with expected behavior, across a large number of trials,\nstatistically significant distribution shifts can emerge. To test this, we\nconstruct a well-defined environment with known outcome logic: blackjack. In\nmore than 1,000 trials, we uncover statistically significant evidence\nsuggesting behavioral misalignment in the learned representations of LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models (LLMs) are asked to perform certain tasks, how can\nwe be sure that their learned representations align with reality? We propose a\ndomain-agnostic framework for systematically evaluating distribution shifts in\nLLMs decision-making processes, where they are given control of mechanisms\ngoverned by pre-defined rules. While individual LLM actions may appear\nconsistent with expected behavior, across a large number of trials,\nstatistically significant distribution shifts can emerge. To test this, we\nconstruct a well-defined environment with known outcome logic: blackjack. In\nmore than 1,000 trials, we uncover statistically significant evidence\nsuggesting behavioral misalignment in the learned representations of LLM."
                },
                "authors": [
                    {
                        "name": "Tanush Chopra"
                    },
                    {
                        "name": "Michael Li"
                    },
                    {
                        "name": "Jacob Haimes"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Haimes"
                },
                "author": "Jacob Haimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19868v3",
                "updated": "2024-09-26T00:17:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    17,
                    14,
                    3,
                    270,
                    0
                ],
                "published": "2024-06-28T12:19:15Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    12,
                    19,
                    15,
                    4,
                    180,
                    0
                ],
                "title": "Reconfigurable Intelligent Surfaces for 6G Mobile Networks: An Industry\n  R&D Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces for 6G Mobile Networks: An Industry\n  R&D Perspective"
                },
                "summary": "The reconfigurable intelligent surface (RIS) technology is a potential\nsolution to enhance network capacity and coverage without significant\ninvestment into additional infrastructure in 6G networks. This work highlights\nthe interest of the mobile communication industry on RIS, and discusses the\ndevelopment of liquid crystal-based RIS for improved energy efficiency and\ncoverage in the millimeter-wave band. Furthermore, the paper discusses\nperspectives and insights from an industry R&D point of view, addressing\nrelevant use cases, technical requirements, implementation challenges, and\npractical considerations for RIS deployment optimization in the context of 6G\nnetworks. A hardware design of a RIS with liquid crystal at 28 GHz is\npresented. A propagation model for RIS as a new part in the system architecture\nis discussed with the approaches of semi-empirical models, geometric models and\nits combination by the application of artificial intelligence / machine\nlearning. Finally, a channel model for deployment optimization and dimensioning\nis presented with the findings that rather large RIS is in favor for coverage\nimprovement as well as a greater attenuation at higher frequencies combined\nwith a smaller RIS size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reconfigurable intelligent surface (RIS) technology is a potential\nsolution to enhance network capacity and coverage without significant\ninvestment into additional infrastructure in 6G networks. This work highlights\nthe interest of the mobile communication industry on RIS, and discusses the\ndevelopment of liquid crystal-based RIS for improved energy efficiency and\ncoverage in the millimeter-wave band. Furthermore, the paper discusses\nperspectives and insights from an industry R&D point of view, addressing\nrelevant use cases, technical requirements, implementation challenges, and\npractical considerations for RIS deployment optimization in the context of 6G\nnetworks. A hardware design of a RIS with liquid crystal at 28 GHz is\npresented. A propagation model for RIS as a new part in the system architecture\nis discussed with the approaches of semi-empirical models, geometric models and\nits combination by the application of artificial intelligence / machine\nlearning. Finally, a channel model for deployment optimization and dimensioning\nis presented with the findings that rather large RIS is in favor for coverage\nimprovement as well as a greater attenuation at higher frequencies combined\nwith a smaller RIS size."
                },
                "authors": [
                    {
                        "name": "Maik Sode"
                    },
                    {
                        "name": "Michael Ponschab"
                    },
                    {
                        "name": "Lucas N. Ribeiro"
                    },
                    {
                        "name": "Sven Haesloop"
                    },
                    {
                        "name": "Ehsan Tohidi"
                    },
                    {
                        "name": "Michael Peter"
                    },
                    {
                        "name": "Sławomir Stańczak"
                    },
                    {
                        "name": "Bilal H. Mohamed"
                    },
                    {
                        "name": "Wilhelm Keusgen"
                    },
                    {
                        "name": "Heinz Mellein"
                    },
                    {
                        "name": "Eslam Yassin"
                    },
                    {
                        "name": "Bernd Schroeder"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Schroeder"
                },
                "arxiv_affiliation": "brown-iposs GmbH, Bonn, Germany",
                "author": "Bernd Schroeder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17436v1",
                "updated": "2024-09-26T00:08:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    8,
                    46,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T00:08:46Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    0,
                    8,
                    46,
                    3,
                    270,
                    0
                ],
                "title": "Minimizing Live Experiments in Recommender Systems: User Simulation to\n  Evaluate Preference Elicitation Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimizing Live Experiments in Recommender Systems: User Simulation to\n  Evaluate Preference Elicitation Policies"
                },
                "summary": "Evaluation of policies in recommender systems typically involves A/B testing\nusing live experiments on real users to assess a new policy's impact on\nrelevant metrics. This ``gold standard'' comes at a high cost, however, in\nterms of cycle time, user cost, and potential user retention. In developing\npolicies for ``onboarding'' new users, these costs can be especially\nproblematic, since on-boarding occurs only once. In this work, we describe a\nsimulation methodology used to augment (and reduce) the use of live\nexperiments. We illustrate its deployment for the evaluation of ``preference\nelicitation'' algorithms used to onboard new users of the YouTube Music\nplatform. By developing counterfactually robust user behavior models, and a\nsimulation service that couples such models with production infrastructure, we\nare able to test new algorithms in a way that reliably predicts their\nperformance on key metrics when deployed live. We describe our domain, our\nsimulation models and platform, results of experiments and deployment, and\nsuggest future steps needed to further realistic simulation as a powerful\ncomplement to live experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of policies in recommender systems typically involves A/B testing\nusing live experiments on real users to assess a new policy's impact on\nrelevant metrics. This ``gold standard'' comes at a high cost, however, in\nterms of cycle time, user cost, and potential user retention. In developing\npolicies for ``onboarding'' new users, these costs can be especially\nproblematic, since on-boarding occurs only once. In this work, we describe a\nsimulation methodology used to augment (and reduce) the use of live\nexperiments. We illustrate its deployment for the evaluation of ``preference\nelicitation'' algorithms used to onboard new users of the YouTube Music\nplatform. By developing counterfactually robust user behavior models, and a\nsimulation service that couples such models with production infrastructure, we\nare able to test new algorithms in a way that reliably predicts their\nperformance on key metrics when deployed live. We describe our domain, our\nsimulation models and platform, results of experiments and deployment, and\nsuggest future steps needed to further realistic simulation as a powerful\ncomplement to live experiments."
                },
                "authors": [
                    {
                        "name": "Chih-Wei Hsu"
                    },
                    {
                        "name": "Martin Mladenov"
                    },
                    {
                        "name": "Ofer Meshi"
                    },
                    {
                        "name": "James Pine"
                    },
                    {
                        "name": "Hubert Pham"
                    },
                    {
                        "name": "Shane Li"
                    },
                    {
                        "name": "Xujian Liang"
                    },
                    {
                        "name": "Anton Polishko"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Ben Scheetz"
                    },
                    {
                        "name": "Craig Boutilier"
                    }
                ],
                "author_detail": {
                    "name": "Craig Boutilier"
                },
                "author": "Craig Boutilier",
                "arxiv_doi": "10.1145/3626772.3661358",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626772.3661358",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17433v1",
                "updated": "2024-09-25T23:52:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    23,
                    52,
                    17,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T23:52:17Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    23,
                    52,
                    17,
                    2,
                    269,
                    0
                ],
                "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and\n  Dynamic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and\n  Dynamic Workflows"
                },
                "summary": "Despite recent advancements in large language models (LLMs), their\nperformance on complex reasoning problems requiring multi-step thinking and\ncombining various skills is still limited. To address this, we propose a novel\nframework HDFlow for complex reasoning with LLMs that combines fast and slow\nthinking modes in an adaptive manner. Our approach consists of two key\ncomponents: 1) a new approach for slow, deliberate reasoning called Dynamic\nWorkflow, which automatically decomposes complex problems into more manageable\nsub-tasks and dynamically designs a workflow to assemble specialized LLM or\nsymbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general\nframework that dynamically combines fast and slow thinking based on problem\ncomplexity. Finally, we propose an easy-to-scale method for automatically\nsynthesizing a large-scale dataset of 27K challenging reasoning problems for\ncomplex reasoning and a hybrid thinking tuning method that trains smaller LLMs\non this dataset to internalize the fast/slow hybrid reasoning strategies.\nExperiments on four reasoning benchmark datasets demonstrate that our slow\nthinking with dynamic workflows significantly outperforms Chain-of-Thought, and\nhybrid thinking achieves the highest accuracy while providing an effective\nbalance between computational efficiency and performance. Fine-tuning using our\nhybrid thinking approach also significantly boosts the complex reasoning\ncapabilities of open-source language models. The results showcase the promise\nof slow thinking, dynamic workflows, and hybrid thinking in expanding the\nfrontier of complex problem-solving with LLMs\\footnote{Code and data will be\nreleased at \\url{https://github.com/wenlinyao/HDFlow}.}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in large language models (LLMs), their\nperformance on complex reasoning problems requiring multi-step thinking and\ncombining various skills is still limited. To address this, we propose a novel\nframework HDFlow for complex reasoning with LLMs that combines fast and slow\nthinking modes in an adaptive manner. Our approach consists of two key\ncomponents: 1) a new approach for slow, deliberate reasoning called Dynamic\nWorkflow, which automatically decomposes complex problems into more manageable\nsub-tasks and dynamically designs a workflow to assemble specialized LLM or\nsymbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general\nframework that dynamically combines fast and slow thinking based on problem\ncomplexity. Finally, we propose an easy-to-scale method for automatically\nsynthesizing a large-scale dataset of 27K challenging reasoning problems for\ncomplex reasoning and a hybrid thinking tuning method that trains smaller LLMs\non this dataset to internalize the fast/slow hybrid reasoning strategies.\nExperiments on four reasoning benchmark datasets demonstrate that our slow\nthinking with dynamic workflows significantly outperforms Chain-of-Thought, and\nhybrid thinking achieves the highest accuracy while providing an effective\nbalance between computational efficiency and performance. Fine-tuning using our\nhybrid thinking approach also significantly boosts the complex reasoning\ncapabilities of open-source language models. The results showcase the promise\nof slow thinking, dynamic workflows, and hybrid thinking in expanding the\nfrontier of complex problem-solving with LLMs\\footnote{Code and data will be\nreleased at \\url{https://github.com/wenlinyao/HDFlow}.}."
                },
                "authors": [
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17429v1",
                "updated": "2024-09-25T23:32:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    23,
                    32,
                    59,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T23:32:59Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    23,
                    32,
                    59,
                    2,
                    269,
                    0
                ],
                "title": "Real-World Data Inspired Interactive Connected Traffic Scenario\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Data Inspired Interactive Connected Traffic Scenario\n  Generation"
                },
                "summary": "Simulation is a crucial step in ensuring accurate, efficient, and realistic\nConnected and Autonomous Vehicles (CAVs) testing and validation. As the\nadoption of CAV accelerates, the integration of real-world data into simulation\nenvironments becomes increasingly critical. Among various technologies utilized\nby CAVs, Vehicle-to-Everything (V2X) communication plays a crucial role in\nensuring a seamless transmission of information between CAVs, infrastructure,\nand other road users. However, most existing studies have focused on developing\nand testing communication protocols, resource allocation strategies, and data\ndissemination techniques in V2X. There is a gap where real-world V2X data is\nintegrated into simulations to generate diverse and high-fidelity traffic\nscenarios. To fulfill this research gap, we leverage real-world Signal Phase\nand Timing (SPaT) data from Roadside Units (RSUs) to enhance the fidelity of\nCAV simulations. Moreover, we developed an algorithm that enables Autonomous\nVehicles (AVs) to respond dynamically to real-time traffic signal data,\nsimulating realistic V2X communication scenarios. Such high-fidelity simulation\nenvironments can generate multimodal data, including trajectory, semantic\ncamera, depth camera, and bird's eye view data for various traffic scenarios.\nThe generated scenarios and data provide invaluable insights into AVs'\ninteractions with traffic infrastructure and other road users. This work aims\nto bridge the gap between theoretical research and practical deployment of\nCAVs, facilitating the development of smarter and safer transportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation is a crucial step in ensuring accurate, efficient, and realistic\nConnected and Autonomous Vehicles (CAVs) testing and validation. As the\nadoption of CAV accelerates, the integration of real-world data into simulation\nenvironments becomes increasingly critical. Among various technologies utilized\nby CAVs, Vehicle-to-Everything (V2X) communication plays a crucial role in\nensuring a seamless transmission of information between CAVs, infrastructure,\nand other road users. However, most existing studies have focused on developing\nand testing communication protocols, resource allocation strategies, and data\ndissemination techniques in V2X. There is a gap where real-world V2X data is\nintegrated into simulations to generate diverse and high-fidelity traffic\nscenarios. To fulfill this research gap, we leverage real-world Signal Phase\nand Timing (SPaT) data from Roadside Units (RSUs) to enhance the fidelity of\nCAV simulations. Moreover, we developed an algorithm that enables Autonomous\nVehicles (AVs) to respond dynamically to real-time traffic signal data,\nsimulating realistic V2X communication scenarios. Such high-fidelity simulation\nenvironments can generate multimodal data, including trajectory, semantic\ncamera, depth camera, and bird's eye view data for various traffic scenarios.\nThe generated scenarios and data provide invaluable insights into AVs'\ninteractions with traffic infrastructure and other road users. This work aims\nto bridge the gap between theoretical research and practical deployment of\nCAVs, facilitating the development of smarter and safer transportation systems."
                },
                "authors": [
                    {
                        "name": "Junwei You"
                    },
                    {
                        "name": "Pei Li"
                    },
                    {
                        "name": "Yang Cheng"
                    },
                    {
                        "name": "Keshu Wu"
                    },
                    {
                        "name": "Rui Gan"
                    },
                    {
                        "name": "Steven T. Parker"
                    },
                    {
                        "name": "Bin Ran"
                    }
                ],
                "author_detail": {
                    "name": "Bin Ran"
                },
                "author": "Bin Ran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]