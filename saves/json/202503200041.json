[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v1",
                "updated": "2025-03-17T23:38:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v1",
                "updated": "2025-03-17T11:10:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v4",
                "updated": "2025-03-16T16:25:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    16,
                    25,
                    31,
                    6,
                    75,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.14505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14505v1",
                "updated": "2025-03-18T17:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    58,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    58,
                    1,
                    77,
                    0
                ],
                "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MusicInfuser: Making Video Diffusion Listen and Dance"
                },
                "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser."
                },
                "authors": [
                    {
                        "name": "Susung Hong"
                    },
                    {
                        "name": "Ira Kemelmacher-Shlizerman"
                    },
                    {
                        "name": "Brian Curless"
                    },
                    {
                        "name": "Steven M. Seitz"
                    }
                ],
                "author_detail": {
                    "name": "Steven M. Seitz"
                },
                "author": "Steven M. Seitz",
                "arxiv_comment": "Project page: https://susunghong.github.io/MusicInfuser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14504v1",
                "updated": "2025-03-18T17:59:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    56,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Aligning Multimodal LLM with Human Preference: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Multimodal LLM with Human Preference: A Survey"
                },
                "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment."
                },
                "authors": [
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14495v1",
                "updated": "2025-03-18T17:58:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    58,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:58:28Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    58,
                    28,
                    1,
                    77,
                    0
                ],
                "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Consistency for LLM Reasoning Process Error Identification"
                },
                "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency"
                },
                "authors": [
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Kaixuan Huang"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14492v1",
                "updated": "2025-03-18T17:57:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    57,
                    54,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:57:54Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    57,
                    54,
                    1,
                    77,
                    0
                ],
                "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control"
                },
                "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1."
                },
                "authors": [
                    {
                        "name": "NVIDIA"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Hassan Abu Alhaija"
                    },
                    {
                        "name": "Jose Alvarez"
                    },
                    {
                        "name": "Maciej Bala"
                    },
                    {
                        "name": "Tiffany Cai"
                    },
                    {
                        "name": "Tianshi Cao"
                    },
                    {
                        "name": "Liz Cha"
                    },
                    {
                        "name": "Joshua Chen"
                    },
                    {
                        "name": "Mike Chen"
                    },
                    {
                        "name": "Francesco Ferroni"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Dieter Fox"
                    },
                    {
                        "name": "Yunhao Ge"
                    },
                    {
                        "name": "Jinwei Gu"
                    },
                    {
                        "name": "Ali Hassani"
                    },
                    {
                        "name": "Michael Isaev"
                    },
                    {
                        "name": "Pooya Jannaty"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Tobias Lasser"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Xian Liu"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Alice Luo"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Hanzi Mao"
                    },
                    {
                        "name": "Fabio Ramos"
                    },
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Shitao Tang"
                    },
                    {
                        "name": "Ting-Chun Wang"
                    },
                    {
                        "name": "Jay Wu"
                    },
                    {
                        "name": "Jiashu Xu"
                    },
                    {
                        "name": "Stella Xu"
                    },
                    {
                        "name": "Kevin Xie"
                    },
                    {
                        "name": "Yuchong Ye"
                    },
                    {
                        "name": "Xiaodong Yang"
                    },
                    {
                        "name": "Xiaohui Zeng"
                    },
                    {
                        "name": "Yu Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zeng"
                },
                "author": "Yu Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14488v1",
                "updated": "2025-03-18T17:57:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    57,
                    16,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:57:16Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    57,
                    16,
                    1,
                    77,
                    0
                ],
                "title": "Engineering Scientific Assistants using Interactive Structured Induction\n  of Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Scientific Assistants using Interactive Structured Induction\n  of Programs"
                },
                "summary": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants."
                },
                "authors": [
                    {
                        "name": "Shraddha Surana"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14484v1",
                "updated": "2025-03-18T17:54:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    54,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:54:14Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    54,
                    14,
                    1,
                    77,
                    0
                ],
                "title": "Gricean Norms as a Basis for Effective Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gricean Norms as a Basis for Effective Collaboration"
                },
                "summary": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Fardin Saad"
                    },
                    {
                        "name": "Pradeep K. Murukannaiah"
                    },
                    {
                        "name": "Munindar P. Singh"
                    }
                ],
                "author_detail": {
                    "name": "Munindar P. Singh"
                },
                "author": "Munindar P. Singh",
                "arxiv_comment": "Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures/tables.\n  (Appendix: 5 pages, 6 figures/tables). Code available at:\n  https://github.com/fardinsaad/Gricean-Norms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05002v2",
                "updated": "2025-03-18T17:52:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    52,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-07T15:16:46Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    15,
                    16,
                    46,
                    4,
                    159,
                    0
                ],
                "title": "Deep Jansen-Rit Parameter Inference for Model-Driven Analysis of Brain\n  Activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Jansen-Rit Parameter Inference for Model-Driven Analysis of Brain\n  Activity"
                },
                "summary": "Accurately modeling effective connectivity (EC) is critical for understanding\nhow the brain processes and integrates sensory information. Yet, it remains a\nformidable challenge due to complex neural dynamics and noisy measurements such\nas those obtained from the electroencephalogram (EEG). Model-driven EC infers\nlocal (within a brain region) and global (between brain regions) EC parameters\nby fitting a generative model of neural activity onto experimental data. This\napproach offers a promising route for various applications, including\ninvestigating neurodevelopmental disorders. However, current approaches fail to\nscale to whole-brain analyses and are highly noise-sensitive. In this work, we\nemploy three deep-learning architectures--a transformer, a long short-term\nmemory (LSTM) network, and a convolutional neural network and bidirectional\nLSTM (CNN-BiLSTM) network--for inverse modeling and compare their performance\nwith simulation-based inference in estimating the Jansen-Rit neural mass model\n(JR-NMM) parameters from simulated EEG data under various noise conditions. We\ndemonstrate a reliable estimation of key local parameters, such as synaptic\ngains and time constants. However, other parameters like local JR-NMM\nconnectivity cannot be evaluated reliably from evoked-related potentials (ERP).\nWe also conduct a sensitivity analysis to characterize the influence of JR-NMM\nparameters on ERP and evaluate their learnability. Our results show the\nfeasibility of deep-learning approaches to estimate the subset of learnable\nJR-NMM parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately modeling effective connectivity (EC) is critical for understanding\nhow the brain processes and integrates sensory information. Yet, it remains a\nformidable challenge due to complex neural dynamics and noisy measurements such\nas those obtained from the electroencephalogram (EEG). Model-driven EC infers\nlocal (within a brain region) and global (between brain regions) EC parameters\nby fitting a generative model of neural activity onto experimental data. This\napproach offers a promising route for various applications, including\ninvestigating neurodevelopmental disorders. However, current approaches fail to\nscale to whole-brain analyses and are highly noise-sensitive. In this work, we\nemploy three deep-learning architectures--a transformer, a long short-term\nmemory (LSTM) network, and a convolutional neural network and bidirectional\nLSTM (CNN-BiLSTM) network--for inverse modeling and compare their performance\nwith simulation-based inference in estimating the Jansen-Rit neural mass model\n(JR-NMM) parameters from simulated EEG data under various noise conditions. We\ndemonstrate a reliable estimation of key local parameters, such as synaptic\ngains and time constants. However, other parameters like local JR-NMM\nconnectivity cannot be evaluated reliably from evoked-related potentials (ERP).\nWe also conduct a sensitivity analysis to characterize the influence of JR-NMM\nparameters on ERP and evaluate their learnability. Our results show the\nfeasibility of deep-learning approaches to estimate the subset of learnable\nJR-NMM parameters."
                },
                "authors": [
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Christian O'Reilly"
                    }
                ],
                "author_detail": {
                    "name": "Christian O'Reilly"
                },
                "author": "Christian O'Reilly",
                "arxiv_comment": "Accepted at 7th International Conference on Advances in Signal\n  Processing and Artificial Intelligence (ASPAI' 2025), 8-10 April 2025,\n  Innsbruck, Austria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04382v2",
                "updated": "2025-03-18T17:51:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    56,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-05T18:58:02Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    2,
                    2,
                    36,
                    0
                ],
                "title": "Sparse Autoencoders for Hypothesis Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders for Hypothesis Generation"
                },
                "summary": "We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines."
                },
                "authors": [
                    {
                        "name": "Rajiv Movva"
                    },
                    {
                        "name": "Kenny Peng"
                    },
                    {
                        "name": "Nikhil Garg"
                    },
                    {
                        "name": "Jon Kleinberg"
                    },
                    {
                        "name": "Emma Pierson"
                    }
                ],
                "author_detail": {
                    "name": "Emma Pierson"
                },
                "author": "Emma Pierson",
                "arxiv_comment": "First two authors contributed equally; working paper. Code is\n  available at https://github.com/rmovva/HypotheSAEs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14478v1",
                "updated": "2025-03-18T17:51:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    34,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:51:34Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    34,
                    1,
                    77,
                    0
                ],
                "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM"
                },
                "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Zhijian Chen"
                    },
                    {
                        "name": "Kai Lan"
                    },
                    {
                        "name": "Shengyuan Ding"
                    },
                    {
                        "name": "Yingji Liang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Farong Wen"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Guofeng Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14477v1",
                "updated": "2025-03-18T17:51:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    4,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:51:04Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    4,
                    1,
                    77,
                    0
                ],
                "title": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce\n  Hallucinations"
                },
                "summary": "LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%."
                },
                "authors": [
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Yeskendir Koishekenov"
                    },
                    {
                        "name": "Yejin Bang"
                    },
                    {
                        "name": "Anthony Hartshorn"
                    },
                    {
                        "name": "Alan Schelten"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Pascale Fung"
                    },
                    {
                        "name": "Nicola Cancedda"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Cancedda"
                },
                "author": "Nicola Cancedda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13803v2",
                "updated": "2025-03-18T17:49:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    49,
                    6,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-19T20:07:37Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    20,
                    7,
                    37,
                    2,
                    171,
                    0
                ],
                "title": "LLMs as Models for Analogical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Models for Analogical Reasoning"
                },
                "summary": "Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations."
                },
                "authors": [
                    {
                        "name": "Sam Musker"
                    },
                    {
                        "name": "Alex Duchnowski"
                    },
                    {
                        "name": "Raphaël Millière"
                    },
                    {
                        "name": "Ellie Pavlick"
                    }
                ],
                "author_detail": {
                    "name": "Ellie Pavlick"
                },
                "author": "Ellie Pavlick",
                "arxiv_comment": "The title has been changed from Semantic Structure-Mapping in LLM and\n  Human Analogical Reasoning to LLMs as Models for Analogical Reasoning to\n  improve clarity and accuracy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14476v1",
                "updated": "2025-03-18T17:49:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    49,
                    6,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:49:06Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    49,
                    6,
                    1,
                    77,
                    0
                ],
                "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
                },
                "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL."
                },
                "authors": [
                    {
                        "name": "Qiying Yu"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Ruofei Zhu"
                    },
                    {
                        "name": "Yufeng Yuan"
                    },
                    {
                        "name": "Xiaochen Zuo"
                    },
                    {
                        "name": "Yu Yue"
                    },
                    {
                        "name": "Tiantian Fan"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Lingjun Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Bole Ma"
                    },
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Yuxuan Tong"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Mofan Zhang"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Jinhua Zhu"
                    },
                    {
                        "name": "Jiaze Chen"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Chengyi Wang"
                    },
                    {
                        "name": "Hongli Yu"
                    },
                    {
                        "name": "Weinan Dai"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Xiangpeng Wei"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Mu Qiao"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Mingxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Wang"
                },
                "author": "Mingxuan Wang",
                "arxiv_comment": "Project Page: https://dapo-sia.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12730v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12730v3",
                "updated": "2025-03-18T17:48:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    48,
                    13,
                    1,
                    77,
                    0
                ],
                "published": "2024-10-16T16:44:12Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    16,
                    44,
                    12,
                    2,
                    290,
                    0
                ],
                "title": "Counterfactual Generative Modeling with Variational Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Generative Modeling with Variational Causal Inference"
                },
                "summary": "Estimating an individual's counterfactual outcomes under interventions is a\nchallenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in the observed outcome in addition to the covariates.\nPrior works using variational inference in counterfactual generative modeling\nhave been focusing on neural adaptations and model variants within the\nconditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage disentangled exogenous noise abduction that aids the correct\nidentification of causal effect in counterfactual generations. In experiments,\nwe demonstrate the advantage of our framework compared to state-of-the-art\nmodels in counterfactual generative modeling on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating an individual's counterfactual outcomes under interventions is a\nchallenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in the observed outcome in addition to the covariates.\nPrior works using variational inference in counterfactual generative modeling\nhave been focusing on neural adaptations and model variants within the\nconditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage disentangled exogenous noise abduction that aids the correct\nidentification of causal effect in counterfactual generations. In experiments,\nwe demonstrate the advantage of our framework compared to state-of-the-art\nmodels in counterfactual generative modeling on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Louie McConnell"
                    },
                    {
                        "name": "Claudia Iriondo"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Iriondo"
                },
                "author": "Claudia Iriondo",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12730v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12730v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02239v2",
                "updated": "2025-03-18T17:38:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    38,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-04T16:32:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    32,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "Powerful batch conformal prediction for classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful batch conformal prediction for classification"
                },
                "summary": "In a split conformal framework with $K$ classes, a calibration sample of $n$\nlabeled examples is observed for inference on the label of a new unlabeled\nexample. We explore the setting where a `batch' of $m$ independent such\nunlabeled examples is given, and the goal is to construct a batch prediction\nset with 1-$\\alpha$ coverage. Unlike individual prediction sets, the batch\nprediction set is a collection of label vectors of size $m$, while the\ncalibration sample consists of univariate labels. A natural approach is to\napply the Bonferroni correction, which concatenates individual prediction sets\nat level $1-\\alpha/m$. We propose a uniformly more powerful solution, based on\nspecific combinations of conformal $p$-values that exploit the Simes\ninequality. We provide a general recipe for valid inference with any\ncombinations of conformal $p$-values, and compare the performance of several\nuseful choices. Intuitively, the pooled evidence of relatively `easy' examples\nwithin the batch can help provide narrower batch prediction sets. Additionally,\nwe introduce a more computationally intensive method that aggregates batch\nscores and can be even more powerful. The theoretical guarantees are\nestablished when all examples are independent and identically distributed\n(iid), as well as more generally when iid is assumed only conditionally within\neach class. Notably, our results remain valid under label distribution shift,\nsince the distribution of the labels need not be the same in the calibration\nsample and in the new batch. The effectiveness of the methods is highlighted\nthrough illustrative synthetic and real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a split conformal framework with $K$ classes, a calibration sample of $n$\nlabeled examples is observed for inference on the label of a new unlabeled\nexample. We explore the setting where a `batch' of $m$ independent such\nunlabeled examples is given, and the goal is to construct a batch prediction\nset with 1-$\\alpha$ coverage. Unlike individual prediction sets, the batch\nprediction set is a collection of label vectors of size $m$, while the\ncalibration sample consists of univariate labels. A natural approach is to\napply the Bonferroni correction, which concatenates individual prediction sets\nat level $1-\\alpha/m$. We propose a uniformly more powerful solution, based on\nspecific combinations of conformal $p$-values that exploit the Simes\ninequality. We provide a general recipe for valid inference with any\ncombinations of conformal $p$-values, and compare the performance of several\nuseful choices. Intuitively, the pooled evidence of relatively `easy' examples\nwithin the batch can help provide narrower batch prediction sets. Additionally,\nwe introduce a more computationally intensive method that aggregates batch\nscores and can be even more powerful. The theoretical guarantees are\nestablished when all examples are independent and identically distributed\n(iid), as well as more generally when iid is assumed only conditionally within\neach class. Notably, our results remain valid under label distribution shift,\nsince the distribution of the labels need not be the same in the calibration\nsample and in the new batch. The effectiveness of the methods is highlighted\nthrough illustrative synthetic and real data examples."
                },
                "authors": [
                    {
                        "name": "Ulysse Gazin"
                    },
                    {
                        "name": "Ruth Heller"
                    },
                    {
                        "name": "Etienne Roquain"
                    },
                    {
                        "name": "Aldo Solari"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Solari"
                },
                "author": "Aldo Solari",
                "arxiv_comment": "34 pages, 7 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14460v1",
                "updated": "2025-03-18T17:36:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    36,
                    26,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:36:26Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    36,
                    26,
                    1,
                    77,
                    0
                ],
                "title": "Low-Metallicity Star Formation Survey in Sh2-284 (LZ-STAR). I. Ordered\n  massive star formation in the outer Galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Metallicity Star Formation Survey in Sh2-284 (LZ-STAR). I. Ordered\n  massive star formation in the outer Galaxy"
                },
                "summary": "Star formation is a fundamental, yet poorly understood, process of the\nUniverse. It is important to study how star formation occurs in different\ngalactic environments. Thus, here, in the first of a series of papers, we\nintroduce the Low-Metallicity Star Formation (LZ-STAR) survey of the Sh2-284\n(hereafter S284) region, which, at $Z\\sim 0.3-0.5Z_\\odot$, is one of the\nlowest-metallicity star-forming regions of our Galaxy. LZ-STAR is a\nmulti-facility survey, including observations with {\\it JWST}, {\\it ALMA}, {\\it\nHST}, {\\it Chandra} and {\\it Gemini}. As a starting point, we report {\\it JWST}\nand {\\it ALMA} observations of one of the most massive protostars in the\nregion, S284p1. The observations of shock-excited molecular hydrogen reveal a\nsymmetric, bipolar outflow originating from the protostar, spanning several\nparsecs, and fully covered by the {\\it JWST} field of view and the {\\it ALMA}\nobservations of CO(2-1) emission. This allows us to infer that the protostar\nhas maintained a relatively stable orientation of disk accretion over its\nformation history. The {\\it JWST} near-IR continuum observations detect a\ncentrally illuminated bipolar outflow cavity around the protostar, as well as a\nsurrounding cluster of low-mass young stars. We develop new radiative transfer\nmodels of massive protostars designed for the low metallicity of S284. Fitting\nthese models to the protostar's spectral energy distribution implies a current\nprotostellar mass of $\\sim11\\:M_\\odot$ has formed from an initially\n$\\sim100\\:M_\\odot$ core over the last $\\sim3\\times10^5$ years. Overall, these\nresults indicate that massive stars can form in an ordered manner in\nlow-metallicity, protocluster environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star formation is a fundamental, yet poorly understood, process of the\nUniverse. It is important to study how star formation occurs in different\ngalactic environments. Thus, here, in the first of a series of papers, we\nintroduce the Low-Metallicity Star Formation (LZ-STAR) survey of the Sh2-284\n(hereafter S284) region, which, at $Z\\sim 0.3-0.5Z_\\odot$, is one of the\nlowest-metallicity star-forming regions of our Galaxy. LZ-STAR is a\nmulti-facility survey, including observations with {\\it JWST}, {\\it ALMA}, {\\it\nHST}, {\\it Chandra} and {\\it Gemini}. As a starting point, we report {\\it JWST}\nand {\\it ALMA} observations of one of the most massive protostars in the\nregion, S284p1. The observations of shock-excited molecular hydrogen reveal a\nsymmetric, bipolar outflow originating from the protostar, spanning several\nparsecs, and fully covered by the {\\it JWST} field of view and the {\\it ALMA}\nobservations of CO(2-1) emission. This allows us to infer that the protostar\nhas maintained a relatively stable orientation of disk accretion over its\nformation history. The {\\it JWST} near-IR continuum observations detect a\ncentrally illuminated bipolar outflow cavity around the protostar, as well as a\nsurrounding cluster of low-mass young stars. We develop new radiative transfer\nmodels of massive protostars designed for the low metallicity of S284. Fitting\nthese models to the protostar's spectral energy distribution implies a current\nprotostellar mass of $\\sim11\\:M_\\odot$ has formed from an initially\n$\\sim100\\:M_\\odot$ core over the last $\\sim3\\times10^5$ years. Overall, these\nresults indicate that massive stars can form in an ordered manner in\nlow-metallicity, protocluster environments."
                },
                "authors": [
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Jonathan C. Tan"
                    },
                    {
                        "name": "Morten Andersen"
                    },
                    {
                        "name": "Rubén Fedriani"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Massimo Robberto"
                    },
                    {
                        "name": "Zhi-Yun Li"
                    },
                    {
                        "name": "Kei E. I. Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Kei E. I. Tanaka"
                },
                "author": "Kei E. I. Tanaka",
                "arxiv_comment": "19 pages, 12 figures, submitted to ApJ, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14459v1",
                "updated": "2025-03-18T17:33:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    33,
                    10,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:33:10Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    33,
                    10,
                    1,
                    77,
                    0
                ],
                "title": "Doubly robust identification of treatment effects from multiple\n  environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly robust identification of treatment effects from multiple\n  environments"
                },
                "summary": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods."
                },
                "authors": [
                    {
                        "name": "Piersilvio De Bartolomeis"
                    },
                    {
                        "name": "Julia Kostin"
                    },
                    {
                        "name": "Javier Abad"
                    },
                    {
                        "name": "Yixin Wang"
                    },
                    {
                        "name": "Fanny Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Yang"
                },
                "author": "Fanny Yang",
                "arxiv_comment": "Accepted for presentation at the International Conference on Learning\n  Representations (ICLR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14456v1",
                "updated": "2025-03-18T17:31:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    31,
                    5,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:31:05Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    31,
                    5,
                    1,
                    77,
                    0
                ],
                "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution"
                },
                "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense."
                },
                "authors": [
                    {
                        "name": "Bo Peng"
                    },
                    {
                        "name": "Ruichong Zhang"
                    },
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Haowen Hou"
                    },
                    {
                        "name": "Janna Lu"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Kaifeng Tan"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Nathan Wilce"
                    },
                    {
                        "name": "Johan S. Wind"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Daniel Wuttke"
                    },
                    {
                        "name": "Christian Zhou-Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Christian Zhou-Zheng"
                },
                "author": "Christian Zhou-Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14454v1",
                "updated": "2025-03-18T17:30:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    30,
                    26,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:30:26Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    30,
                    26,
                    1,
                    77,
                    0
                ],
                "title": "The Atacama Cosmology Telescope: DR6 Constraints on Extended\n  Cosmological Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Atacama Cosmology Telescope: DR6 Constraints on Extended\n  Cosmological Models"
                },
                "summary": "We use new cosmic microwave background (CMB) primary temperature and\npolarization anisotropy measurements from the Atacama Cosmology Telescope (ACT)\nData Release 6 (DR6) to test foundational assumptions of the standard\ncosmological model and set constraints on extensions to it. We derive\nconstraints from the ACT DR6 power spectra alone, as well as in combination\nwith legacy data from Planck. To break geometric degeneracies, we include ACT\nand Planck CMB lensing data and baryon acoustic oscillation data from DESI\nYear-1, and further add supernovae measurements from Pantheon+ for models that\naffect the late-time expansion history. We verify the near-scale-invariance\n(running of the spectral index $d n_s/d\\ln k = 0.0062 \\pm 0.0052$) and\nadiabaticity of the primordial perturbations. Neutrino properties are\nconsistent with Standard Model predictions: we find no evidence for new light,\nrelativistic species that are free-streaming ($N_{\\rm eff} = 2.86 \\pm 0.13$,\nwhich combined with external BBN data becomes $N_{\\rm eff} = 2.89 \\pm 0.11$),\nfor non-zero neutrino masses ($\\sum m_\\nu < 0.082$ eV at 95% CL), or for\nneutrino self-interactions. We also find no evidence for self-interacting dark\nradiation ($N_{\\rm idr} < 0.134$), early-universe variation of fundamental\nconstants, early dark energy, primordial magnetic fields, or modified\nrecombination. Our data are consistent with standard BBN, the FIRAS-inferred\nCMB temperature, a dark matter component that is collisionless and with only a\nsmall fraction allowed as axion-like particles, a cosmological constant, and\nthe late-time growth rate predicted by general relativity. We find no\nstatistically significant preference for a departure from the baseline\n$\\Lambda$CDM model. In general, models introduced to increase the Hubble\nconstant or to decrease the amplitude of density fluctuations inferred from the\nprimary CMB are not favored by our data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use new cosmic microwave background (CMB) primary temperature and\npolarization anisotropy measurements from the Atacama Cosmology Telescope (ACT)\nData Release 6 (DR6) to test foundational assumptions of the standard\ncosmological model and set constraints on extensions to it. We derive\nconstraints from the ACT DR6 power spectra alone, as well as in combination\nwith legacy data from Planck. To break geometric degeneracies, we include ACT\nand Planck CMB lensing data and baryon acoustic oscillation data from DESI\nYear-1, and further add supernovae measurements from Pantheon+ for models that\naffect the late-time expansion history. We verify the near-scale-invariance\n(running of the spectral index $d n_s/d\\ln k = 0.0062 \\pm 0.0052$) and\nadiabaticity of the primordial perturbations. Neutrino properties are\nconsistent with Standard Model predictions: we find no evidence for new light,\nrelativistic species that are free-streaming ($N_{\\rm eff} = 2.86 \\pm 0.13$,\nwhich combined with external BBN data becomes $N_{\\rm eff} = 2.89 \\pm 0.11$),\nfor non-zero neutrino masses ($\\sum m_\\nu < 0.082$ eV at 95% CL), or for\nneutrino self-interactions. We also find no evidence for self-interacting dark\nradiation ($N_{\\rm idr} < 0.134$), early-universe variation of fundamental\nconstants, early dark energy, primordial magnetic fields, or modified\nrecombination. Our data are consistent with standard BBN, the FIRAS-inferred\nCMB temperature, a dark matter component that is collisionless and with only a\nsmall fraction allowed as axion-like particles, a cosmological constant, and\nthe late-time growth rate predicted by general relativity. We find no\nstatistically significant preference for a departure from the baseline\n$\\Lambda$CDM model. In general, models introduced to increase the Hubble\nconstant or to decrease the amplitude of density fluctuations inferred from the\nprimary CMB are not favored by our data."
                },
                "authors": [
                    {
                        "name": "Erminia Calabrese"
                    },
                    {
                        "name": "J. Colin Hill"
                    },
                    {
                        "name": "Hidde T. Jense"
                    },
                    {
                        "name": "Adrien La Posta"
                    },
                    {
                        "name": "Irene Abril-Cabezas"
                    },
                    {
                        "name": "Graeme E. Addison"
                    },
                    {
                        "name": "Peter A. R. Ade"
                    },
                    {
                        "name": "Simone Aiola"
                    },
                    {
                        "name": "Tommy Alford"
                    },
                    {
                        "name": "David Alonso"
                    },
                    {
                        "name": "Mandana Amiri"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Zachary Atkins"
                    },
                    {
                        "name": "Jason E. Austermann"
                    },
                    {
                        "name": "Eleonora Barbavara"
                    },
                    {
                        "name": "Nicola Barbieri"
                    },
                    {
                        "name": "Nicholas Battaglia"
                    },
                    {
                        "name": "Elia Stefano Battistelli"
                    },
                    {
                        "name": "James A. Beall"
                    },
                    {
                        "name": "Rachel Bean"
                    },
                    {
                        "name": "Ali Beheshti"
                    },
                    {
                        "name": "Benjamin Beringue"
                    },
                    {
                        "name": "Tanay Bhandarkar"
                    },
                    {
                        "name": "Emily Biermann"
                    },
                    {
                        "name": "Boris Bolliet"
                    },
                    {
                        "name": "J Richard Bond"
                    },
                    {
                        "name": "Valentina Capalbo"
                    },
                    {
                        "name": "Felipe Carrero"
                    },
                    {
                        "name": "Stephen Chen"
                    },
                    {
                        "name": "Grace Chesmore"
                    },
                    {
                        "name": "Hsiao-mei Cho"
                    },
                    {
                        "name": "Steve K. Choi"
                    },
                    {
                        "name": "Susan E. Clark"
                    },
                    {
                        "name": "Nicholas F. Cothard"
                    },
                    {
                        "name": "Kevin Coughlin"
                    },
                    {
                        "name": "William Coulton"
                    },
                    {
                        "name": "Devin Crichton"
                    },
                    {
                        "name": "Kevin T. Crowley"
                    },
                    {
                        "name": "Omar Darwish"
                    },
                    {
                        "name": "Mark J. Devlin"
                    },
                    {
                        "name": "Simon Dicker"
                    },
                    {
                        "name": "Cody J. Duell"
                    },
                    {
                        "name": "Shannon M. Duff"
                    },
                    {
                        "name": "Adriaan J. Duivenvoorden"
                    },
                    {
                        "name": "Jo Dunkley"
                    },
                    {
                        "name": "Rolando Dunner"
                    },
                    {
                        "name": "Carmen Embil Villagra"
                    },
                    {
                        "name": "Max Fankhanel"
                    },
                    {
                        "name": "Gerrit S. Farren"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Allen Foster"
                    },
                    {
                        "name": "Rodrigo Freundt"
                    },
                    {
                        "name": "Brittany Fuzia"
                    },
                    {
                        "name": "Patricio A. Gallardo"
                    },
                    {
                        "name": "Xavier Garrido"
                    },
                    {
                        "name": "Martina Gerbino"
                    },
                    {
                        "name": "Serena Giardiello"
                    },
                    {
                        "name": "Ajay Gill"
                    },
                    {
                        "name": "Jahmour Givans"
                    },
                    {
                        "name": "Vera Gluscevic"
                    },
                    {
                        "name": "Samuel Goldstein"
                    },
                    {
                        "name": "Joseph E. Golec"
                    },
                    {
                        "name": "Yulin Gong"
                    },
                    {
                        "name": "Yilun Guan"
                    },
                    {
                        "name": "Mark Halpern"
                    },
                    {
                        "name": "Ian Harrison"
                    },
                    {
                        "name": "Matthew Hasselfield"
                    },
                    {
                        "name": "Adam He"
                    },
                    {
                        "name": "Erin Healy"
                    },
                    {
                        "name": "Shawn Henderson"
                    },
                    {
                        "name": "Brandon Hensley"
                    },
                    {
                        "name": "Carlos Hervías-Caimapo"
                    },
                    {
                        "name": "Gene C. Hilton"
                    },
                    {
                        "name": "Matt Hilton"
                    },
                    {
                        "name": "Adam D. Hincks"
                    },
                    {
                        "name": "Renée Hložek"
                    },
                    {
                        "name": "Shuay-Pwu Patty Ho"
                    },
                    {
                        "name": "John Hood"
                    },
                    {
                        "name": "Erika Hornecker"
                    },
                    {
                        "name": "Zachary B. Huber"
                    },
                    {
                        "name": "Johannes Hubmayr"
                    },
                    {
                        "name": "Kevin M. Huffenberger"
                    },
                    {
                        "name": "John P. Hughes"
                    },
                    {
                        "name": "Margaret Ikape"
                    },
                    {
                        "name": "Kent Irwin"
                    },
                    {
                        "name": "Giovanni Isopi"
                    },
                    {
                        "name": "Neha Joshi"
                    },
                    {
                        "name": "Ben Keller"
                    },
                    {
                        "name": "Joshua Kim"
                    },
                    {
                        "name": "Kenda Knowles"
                    },
                    {
                        "name": "Brian J. Koopman"
                    },
                    {
                        "name": "Arthur Kosowsky"
                    },
                    {
                        "name": "Darby Kramer"
                    },
                    {
                        "name": "Aleksandra Kusiak"
                    },
                    {
                        "name": "Alex Lague"
                    },
                    {
                        "name": "Victoria Lakey"
                    },
                    {
                        "name": "Massimiliano Lattanzi"
                    },
                    {
                        "name": "Eunseong Lee"
                    },
                    {
                        "name": "Yaqiong Li"
                    },
                    {
                        "name": "Zack Li"
                    },
                    {
                        "name": "Michele Limon"
                    },
                    {
                        "name": "Martine Lokken"
                    },
                    {
                        "name": "Thibaut Louis"
                    },
                    {
                        "name": "Marius Lungu"
                    },
                    {
                        "name": "Niall MacCrann"
                    },
                    {
                        "name": "Amanda MacInnis"
                    },
                    {
                        "name": "Mathew S. Madhavacheril"
                    },
                    {
                        "name": "Diego Maldonado"
                    },
                    {
                        "name": "Felipe Maldonado"
                    },
                    {
                        "name": "Maya Mallaby-Kay"
                    },
                    {
                        "name": "Gabriela A. Marques"
                    },
                    {
                        "name": "Joshiwa van Marrewijk"
                    },
                    {
                        "name": "Fiona McCarthy"
                    },
                    {
                        "name": "Jeff McMahon"
                    },
                    {
                        "name": "Yogesh Mehta"
                    },
                    {
                        "name": "Felipe Menanteau"
                    },
                    {
                        "name": "Kavilan Moodley"
                    },
                    {
                        "name": "Thomas W. Morris"
                    },
                    {
                        "name": "Tony Mroczkowski"
                    },
                    {
                        "name": "Sigurd Naess"
                    },
                    {
                        "name": "Toshiya Namikawa"
                    },
                    {
                        "name": "Federico Nati"
                    },
                    {
                        "name": "Simran K. Nerval"
                    },
                    {
                        "name": "Laura Newburgh"
                    },
                    {
                        "name": "Andrina Nicola"
                    },
                    {
                        "name": "Michael D. Niemack"
                    },
                    {
                        "name": "Michael R. Nolta"
                    },
                    {
                        "name": "John Orlowski-Scherer"
                    },
                    {
                        "name": "Luca Pagano"
                    },
                    {
                        "name": "Lyman A. Page"
                    },
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "Bruce Partridge"
                    },
                    {
                        "name": "Karen Perez Sarmiento"
                    },
                    {
                        "name": "Heather Prince"
                    },
                    {
                        "name": "Roberto Puddu"
                    },
                    {
                        "name": "Frank J. Qu"
                    },
                    {
                        "name": "Damien C. Ragavan"
                    },
                    {
                        "name": "Bernardita Ried Guachalla"
                    },
                    {
                        "name": "Keir K. Rogers"
                    },
                    {
                        "name": "Felipe Rojas"
                    },
                    {
                        "name": "Tai Sakuma"
                    },
                    {
                        "name": "Emmanuel Schaan"
                    },
                    {
                        "name": "Benjamin L. Schmitt"
                    },
                    {
                        "name": "Neelima Sehgal"
                    },
                    {
                        "name": "Shabbir Shaikh"
                    },
                    {
                        "name": "Blake D. Sherwin"
                    },
                    {
                        "name": "Carlos Sierra"
                    },
                    {
                        "name": "Jon Sievers"
                    },
                    {
                        "name": "Cristóbal Sifón"
                    },
                    {
                        "name": "Sara Simon"
                    },
                    {
                        "name": "Rita Sonka"
                    },
                    {
                        "name": "David N. Spergel"
                    },
                    {
                        "name": "Suzanne T. Staggs"
                    },
                    {
                        "name": "Emilie Storer"
                    },
                    {
                        "name": "Kristen Surrao"
                    },
                    {
                        "name": "Eric R. Switzer"
                    },
                    {
                        "name": "Niklas Tampier"
                    },
                    {
                        "name": "Leander Thiele"
                    },
                    {
                        "name": "Robert Thornton"
                    },
                    {
                        "name": "Hy Trac"
                    },
                    {
                        "name": "Carole Tucker"
                    },
                    {
                        "name": "Joel Ullom"
                    },
                    {
                        "name": "Leila R. Vale"
                    },
                    {
                        "name": "Alexander Van Engelen"
                    },
                    {
                        "name": "Jeff Van Lanen"
                    },
                    {
                        "name": "Cristian Vargas"
                    },
                    {
                        "name": "Eve M. Vavagiakis"
                    },
                    {
                        "name": "Kasey Wagoner"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Lukas Wenzl"
                    },
                    {
                        "name": "Edward J. Wollack"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Kaiwen Zheng"
                },
                "author": "Kaiwen Zheng",
                "arxiv_comment": "56+32 pages, 46+9 figures, abstract abridged here. Part of ACT DR6\n  suite of papers submitted to JCAP. Data located at\n  https://lambda.gsfc.nasa.gov/product/act/actadv_prod_table.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14445v1",
                "updated": "2025-03-18T17:24:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    24,
                    19,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:24:19Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    24,
                    19,
                    1,
                    77,
                    0
                ],
                "title": "Bolt3D: Generating 3D Scenes in Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bolt3D: Generating 3D Scenes in Seconds"
                },
                "summary": "We present a latent diffusion model for fast feed-forward 3D scene\ngeneration. Given one or more images, our model Bolt3D directly samples a 3D\nscene representation in less than seven seconds on a single GPU. We achieve\nthis by leveraging powerful and scalable existing 2D diffusion network\narchitectures to produce consistent high-fidelity 3D scene representations. To\ntrain this model, we create a large-scale multiview-consistent dataset of 3D\ngeometry and appearance by applying state-of-the-art dense 3D reconstruction\ntechniques to existing multiview image datasets. Compared to prior multiview\ngenerative models that require per-scene optimization for 3D reconstruction,\nBolt3D reduces the inference cost by a factor of up to 300 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a latent diffusion model for fast feed-forward 3D scene\ngeneration. Given one or more images, our model Bolt3D directly samples a 3D\nscene representation in less than seven seconds on a single GPU. We achieve\nthis by leveraging powerful and scalable existing 2D diffusion network\narchitectures to produce consistent high-fidelity 3D scene representations. To\ntrain this model, we create a large-scale multiview-consistent dataset of 3D\ngeometry and appearance by applying state-of-the-art dense 3D reconstruction\ntechniques to existing multiview image datasets. Compared to prior multiview\ngenerative models that require per-scene optimization for 3D reconstruction,\nBolt3D reduces the inference cost by a factor of up to 300 times."
                },
                "authors": [
                    {
                        "name": "Stanislaw Szymanowicz"
                    },
                    {
                        "name": "Jason Y. Zhang"
                    },
                    {
                        "name": "Pratul Srinivasan"
                    },
                    {
                        "name": "Ruiqi Gao"
                    },
                    {
                        "name": "Arthur Brussee"
                    },
                    {
                        "name": "Aleksander Holynski"
                    },
                    {
                        "name": "Ricardo Martin-Brualla"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Philipp Henzler"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Henzler"
                },
                "author": "Philipp Henzler",
                "arxiv_comment": "Project page: https://szymanowiczs.github.io/bolt3d",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14443v1",
                "updated": "2025-03-18T17:19:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    19,
                    12,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:19:12Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    19,
                    12,
                    1,
                    77,
                    0
                ],
                "title": "EnvBench: A Benchmark for Automated Environment Setup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnvBench: A Benchmark for Automated Environment Setup"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench."
                },
                "authors": [
                    {
                        "name": "Aleksandra Eliseeva"
                    },
                    {
                        "name": "Alexander Kovrigin"
                    },
                    {
                        "name": "Ilia Kholkin"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    }
                ],
                "author_detail": {
                    "name": "Yaroslav Zharov"
                },
                "author": "Yaroslav Zharov",
                "arxiv_comment": "Accepted at the DL4Code workshop at ICLR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15378v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15378v5",
                "updated": "2025-03-18T17:14:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    14,
                    43,
                    1,
                    77,
                    0
                ],
                "published": "2024-01-27T10:50:11Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    10,
                    50,
                    11,
                    5,
                    27,
                    0
                ],
                "title": "A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM"
                },
                "summary": "Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given."
                },
                "authors": [
                    {
                        "name": "Ahmet Yusuf Alan"
                    },
                    {
                        "name": "Enis Karaarslan"
                    },
                    {
                        "name": "Ömer Aydin"
                    }
                ],
                "author_detail": {
                    "name": "Ömer Aydin"
                },
                "author": "Ömer Aydin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15378v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15378v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14434v1",
                "updated": "2025-03-18T17:11:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    11,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:11:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    11,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers"
                },
                "summary": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Nikhil Abhyankar"
                    },
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14432v1",
                "updated": "2025-03-18T17:09:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    9,
                    57,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:09:57Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    9,
                    57,
                    1,
                    77,
                    0
                ],
                "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play"
                },
                "summary": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration."
                },
                "authors": [
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Kaizhi Qian"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18841v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18841v4",
                "updated": "2025-03-18T16:57:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    57,
                    17,
                    1,
                    77,
                    0
                ],
                "published": "2024-05-14T15:03:05Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    15,
                    3,
                    5,
                    1,
                    135,
                    0
                ],
                "title": "Navigating LLM Ethics: Advancements, Challenges, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating LLM Ethics: Advancements, Challenges, and Future Directions"
                },
                "summary": "This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Connor Phillips"
                    }
                ],
                "author_detail": {
                    "name": "Connor Phillips"
                },
                "author": "Connor Phillips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18841v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18841v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14411v1",
                "updated": "2025-03-18T16:50:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    50,
                    10,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:50:10Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    50,
                    10,
                    1,
                    77,
                    0
                ],
                "title": "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models"
                },
                "summary": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness."
                },
                "authors": [
                    {
                        "name": "Siwei Zhang"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Yateng Tang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zian Jia"
                    },
                    {
                        "name": "Zehao Gu"
                    },
                    {
                        "name": "Jiarong Xu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "arxiv_comment": "Submit to ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14409v1",
                "updated": "2025-03-18T16:49:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    49,
                    56,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    49,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Inference and Learning of Nonlinear LFR State-space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference and Learning of Nonlinear LFR State-space Models"
                },
                "summary": "Estimating the parameters of nonlinear block-oriented state-space models from\ninput-output data typically involves solving a highly non-convex optimization\nproblem, making it susceptible to poor local minima and slow convergence. This\npaper presents a computationally efficient initialization method for fully\nparametrizing nonlinear linear fractional representation (NL-LFR) models using\nperiodic data. The approach first infers the latent variables and then\nestimates the model parameters, yielding initial estimates that serve as a\nstarting point for further nonlinear optimization. The proposed method shows\nrobustness against poor local minima, and achieves a twofold error reduction\ncompared to the state-of-the-art on a challenging benchmark dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the parameters of nonlinear block-oriented state-space models from\ninput-output data typically involves solving a highly non-convex optimization\nproblem, making it susceptible to poor local minima and slow convergence. This\npaper presents a computationally efficient initialization method for fully\nparametrizing nonlinear linear fractional representation (NL-LFR) models using\nperiodic data. The approach first infers the latent variables and then\nestimates the model parameters, yielding initial estimates that serve as a\nstarting point for further nonlinear optimization. The proposed method shows\nrobustness against poor local minima, and achieves a twofold error reduction\ncompared to the state-of-the-art on a challenging benchmark dataset."
                },
                "authors": [
                    {
                        "name": "Merijn Floren"
                    },
                    {
                        "name": "Jean-Philippe Noël"
                    },
                    {
                        "name": "Jan Swevers"
                    }
                ],
                "author_detail": {
                    "name": "Jan Swevers"
                },
                "author": "Jan Swevers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02063v3",
                "updated": "2025-03-18T16:45:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    45,
                    54,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-03T19:16:36Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    19,
                    16,
                    36,
                    4,
                    3,
                    0
                ],
                "title": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models"
                },
                "summary": "This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.18842,\n  arXiv:2501.00959",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00959v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00959v3",
                "updated": "2025-03-18T16:44:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    44,
                    15,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-01T21:31:47Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    31,
                    47,
                    2,
                    1,
                    0
                ],
                "title": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs"
                },
                "summary": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00959v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00959v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18842v3",
                "updated": "2025-03-18T16:42:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    42,
                    30,
                    1,
                    77,
                    0
                ],
                "published": "2024-05-26T15:28:24Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    15,
                    28,
                    24,
                    6,
                    147,
                    0
                ],
                "title": "The global landscape of academic guidelines for generative AI and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global landscape of academic guidelines for generative AI and Large\n  Language Models"
                },
                "summary": "The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09292v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09292v3",
                "updated": "2025-03-18T16:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    42,
                    17,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-16T04:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    56,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    }
                ],
                "author_detail": {
                    "name": "Kaustubh D. Dhole"
                },
                "author": "Kaustubh D. Dhole",
                "arxiv_comment": "1st workshop of \"Quantify Uncertainty and Hallucination in Foundation\n  Models: The Next Frontier in Reliable AI\" at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09292v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09292v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16970v3",
                "updated": "2025-03-18T16:34:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    34,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-24T03:32:05Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    3,
                    32,
                    5,
                    2,
                    206,
                    0
                ],
                "title": "Towards Aligning Language Models with Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Aligning Language Models with Textual Feedback"
                },
                "summary": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback."
                },
                "authors": [
                    {
                        "name": "Saüc Abadal Lloret"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14392v1",
                "updated": "2025-03-18T16:27:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    27,
                    1,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:27:01Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    27,
                    1,
                    1,
                    77,
                    0
                ],
                "title": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to\n  Enhance Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to\n  Enhance Large Language Models"
                },
                "summary": "This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems."
                },
                "authors": [
                    {
                        "name": "Qiantong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qiantong Wang"
                },
                "author": "Qiantong Wang",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14391v1",
                "updated": "2025-03-18T16:26:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    26,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:26:29Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    26,
                    29,
                    1,
                    77,
                    0
                ],
                "title": "How much do LLMs learn from negative examples?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much do LLMs learn from negative examples?"
                },
                "summary": "Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs."
                },
                "authors": [
                    {
                        "name": "Shadi Hamdan"
                    },
                    {
                        "name": "Deniz Yuret"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Yuret"
                },
                "author": "Deniz Yuret",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14382v1",
                "updated": "2025-03-18T16:15:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    15,
                    55,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:15:55Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    15,
                    55,
                    1,
                    77,
                    0
                ],
                "title": "Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation"
                },
                "summary": "The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions."
                },
                "authors": [
                    {
                        "name": "Rikuto Tsuchida"
                    },
                    {
                        "name": "Hibiki Yokoyama"
                    },
                    {
                        "name": "Takehito Utsuro"
                    }
                ],
                "author_detail": {
                    "name": "Takehito Utsuro"
                },
                "author": "Takehito Utsuro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10616v2",
                "updated": "2025-03-18T16:12:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    12,
                    19,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-13T17:56:10Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    56,
                    10,
                    3,
                    72,
                    0
                ],
                "title": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with\n  Transformer"
                },
                "summary": "Open-vocabulary multiple object tracking aims to generalize trackers to\nunseen categories during training, enabling their application across a variety\nof real-world scenarios. However, the existing open-vocabulary tracker is\nconstrained by its framework structure, isolated frame-level perception, and\ninsufficient modal interactions, which hinder its performance in\nopen-vocabulary classification and tracking. In this paper, we propose OVTR\n(End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the\nfirst end-to-end open-vocabulary tracker that models motion, appearance, and\ncategory simultaneously. To achieve stable classification and continuous\ntracking, we design the CIP (Category Information Propagation) strategy, which\nestablishes multiple high-level category information priors for subsequent\nframes. Additionally, we introduce a dual-branch structure for generalization\ncapability and deep multimodal interaction, and incorporate protective\nstrategies in the decoder to enhance performance. Experimental results show\nthat our method surpasses previous trackers on the open-vocabulary MOT\nbenchmark while also achieving faster inference speeds and significantly\nreducing preprocessing requirements. Moreover, the experiment transferring the\nmodel to another dataset demonstrates its strong adaptability. Models and code\nare released at https://github.com/jinyanglii/OVTR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary multiple object tracking aims to generalize trackers to\nunseen categories during training, enabling their application across a variety\nof real-world scenarios. However, the existing open-vocabulary tracker is\nconstrained by its framework structure, isolated frame-level perception, and\ninsufficient modal interactions, which hinder its performance in\nopen-vocabulary classification and tracking. In this paper, we propose OVTR\n(End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the\nfirst end-to-end open-vocabulary tracker that models motion, appearance, and\ncategory simultaneously. To achieve stable classification and continuous\ntracking, we design the CIP (Category Information Propagation) strategy, which\nestablishes multiple high-level category information priors for subsequent\nframes. Additionally, we introduce a dual-branch structure for generalization\ncapability and deep multimodal interaction, and incorporate protective\nstrategies in the decoder to enhance performance. Experimental results show\nthat our method surpasses previous trackers on the open-vocabulary MOT\nbenchmark while also achieving faster inference speeds and significantly\nreducing preprocessing requirements. Moreover, the experiment transferring the\nmodel to another dataset demonstrates its strong adaptability. Models and code\nare released at https://github.com/jinyanglii/OVTR."
                },
                "authors": [
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Wenbing Tao"
                    }
                ],
                "author_detail": {
                    "name": "Wenbing Tao"
                },
                "author": "Wenbing Tao",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14379v1",
                "updated": "2025-03-18T16:12:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    12,
                    4,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:12:04Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    12,
                    4,
                    1,
                    77,
                    0
                ],
                "title": "On the Standard Performance Criteria for Applied Control Design: PID,\n  MPC or Machine Learning Controller?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Standard Performance Criteria for Applied Control Design: PID,\n  MPC or Machine Learning Controller?"
                },
                "summary": "The traditional control theory and its application to basic and complex\nsystems have reached an advanced level of maturity. This includes aerial,\nmarine, and ground vehicles, as well as robotics, chemical, transportation, and\nelectrical systems widely used in our daily lives. The emerging era of\ndata-driven methods, Large Language Models (LLMs), and AI-based controllers\ndoes not indicate a weakness in well-established control theory. Instead, it\naims to reduce dependence on models and uncertainties, address increasingly\ncomplex systems, and potentially achieve decision-making capabilities\ncomparable to human-level performance. This revolution integrates knowledge\nfrom computer science, machine learning, biology, and classical control,\nproducing promising algorithms that are yet to demonstrate widespread\nreal-world applicability. Despite the maturity of control theory and the\npresence of various performance criteria, there is still a lack of standardised\nmetrics for testing, evaluation, Verification and Validation ($V\\&V$) of\nalgorithms. This gap can lead to algorithms that, while optimal in certain\naspects, may fall short of practical implementation, sparking debates within\nthe literature. For a controller to succeed in real-world applications, it must\nsatisfy three key categories of performance metrics: tracking quality, control\neffort (energy consumption), and robustness. This paper rather takes an applied\nperspective, proposing and consolidating standard performance criteria for\ntesting and analysing control systems, intended for researchers and students.\nThe proposed framework ensures the post-design applicability of a black-box\nalgorithm, aligning with modern data analysis and $V\\&V$ perspectives to\nprevent resource allocation to systems with limited impact or imprecise claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The traditional control theory and its application to basic and complex\nsystems have reached an advanced level of maturity. This includes aerial,\nmarine, and ground vehicles, as well as robotics, chemical, transportation, and\nelectrical systems widely used in our daily lives. The emerging era of\ndata-driven methods, Large Language Models (LLMs), and AI-based controllers\ndoes not indicate a weakness in well-established control theory. Instead, it\naims to reduce dependence on models and uncertainties, address increasingly\ncomplex systems, and potentially achieve decision-making capabilities\ncomparable to human-level performance. This revolution integrates knowledge\nfrom computer science, machine learning, biology, and classical control,\nproducing promising algorithms that are yet to demonstrate widespread\nreal-world applicability. Despite the maturity of control theory and the\npresence of various performance criteria, there is still a lack of standardised\nmetrics for testing, evaluation, Verification and Validation ($V\\&V$) of\nalgorithms. This gap can lead to algorithms that, while optimal in certain\naspects, may fall short of practical implementation, sparking debates within\nthe literature. For a controller to succeed in real-world applications, it must\nsatisfy three key categories of performance metrics: tracking quality, control\neffort (energy consumption), and robustness. This paper rather takes an applied\nperspective, proposing and consolidating standard performance criteria for\ntesting and analysing control systems, intended for researchers and students.\nThe proposed framework ensures the post-design applicability of a black-box\nalgorithm, aligning with modern data analysis and $V\\&V$ perspectives to\nprevent resource allocation to systems with limited impact or imprecise claims."
                },
                "authors": [
                    {
                        "name": "Pouria Sarhadi"
                    }
                ],
                "author_detail": {
                    "name": "Pouria Sarhadi"
                },
                "author": "Pouria Sarhadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14378v1",
                "updated": "2025-03-18T16:10:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    10,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:10:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    10,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "Impossible Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impossible Videos"
                },
                "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models."
                },
                "authors": [
                    {
                        "name": "Zechen Bai"
                    },
                    {
                        "name": "Hai Ci"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01250v3",
                "updated": "2025-03-18T16:09:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    9,
                    20,
                    1,
                    77,
                    0
                ],
                "published": "2024-12-02T08:16:38Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    16,
                    38,
                    0,
                    337,
                    0
                ],
                "title": "Collaborative Instance Object Navigation: Leveraging\n  Uncertainty-Awareness to Minimize Human-Agent Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Instance Object Navigation: Leveraging\n  Uncertainty-Awareness to Minimize Human-Agent Dialogues"
                },
                "summary": "Language-driven instance object navigation assumes that human users initiate\nthe task by providing a detailed description of the target instance to the\nembodied agent. While this description is crucial for distinguishing the target\nfrom visually similar instances in a scene, providing it prior to navigation\ncan be demanding for human. To bridge this gap, we introduce Collaborative\nInstance object Navigation (CoIN), a new task setting where the agent actively\nresolve uncertainties about the target instance during navigation in natural,\ntemplate-free, open-ended dialogues with human. We propose a novel\ntraining-free method, Agent-user Interaction with UncerTainty Awareness\n(AIUTA), which operates independently from the navigation policy, and focuses\non the human-agent interaction reasoning with Vision-Language Models (VLMs) and\nLarge Language Models (LLMs). First, upon object detection, a Self-Questioner\nmodel initiates a self-dialogue within the agent to obtain a complete and\naccurate observation description with a novel uncertainty estimation technique.\nThen, an Interaction Trigger module determines whether to ask a question to the\nhuman, continue or halt navigation, minimizing user input. For evaluation, we\nintroduce CoIN-Bench, with a curated dataset designed for challenging\nmulti-instance scenarios. CoIN-Bench supports both online evaluation with\nhumans and reproducible experiments with simulated user-agent interactions. On\nCoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing\nlanguage-driven instance navigation methods struggle in complex multi-instance\nscenes. Code and benchmark will be available upon acceptance at\nhttps://intelligolabs.github.io/CoIN/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-driven instance object navigation assumes that human users initiate\nthe task by providing a detailed description of the target instance to the\nembodied agent. While this description is crucial for distinguishing the target\nfrom visually similar instances in a scene, providing it prior to navigation\ncan be demanding for human. To bridge this gap, we introduce Collaborative\nInstance object Navigation (CoIN), a new task setting where the agent actively\nresolve uncertainties about the target instance during navigation in natural,\ntemplate-free, open-ended dialogues with human. We propose a novel\ntraining-free method, Agent-user Interaction with UncerTainty Awareness\n(AIUTA), which operates independently from the navigation policy, and focuses\non the human-agent interaction reasoning with Vision-Language Models (VLMs) and\nLarge Language Models (LLMs). First, upon object detection, a Self-Questioner\nmodel initiates a self-dialogue within the agent to obtain a complete and\naccurate observation description with a novel uncertainty estimation technique.\nThen, an Interaction Trigger module determines whether to ask a question to the\nhuman, continue or halt navigation, minimizing user input. For evaluation, we\nintroduce CoIN-Bench, with a curated dataset designed for challenging\nmulti-instance scenarios. CoIN-Bench supports both online evaluation with\nhumans and reproducible experiments with simulated user-agent interactions. On\nCoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing\nlanguage-driven instance navigation methods struggle in complex multi-instance\nscenes. Code and benchmark will be available upon acceptance at\nhttps://intelligolabs.github.io/CoIN/"
                },
                "authors": [
                    {
                        "name": "Francesco Taioli"
                    },
                    {
                        "name": "Edoardo Zorzi"
                    },
                    {
                        "name": "Gianni Franchi"
                    },
                    {
                        "name": "Alberto Castellini"
                    },
                    {
                        "name": "Alessandro Farinelli"
                    },
                    {
                        "name": "Marco Cristani"
                    },
                    {
                        "name": "Yiming Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Wang"
                },
                "author": "Yiming Wang",
                "arxiv_comment": "https://intelligolabs.github.io/CoIN/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07917v2",
                "updated": "2025-03-18T15:49:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    49,
                    8,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-12T16:49:51Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    16,
                    49,
                    51,
                    1,
                    317,
                    0
                ],
                "title": "CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts"
                },
                "summary": "The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Updated and Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14353v1",
                "updated": "2025-03-18T15:36:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    36,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T15:36:36Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    36,
                    36,
                    1,
                    77,
                    0
                ],
                "title": "Unified Analysis of Decentralized Gradient Descent: a Contraction\n  Mapping Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Analysis of Decentralized Gradient Descent: a Contraction\n  Mapping Framework"
                },
                "summary": "The decentralized gradient descent (DGD) algorithm, and its sibling,\ndiffusion, are workhorses in decentralized machine learning, distributed\ninference and estimation, and multi-agent coordination. We propose a novel,\nprincipled framework for the analysis of DGD and diffusion for strongly convex,\nsmooth objectives, and arbitrary undirected topologies, using contraction\nmappings coupled with a result called the mean Hessian theorem (MHT). The use\nof these tools yields tight convergence bounds, both in the noise-free and\nnoisy regimes. While these bounds are qualitatively similar to results found in\nthe literature, our approach using contractions together with the MHT decouples\nthe algorithm dynamics (how quickly the algorithm converges to its fixed point)\nfrom its asymptotic convergence properties (how far the fixed point is from the\nglobal optimum). This yields a simple, intuitive analysis that is accessible to\na broader audience. Extensions are provided to multiple local gradient updates,\ntime-varying step sizes, noisy gradients (stochastic DGD and diffusion),\ncommunication noise, and random topologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The decentralized gradient descent (DGD) algorithm, and its sibling,\ndiffusion, are workhorses in decentralized machine learning, distributed\ninference and estimation, and multi-agent coordination. We propose a novel,\nprincipled framework for the analysis of DGD and diffusion for strongly convex,\nsmooth objectives, and arbitrary undirected topologies, using contraction\nmappings coupled with a result called the mean Hessian theorem (MHT). The use\nof these tools yields tight convergence bounds, both in the noise-free and\nnoisy regimes. While these bounds are qualitatively similar to results found in\nthe literature, our approach using contractions together with the MHT decouples\nthe algorithm dynamics (how quickly the algorithm converges to its fixed point)\nfrom its asymptotic convergence properties (how far the fixed point is from the\nglobal optimum). This yields a simple, intuitive analysis that is accessible to\na broader audience. Extensions are provided to multiple local gradient updates,\ntime-varying step sizes, noisy gradients (stochastic DGD and diffusion),\ncommunication noise, and random topologies."
                },
                "authors": [
                    {
                        "name": "Erik G. Larsson"
                    },
                    {
                        "name": "Nicolo Michelusi"
                    }
                ],
                "author_detail": {
                    "name": "Nicolo Michelusi"
                },
                "author": "Nicolo Michelusi",
                "arxiv_comment": "submitted to the IEEE Open Journal of Signal Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06946v2",
                "updated": "2025-03-18T15:36:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    36,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-11T12:54:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    54,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models"
                },
                "summary": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Updated and Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01023v2",
                "updated": "2025-03-18T15:30:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    30,
                    22,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-02T02:51:16Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    51,
                    16,
                    3,
                    2,
                    0
                ],
                "title": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo\n  Matching Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo\n  Matching Transformer"
                },
                "summary": "In light of the advancements in transformer technology, extant research\nposits the construction of stereo transformers as a potential solution to the\nbinocular stereo matching challenge. However, constrained by the low-rank\nbottleneck and quadratic complexity of attention mechanisms, stereo\ntransformers still fail to demonstrate sufficient nonlinear expressiveness\nwithin a reasonable inference time. The lack of focus on key homonymous points\nrenders the representations of such methods vulnerable to challenging\nconditions, including reflections and weak textures. Furthermore, a slow\ncomputing speed is not conducive to the application. To overcome these\ndifficulties, we present the Hadamard Attention Recurrent Stereo Transformer\n(HART) that incorporates the following components: 1) For faster inference, we\npresent a Hadamard product paradigm for the attention mechanism, achieving\nlinear computational complexity. 2) We designed a Dense Attention Kernel (DAK)\nto amplify the differences between relevant and irrelevant feature responses.\nThis allows HART to focus on important details. DAK also converts zero elements\nto non-zero elements to mitigate the reduced expressiveness caused by the\nlow-rank bottleneck. 3) To compensate for the spatial and channel interaction\nmissing in the Hadamard product, we propose MKOI to capture both global and\nlocal information through the interleaving of large and small kernel\nconvolutions. Experimental results demonstrate the effectiveness of our HART.\nIn reflective area, HART ranked 1st on the KITTI 2012 benchmark among all\npublished methods at the time of submission. Code is available at\nhttps://github.com/ZYangChen/HART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the advancements in transformer technology, extant research\nposits the construction of stereo transformers as a potential solution to the\nbinocular stereo matching challenge. However, constrained by the low-rank\nbottleneck and quadratic complexity of attention mechanisms, stereo\ntransformers still fail to demonstrate sufficient nonlinear expressiveness\nwithin a reasonable inference time. The lack of focus on key homonymous points\nrenders the representations of such methods vulnerable to challenging\nconditions, including reflections and weak textures. Furthermore, a slow\ncomputing speed is not conducive to the application. To overcome these\ndifficulties, we present the Hadamard Attention Recurrent Stereo Transformer\n(HART) that incorporates the following components: 1) For faster inference, we\npresent a Hadamard product paradigm for the attention mechanism, achieving\nlinear computational complexity. 2) We designed a Dense Attention Kernel (DAK)\nto amplify the differences between relevant and irrelevant feature responses.\nThis allows HART to focus on important details. DAK also converts zero elements\nto non-zero elements to mitigate the reduced expressiveness caused by the\nlow-rank bottleneck. 3) To compensate for the spatial and channel interaction\nmissing in the Hadamard product, we propose MKOI to capture both global and\nlocal information through the interleaving of large and small kernel\nconvolutions. Experimental results demonstrate the effectiveness of our HART.\nIn reflective area, HART ranked 1st on the KITTI 2012 benchmark among all\npublished methods at the time of submission. Code is available at\nhttps://github.com/ZYangChen/HART."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yongjun Zhang"
                    },
                    {
                        "name": "Wenting Li"
                    },
                    {
                        "name": "Bingshu Wang"
                    },
                    {
                        "name": "Yabo Wu"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "C. L. Philip Chen"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Philip Chen"
                },
                "author": "C. L. Philip Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16657v3",
                "updated": "2025-03-18T15:19:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    19,
                    15,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-25T18:41:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation"
                },
                "summary": "Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples."
                },
                "authors": [
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project website: https://zunwang1.github.io/DreamRunner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14340v1",
                "updated": "2025-03-18T15:16:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    16,
                    51,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T15:16:51Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    16,
                    51,
                    1,
                    77,
                    0
                ],
                "title": "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration"
                },
                "summary": "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks."
                },
                "authors": [
                    {
                        "name": "Yisen Xu"
                    },
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Jinqiu Yang"
                    },
                    {
                        "name": "Tse-Hsun"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Nikolaos Tsantalis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Tsantalis"
                },
                "arxiv_affiliation": "Peter",
                "author": "Nikolaos Tsantalis",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14334v1",
                "updated": "2025-03-18T15:12:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    12,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T15:12:28Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    12,
                    28,
                    1,
                    77,
                    0
                ],
                "title": "Partially Directed Configuration Model with Homophily and\n  Respondent-Driven Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Directed Configuration Model with Homophily and\n  Respondent-Driven Sampling"
                },
                "summary": "Respondent-driven sampling (RDS) is a sampling scheme used in socially\nconnected human populations lacking a sampling frame. One of the first steps to\nmake design-based inferences from RDS data is to estimate the sampling\nprobabilities. A classical approach for such estimation assumes that a\nfirst-order Markov chain over a fully connected and undirected network may\nadequately represent RDS. This convenient model, however, does not reflect that\nthe network may be directed and homophilous. The methods proposed in this work\naim to address this issue. The main methodological contributions of this\nmanuscript are two fold: first, we introduce a partially directed and\nhomophilous network configuration model, and second, we develop two\nmathematical representations of the RDS sampling process over the proposed\nconfiguration model. Our simulation study shows that the resulting sampling\nprobabilities are similar to those of RDS, and they improve the prevalence\nestimation under various realistic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Respondent-driven sampling (RDS) is a sampling scheme used in socially\nconnected human populations lacking a sampling frame. One of the first steps to\nmake design-based inferences from RDS data is to estimate the sampling\nprobabilities. A classical approach for such estimation assumes that a\nfirst-order Markov chain over a fully connected and undirected network may\nadequately represent RDS. This convenient model, however, does not reflect that\nthe network may be directed and homophilous. The methods proposed in this work\naim to address this issue. The main methodological contributions of this\nmanuscript are two fold: first, we introduce a partially directed and\nhomophilous network configuration model, and second, we develop two\nmathematical representations of the RDS sampling process over the proposed\nconfiguration model. Our simulation study shows that the resulting sampling\nprobabilities are similar to those of RDS, and they improve the prevalence\nestimation under various realistic scenarios."
                },
                "authors": [
                    {
                        "name": "Alejandro Sepulveda-Peñaloza"
                    },
                    {
                        "name": "Isabelle S. Beaudry"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle S. Beaudry"
                },
                "author": "Isabelle S. Beaudry",
                "arxiv_comment": "This paper contains 25 pages and 4 figures. This work has not yet\n  been submitted for publication elsewhere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14506v2",
                "updated": "2025-03-18T15:03:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    3,
                    47,
                    1,
                    77,
                    0
                ],
                "published": "2024-09-22T16:10:10Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    10,
                    10,
                    6,
                    266,
                    0
                ],
                "title": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy"
                },
                "summary": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot (HSR). Our method achieves a 93%\nsuccess rate in the 'fetch me' task completion with failure recovery,\nhighlighting its capability in both failure reasoning and task planning.\nInteLiPlan achieves comparable performance to state-of-the-art large-scale\nLLM-based robotics planners, while using only real-time onboard computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot (HSR). Our method achieves a 93%\nsuccess rate in the 'fetch me' task completion with failure recovery,\nhighlighting its capability in both failure reasoning and task planning.\nInteLiPlan achieves comparable performance to state-of-the-art large-scale\nLLM-based robotics planners, while using only real-time onboard computing."
                },
                "authors": [
                    {
                        "name": "Kim Tien Ly"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Ioannis Havoutis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Havoutis"
                },
                "author": "Ioannis Havoutis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14329v2",
                "updated": "2025-03-19T08:55:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    55,
                    21,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T15:01:47Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    1,
                    47,
                    1,
                    77,
                    0
                ],
                "title": "EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference\n  Alignment"
                },
                "summary": "Dexterous robotic hands often struggle to generalize effectively in complex\nenvironments due to the limitations of models trained on low-diversity data.\nHowever, the real world presents an inherently unbounded range of scenarios,\nmaking it impractical to account for every possible variation. A natural\nsolution is to enable robots learning from experience in complex environments,\nan approach akin to evolution, where systems improve through continuous\nfeedback, learning from both failures and successes, and iterating toward\noptimal performance. Motivated by this, we propose EvolvingGrasp, an\nevolutionary grasp generation method that continuously enhances grasping\nperformance through efficient preference alignment. Specifically, we introduce\nHandpose wise Preference Optimization (HPO), which allows the model to\ncontinuously align with preferences from both positive and negative feedback\nwhile progressively refining its grasping strategies. To further enhance\nefficiency and reliability during online adjustments, we incorporate a\nPhysics-aware Consistency Model within HPO, which accelerates inference,\nreduces the number of timesteps needed for preference finetuning, and ensures\nphysical plausibility throughout the process. Extensive experiments across four\nbenchmark datasets demonstrate state of the art performance of our method in\ngrasp success rate and sampling efficiency. Our results validate that\nEvolvingGrasp enables evolutionary grasp generation, ensuring robust,\nphysically feasible, and preference-aligned grasping in both simulation and\nreal scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous robotic hands often struggle to generalize effectively in complex\nenvironments due to the limitations of models trained on low-diversity data.\nHowever, the real world presents an inherently unbounded range of scenarios,\nmaking it impractical to account for every possible variation. A natural\nsolution is to enable robots learning from experience in complex environments,\nan approach akin to evolution, where systems improve through continuous\nfeedback, learning from both failures and successes, and iterating toward\noptimal performance. Motivated by this, we propose EvolvingGrasp, an\nevolutionary grasp generation method that continuously enhances grasping\nperformance through efficient preference alignment. Specifically, we introduce\nHandpose wise Preference Optimization (HPO), which allows the model to\ncontinuously align with preferences from both positive and negative feedback\nwhile progressively refining its grasping strategies. To further enhance\nefficiency and reliability during online adjustments, we incorporate a\nPhysics-aware Consistency Model within HPO, which accelerates inference,\nreduces the number of timesteps needed for preference finetuning, and ensures\nphysical plausibility throughout the process. Extensive experiments across four\nbenchmark datasets demonstrate state of the art performance of our method in\ngrasp success rate and sampling efficiency. Our results validate that\nEvolvingGrasp enables evolutionary grasp generation, ensuring robust,\nphysically feasible, and preference-aligned grasping in both simulation and\nreal scenarios."
                },
                "authors": [
                    {
                        "name": "Yufei Zhu"
                    },
                    {
                        "name": "Yiming Zhong"
                    },
                    {
                        "name": "Zemin Yang"
                    },
                    {
                        "name": "Peishan Cong"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xinge Zhu"
                    },
                    {
                        "name": "Yuexin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuexin Ma"
                },
                "author": "Yuexin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14325v1",
                "updated": "2025-03-18T14:58:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    58,
                    59,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:58:59Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    58,
                    59,
                    1,
                    77,
                    0
                ],
                "title": "LeanVAE: An Ultra-Efficient Reconstruction VAE for Video Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanVAE: An Ultra-Efficient Reconstruction VAE for Video Diffusion\n  Models"
                },
                "summary": "Recent advances in Latent Video Diffusion Models (LVDMs) have revolutionized\nvideo generation by leveraging Video Variational Autoencoders (Video VAEs) to\ncompress intricate video data into a compact latent space. However, as LVDM\ntraining scales, the computational overhead of Video VAEs becomes a critical\nbottleneck, particularly for encoding high-resolution videos. To address this,\nwe propose LeanVAE, a novel and ultra-efficient Video VAE framework that\nintroduces two key innovations: (1) a lightweight architecture based on a\nNeighborhood-Aware Feedforward (NAF) module and non-overlapping patch\noperations, drastically reducing computational cost, and (2) the integration of\nwavelet transforms and compressed sensing techniques to enhance reconstruction\nquality. Extensive experiments validate LeanVAE's superiority in video\nreconstruction and generation, particularly in enhancing efficiency over\nexisting Video VAEs. Our model offers up to 50x fewer FLOPs and 44x faster\ninference speed while maintaining competitive reconstruction quality, providing\ninsights for scalable, efficient video generation. Our models and code are\navailable at https://github.com/westlake-repl/LeanVAE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Latent Video Diffusion Models (LVDMs) have revolutionized\nvideo generation by leveraging Video Variational Autoencoders (Video VAEs) to\ncompress intricate video data into a compact latent space. However, as LVDM\ntraining scales, the computational overhead of Video VAEs becomes a critical\nbottleneck, particularly for encoding high-resolution videos. To address this,\nwe propose LeanVAE, a novel and ultra-efficient Video VAE framework that\nintroduces two key innovations: (1) a lightweight architecture based on a\nNeighborhood-Aware Feedforward (NAF) module and non-overlapping patch\noperations, drastically reducing computational cost, and (2) the integration of\nwavelet transforms and compressed sensing techniques to enhance reconstruction\nquality. Extensive experiments validate LeanVAE's superiority in video\nreconstruction and generation, particularly in enhancing efficiency over\nexisting Video VAEs. Our model offers up to 50x fewer FLOPs and 44x faster\ninference speed while maintaining competitive reconstruction quality, providing\ninsights for scalable, efficient video generation. Our models and code are\navailable at https://github.com/westlake-repl/LeanVAE"
                },
                "authors": [
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Fajie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Fajie Yuan"
                },
                "author": "Fajie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13281v2",
                "updated": "2025-03-18T14:56:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    56,
                    41,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-17T15:31:55Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    55,
                    0,
                    76,
                    0
                ],
                "title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation"
                },
                "summary": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines."
                },
                "authors": [
                    {
                        "name": "Xiaodi Li"
                    },
                    {
                        "name": "Shaika Chowdhury"
                    },
                    {
                        "name": "Chung Il Wi"
                    },
                    {
                        "name": "Maria Vassilaki"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Terence T Sio"
                    },
                    {
                        "name": "Owen Garrick"
                    },
                    {
                        "name": "Young J Juhn"
                    },
                    {
                        "name": "James R Cerhan"
                    },
                    {
                        "name": "Cui Tao"
                    },
                    {
                        "name": "Nansu Zong"
                    }
                ],
                "author_detail": {
                    "name": "Nansu Zong"
                },
                "author": "Nansu Zong",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14321v1",
                "updated": "2025-03-18T14:51:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    51,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:51:42Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    51,
                    42,
                    1,
                    77,
                    0
                ],
                "title": "COPA: Comparing the Incomparable to Explore the Pareto Front",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COPA: Comparing the Incomparable to Explore the Pareto Front"
                },
                "summary": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail."
                },
                "authors": [
                    {
                        "name": "Adrián Javaloy"
                    },
                    {
                        "name": "Antonio Vergari"
                    },
                    {
                        "name": "Isabel Valera"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Valera"
                },
                "author": "Isabel Valera",
                "arxiv_comment": "19 pages, 14 figures. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18231v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18231v3",
                "updated": "2025-03-18T14:46:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    46,
                    11,
                    1,
                    77,
                    0
                ],
                "published": "2024-10-23T19:26:20Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    26,
                    20,
                    2,
                    297,
                    0
                ],
                "title": "Relativisitic non-pascalian fluid as a density contribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relativisitic non-pascalian fluid as a density contribution"
                },
                "summary": "Understanding the role of pressure anisotropy and dissipation is crucial for\nmodelling compact objects' internal structure and observable properties. In\nthis work, we reinterpret local pressure anisotropy in relativistic stellar\nstructures as an additional contribution to the energy density. This\nperspective enables the formulation of anisotropic equations of state for\nself-gravitating systems by incorporating anisotropy as a fundamental\ncomponent. We demonstrate that this approach yields more realistic stellar\nmodels that satisfy key physical constraints, including mass-radius\nrelationships and stability conditions. Our results are compared with\nobservational data, particularly the inferred compactness of pulsars PSR\nJ0740+6620 and PSR J0030+0451, showing that both anisotropic and isotropic\nmodels can describe these objects. Additionally, we examine the influence of\ndissipation -- such as temperature gradients -- on radial pressure,\ndemonstrating that it can be modelled similarly to anisotropy. This\ninterpretation allows the transformation of dissipative anisotropic models into\nequivalent non-dissipative isotropic configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the role of pressure anisotropy and dissipation is crucial for\nmodelling compact objects' internal structure and observable properties. In\nthis work, we reinterpret local pressure anisotropy in relativistic stellar\nstructures as an additional contribution to the energy density. This\nperspective enables the formulation of anisotropic equations of state for\nself-gravitating systems by incorporating anisotropy as a fundamental\ncomponent. We demonstrate that this approach yields more realistic stellar\nmodels that satisfy key physical constraints, including mass-radius\nrelationships and stability conditions. Our results are compared with\nobservational data, particularly the inferred compactness of pulsars PSR\nJ0740+6620 and PSR J0030+0451, showing that both anisotropic and isotropic\nmodels can describe these objects. Additionally, we examine the influence of\ndissipation -- such as temperature gradients -- on radial pressure,\ndemonstrating that it can be modelled similarly to anisotropy. This\ninterpretation allows the transformation of dissipative anisotropic models into\nequivalent non-dissipative isotropic configurations."
                },
                "authors": [
                    {
                        "name": "Justo Ospino"
                    },
                    {
                        "name": "Daniel Suárez-Urango"
                    },
                    {
                        "name": "Laura M. Becerra"
                    },
                    {
                        "name": "Héctor Hernández"
                    },
                    {
                        "name": "Luis A. Núñez"
                    }
                ],
                "author_detail": {
                    "name": "Luis A. Núñez"
                },
                "author": "Luis A. Núñez",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18231v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18231v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12334v2",
                "updated": "2025-03-18T14:39:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    39,
                    38,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-17T21:44:19Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    21,
                    44,
                    19,
                    0,
                    48,
                    0
                ],
                "title": "Inference for Log-Gaussian Cox Point Processes using Bayesian Deep\n  Learning: Application to Human Oral Microbiome Image Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Log-Gaussian Cox Point Processes using Bayesian Deep\n  Learning: Application to Human Oral Microbiome Image Data"
                },
                "summary": "It is common in nature to see aggregation of objects in space. Exploring the\nmechanism associated with the locations of such clustered observations can be\nessential to understanding the phenomenon, such as the source of spatial\nheterogeneity, or comparison to other event generating processes in the same\ndomain. Log-Gaussian Cox processes (LGCPs) represent an important class of\nmodels for quantifying aggregation in a spatial point pattern. However,\nimplementing likelihood-based Bayesian inference for such models presents many\ncomputational challenges, particularly in high dimensions. In this paper, we\npropose a novel likelihood-free inference approach for LGCPs using the recently\ndeveloped BayesFlow approach, where invertible neural networks are employed to\napproximate the posterior distribution of the parameters of interest. BayesFlow\nis a neural simulation-based method based on \"amortized\" posterior estimation.\nThat is, after an initial training procedure, fast feed-forward operations\nallow rapid posterior inference for any data within the same model family.\nComprehensive numerical studies validate the reliability of the framework and\nshow that BayesFlow achieves substantial computational gain in repeated\napplication, especially for two-dimensional LGCPs. We demonstrate the utility\nand robustness of the method by applying it to two distinct oral microbial\nbiofilm images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is common in nature to see aggregation of objects in space. Exploring the\nmechanism associated with the locations of such clustered observations can be\nessential to understanding the phenomenon, such as the source of spatial\nheterogeneity, or comparison to other event generating processes in the same\ndomain. Log-Gaussian Cox processes (LGCPs) represent an important class of\nmodels for quantifying aggregation in a spatial point pattern. However,\nimplementing likelihood-based Bayesian inference for such models presents many\ncomputational challenges, particularly in high dimensions. In this paper, we\npropose a novel likelihood-free inference approach for LGCPs using the recently\ndeveloped BayesFlow approach, where invertible neural networks are employed to\napproximate the posterior distribution of the parameters of interest. BayesFlow\nis a neural simulation-based method based on \"amortized\" posterior estimation.\nThat is, after an initial training procedure, fast feed-forward operations\nallow rapid posterior inference for any data within the same model family.\nComprehensive numerical studies validate the reliability of the framework and\nshow that BayesFlow achieves substantial computational gain in repeated\napplication, especially for two-dimensional LGCPs. We demonstrate the utility\nand robustness of the method by applying it to two distinct oral microbial\nbiofilm images."
                },
                "authors": [
                    {
                        "name": "Shuwan Wang"
                    },
                    {
                        "name": "Christopher K. Wikle"
                    },
                    {
                        "name": "Athanasios C. Micheas"
                    },
                    {
                        "name": "Jessica L. Mark Welch"
                    },
                    {
                        "name": "Jacqueline R. Starr"
                    },
                    {
                        "name": "Kyu Ha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Ha Lee"
                },
                "author": "Kyu Ha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09647v2",
                "updated": "2025-03-18T14:37:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    37,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T08:41:36Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    41,
                    36,
                    2,
                    71,
                    0
                ],
                "title": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading"
                },
                "summary": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level."
                },
                "authors": [
                    {
                        "name": "Ryan Quek Wei Heng"
                    },
                    {
                        "name": "Edoardo Vittori"
                    },
                    {
                        "name": "Keane Ong"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Gianmarco Mengaldo"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco Mengaldo"
                },
                "author": "Gianmarco Mengaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14286v1",
                "updated": "2025-03-18T14:23:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    23,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:23:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    23,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs"
                },
                "summary": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance."
                },
                "authors": [
                    {
                        "name": "Nicolas Le Roux"
                    },
                    {
                        "name": "Marc G. Bellemare"
                    },
                    {
                        "name": "Jonathan Lebensold"
                    },
                    {
                        "name": "Arnaud Bergeron"
                    },
                    {
                        "name": "Joshua Greaves"
                    },
                    {
                        "name": "Alex Fréchette"
                    },
                    {
                        "name": "Carolyne Pelletier"
                    },
                    {
                        "name": "Eric Thibodeau-Laufer Sándor Toth"
                    },
                    {
                        "name": "Samantha Work"
                    }
                ],
                "author_detail": {
                    "name": "Samantha Work"
                },
                "author": "Samantha Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14281v1",
                "updated": "2025-03-18T14:20:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    20,
                    54,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:20:54Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    20,
                    54,
                    1,
                    77,
                    0
                ],
                "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants"
                },
                "summary": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools."
                },
                "authors": [
                    {
                        "name": "Adam Štorek"
                    },
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Noopur Bhatt"
                    },
                    {
                        "name": "Aditya Gupta"
                    },
                    {
                        "name": "Janie Kim"
                    },
                    {
                        "name": "Prashast Srivastava"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14280v1",
                "updated": "2025-03-18T14:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    19,
                    30,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:19:30Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    19,
                    30,
                    1,
                    77,
                    0
                ],
                "title": "Constraints on the early Universe star formation efficiency from galaxy\n  clustering and halo modeling of H$α$ and [O III] emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the early Universe star formation efficiency from galaxy\n  clustering and halo modeling of H$α$ and [O III] emitters"
                },
                "summary": "We develop a theoretical framework to provide observational constraints on\nthe early Universe galaxy-halo connection by combining measurements of the UV\nluminosity function (UVLF) and galaxy clustering via the 2-point correlation\nfunction (2PCF). We implemented this framework in the FRESCO and CONGRESS JWST\nNIRCam/grism surveys by measuring the 2PCF of spectroscopically selected\nsamples of H$\\alpha$ and [OIII] emitters at $3.8<z<9$ in 124 arcmin$^2$ in\nGOODS-N and GOODS-S. By fitting the 2PCF and UVLF at $3.8<z<9$ we inferred that\nthe H$\\alpha$ and [OIII] samples at $\\langle z \\rangle \\sim4.3, 5.4$ and $7.3$\nreside in halos of masses of log$(M_{\\rm h}/$M$_{\\odot}) = 11.5$, $11.2$,\n$11.0$ respectively, while their galaxy bias increases with redshift with\nvalues of $b_{\\rm g} = 4.0$, $5.0$, $7.6$. These halos do not represent extreme\noverdense environments at these epochs. We constrain the instantaneous star\nformation efficiency (SFE), defined as the ratio of the star formation rate\nover the baryonic accretion rate as a function of halo mass. The SFE rises with\nhalo mass, peaks at $\\sim20\\%$ at $M_{\\rm h} \\sim 3 \\times 10^{11}\\,\nM_{\\odot}$, and declines at higher halo masses. The SFE-$M_{\\rm h}$ shows only\na mild evolution with redshift with tentative indications that low mass halos\ndecrease but the high mass halos increase in efficiency with redshift. The\nscatter in the $M_{\\rm UV}-M_{\\rm h}$ relation, quantified by $\\sigma_{\\rm\nUV}$, implies stochasticity in the UV luminosities of $\\sim 0.7$ mag,\nrelatively constant with z. Extrapolating our model to $z>9$ shows that a\nconstant SFE-$M_{\\rm h}$ fixed at $z=8$ cannot reproduce the observed UVLF and\nneither high maximum SFE nor high stochasticity alone can explain the high\nabundances of luminous galaxies seen by JWST. Extending the analysis of the\nUVLF and 2PCF to $z>9$ measured from wider surveys will be crucial in breaking\ndegeneracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a theoretical framework to provide observational constraints on\nthe early Universe galaxy-halo connection by combining measurements of the UV\nluminosity function (UVLF) and galaxy clustering via the 2-point correlation\nfunction (2PCF). We implemented this framework in the FRESCO and CONGRESS JWST\nNIRCam/grism surveys by measuring the 2PCF of spectroscopically selected\nsamples of H$\\alpha$ and [OIII] emitters at $3.8<z<9$ in 124 arcmin$^2$ in\nGOODS-N and GOODS-S. By fitting the 2PCF and UVLF at $3.8<z<9$ we inferred that\nthe H$\\alpha$ and [OIII] samples at $\\langle z \\rangle \\sim4.3, 5.4$ and $7.3$\nreside in halos of masses of log$(M_{\\rm h}/$M$_{\\odot}) = 11.5$, $11.2$,\n$11.0$ respectively, while their galaxy bias increases with redshift with\nvalues of $b_{\\rm g} = 4.0$, $5.0$, $7.6$. These halos do not represent extreme\noverdense environments at these epochs. We constrain the instantaneous star\nformation efficiency (SFE), defined as the ratio of the star formation rate\nover the baryonic accretion rate as a function of halo mass. The SFE rises with\nhalo mass, peaks at $\\sim20\\%$ at $M_{\\rm h} \\sim 3 \\times 10^{11}\\,\nM_{\\odot}$, and declines at higher halo masses. The SFE-$M_{\\rm h}$ shows only\na mild evolution with redshift with tentative indications that low mass halos\ndecrease but the high mass halos increase in efficiency with redshift. The\nscatter in the $M_{\\rm UV}-M_{\\rm h}$ relation, quantified by $\\sigma_{\\rm\nUV}$, implies stochasticity in the UV luminosities of $\\sim 0.7$ mag,\nrelatively constant with z. Extrapolating our model to $z>9$ shows that a\nconstant SFE-$M_{\\rm h}$ fixed at $z=8$ cannot reproduce the observed UVLF and\nneither high maximum SFE nor high stochasticity alone can explain the high\nabundances of luminous galaxies seen by JWST. Extending the analysis of the\nUVLF and 2PCF to $z>9$ measured from wider surveys will be crucial in breaking\ndegeneracies."
                },
                "authors": [
                    {
                        "name": "Marko Shuntov"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Sune Toft"
                    },
                    {
                        "name": "Romain A. Meyer"
                    },
                    {
                        "name": "Alba Covelo-Paz"
                    },
                    {
                        "name": "Louise Paquereau"
                    },
                    {
                        "name": "Rychard Bouwens"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Viola Gelli"
                    },
                    {
                        "name": "Emma Giovinazzo"
                    },
                    {
                        "name": "Thomas Herard-Demanche"
                    },
                    {
                        "name": "Garth D. Illingworth"
                    },
                    {
                        "name": "Charlotte Mason"
                    },
                    {
                        "name": "Rohan P. Naidu"
                    },
                    {
                        "name": "Andrea Weibel"
                    },
                    {
                        "name": "Mengyuan Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Xiao"
                },
                "author": "Mengyuan Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13788v2",
                "updated": "2025-03-18T14:17:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    17,
                    47,
                    1,
                    77,
                    0
                ],
                "published": "2024-10-17T17:29:04Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    29,
                    4,
                    3,
                    291,
                    0
                ],
                "title": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions"
                },
                "summary": "Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Michael J. Q. Zhang"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "arxiv_comment": "Presented at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14269v1",
                "updated": "2025-03-18T14:02:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    2,
                    59,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:02:59Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    2,
                    59,
                    1,
                    77,
                    0
                ],
                "title": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks."
                },
                "authors": [
                    {
                        "name": "Vaibhav Aggarwal"
                    },
                    {
                        "name": "Ojasv Kamal"
                    },
                    {
                        "name": "Abhinav Japesh"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13501v2",
                "updated": "2025-03-18T13:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    55,
                    22,
                    1,
                    77,
                    0
                ],
                "published": "2024-03-20T10:58:58Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    10,
                    58,
                    58,
                    2,
                    80,
                    0
                ],
                "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis"
                },
                "summary": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time."
                },
                "authors": [
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "William Beluch"
                    },
                    {
                        "name": "Margret Keuper"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Anna Khoreva"
                    }
                ],
                "author_detail": {
                    "name": "Anna Khoreva"
                },
                "author": "Anna Khoreva",
                "arxiv_comment": "Accepted at ICLR 2025. Code: https://github.com/boschresearch/VSTAR\n  and project page: https://yumengli007.github.io/VSTAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14263v1",
                "updated": "2025-03-18T13:54:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    54,
                    12,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:54:12Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    54,
                    12,
                    1,
                    77,
                    0
                ],
                "title": "Conversational Agents as Catalysts for Critical Thinking: Challenging\n  Social Influence in Group Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Agents as Catalysts for Critical Thinking: Challenging\n  Social Influence in Group Decision-making"
                },
                "summary": "Group decision-making processes frequently suffer when social influence and\npower dynamics suppress minority viewpoints, leading to compliance and\ngroupthink. Conversational agents can counteract these harmful dynamics by\nencouraging critical thinking. This study investigates how LLM-powered devil's\nadvocate systems affect psychological safety, opinion expression, and\nsatisfaction in power-imbalanced group dynamics. We conducted an experiment\nwith 48 participants in 12 four-person groups, each containing three high-power\n(senior) and one low-power (junior) member. Each group completed decision tasks\nin both baseline and AI intervention conditions. Results show AI\ncounterarguments fostered a more flexible atmosphere and significantly enhanced\nboth process and outcome satisfaction for all participants, with particularly\nnotable improvements for minority members. Cognitive workload increased\nslightly, though not significantly. This research contributes empirical\nevidence on how AI systems can effectively navigate power hierarchies to foster\nmore inclusive decision-making environments, highlighting the importance of\nbalancing intervention frequency, maintaining conversational flow, and\npreserving group cohesion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group decision-making processes frequently suffer when social influence and\npower dynamics suppress minority viewpoints, leading to compliance and\ngroupthink. Conversational agents can counteract these harmful dynamics by\nencouraging critical thinking. This study investigates how LLM-powered devil's\nadvocate systems affect psychological safety, opinion expression, and\nsatisfaction in power-imbalanced group dynamics. We conducted an experiment\nwith 48 participants in 12 four-person groups, each containing three high-power\n(senior) and one low-power (junior) member. Each group completed decision tasks\nin both baseline and AI intervention conditions. Results show AI\ncounterarguments fostered a more flexible atmosphere and significantly enhanced\nboth process and outcome satisfaction for all participants, with particularly\nnotable improvements for minority members. Cognitive workload increased\nslightly, though not significantly. This research contributes empirical\nevidence on how AI systems can effectively navigate power hierarchies to foster\nmore inclusive decision-making environments, highlighting the importance of\nbalancing intervention frequency, maintaining conversational flow, and\npreserving group cohesion."
                },
                "authors": [
                    {
                        "name": "Soohwan Lee"
                    },
                    {
                        "name": "Seoyeong Hwang"
                    },
                    {
                        "name": "Dajung Kim"
                    },
                    {
                        "name": "Kyungho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyungho Lee"
                },
                "author": "Kyungho Lee",
                "arxiv_comment": "12 pages, 8 figures, 1 table, This is a preprint version of the Late\n  Breaking Work accepted to CHI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14258v1",
                "updated": "2025-03-18T13:48:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System"
                },
                "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Baoqing Yue"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14251v1",
                "updated": "2025-03-18T13:39:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    39,
                    46,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:39:46Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    39,
                    46,
                    1,
                    77,
                    0
                ],
                "title": "Towards a Barrier-free GeoQA Portal: Natural Language Interaction with\n  Geospatial Data Using Multi-Agent LLMs and Semantic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Barrier-free GeoQA Portal: Natural Language Interaction with\n  Geospatial Data Using Multi-Agent LLMs and Semantic Search"
                },
                "summary": "A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a\nMulti-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting\nopen spatial data sharing and online geo-information management. Designed with\nGIS-like interaction and layered visualization, they often challenge non-expert\nusers with complex functionalities and overlapping layers that obscure spatial\nrelationships. We propose a GeoQA Portal using a multi-agent Large Language\nModel framework for seamless natural language interaction with geospatial data.\nComplex queries are broken into subtasks handled by specialized agents,\nretrieving relevant geographic data efficiently. Task plans are shown to users,\nboosting transparency. The portal supports default and custom data inputs for\nflexibility. Semantic search via word vector similarity aids data retrieval\ndespite imperfect terms. Case studies, evaluations, and user tests confirm its\neffectiveness for non-experts, bridging GIS complexity and public access, and\noffering an intuitive solution for future geoportals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a\nMulti-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting\nopen spatial data sharing and online geo-information management. Designed with\nGIS-like interaction and layered visualization, they often challenge non-expert\nusers with complex functionalities and overlapping layers that obscure spatial\nrelationships. We propose a GeoQA Portal using a multi-agent Large Language\nModel framework for seamless natural language interaction with geospatial data.\nComplex queries are broken into subtasks handled by specialized agents,\nretrieving relevant geographic data efficiently. Task plans are shown to users,\nboosting transparency. The portal supports default and custom data inputs for\nflexibility. Semantic search via word vector similarity aids data retrieval\ndespite imperfect terms. Case studies, evaluations, and user tests confirm its\neffectiveness for non-experts, bridging GIS complexity and public access, and\noffering an intuitive solution for future geoportals."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Puzhen Zhang"
                    },
                    {
                        "name": "Guohui Xiao"
                    },
                    {
                        "name": "Linfang Ding"
                    },
                    {
                        "name": "Liqiu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Meng"
                },
                "author": "Liqiu Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14242v1",
                "updated": "2025-03-18T13:26:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    26,
                    22,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:26:22Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    26,
                    22,
                    1,
                    77,
                    0
                ],
                "title": "The covariance of causal effect estimators for binary v-structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The covariance of causal effect estimators for binary v-structures"
                },
                "summary": "Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the\nvariance of two estimators of causal effects for a v-structure of binary\nvariables. Here we show that a linear combination of these estimators has lower\nvariance than either. Furthermore, we show that this holds also when the\ntreatment variable is block randomised with a predefined number receiving\ntreatment, with analogous results to when it is sampled randomly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the\nvariance of two estimators of causal effects for a v-structure of binary\nvariables. Here we show that a linear combination of these estimators has lower\nvariance than either. Furthermore, we show that this holds also when the\ntreatment variable is block randomised with a predefined number receiving\ntreatment, with analogous results to when it is sampled randomly."
                },
                "authors": [
                    {
                        "name": "Jack Kuipers"
                    },
                    {
                        "name": "Giusi Moffa"
                    }
                ],
                "author_detail": {
                    "name": "Giusi Moffa"
                },
                "author": "Giusi Moffa",
                "arxiv_comment": "6 page technical report with 2 figures and 8 pages of supplementary\n  appendices (with 3 more figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14234v2",
                "updated": "2025-03-19T04:49:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    49,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T13:11:43Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    11,
                    43,
                    1,
                    77,
                    0
                ],
                "title": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented\n  Generation Framework for Temporal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented\n  Generation Framework for Temporal Reasoning"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14222v1",
                "updated": "2025-03-18T12:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    59,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    59,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "Stacked-Residual PINN for State Reconstruction of Hyperbolic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked-Residual PINN for State Reconstruction of Hyperbolic Systems"
                },
                "summary": "In a more connected world, modeling multi-agent systems with hyperbolic\npartial differential equations (PDEs) offers a potential solution to the curse\nof dimensionality. However, classical control tools need adaptation for these\ncomplex systems. Physics-informed neural networks (PINNs) provide a powerful\nframework to fix this issue by inferring solutions to PDEs by embedding\ngoverning equations into the neural network. A major limitation of original\nPINNs is their inability to capture steep gradients and discontinuities in\nhyperbolic PDEs. This paper proposes a stacked residual PINN method enhanced\nwith a vanishing viscosity mechanism. Initially, a basic PINN with a small\nviscosity coefficient provides a stable, low-fidelity solution. Residual\ncorrection blocks with learnable scaling parameters then iteratively refine\nthis solution, progressively decreasing the viscosity coefficient to transition\nfrom parabolic to hyperbolic PDEs. Applying this method to traffic state\nreconstruction improved results by an order of magnitude in relative\n$\\mathcal{L}^2$ error, demonstrating its potential to accurately estimate\nsolutions where original PINNs struggle with instability and low fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a more connected world, modeling multi-agent systems with hyperbolic\npartial differential equations (PDEs) offers a potential solution to the curse\nof dimensionality. However, classical control tools need adaptation for these\ncomplex systems. Physics-informed neural networks (PINNs) provide a powerful\nframework to fix this issue by inferring solutions to PDEs by embedding\ngoverning equations into the neural network. A major limitation of original\nPINNs is their inability to capture steep gradients and discontinuities in\nhyperbolic PDEs. This paper proposes a stacked residual PINN method enhanced\nwith a vanishing viscosity mechanism. Initially, a basic PINN with a small\nviscosity coefficient provides a stable, low-fidelity solution. Residual\ncorrection blocks with learnable scaling parameters then iteratively refine\nthis solution, progressively decreasing the viscosity coefficient to transition\nfrom parabolic to hyperbolic PDEs. Applying this method to traffic state\nreconstruction improved results by an order of magnitude in relative\n$\\mathcal{L}^2$ error, demonstrating its potential to accurately estimate\nsolutions where original PINNs struggle with instability and low fidelity."
                },
                "authors": [
                    {
                        "name": "Katayoun Eshkofti"
                    },
                    {
                        "name": "Matthieu Barreau"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Barreau"
                },
                "author": "Matthieu Barreau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14217v1",
                "updated": "2025-03-18T12:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Decision Tree Induction Through LLMs via Semantically-Aware Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision Tree Induction Through LLMs via Semantically-Aware Evolution"
                },
                "summary": "Decision trees are a crucial class of models offering robust predictive\nperformance and inherent interpretability across various domains, including\nhealthcare, finance, and logistics. However, current tree induction methods\noften face limitations such as suboptimal solutions from greedy methods or\nprohibitive computational costs and limited applicability of exact optimization\napproaches. To address these challenges, we propose an evolutionary\noptimization method for decision tree induction based on genetic programming\n(GP). Our key innovation is the integration of semantic priors and\ndomain-specific knowledge about the search space into the optimization\nalgorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that\nincorporates semantic priors into genetic search operators through the use of\nLarge Language Models (LLMs), thereby enhancing search efficiency and targeting\nregions of the search space that yield decision trees with superior\ngeneralization performance. This is operationalized through novel genetic\noperators that work with structured natural language prompts, effectively\nutilizing LLMs as conditional generative models and sources of semantic\nknowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to\nexploit high-performing regions, and $\\textit{diversity-guided}$ mutation for\nefficient global exploration of the search space. These operators are\ncontrolled by corresponding hyperparameters that enable a more nuanced balance\nbetween exploration and exploitation across the search space. Empirically, we\ndemonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves\nsuperior-performing trees compared to existing tree induction methods, and\nexhibits significantly more efficient search performance compared to\nconventional GP approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision trees are a crucial class of models offering robust predictive\nperformance and inherent interpretability across various domains, including\nhealthcare, finance, and logistics. However, current tree induction methods\noften face limitations such as suboptimal solutions from greedy methods or\nprohibitive computational costs and limited applicability of exact optimization\napproaches. To address these challenges, we propose an evolutionary\noptimization method for decision tree induction based on genetic programming\n(GP). Our key innovation is the integration of semantic priors and\ndomain-specific knowledge about the search space into the optimization\nalgorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that\nincorporates semantic priors into genetic search operators through the use of\nLarge Language Models (LLMs), thereby enhancing search efficiency and targeting\nregions of the search space that yield decision trees with superior\ngeneralization performance. This is operationalized through novel genetic\noperators that work with structured natural language prompts, effectively\nutilizing LLMs as conditional generative models and sources of semantic\nknowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to\nexploit high-performing regions, and $\\textit{diversity-guided}$ mutation for\nefficient global exploration of the search space. These operators are\ncontrolled by corresponding hyperparameters that enable a more nuanced balance\nbetween exploration and exploitation across the search space. Empirically, we\ndemonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves\nsuperior-performing trees compared to existing tree induction methods, and\nexhibits significantly more efficient search performance compared to\nconventional GP approaches."
                },
                "authors": [
                    {
                        "name": "Tennison Liu"
                    },
                    {
                        "name": "Nicolas Huynh"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "*Liu and Huynh contributed equally. Published as a conference paper\n  at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.12351v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.12351v4",
                "updated": "2025-03-18T12:39:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    39,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2023-01-29T04:10:12Z",
                "published_parsed": [
                    2023,
                    1,
                    29,
                    4,
                    10,
                    12,
                    6,
                    29,
                    0
                ],
                "title": "Emerging Synergies in Causality and Deep Generative Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Synergies in Causality and Deep Generative Models: A Survey"
                },
                "summary": "In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area."
                },
                "authors": [
                    {
                        "name": "Guanglin Zhou"
                    },
                    {
                        "name": "Shaoan Xie"
                    },
                    {
                        "name": "Guang-Yuan Hao"
                    },
                    {
                        "name": "Shiming Chen"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.12351v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.12351v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14201v1",
                "updated": "2025-03-18T12:26:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    26,
                    6,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:26:06Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    26,
                    6,
                    1,
                    77,
                    0
                ],
                "title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Personalizing Deep Learning-Based Code Completion Tools Matters"
                },
                "summary": "Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being $\\sim$10$\\times$ larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being $\\sim$10$\\times$ larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed)."
                },
                "authors": [
                    {
                        "name": "Alessandro Giagnorio"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "arxiv_comment": "Accepted for publication at ACM TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01632v3",
                "updated": "2025-03-18T12:25:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    25,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-01-03T09:07:10Z",
                "published_parsed": [
                    2024,
                    1,
                    3,
                    9,
                    7,
                    10,
                    2,
                    3,
                    0
                ],
                "title": "Out-of-equlibrium inference of feeding rates through population data\n  from generic consumer-resource stochastic dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-equlibrium inference of feeding rates through population data\n  from generic consumer-resource stochastic dynamics"
                },
                "summary": "Statistical models are often structurally unidentifiable, because different\nsets of parameters can lead to equal model outcomes. To be useful for\nprediction and parameter inference from data, stochastic population models need\nto be identifiable, this meaning that model parameters can be uniquely inferred\nfrom a large number of model observations. In particular, precise estimation of\nfeeding rates in consumer-resource dynamics is crucial, because\nconsumer-resource processes are central in determining biomass transport across\necosystems. Model parameters are usually estimated at stationarity, because in\nthat case model analyses are often easier. In this contribution we analyze the\nproblem of parameter redundancy in a multi-resource consumer-resource model,\nshowing that model indentifiability depends on whether the dynamics have\nreached stationarity or not. To be precise, we: (i) Calculate the steady-state\nand out-of-equilibrium probability distributions of predator's abundances\nanalytically using generating functions, which allow us to unveil parameter\nredundancy and carry out proper maximum likelihood estimation. (ii) Conduct\n\\emph{in silico} experiments by tracking the abundance of consumers that are\neither searching for or handling prey, data then used for maximum likelihood\nparameter estimation. (iii) Show that, when model observations are recorded out\nof equilibrium, feeding parameters are truly identifiable, whereas if sampling\nis done solely at stationarity, only ratios of rates can be inferred from data\n(i.e., parameters are redundant). We discuss the implications of our results\nwhen inferring parameters of general dynamical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical models are often structurally unidentifiable, because different\nsets of parameters can lead to equal model outcomes. To be useful for\nprediction and parameter inference from data, stochastic population models need\nto be identifiable, this meaning that model parameters can be uniquely inferred\nfrom a large number of model observations. In particular, precise estimation of\nfeeding rates in consumer-resource dynamics is crucial, because\nconsumer-resource processes are central in determining biomass transport across\necosystems. Model parameters are usually estimated at stationarity, because in\nthat case model analyses are often easier. In this contribution we analyze the\nproblem of parameter redundancy in a multi-resource consumer-resource model,\nshowing that model indentifiability depends on whether the dynamics have\nreached stationarity or not. To be precise, we: (i) Calculate the steady-state\nand out-of-equilibrium probability distributions of predator's abundances\nanalytically using generating functions, which allow us to unveil parameter\nredundancy and carry out proper maximum likelihood estimation. (ii) Conduct\n\\emph{in silico} experiments by tracking the abundance of consumers that are\neither searching for or handling prey, data then used for maximum likelihood\nparameter estimation. (iii) Show that, when model observations are recorded out\nof equilibrium, feeding parameters are truly identifiable, whereas if sampling\nis done solely at stationarity, only ratios of rates can be inferred from data\n(i.e., parameters are redundant). We discuss the implications of our results\nwhen inferring parameters of general dynamical models."
                },
                "authors": [
                    {
                        "name": "Jose A. Capitan"
                    },
                    {
                        "name": "David Alonso"
                    }
                ],
                "author_detail": {
                    "name": "David Alonso"
                },
                "author": "David Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92-10 60Gxx 62Fxx (Primary), 05A15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04614v2",
                "updated": "2025-03-18T12:23:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    23,
                    39,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-06T16:59:10Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    59,
                    10,
                    3,
                    65,
                    0
                ],
                "title": "Current Flow Mapping in Conducting Ferroelectric Domain Walls using\n  Scanning NV-Magnetometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Flow Mapping in Conducting Ferroelectric Domain Walls using\n  Scanning NV-Magnetometry"
                },
                "summary": "The electrical conductivity of parallel plate capacitors, with ferroelectric\nlithium niobate as the dielectric layer, can be extensively and progressively\nmodified by the controlled injection of conducting domain walls. Domain\nwall-based memristor devices hence result. Microstructures, developed as a\nresult of partial switching, are complex and so simple models of equivalent\ncircuits, based on the collective action of all conducting domain wall channels\nacting identically and in parallel, may not be appropriate. Here, we directly\nmap the current density in ferroelectric domain wall memristors in-situ, by\nmapping Oersted fields, using nitrogen vacancy centre microscopy. Current\ndensity maps were found to directly correlate with the domain microstructure,\nrevealing that a strikingly small fraction of the total domain wall network is\nresponsible for the majority of the current flow. This insight forces a two\norder of magnitude correction to the carrier densities, previously inferred\nfrom standard scanning probe or macroscopic electrical characterisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrical conductivity of parallel plate capacitors, with ferroelectric\nlithium niobate as the dielectric layer, can be extensively and progressively\nmodified by the controlled injection of conducting domain walls. Domain\nwall-based memristor devices hence result. Microstructures, developed as a\nresult of partial switching, are complex and so simple models of equivalent\ncircuits, based on the collective action of all conducting domain wall channels\nacting identically and in parallel, may not be appropriate. Here, we directly\nmap the current density in ferroelectric domain wall memristors in-situ, by\nmapping Oersted fields, using nitrogen vacancy centre microscopy. Current\ndensity maps were found to directly correlate with the domain microstructure,\nrevealing that a strikingly small fraction of the total domain wall network is\nresponsible for the majority of the current flow. This insight forces a two\norder of magnitude correction to the carrier densities, previously inferred\nfrom standard scanning probe or macroscopic electrical characterisation."
                },
                "authors": [
                    {
                        "name": "Conor J. McCluskey"
                    },
                    {
                        "name": "James Dalzell"
                    },
                    {
                        "name": "Amit Kumar"
                    },
                    {
                        "name": "J. Marty Gregg"
                    }
                ],
                "author_detail": {
                    "name": "J. Marty Gregg"
                },
                "author": "J. Marty Gregg",
                "arxiv_comment": "Main Text: 19 Pages, 4 Figures. Supplementary Information: 7 Pages, 3\n  Figures. Typo in title corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18425v2",
                "updated": "2025-03-18T12:18:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    18,
                    4,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-27T15:07:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Streamlining Prediction in Bayesian Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Prediction in Bayesian Deep Learning"
                },
                "summary": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Marcus Klasson"
                    },
                    {
                        "name": "Arno Solin"
                    },
                    {
                        "name": "Martin Trapp"
                    }
                ],
                "author_detail": {
                    "name": "Martin Trapp"
                },
                "author": "Martin Trapp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07604v2",
                "updated": "2025-03-18T12:08:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    8,
                    17,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-10T17:58:31Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    58,
                    31,
                    0,
                    69,
                    0
                ],
                "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Reasoning in Transformers is Reasoning through Shortcuts"
                },
                "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization."
                },
                "authors": [
                    {
                        "name": "Tianhe Lin"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14190v1",
                "updated": "2025-03-18T12:07:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    7,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:07:33Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    7,
                    33,
                    1,
                    77,
                    0
                ],
                "title": "Inferring Event Descriptions from Time Series with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Event Descriptions from Time Series with Language Models"
                },
                "summary": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)"
                },
                "authors": [
                    {
                        "name": "Mingtian Tan"
                    },
                    {
                        "name": "Mike A. Merrill"
                    },
                    {
                        "name": "Zack Gottesman"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "David Evans"
                    },
                    {
                        "name": "Tom Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hartvigsen"
                },
                "author": "Tom Hartvigsen",
                "arxiv_comment": "17 pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 68T07,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20963v2",
                "updated": "2025-03-18T12:00:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    0,
                    26,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-28T11:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    25,
                    11,
                    4,
                    59,
                    0
                ],
                "title": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration"
                },
                "summary": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research."
                },
                "authors": [
                    {
                        "name": "Gerion Spielberger"
                    },
                    {
                        "name": "Florian M. Artinger"
                    },
                    {
                        "name": "Jochen Reb"
                    },
                    {
                        "name": "Rudolf Kerschreiter"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Kerschreiter"
                },
                "author": "Rudolf Kerschreiter",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11139v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11139v4",
                "updated": "2025-03-18T11:58:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    58,
                    48,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-17T01:54:27Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    1,
                    54,
                    27,
                    0,
                    169,
                    0
                ],
                "title": "Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance"
                },
                "summary": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Avik Halder"
                    },
                    {
                        "name": "Rajarshi Mandal"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Ian Soboroff"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "arxiv_comment": "Accepted at NAACL 2025 (Industry track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11139v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11139v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14183v1",
                "updated": "2025-03-18T11:58:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    58,
                    0,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:58:00Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    58,
                    0,
                    1,
                    77,
                    0
                ],
                "title": "Can LLMs Enable Verification in Mainstream Programming?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Enable Verification in Mainstream Programming?"
                },
                "summary": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results."
                },
                "authors": [
                    {
                        "name": "Aleksandr Shefer"
                    },
                    {
                        "name": "Igor Engel"
                    },
                    {
                        "name": "Stanislav Alekseev"
                    },
                    {
                        "name": "Daniil Berezun"
                    },
                    {
                        "name": "Ekaterina Verbitskaia"
                    },
                    {
                        "name": "Anton Podkopaev"
                    }
                ],
                "author_detail": {
                    "name": "Anton Podkopaev"
                },
                "author": "Anton Podkopaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14177v1",
                "updated": "2025-03-18T11:51:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    51,
                    11,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:51:11Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    51,
                    11,
                    1,
                    77,
                    0
                ],
                "title": "Distributions and Direct Parametrization for Stable Stochastic\n  State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributions and Direct Parametrization for Stable Stochastic\n  State-Space Models"
                },
                "summary": "We present a direct parametrization for continuous-time stochastic\nstate-space models that ensures external stability via the stochastic\nbounded-real lemma. Our formulation facilitates the construction of\nprobabilistic priors that enforce almost-sure stability which are suitable for\nsampling-based Bayesian inference methods. We validate our work with a\nsimulation example and demonstrate its ability to yield stable predictions with\nuncertainty quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a direct parametrization for continuous-time stochastic\nstate-space models that ensures external stability via the stochastic\nbounded-real lemma. Our formulation facilitates the construction of\nprobabilistic priors that enforce almost-sure stability which are suitable for\nsampling-based Bayesian inference methods. We validate our work with a\nsimulation example and demonstrate its ability to yield stable predictions with\nuncertainty quantification."
                },
                "authors": [
                    {
                        "name": "Mohamad Al Ahdab"
                    },
                    {
                        "name": "Zheng-Hua Tan"
                    },
                    {
                        "name": "John Leth"
                    }
                ],
                "author_detail": {
                    "name": "John Leth"
                },
                "author": "John Leth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16635v2",
                "updated": "2025-03-18T11:50:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    50,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-28T02:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    16,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation"
                },
                "summary": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4o's performance\nin recognizing the majority labels of laughable contexts, achieving an F1 score\nof 43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4o's performance\nin recognizing the majority labels of laughable contexts, achieving an F1 score\nof 43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions."
                },
                "authors": [
                    {
                        "name": "Koji Inoue"
                    },
                    {
                        "name": "Mikey Elmers"
                    },
                    {
                        "name": "Divesh Lala"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "arxiv_comment": "This paper has been accepted for presentation at International\n  Workshop on Spoken Dialogue Systems Technology 2025 (IWSDS 2025) and\n  represents the author's version of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05092v2",
                "updated": "2025-03-18T11:43:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    43,
                    52,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-07T17:11:23Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    11,
                    23,
                    4,
                    38,
                    0
                ],
                "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs"
                },
                "summary": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "Accepted at the ICLR 2025 Workshop on Reasoning and Planning for\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14167v1",
                "updated": "2025-03-18T11:37:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    37,
                    25,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:37:25Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    37,
                    25,
                    1,
                    77,
                    0
                ],
                "title": "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach"
                },
                "summary": "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections."
                },
                "authors": [
                    {
                        "name": "Christian Poelitz"
                    },
                    {
                        "name": "Nick McKenna"
                    }
                ],
                "author_detail": {
                    "name": "Nick McKenna"
                },
                "author": "Nick McKenna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14153v1",
                "updated": "2025-03-18T11:21:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    21,
                    53,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:21:53Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    21,
                    53,
                    1,
                    77,
                    0
                ],
                "title": "Speculative Decoding for Verilog: Speed and Quality, All in One",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding for Verilog: Speed and Quality, All in One"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages."
                },
                "authors": [
                    {
                        "name": "Changran Xu"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yunhao Zhou"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Ningyi Xu"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "arxiv_comment": "Accepted by the 62nd Design Automation Conference (DAC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14140v1",
                "updated": "2025-03-18T11:07:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    7,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:07:14Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    7,
                    14,
                    1,
                    77,
                    0
                ],
                "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marten: Visual Question Answering with Mask Generation for Multi-modal\n  Document Understanding"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps://github.com/PriNing/Marten.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps://github.com/PriNing/Marten."
                },
                "authors": [
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Tongkun Guan"
                    },
                    {
                        "name": "Pei Fu"
                    },
                    {
                        "name": "Chen Duan"
                    },
                    {
                        "name": "Qianyi Jiang"
                    },
                    {
                        "name": "Zhentao Guo"
                    },
                    {
                        "name": "Shan Guo"
                    },
                    {
                        "name": "Junfeng Luo"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17002v2",
                "updated": "2025-03-18T11:02:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    2,
                    5,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-26T00:15:37Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    15,
                    37,
                    1,
                    331,
                    0
                ],
                "title": "Words Matter: Leveraging Individual Text Embeddings for Code Generation\n  in CLIP Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Words Matter: Leveraging Individual Text Embeddings for Code Generation\n  in CLIP Test-Time Adaptation"
                },
                "summary": "Vision-language foundation models, such as CLIP, have shown unprecedented\nzero-shot performance across a wide range of tasks. Nevertheless, these models\nmay be unreliable under distributional shifts, as their performance is\nsignificantly degraded. In this work, we explore how to efficiently leverage\nclass text information to mitigate these distribution drifts encountered by\nlarge pre-trained vision-language models (VLMs) during test-time inference. In\nparticular, we propose to generate pseudo-labels for the test-time samples by\nexploiting generic class text embeddings as fixed centroids of a label\nassignment problem, which is efficiently solved with Optimal Transport.\nFurthermore, the proposed adaptation method (CLIP-OT) integrates a multiple\ntemplate knowledge distillation approach, which replicates multi-view\ncontrastive learning strategies in unsupervised representation learning but\nwithout incurring additional computational complexity. Extensive experiments on\nmultiple popular test-time adaptation benchmarks presenting diverse complexity\nempirically show the superiority of CLIP-OT, achieving performance gains of up\nto 7% over recent state-of-the-art methods, yet being computationally and\nmemory efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models, such as CLIP, have shown unprecedented\nzero-shot performance across a wide range of tasks. Nevertheless, these models\nmay be unreliable under distributional shifts, as their performance is\nsignificantly degraded. In this work, we explore how to efficiently leverage\nclass text information to mitigate these distribution drifts encountered by\nlarge pre-trained vision-language models (VLMs) during test-time inference. In\nparticular, we propose to generate pseudo-labels for the test-time samples by\nexploiting generic class text embeddings as fixed centroids of a label\nassignment problem, which is efficiently solved with Optimal Transport.\nFurthermore, the proposed adaptation method (CLIP-OT) integrates a multiple\ntemplate knowledge distillation approach, which replicates multi-view\ncontrastive learning strategies in unsupervised representation learning but\nwithout incurring additional computational complexity. Extensive experiments on\nmultiple popular test-time adaptation benchmarks presenting diverse complexity\nempirically show the superiority of CLIP-OT, achieving performance gains of up\nto 7% over recent state-of-the-art methods, yet being computationally and\nmemory efficient."
                },
                "authors": [
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Julio Silva-Rodrıguez"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    },
                    {
                        "name": "Marco Pedersoli"
                    },
                    {
                        "name": "Jose Dolz"
                    }
                ],
                "author_detail": {
                    "name": "Jose Dolz"
                },
                "author": "Jose Dolz",
                "arxiv_comment": "Added additional figures to communicate the algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14131v1",
                "updated": "2025-03-18T10:53:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    53,
                    57,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:53:57Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    53,
                    57,
                    1,
                    77,
                    0
                ],
                "title": "Spinor ice correlation in flat-band electronic states on kagome and\n  pyrochlore lattices with spin-orbit coupling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spinor ice correlation in flat-band electronic states on kagome and\n  pyrochlore lattices with spin-orbit coupling"
                },
                "summary": "We investigate the emergence and transformation of pinch-point singularities\nin the excitation spectrum of electronic flat band systems on kagome and\npyrochlore lattices with spin-orbit coupling (SOC) and Coulomb interactions.\nWhile pinch points are widely recognized as signatures of classical spin\nliquids, they also appear in electronic flat-band systems when there exists a\nsingular band-touching point to dispersive bands. We explore how SOC modifies\nthe pinch-point structure in the chiral spin flat-band metallic state, which we\nterm spinor-ice. The pinch point profile can rotate or redistribute its\nspectral weight, governed by a prefactor in the spectral function that\nprimarily depends on the direction of the ground-state spin polarization, where\nwe show that SOC flat bands could be experimentally probed by rotating the spin\npolarization of the injected electron to infer internal magnetic structures.\nThese observations are discussed in conjunction with the angle-resolved\nphotoemission spectroscopy (ARPES) and the application to the potential SOC\nflat-band material $\\rm CsW_2O_6$. We also demonstrate the persistent residual\npinch-point features under Coulomb interactions and deviations from the ideal\nflat-band limit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the emergence and transformation of pinch-point singularities\nin the excitation spectrum of electronic flat band systems on kagome and\npyrochlore lattices with spin-orbit coupling (SOC) and Coulomb interactions.\nWhile pinch points are widely recognized as signatures of classical spin\nliquids, they also appear in electronic flat-band systems when there exists a\nsingular band-touching point to dispersive bands. We explore how SOC modifies\nthe pinch-point structure in the chiral spin flat-band metallic state, which we\nterm spinor-ice. The pinch point profile can rotate or redistribute its\nspectral weight, governed by a prefactor in the spectral function that\nprimarily depends on the direction of the ground-state spin polarization, where\nwe show that SOC flat bands could be experimentally probed by rotating the spin\npolarization of the injected electron to infer internal magnetic structures.\nThese observations are discussed in conjunction with the angle-resolved\nphotoemission spectroscopy (ARPES) and the application to the potential SOC\nflat-band material $\\rm CsW_2O_6$. We also demonstrate the persistent residual\npinch-point features under Coulomb interactions and deviations from the ideal\nflat-band limit."
                },
                "authors": [
                    {
                        "name": "Hiroki Nakai"
                    },
                    {
                        "name": "Masafumi Udagawa"
                    },
                    {
                        "name": "Chisa Hotta"
                    }
                ],
                "author_detail": {
                    "name": "Chisa Hotta"
                },
                "author": "Chisa Hotta",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12687v3",
                "updated": "2025-03-18T10:50:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    50,
                    58,
                    1,
                    77,
                    0
                ],
                "published": "2024-12-17T09:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models"
                },
                "summary": "This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping."
                },
                "authors": [
                    {
                        "name": "Seungeun Oh"
                    },
                    {
                        "name": "Jinhyuk Kim"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Seung-Woo Ko"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    },
                    {
                        "name": "Seong-Lyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Lyun Kim"
                },
                "author": "Seong-Lyun Kim",
                "arxiv_comment": "7 pages, 6 figures; to be presented at IEEE International Conference\n  on Machine Learning for Communication and Networking (ICMLCN) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14130v1",
                "updated": "2025-03-18T10:49:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    49,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:49:36Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    49,
                    36,
                    1,
                    77,
                    0
                ],
                "title": "Inference-Time Intervention in Large Language Models for Reliable\n  Requirement Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Intervention in Large Language Models for Reliable\n  Requirement Verification"
                },
                "summary": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set."
                },
                "authors": [
                    {
                        "name": "Paul Darm"
                    },
                    {
                        "name": "James Xie"
                    },
                    {
                        "name": "Annalisa Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Annalisa Riccardi"
                },
                "author": "Annalisa Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.2; I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09123v3",
                "updated": "2025-03-18T10:28:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    28,
                    1,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-13T13:56:55Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    13,
                    56,
                    55,
                    3,
                    165,
                    0
                ],
                "title": "Can I introduce my boyfriend to my grandmother? Evaluating Large\n  Language Models Capabilities on Iranian Social Norm Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can I introduce my boyfriend to my grandmother? Evaluating Large\n  Language Models Capabilities on Iranian Social Norm Classification"
                },
                "summary": "Creating globally inclusive AI systems demands datasets reflecting diverse\nsocial norms. Iran, with its unique cultural blend, offers an ideal case study,\nwith Farsi adding linguistic complexity. In this work, we introduce the Iranian\nSocial Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms,\nincluding environments, demographic features, and scope annotation, alongside\nEnglish translations. Our evaluation of 6 Large Language Models (LLMs) in\nclassifying Iranian social norms, using a variety of prompts, uncovered\ncritical insights into the impact of geographic and linguistic context. Results\nrevealed a substantial performance gap in LLMs' comprehension of Iranian norms.\nNotably, while the geographic context in English prompts enhanced the\nperformance, this effect was absent in Farsi, pointing to nuanced linguistic\nchallenges. Particularly, performance was significantly worse for Iran-specific\nnorms, emphasizing the importance of culturally tailored datasets. As the first\nFarsi dataset for social norm classification, ISN will facilitate crucial\ncross-cultural analyses, shedding light on how values differ across contexts\nand cultures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating globally inclusive AI systems demands datasets reflecting diverse\nsocial norms. Iran, with its unique cultural blend, offers an ideal case study,\nwith Farsi adding linguistic complexity. In this work, we introduce the Iranian\nSocial Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms,\nincluding environments, demographic features, and scope annotation, alongside\nEnglish translations. Our evaluation of 6 Large Language Models (LLMs) in\nclassifying Iranian social norms, using a variety of prompts, uncovered\ncritical insights into the impact of geographic and linguistic context. Results\nrevealed a substantial performance gap in LLMs' comprehension of Iranian norms.\nNotably, while the geographic context in English prompts enhanced the\nperformance, this effect was absent in Farsi, pointing to nuanced linguistic\nchallenges. Particularly, performance was significantly worse for Iran-specific\nnorms, emphasizing the importance of culturally tailored datasets. As the first\nFarsi dataset for social norm classification, ISN will facilitate crucial\ncross-cultural analyses, shedding light on how values differ across contexts\nand cultures."
                },
                "authors": [
                    {
                        "name": "Hamidreza Saffari"
                    },
                    {
                        "name": "Mohammadamin Shafiei"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Francesco Pierri"
                    },
                    {
                        "name": "Debora Nozza"
                    }
                ],
                "author_detail": {
                    "name": "Debora Nozza"
                },
                "author": "Debora Nozza",
                "arxiv_comment": "15 pages, 1 figure, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13839v3",
                "updated": "2025-03-18T10:25:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    25,
                    10,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-19T21:06:44Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    21,
                    6,
                    44,
                    2,
                    171,
                    0
                ],
                "title": "RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design"
                },
                "summary": "We introduce RNA-FrameFlow, the first generative model for 3D RNA backbone\ndesign. We build upon SE(3) flow matching for protein backbone generation and\nestablish protocols for data preparation and evaluation to address unique\nchallenges posed by RNA modeling. We formulate RNA structures as a set of\nrigid-body frames and associated loss functions which account for larger, more\nconformationally flexible RNA backbones (13 atoms per nucleotide) vs. proteins\n(4 atoms per residue). Toward tackling the lack of diversity in 3D RNA\ndatasets, we explore training with structural clustering and cropping\naugmentations. Additionally, we define a suite of evaluation metrics to measure\nwhether the generated RNA structures are globally self-consistent (via inverse\nfolding followed by forward folding) and locally recover RNA-specific\nstructural descriptors. The most performant version of RNA-FrameFlow generates\nlocally realistic RNA backbones of 40-150 nucleotides, over 40% of which pass\nour validity criteria as measured by a self-consistency TM-score >= 0.45, at\nwhich two RNAs have the same global fold. Open-source code:\nhttps://github.com/rish-16/rna-backbone-design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RNA-FrameFlow, the first generative model for 3D RNA backbone\ndesign. We build upon SE(3) flow matching for protein backbone generation and\nestablish protocols for data preparation and evaluation to address unique\nchallenges posed by RNA modeling. We formulate RNA structures as a set of\nrigid-body frames and associated loss functions which account for larger, more\nconformationally flexible RNA backbones (13 atoms per nucleotide) vs. proteins\n(4 atoms per residue). Toward tackling the lack of diversity in 3D RNA\ndatasets, we explore training with structural clustering and cropping\naugmentations. Additionally, we define a suite of evaluation metrics to measure\nwhether the generated RNA structures are globally self-consistent (via inverse\nfolding followed by forward folding) and locally recover RNA-specific\nstructural descriptors. The most performant version of RNA-FrameFlow generates\nlocally realistic RNA backbones of 40-150 nucleotides, over 40% of which pass\nour validity criteria as measured by a self-consistency TM-score >= 0.45, at\nwhich two RNAs have the same global fold. Open-source code:\nhttps://github.com/rish-16/rna-backbone-design"
                },
                "authors": [
                    {
                        "name": "Rishabh Anand"
                    },
                    {
                        "name": "Chaitanya K. Joshi"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Arian R. Jamasb"
                    },
                    {
                        "name": "Charles Harris"
                    },
                    {
                        "name": "Simon V. Mathis"
                    },
                    {
                        "name": "Kieran Didi"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Pietro Liò"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Liò"
                },
                "author": "Pietro Liò",
                "arxiv_comment": "Oral presentation at Machine Learning in Computational Biology\n  (MLCB), 2024. Also presented as an Oral at ICML 2024 Structured Probabilistic\n  Inference & Generative Modeling Workshop, and a Spotlight at ICML 2024\n  AI4Science Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14103v1",
                "updated": "2025-03-18T10:18:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    18,
                    7,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:18:07Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    18,
                    7,
                    1,
                    77,
                    0
                ],
                "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model"
                },
                "summary": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research."
                },
                "authors": [
                    {
                        "name": "Jonas Oppenlaender"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Oppenlaender"
                },
                "author": "Jonas Oppenlaender",
                "arxiv_comment": "17 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14097v1",
                "updated": "2025-03-18T10:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    14,
                    49,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:14:49Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    14,
                    49,
                    1,
                    77,
                    0
                ],
                "title": "SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human\n  Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human\n  Pose Estimation"
                },
                "summary": "Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but\nsuffer from computational overhead and slow inference, while knowledge\ndistillation methods fail to address spatial relationships between joints and\ntemporal correlations in multi-frame inputs. In this paper, we propose Sparse\nCorrelation and Joint Distillation (SCJD), a novel framework that balances\nefficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input\nSequence Downsampling to reduce redundancy in student network inputs while\npreserving inter-frame correlations. For effective knowledge transfer, we\npropose Dynamic Joint Spatial Attention Distillation, which includes Dynamic\nJoint Embedding Distillation to enhance the student's feature representation\nusing the teacher's multi-frame context feature, and Adjacent Joint Attention\nDistillation to improve the student network's focus on adjacent joint\nrelationships for better spatial understanding. Additionally, Temporal\nConsistency Distillation aligns the temporal correlations between teacher and\nstudent networks through upsampling and global supervision. Extensive\nexperiments demonstrate that SCJD achieves state-of-the-art performance. Code\nis available at https://github.com/wileychan/SCJD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but\nsuffer from computational overhead and slow inference, while knowledge\ndistillation methods fail to address spatial relationships between joints and\ntemporal correlations in multi-frame inputs. In this paper, we propose Sparse\nCorrelation and Joint Distillation (SCJD), a novel framework that balances\nefficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input\nSequence Downsampling to reduce redundancy in student network inputs while\npreserving inter-frame correlations. For effective knowledge transfer, we\npropose Dynamic Joint Spatial Attention Distillation, which includes Dynamic\nJoint Embedding Distillation to enhance the student's feature representation\nusing the teacher's multi-frame context feature, and Adjacent Joint Attention\nDistillation to improve the student network's focus on adjacent joint\nrelationships for better spatial understanding. Additionally, Temporal\nConsistency Distillation aligns the temporal correlations between teacher and\nstudent networks through upsampling and global supervision. Extensive\nexperiments demonstrate that SCJD achieves state-of-the-art performance. Code\nis available at https://github.com/wileychan/SCJD."
                },
                "authors": [
                    {
                        "name": "Weihong Chen"
                    },
                    {
                        "name": "Xuemiao Xu"
                    },
                    {
                        "name": "Haoxin Yang"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Peng Xiao"
                    },
                    {
                        "name": "Cheng Xu"
                    },
                    {
                        "name": "Huaidong Zhang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14096v1",
                "updated": "2025-03-18T10:12:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    12,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:12:29Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    12,
                    29,
                    1,
                    77,
                    0
                ],
                "title": "GenPara: Enhancing the 3D Design Editing Process by Inferring Users'\n  Regions of Interest with Text-Conditional Shape Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenPara: Enhancing the 3D Design Editing Process by Inferring Users'\n  Regions of Interest with Text-Conditional Shape Parameters"
                },
                "summary": "In 3D design, specifying design objectives and visualizing complex shapes\nthrough text alone proves to be a significant challenge. Although advancements\nin 3D GenAI have significantly enhanced part assembly and the creation of\nhigh-quality 3D designs, many systems still to dynamically generate and edit\ndesign elements based on the shape parameters. To bridge this gap, we propose\nGenPara, an interactive 3D design editing system that leverages\ntext-conditional shape parameters of part-aware 3D designs and visualizes\ndesign space within the Exploration Map and Design Versioning Tree.\nAdditionally, among the various shape parameters generated by LLM, the system\nextracts and provides design outcomes within the user's regions of interest\nbased on Bayesian inference. A user study N = 16 revealed that \\textit{GenPara}\nenhanced the comprehension and management of designers with text-conditional\nshape parameters, streamlining design exploration and concretization. This\nimprovement boosted efficiency and creativity of the 3D design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 3D design, specifying design objectives and visualizing complex shapes\nthrough text alone proves to be a significant challenge. Although advancements\nin 3D GenAI have significantly enhanced part assembly and the creation of\nhigh-quality 3D designs, many systems still to dynamically generate and edit\ndesign elements based on the shape parameters. To bridge this gap, we propose\nGenPara, an interactive 3D design editing system that leverages\ntext-conditional shape parameters of part-aware 3D designs and visualizes\ndesign space within the Exploration Map and Design Versioning Tree.\nAdditionally, among the various shape parameters generated by LLM, the system\nextracts and provides design outcomes within the user's regions of interest\nbased on Bayesian inference. A user study N = 16 revealed that \\textit{GenPara}\nenhanced the comprehension and management of designers with text-conditional\nshape parameters, streamlining design exploration and concretization. This\nimprovement boosted efficiency and creativity of the 3D design process."
                },
                "authors": [
                    {
                        "name": "Jiin Choi"
                    },
                    {
                        "name": "Seung Won Lee"
                    },
                    {
                        "name": "Kyung Hoon Hyun"
                    }
                ],
                "author_detail": {
                    "name": "Kyung Hoon Hyun"
                },
                "author": "Kyung Hoon Hyun",
                "arxiv_doi": "10.1145/3706598.3713502",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713502",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10630v3",
                "updated": "2025-03-18T10:07:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    7,
                    7,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-13T17:59:48Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    48,
                    3,
                    72,
                    0
                ],
                "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"
                },
                "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods."
                },
                "authors": [
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Lingqing Zhao"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to CVPR 2025. Project page:\n  https://bagh2178.github.io/UniGoal/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13454v2",
                "updated": "2025-03-18T10:02:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    2,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2024-05-22T08:50:54Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    8,
                    50,
                    54,
                    2,
                    143,
                    0
                ],
                "title": "The Erdős-Rényi Random Graph Conditioned on Every Component Being\n  a Clique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Erdős-Rényi Random Graph Conditioned on Every Component Being\n  a Clique"
                },
                "summary": "Motivated by an application in community detection, we consider an \\ER random\ngraph conditioned on the rare event that all connected components are fully\nconnected. Such graphs can be considered as partitions of vertices into\ncliques. Hence, this conditional distribution defines a distribution over\npartitions. We show that a popular community detection method is equivalent to\nBayesian inference with this distribution as prior over the community\npartitions. Using tools from analytic combinatorics, we prove limit theorems\nfor several graph observables in this conditional distribution: the number of\ncliques; the number of edges; and the degree distribution. We consider several\nregimes of the connection probability $p$ as the number of vertices $n$\ndiverges. For $p=\\tfrac{1}{2}$, the conditioning yields the uniform\ndistribution over set partitions, which is well-studied, but has not been\nstudied as a graph distribution before. For $p<\\tfrac{1}{2}$, we show that the\nnumber of cliques is of the order $n/\\sqrt{\\log n}$, while for\n$p>\\tfrac{1}{2}$, we prove that the graph consists of a single clique with high\nprobability. This shows that there is a phase transition at $p=\\tfrac{1}{2}$.\nWe additionally study the near-critical regime $p_n\\downarrow\\tfrac{1}{2}$, as\nwell as the sparse regime $p_n\\downarrow0$. Finally, we discuss the\nimplications of these results for community detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by an application in community detection, we consider an \\ER random\ngraph conditioned on the rare event that all connected components are fully\nconnected. Such graphs can be considered as partitions of vertices into\ncliques. Hence, this conditional distribution defines a distribution over\npartitions. We show that a popular community detection method is equivalent to\nBayesian inference with this distribution as prior over the community\npartitions. Using tools from analytic combinatorics, we prove limit theorems\nfor several graph observables in this conditional distribution: the number of\ncliques; the number of edges; and the degree distribution. We consider several\nregimes of the connection probability $p$ as the number of vertices $n$\ndiverges. For $p=\\tfrac{1}{2}$, the conditioning yields the uniform\ndistribution over set partitions, which is well-studied, but has not been\nstudied as a graph distribution before. For $p<\\tfrac{1}{2}$, we show that the\nnumber of cliques is of the order $n/\\sqrt{\\log n}$, while for\n$p>\\tfrac{1}{2}$, we prove that the graph consists of a single clique with high\nprobability. This shows that there is a phase transition at $p=\\tfrac{1}{2}$.\nWe additionally study the near-critical regime $p_n\\downarrow\\tfrac{1}{2}$, as\nwell as the sparse regime $p_n\\downarrow0$. Finally, we discuss the\nimplications of these results for community detection."
                },
                "authors": [
                    {
                        "name": "Martijn Gösgens"
                    },
                    {
                        "name": "Lukas Lüchtrath"
                    },
                    {
                        "name": "Elena Magnanini"
                    },
                    {
                        "name": "Marc Noy"
                    },
                    {
                        "name": "Élie de Panafieu"
                    }
                ],
                "author_detail": {
                    "name": "Élie de Panafieu"
                },
                "author": "Élie de Panafieu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14079v1",
                "updated": "2025-03-18T10:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    0,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    0,
                    36,
                    1,
                    77,
                    0
                ],
                "title": "Testing Uniform Random Samplers: Methods, Datasets and Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Uniform Random Samplers: Methods, Datasets and Protocols"
                },
                "summary": "Boolean formulae compactly encode huge, constrained search spaces. Thus,\nvariability-intensive systems are often encoded with Boolean formulae. The\nsearch space of a variability-intensive system is usually too large to explore\nwithout statistical inference (e.g. testing). Testing every valid configuration\nis computationally expensive (if not impossible) for most systems. This leads\nmost testing approaches to sample a few configurations before analyzing them. A\ndesirable property of such samples is uniformity: Each solution should have the\nsame selection probability. Uniformity is the property that facilitates\nstatistical inference. This property motivated the design of uniform random\nsamplers, relying on SAT solvers and counters and achieving different\ntrade-offs between uniformity and scalability. Though we can observe their\nperformance in practice, judging the quality of the generated samples is\ndifferent. Assessing the uniformity of a sampler is similar in nature to\nassessing the uniformity of a pseudo-random number (PRNG) generator. However,\nsampling is much slower and the nature of sampling also implies that the\nhyperspace containing the samples is constrained. This means that testing PRNGs\nis subject to fewer constraints than testing samplers. We propose a framework\nthat contains five statistical tests which are suited to test uniform random\nsamplers. Moreover, we demonstrate their use by testing seven samplers.\nFinally, we demonstrate the influence of the Boolean formula given as input to\nthe samplers under test on the test results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boolean formulae compactly encode huge, constrained search spaces. Thus,\nvariability-intensive systems are often encoded with Boolean formulae. The\nsearch space of a variability-intensive system is usually too large to explore\nwithout statistical inference (e.g. testing). Testing every valid configuration\nis computationally expensive (if not impossible) for most systems. This leads\nmost testing approaches to sample a few configurations before analyzing them. A\ndesirable property of such samples is uniformity: Each solution should have the\nsame selection probability. Uniformity is the property that facilitates\nstatistical inference. This property motivated the design of uniform random\nsamplers, relying on SAT solvers and counters and achieving different\ntrade-offs between uniformity and scalability. Though we can observe their\nperformance in practice, judging the quality of the generated samples is\ndifferent. Assessing the uniformity of a sampler is similar in nature to\nassessing the uniformity of a pseudo-random number (PRNG) generator. However,\nsampling is much slower and the nature of sampling also implies that the\nhyperspace containing the samples is constrained. This means that testing PRNGs\nis subject to fewer constraints than testing samplers. We propose a framework\nthat contains five statistical tests which are suited to test uniform random\nsamplers. Moreover, we demonstrate their use by testing seven samplers.\nFinally, we demonstrate the influence of the Boolean formula given as input to\nthe samplers under test on the test results."
                },
                "authors": [
                    {
                        "name": "Olivier Zeyen"
                    },
                    {
                        "name": "Maxime Cordy"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Gilles Perrouin"
                    },
                    {
                        "name": "Mathieu Acher"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Acher"
                },
                "author": "Mathieu Acher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14070v1",
                "updated": "2025-03-18T09:42:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    42,
                    55,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T09:42:55Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    42,
                    55,
                    1,
                    77,
                    0
                ],
                "title": "Fast Autoregressive Video Generation with Diagonal Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Autoregressive Video Generation with Diagonal Decoding"
                },
                "summary": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity."
                },
                "authors": [
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Junliang Guo"
                    },
                    {
                        "name": "Haoyu Wu"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Tim Pearce"
                    },
                    {
                        "name": "Tabish Rashid"
                    },
                    {
                        "name": "Katja Hofmann"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17178v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17178v3",
                "updated": "2025-03-18T09:09:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    9,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-24T17:01:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    1,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost"
                },
                "summary": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility."
                },
                "authors": [
                    {
                        "name": "David Salinas"
                    },
                    {
                        "name": "Omar Swelam"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17178v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17178v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14043v1",
                "updated": "2025-03-18T09:04:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    4,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T09:04:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    4,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Learning on LLM Output Signatures for gray-box LLM Behavior Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning on LLM Output Signatures for gray-box LLM Behavior Analysis"
                },
                "summary": "Large Language Models (LLMs) have achieved widespread adoption, yet our\nunderstanding of their behavior remains limited, particularly in detecting data\ncontamination and hallucinations. While recently proposed probing techniques\nprovide insights through activation analysis, they require \"white-box\" access\nto model internals, often unavailable. Current \"gray-box\" approaches typically\nanalyze only the probability of the actual tokens in the sequence with simple\ntask-specific heuristics. Importantly, these methods overlook the rich\ninformation contained in the full token distribution at each processing step.\nTo address these limitations, we propose that gray-box analysis should leverage\nthe complete observable output of LLMs, consisting of both the previously used\ntoken probabilities as well as the complete token distribution sequences - a\nunified data type we term LOS (LLM Output Signature). To this end, we develop a\ntransformer-based approach to process LOS that theoretically guarantees\napproximation of existing techniques while enabling more nuanced analysis. Our\napproach achieves superior performance on hallucination and data contamination\ndetection in gray-box settings, significantly outperforming existing baselines.\nFurthermore, it demonstrates strong transfer capabilities across datasets and\nLLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our\ncode is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved widespread adoption, yet our\nunderstanding of their behavior remains limited, particularly in detecting data\ncontamination and hallucinations. While recently proposed probing techniques\nprovide insights through activation analysis, they require \"white-box\" access\nto model internals, often unavailable. Current \"gray-box\" approaches typically\nanalyze only the probability of the actual tokens in the sequence with simple\ntask-specific heuristics. Importantly, these methods overlook the rich\ninformation contained in the full token distribution at each processing step.\nTo address these limitations, we propose that gray-box analysis should leverage\nthe complete observable output of LLMs, consisting of both the previously used\ntoken probabilities as well as the complete token distribution sequences - a\nunified data type we term LOS (LLM Output Signature). To this end, we develop a\ntransformer-based approach to process LOS that theoretically guarantees\napproximation of existing techniques while enabling more nuanced analysis. Our\napproach achieves superior performance on hallucination and data contamination\ndetection in gray-box settings, significantly outperforming existing baselines.\nFurthermore, it demonstrates strong transfer capabilities across datasets and\nLLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our\ncode is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network."
                },
                "authors": [
                    {
                        "name": "Guy Bar-Shalom"
                    },
                    {
                        "name": "Fabrizio Frasca"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Yoav Gelberg"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18326v2",
                "updated": "2025-03-18T08:57:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    57,
                    39,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-26T13:12:40Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    13,
                    12,
                    40,
                    2,
                    178,
                    0
                ],
                "title": "PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models"
                },
                "summary": "Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods."
                },
                "authors": [
                    {
                        "name": "Huixuan Zhang"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.14505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14505v1",
                "updated": "2025-03-18T17:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    58,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:59:58Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    58,
                    1,
                    77,
                    0
                ],
                "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MusicInfuser: Making Video Diffusion Listen and Dance"
                },
                "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser."
                },
                "authors": [
                    {
                        "name": "Susung Hong"
                    },
                    {
                        "name": "Ira Kemelmacher-Shlizerman"
                    },
                    {
                        "name": "Brian Curless"
                    },
                    {
                        "name": "Steven M. Seitz"
                    }
                ],
                "author_detail": {
                    "name": "Steven M. Seitz"
                },
                "author": "Steven M. Seitz",
                "arxiv_comment": "Project page: https://susunghong.github.io/MusicInfuser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14504v1",
                "updated": "2025-03-18T17:59:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    56,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    59,
                    56,
                    1,
                    77,
                    0
                ],
                "title": "Aligning Multimodal LLM with Human Preference: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Multimodal LLM with Human Preference: A Survey"
                },
                "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment."
                },
                "authors": [
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14495v1",
                "updated": "2025-03-18T17:58:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    58,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:58:28Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    58,
                    28,
                    1,
                    77,
                    0
                ],
                "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Consistency for LLM Reasoning Process Error Identification"
                },
                "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency"
                },
                "authors": [
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Kaixuan Huang"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14488v1",
                "updated": "2025-03-18T17:57:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    57,
                    16,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:57:16Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    57,
                    16,
                    1,
                    77,
                    0
                ],
                "title": "Engineering Scientific Assistants using Interactive Structured Induction\n  of Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Scientific Assistants using Interactive Structured Induction\n  of Programs"
                },
                "summary": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are interested in the construction of software that can act as scientific\nassistants to domain specialists. It is expected that such assistants will be\nneeded to accelerate the identification of ways to address complex problems\nrequiring urgent solutions. In this paper, our focus is not on a specific\nscientific problem, but on the software-engineering of such 'science\naccelerators'. Recent developments in 'No Code' techniques would seem to\nsuggest that scientist can simply hypothesise solutions simply by conversing\nwith a large language model (LLM). However, for complex scientific problems,\nthis seems unlikely given the current state of LLM technology. What does appear\nfeasible is that a software engineer can use LLMs to rapidly construct programs\nfor use by a domain-specialist, including the specialist's requirements\nexpressed in natural language. We propose the design of an interactive form of\n'structured' inductive programming in which a software-engineer and an LLM\ncollaboratively construct an 'assistant' for a scientific data analysis. The\npaper describes a simple implementation called iStrucInd that adapts a '2-way\nIntelligibility' protocol to implement the interaction between the software\nengineer and the LLM. We test the tool on two different non-trivial scientific\ndata analysis tasks. Specifically, we compare the system constructed by\niStrucInd against systems constructed manually and by Low Code/No Code methods\nalong dimensions of: (a) program performance; (b) program quality; and (c)\nprogramming effort. The results show iStrucInd allows a software engineer to\ndevelop better programs faster suggesting interactive structured induction can\nplay a useful role in the rapid construction of scientific assistants."
                },
                "authors": [
                    {
                        "name": "Shraddha Surana"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14484v1",
                "updated": "2025-03-18T17:54:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    54,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:54:14Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    54,
                    14,
                    1,
                    77,
                    0
                ],
                "title": "Gricean Norms as a Basis for Effective Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gricean Norms as a Basis for Effective Collaboration"
                },
                "summary": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Fardin Saad"
                    },
                    {
                        "name": "Pradeep K. Murukannaiah"
                    },
                    {
                        "name": "Munindar P. Singh"
                    }
                ],
                "author_detail": {
                    "name": "Munindar P. Singh"
                },
                "author": "Munindar P. Singh",
                "arxiv_comment": "Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures/tables.\n  (Appendix: 5 pages, 6 figures/tables). Code available at:\n  https://github.com/fardinsaad/Gricean-Norms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04382v2",
                "updated": "2025-03-18T17:51:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    56,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-05T18:58:02Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    2,
                    2,
                    36,
                    0
                ],
                "title": "Sparse Autoencoders for Hypothesis Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders for Hypothesis Generation"
                },
                "summary": "We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines."
                },
                "authors": [
                    {
                        "name": "Rajiv Movva"
                    },
                    {
                        "name": "Kenny Peng"
                    },
                    {
                        "name": "Nikhil Garg"
                    },
                    {
                        "name": "Jon Kleinberg"
                    },
                    {
                        "name": "Emma Pierson"
                    }
                ],
                "author_detail": {
                    "name": "Emma Pierson"
                },
                "author": "Emma Pierson",
                "arxiv_comment": "First two authors contributed equally; working paper. Code is\n  available at https://github.com/rmovva/HypotheSAEs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14478v1",
                "updated": "2025-03-18T17:51:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    34,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:51:34Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    34,
                    1,
                    77,
                    0
                ],
                "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM"
                },
                "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Zhijian Chen"
                    },
                    {
                        "name": "Kai Lan"
                    },
                    {
                        "name": "Shengyuan Ding"
                    },
                    {
                        "name": "Yingji Liang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Farong Wen"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Guofeng Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14477v1",
                "updated": "2025-03-18T17:51:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    4,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:51:04Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    51,
                    4,
                    1,
                    77,
                    0
                ],
                "title": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce\n  Hallucinations"
                },
                "summary": "LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%."
                },
                "authors": [
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Yeskendir Koishekenov"
                    },
                    {
                        "name": "Yejin Bang"
                    },
                    {
                        "name": "Anthony Hartshorn"
                    },
                    {
                        "name": "Alan Schelten"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Pascale Fung"
                    },
                    {
                        "name": "Nicola Cancedda"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Cancedda"
                },
                "author": "Nicola Cancedda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13803v2",
                "updated": "2025-03-18T17:49:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    49,
                    6,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-19T20:07:37Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    20,
                    7,
                    37,
                    2,
                    171,
                    0
                ],
                "title": "LLMs as Models for Analogical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Models for Analogical Reasoning"
                },
                "summary": "Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations."
                },
                "authors": [
                    {
                        "name": "Sam Musker"
                    },
                    {
                        "name": "Alex Duchnowski"
                    },
                    {
                        "name": "Raphaël Millière"
                    },
                    {
                        "name": "Ellie Pavlick"
                    }
                ],
                "author_detail": {
                    "name": "Ellie Pavlick"
                },
                "author": "Ellie Pavlick",
                "arxiv_comment": "The title has been changed from Semantic Structure-Mapping in LLM and\n  Human Analogical Reasoning to LLMs as Models for Analogical Reasoning to\n  improve clarity and accuracy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14476v1",
                "updated": "2025-03-18T17:49:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    49,
                    6,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:49:06Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    49,
                    6,
                    1,
                    77,
                    0
                ],
                "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale"
                },
                "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL."
                },
                "authors": [
                    {
                        "name": "Qiying Yu"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Ruofei Zhu"
                    },
                    {
                        "name": "Yufeng Yuan"
                    },
                    {
                        "name": "Xiaochen Zuo"
                    },
                    {
                        "name": "Yu Yue"
                    },
                    {
                        "name": "Tiantian Fan"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Lingjun Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Bole Ma"
                    },
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Yuxuan Tong"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Mofan Zhang"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Jinhua Zhu"
                    },
                    {
                        "name": "Jiaze Chen"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Chengyi Wang"
                    },
                    {
                        "name": "Hongli Yu"
                    },
                    {
                        "name": "Weinan Dai"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Xiangpeng Wei"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Mu Qiao"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Mingxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Wang"
                },
                "author": "Mingxuan Wang",
                "arxiv_comment": "Project Page: https://dapo-sia.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.04058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.04058v3",
                "updated": "2025-03-18T17:46:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    46,
                    26,
                    1,
                    77,
                    0
                ],
                "published": "2022-06-08T16:24:30Z",
                "published_parsed": [
                    2022,
                    6,
                    8,
                    16,
                    24,
                    30,
                    2,
                    159,
                    0
                ],
                "title": "Measuring the Evolution of the NuSTAR Detector Gains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Evolution of the NuSTAR Detector Gains"
                },
                "summary": "The memo describes the methods used to track the long-term gain variations in\nthe NuSTAR detectors. It builds on the analysis presented in Madsen et al.\n(2015) using the deployable calibration source to measure the gain drift in the\nNuSTAR CdZnTe detectors. This is intended to be a live document that is\nperiodically updated as new entries are required in the NuSTAR gain CALDB\nfiles. This document covers analysis up through late-2024 and the gain v011\nCALDB file released in version 20240226.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memo describes the methods used to track the long-term gain variations in\nthe NuSTAR detectors. It builds on the analysis presented in Madsen et al.\n(2015) using the deployable calibration source to measure the gain drift in the\nNuSTAR CdZnTe detectors. This is intended to be a live document that is\nperiodically updated as new entries are required in the NuSTAR gain CALDB\nfiles. This document covers analysis up through late-2024 and the gain v011\nCALDB file released in version 20240226."
                },
                "authors": [
                    {
                        "name": "Brian Grefenstette"
                    },
                    {
                        "name": "Murray Brightman"
                    },
                    {
                        "name": "Hannah P. Earnshaw"
                    },
                    {
                        "name": "Karl Forster"
                    },
                    {
                        "name": "Kristin K. Madsen"
                    },
                    {
                        "name": "Hiromasa Miyasaka"
                    }
                ],
                "author_detail": {
                    "name": "Hiromasa Miyasaka"
                },
                "author": "Hiromasa Miyasaka",
                "arxiv_comment": "11 page, 7 figures. Intended as a living, easy-to-find document. No\n  intention of submitting this to a journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.04058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.04058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14443v1",
                "updated": "2025-03-18T17:19:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    19,
                    12,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:19:12Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    19,
                    12,
                    1,
                    77,
                    0
                ],
                "title": "EnvBench: A Benchmark for Automated Environment Setup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnvBench: A Benchmark for Automated Environment Setup"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench."
                },
                "authors": [
                    {
                        "name": "Aleksandra Eliseeva"
                    },
                    {
                        "name": "Alexander Kovrigin"
                    },
                    {
                        "name": "Ilia Kholkin"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    }
                ],
                "author_detail": {
                    "name": "Yaroslav Zharov"
                },
                "author": "Yaroslav Zharov",
                "arxiv_comment": "Accepted at the DL4Code workshop at ICLR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15378v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15378v5",
                "updated": "2025-03-18T17:14:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    14,
                    43,
                    1,
                    77,
                    0
                ],
                "published": "2024-01-27T10:50:11Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    10,
                    50,
                    11,
                    5,
                    27,
                    0
                ],
                "title": "A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM"
                },
                "summary": "Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given."
                },
                "authors": [
                    {
                        "name": "Ahmet Yusuf Alan"
                    },
                    {
                        "name": "Enis Karaarslan"
                    },
                    {
                        "name": "Ömer Aydin"
                    }
                ],
                "author_detail": {
                    "name": "Ömer Aydin"
                },
                "author": "Ömer Aydin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15378v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15378v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14434v1",
                "updated": "2025-03-18T17:11:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    11,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:11:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    11,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers"
                },
                "summary": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Nikhil Abhyankar"
                    },
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14432v1",
                "updated": "2025-03-18T17:09:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    9,
                    57,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T17:09:57Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    9,
                    57,
                    1,
                    77,
                    0
                ],
                "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play"
                },
                "summary": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration."
                },
                "authors": [
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Kaizhi Qian"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18841v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18841v4",
                "updated": "2025-03-18T16:57:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    57,
                    17,
                    1,
                    77,
                    0
                ],
                "published": "2024-05-14T15:03:05Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    15,
                    3,
                    5,
                    1,
                    135,
                    0
                ],
                "title": "Navigating LLM Ethics: Advancements, Challenges, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating LLM Ethics: Advancements, Challenges, and Future Directions"
                },
                "summary": "This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Yiming Xu"
                    },
                    {
                        "name": "Connor Phillips"
                    }
                ],
                "author_detail": {
                    "name": "Connor Phillips"
                },
                "author": "Connor Phillips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18841v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18841v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14411v1",
                "updated": "2025-03-18T16:50:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    50,
                    10,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:50:10Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    50,
                    10,
                    1,
                    77,
                    0
                ],
                "title": "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models"
                },
                "summary": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness."
                },
                "authors": [
                    {
                        "name": "Siwei Zhang"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Yateng Tang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Zian Jia"
                    },
                    {
                        "name": "Zehao Gu"
                    },
                    {
                        "name": "Jiarong Xu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "arxiv_comment": "Submit to ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02063v3",
                "updated": "2025-03-18T16:45:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    45,
                    54,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-03T19:16:36Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    19,
                    16,
                    36,
                    4,
                    3,
                    0
                ],
                "title": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models"
                },
                "summary": "This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.18842,\n  arXiv:2501.00959",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00959v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00959v3",
                "updated": "2025-03-18T16:44:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    44,
                    15,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-01T21:31:47Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    31,
                    47,
                    2,
                    1,
                    0
                ],
                "title": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs"
                },
                "summary": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00959v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00959v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18842v3",
                "updated": "2025-03-18T16:42:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    42,
                    30,
                    1,
                    77,
                    0
                ],
                "published": "2024-05-26T15:28:24Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    15,
                    28,
                    24,
                    6,
                    147,
                    0
                ],
                "title": "The global landscape of academic guidelines for generative AI and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global landscape of academic guidelines for generative AI and Large\n  Language Models"
                },
                "summary": "The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09292v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09292v3",
                "updated": "2025-03-18T16:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    42,
                    17,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-16T04:56:33Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    56,
                    33,
                    3,
                    16,
                    0
                ],
                "title": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy."
                },
                "authors": [
                    {
                        "name": "Kaustubh D. Dhole"
                    }
                ],
                "author_detail": {
                    "name": "Kaustubh D. Dhole"
                },
                "author": "Kaustubh D. Dhole",
                "arxiv_comment": "1st workshop of \"Quantify Uncertainty and Hallucination in Foundation\n  Models: The Next Frontier in Reliable AI\" at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09292v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09292v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16970v3",
                "updated": "2025-03-18T16:34:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    34,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-24T03:32:05Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    3,
                    32,
                    5,
                    2,
                    206,
                    0
                ],
                "title": "Towards Aligning Language Models with Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Aligning Language Models with Textual Feedback"
                },
                "summary": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback."
                },
                "authors": [
                    {
                        "name": "Saüc Abadal Lloret"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10647v2",
                "updated": "2025-03-18T16:27:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    27,
                    1,
                    1,
                    77,
                    0
                ],
                "published": "2024-09-16T18:23:20Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    23,
                    20,
                    0,
                    260,
                    0
                ],
                "title": "Safe Interval Motion Planning for Quadrotors in Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Interval Motion Planning for Quadrotors in Dynamic Environments"
                },
                "summary": "Trajectory generation in dynamic environments presents a significant\nchallenge for quadrotors, particularly due to the non-convexity in the\nspatial-temporal domain. Many existing methods either assume simplified static\nenvironments or struggle to produce optimal solutions in real-time. In this\nwork, we propose an efficient safe interval motion planning framework for\nnavigation in dynamic environments. A safe interval refers to a time window\nduring which a specific configuration is safe. Our approach addresses\ntrajectory generation through a two-stage process: a front-end graph search\nstep followed by a back-end gradient-based optimization. We ensure completeness\nand optimality by constructing a dynamic connected visibility graph and\nincorporating low-order dynamic bounds within safe intervals and temporal\ncorridors. To avoid local minima, we propose a Uniform Temporal Visibility\nDeformation (UTVD) for the complete evaluation of spatial-temporal topological\nequivalence. We represent trajectories with B-Spline curves and apply\ngradient-based optimization to navigate around static and moving obstacles\nwithin spatial-temporal corridors. Through simulation and real-world\nexperiments, we show that our method can achieve a success rate of over 95% in\nenvironments with different density levels, exceeding the performance of other\napproaches, demonstrating its potential for practical deployment in highly\ndynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory generation in dynamic environments presents a significant\nchallenge for quadrotors, particularly due to the non-convexity in the\nspatial-temporal domain. Many existing methods either assume simplified static\nenvironments or struggle to produce optimal solutions in real-time. In this\nwork, we propose an efficient safe interval motion planning framework for\nnavigation in dynamic environments. A safe interval refers to a time window\nduring which a specific configuration is safe. Our approach addresses\ntrajectory generation through a two-stage process: a front-end graph search\nstep followed by a back-end gradient-based optimization. We ensure completeness\nand optimality by constructing a dynamic connected visibility graph and\nincorporating low-order dynamic bounds within safe intervals and temporal\ncorridors. To avoid local minima, we propose a Uniform Temporal Visibility\nDeformation (UTVD) for the complete evaluation of spatial-temporal topological\nequivalence. We represent trajectories with B-Spline curves and apply\ngradient-based optimization to navigate around static and moving obstacles\nwithin spatial-temporal corridors. Through simulation and real-world\nexperiments, we show that our method can achieve a success rate of over 95% in\nenvironments with different density levels, exceeding the performance of other\napproaches, demonstrating its potential for practical deployment in highly\ndynamic environments."
                },
                "authors": [
                    {
                        "name": "Songhao Huang"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yuezhan Tao"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "2025 IEEE International Conference on Robotics & Automation(ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14392v1",
                "updated": "2025-03-18T16:27:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    27,
                    1,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:27:01Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    27,
                    1,
                    1,
                    77,
                    0
                ],
                "title": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to\n  Enhance Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to\n  Enhance Large Language Models"
                },
                "summary": "This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems."
                },
                "authors": [
                    {
                        "name": "Qiantong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qiantong Wang"
                },
                "author": "Qiantong Wang",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14391v1",
                "updated": "2025-03-18T16:26:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    26,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:26:29Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    26,
                    29,
                    1,
                    77,
                    0
                ],
                "title": "How much do LLMs learn from negative examples?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much do LLMs learn from negative examples?"
                },
                "summary": "Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs."
                },
                "authors": [
                    {
                        "name": "Shadi Hamdan"
                    },
                    {
                        "name": "Deniz Yuret"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Yuret"
                },
                "author": "Deniz Yuret",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14382v1",
                "updated": "2025-03-18T16:15:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    15,
                    55,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:15:55Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    15,
                    55,
                    1,
                    77,
                    0
                ],
                "title": "Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation"
                },
                "summary": "The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions."
                },
                "authors": [
                    {
                        "name": "Rikuto Tsuchida"
                    },
                    {
                        "name": "Hibiki Yokoyama"
                    },
                    {
                        "name": "Takehito Utsuro"
                    }
                ],
                "author_detail": {
                    "name": "Takehito Utsuro"
                },
                "author": "Takehito Utsuro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14379v1",
                "updated": "2025-03-18T16:12:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    12,
                    4,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:12:04Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    12,
                    4,
                    1,
                    77,
                    0
                ],
                "title": "On the Standard Performance Criteria for Applied Control Design: PID,\n  MPC or Machine Learning Controller?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Standard Performance Criteria for Applied Control Design: PID,\n  MPC or Machine Learning Controller?"
                },
                "summary": "The traditional control theory and its application to basic and complex\nsystems have reached an advanced level of maturity. This includes aerial,\nmarine, and ground vehicles, as well as robotics, chemical, transportation, and\nelectrical systems widely used in our daily lives. The emerging era of\ndata-driven methods, Large Language Models (LLMs), and AI-based controllers\ndoes not indicate a weakness in well-established control theory. Instead, it\naims to reduce dependence on models and uncertainties, address increasingly\ncomplex systems, and potentially achieve decision-making capabilities\ncomparable to human-level performance. This revolution integrates knowledge\nfrom computer science, machine learning, biology, and classical control,\nproducing promising algorithms that are yet to demonstrate widespread\nreal-world applicability. Despite the maturity of control theory and the\npresence of various performance criteria, there is still a lack of standardised\nmetrics for testing, evaluation, Verification and Validation ($V\\&V$) of\nalgorithms. This gap can lead to algorithms that, while optimal in certain\naspects, may fall short of practical implementation, sparking debates within\nthe literature. For a controller to succeed in real-world applications, it must\nsatisfy three key categories of performance metrics: tracking quality, control\neffort (energy consumption), and robustness. This paper rather takes an applied\nperspective, proposing and consolidating standard performance criteria for\ntesting and analysing control systems, intended for researchers and students.\nThe proposed framework ensures the post-design applicability of a black-box\nalgorithm, aligning with modern data analysis and $V\\&V$ perspectives to\nprevent resource allocation to systems with limited impact or imprecise claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The traditional control theory and its application to basic and complex\nsystems have reached an advanced level of maturity. This includes aerial,\nmarine, and ground vehicles, as well as robotics, chemical, transportation, and\nelectrical systems widely used in our daily lives. The emerging era of\ndata-driven methods, Large Language Models (LLMs), and AI-based controllers\ndoes not indicate a weakness in well-established control theory. Instead, it\naims to reduce dependence on models and uncertainties, address increasingly\ncomplex systems, and potentially achieve decision-making capabilities\ncomparable to human-level performance. This revolution integrates knowledge\nfrom computer science, machine learning, biology, and classical control,\nproducing promising algorithms that are yet to demonstrate widespread\nreal-world applicability. Despite the maturity of control theory and the\npresence of various performance criteria, there is still a lack of standardised\nmetrics for testing, evaluation, Verification and Validation ($V\\&V$) of\nalgorithms. This gap can lead to algorithms that, while optimal in certain\naspects, may fall short of practical implementation, sparking debates within\nthe literature. For a controller to succeed in real-world applications, it must\nsatisfy three key categories of performance metrics: tracking quality, control\neffort (energy consumption), and robustness. This paper rather takes an applied\nperspective, proposing and consolidating standard performance criteria for\ntesting and analysing control systems, intended for researchers and students.\nThe proposed framework ensures the post-design applicability of a black-box\nalgorithm, aligning with modern data analysis and $V\\&V$ perspectives to\nprevent resource allocation to systems with limited impact or imprecise claims."
                },
                "authors": [
                    {
                        "name": "Pouria Sarhadi"
                    }
                ],
                "author_detail": {
                    "name": "Pouria Sarhadi"
                },
                "author": "Pouria Sarhadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14378v1",
                "updated": "2025-03-18T16:10:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    10,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T16:10:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    10,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "Impossible Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impossible Videos"
                },
                "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models."
                },
                "authors": [
                    {
                        "name": "Zechen Bai"
                    },
                    {
                        "name": "Hai Ci"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01250v3",
                "updated": "2025-03-18T16:09:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    9,
                    20,
                    1,
                    77,
                    0
                ],
                "published": "2024-12-02T08:16:38Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    16,
                    38,
                    0,
                    337,
                    0
                ],
                "title": "Collaborative Instance Object Navigation: Leveraging\n  Uncertainty-Awareness to Minimize Human-Agent Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Instance Object Navigation: Leveraging\n  Uncertainty-Awareness to Minimize Human-Agent Dialogues"
                },
                "summary": "Language-driven instance object navigation assumes that human users initiate\nthe task by providing a detailed description of the target instance to the\nembodied agent. While this description is crucial for distinguishing the target\nfrom visually similar instances in a scene, providing it prior to navigation\ncan be demanding for human. To bridge this gap, we introduce Collaborative\nInstance object Navigation (CoIN), a new task setting where the agent actively\nresolve uncertainties about the target instance during navigation in natural,\ntemplate-free, open-ended dialogues with human. We propose a novel\ntraining-free method, Agent-user Interaction with UncerTainty Awareness\n(AIUTA), which operates independently from the navigation policy, and focuses\non the human-agent interaction reasoning with Vision-Language Models (VLMs) and\nLarge Language Models (LLMs). First, upon object detection, a Self-Questioner\nmodel initiates a self-dialogue within the agent to obtain a complete and\naccurate observation description with a novel uncertainty estimation technique.\nThen, an Interaction Trigger module determines whether to ask a question to the\nhuman, continue or halt navigation, minimizing user input. For evaluation, we\nintroduce CoIN-Bench, with a curated dataset designed for challenging\nmulti-instance scenarios. CoIN-Bench supports both online evaluation with\nhumans and reproducible experiments with simulated user-agent interactions. On\nCoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing\nlanguage-driven instance navigation methods struggle in complex multi-instance\nscenes. Code and benchmark will be available upon acceptance at\nhttps://intelligolabs.github.io/CoIN/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-driven instance object navigation assumes that human users initiate\nthe task by providing a detailed description of the target instance to the\nembodied agent. While this description is crucial for distinguishing the target\nfrom visually similar instances in a scene, providing it prior to navigation\ncan be demanding for human. To bridge this gap, we introduce Collaborative\nInstance object Navigation (CoIN), a new task setting where the agent actively\nresolve uncertainties about the target instance during navigation in natural,\ntemplate-free, open-ended dialogues with human. We propose a novel\ntraining-free method, Agent-user Interaction with UncerTainty Awareness\n(AIUTA), which operates independently from the navigation policy, and focuses\non the human-agent interaction reasoning with Vision-Language Models (VLMs) and\nLarge Language Models (LLMs). First, upon object detection, a Self-Questioner\nmodel initiates a self-dialogue within the agent to obtain a complete and\naccurate observation description with a novel uncertainty estimation technique.\nThen, an Interaction Trigger module determines whether to ask a question to the\nhuman, continue or halt navigation, minimizing user input. For evaluation, we\nintroduce CoIN-Bench, with a curated dataset designed for challenging\nmulti-instance scenarios. CoIN-Bench supports both online evaluation with\nhumans and reproducible experiments with simulated user-agent interactions. On\nCoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing\nlanguage-driven instance navigation methods struggle in complex multi-instance\nscenes. Code and benchmark will be available upon acceptance at\nhttps://intelligolabs.github.io/CoIN/"
                },
                "authors": [
                    {
                        "name": "Francesco Taioli"
                    },
                    {
                        "name": "Edoardo Zorzi"
                    },
                    {
                        "name": "Gianni Franchi"
                    },
                    {
                        "name": "Alberto Castellini"
                    },
                    {
                        "name": "Alessandro Farinelli"
                    },
                    {
                        "name": "Marco Cristani"
                    },
                    {
                        "name": "Yiming Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Wang"
                },
                "author": "Yiming Wang",
                "arxiv_comment": "https://intelligolabs.github.io/CoIN/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12617v2",
                "updated": "2025-03-18T16:08:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    16,
                    8,
                    31,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T08:02:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    8,
                    2,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft\n  Landing Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft\n  Landing Problem"
                },
                "summary": "The Aircraft Landing Problem (ALP) is one of the challenging problems in\naircraft transportation and management. The challenge is to schedule the\narriving aircraft in a sequence so that the cost and delays are optimized.\nThere are various solution approaches to solving this problem, most of which\nare based on operations research algorithms and meta-heuristics. Although\ntraditional methods perform better on one or the other factors, there remains a\nproblem of solving real-time rescheduling and computational scalability\naltogether. This paper presents a novel deep reinforcement learning (DRL)\nframework that combines graph neural networks with actor-critic architectures\nto address the ALP. This paper introduces three key contributions: A\ngraph-based state representation that efficiently captures temporal and spatial\nrelationships between aircraft, a specialized actor-critic architecture\ndesigned to handle multiple competing objectives in landing scheduling, and a\nrunway balance strategy that ensures efficient resource utilization while\nmaintaining safety constraints. The results show that the trained algorithm can\nbe tested on different problem sets and the results are competitive to\noperation research algorithms. The experimental results on standard benchmark\ndata sets demonstrate a 99.95% reduction in computational time compared to\nMixed Integer Programming (MIP) and 38% higher runway throughput over First\nCome First Serve (FCFS) approaches. Therefore, the proposed solution is\ncompetitive to traditional approaches and achieves substantial advancements.\nNotably, it does not require retraining, making it particularly suitable for\nindustrial deployment. The frameworks capability to generate solutions within 1\nsecond enables real-time rescheduling, addressing critical requirements of air\ntraffic management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aircraft Landing Problem (ALP) is one of the challenging problems in\naircraft transportation and management. The challenge is to schedule the\narriving aircraft in a sequence so that the cost and delays are optimized.\nThere are various solution approaches to solving this problem, most of which\nare based on operations research algorithms and meta-heuristics. Although\ntraditional methods perform better on one or the other factors, there remains a\nproblem of solving real-time rescheduling and computational scalability\naltogether. This paper presents a novel deep reinforcement learning (DRL)\nframework that combines graph neural networks with actor-critic architectures\nto address the ALP. This paper introduces three key contributions: A\ngraph-based state representation that efficiently captures temporal and spatial\nrelationships between aircraft, a specialized actor-critic architecture\ndesigned to handle multiple competing objectives in landing scheduling, and a\nrunway balance strategy that ensures efficient resource utilization while\nmaintaining safety constraints. The results show that the trained algorithm can\nbe tested on different problem sets and the results are competitive to\noperation research algorithms. The experimental results on standard benchmark\ndata sets demonstrate a 99.95% reduction in computational time compared to\nMixed Integer Programming (MIP) and 38% higher runway throughput over First\nCome First Serve (FCFS) approaches. Therefore, the proposed solution is\ncompetitive to traditional approaches and achieves substantial advancements.\nNotably, it does not require retraining, making it particularly suitable for\nindustrial deployment. The frameworks capability to generate solutions within 1\nsecond enables real-time rescheduling, addressing critical requirements of air\ntraffic management."
                },
                "authors": [
                    {
                        "name": "Vatsal Maru"
                    }
                ],
                "author_detail": {
                    "name": "Vatsal Maru"
                },
                "author": "Vatsal Maru",
                "arxiv_comment": "27 pages, submitted to ESWA, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07917v2",
                "updated": "2025-03-18T15:49:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    49,
                    8,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-12T16:49:51Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    16,
                    49,
                    51,
                    1,
                    317,
                    0
                ],
                "title": "CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts"
                },
                "summary": "The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Updated and Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14366v1",
                "updated": "2025-03-18T15:48:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    48,
                    9,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T15:48:09Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    48,
                    9,
                    1,
                    77,
                    0
                ],
                "title": "QuGStep: Refining Step Size Selection in Gradient Estimation for\n  Variational Quantum Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuGStep: Refining Step Size Selection in Gradient Estimation for\n  Variational Quantum Algorithms"
                },
                "summary": "Variational quantum algorithms (VQAs) offer a promising approach to solving\ncomputationally demanding problems by combining parameterized quantum circuits\nwith classical optimization. Estimating probabilistic outcomes on quantum\nhardware requires repeated measurements (shots). However, in practice, the\nlimited shot budget introduces significant noise in the evaluation of the\nobjective function. Gradient estimation in VQAs often relies on the\nfinite-difference, which evaluates the noisy objective function at perturbed\ncircuit parameter values. The accuracy of this estimation is highly dependent\non the choice of step size for these perturbations. An inappropriate step size\ncan exacerbate the impact of noise, causing inaccurate gradient estimates and\nhindering the classical optimization in VQAs. This paper proposes QuGStep, an\nalgorithm that addresses the challenge of determining the appropriate step size\nfor finite-difference gradient estimation under a shot budget. QuGStep is\ngrounded in a theorem that proves the optimal step size, which accounts for the\nshot budget, minimizes the error bound in gradient estimation using finite\ndifferences. Numerical experiments approximating the ground state energy of\nseveral molecules demonstrate that QuGStep can identify the appropriate step\nsize for the given shot budget to obtain effective gradient estimation.\nNotably, the step size identified by QuGStep achieved convergence to the ground\nstate energy with over 96% fewer shots compared to using a default step size.\nThese findings highlight the potential of QuGStep to improve the practical\ndeployment and scalability of quantum computing technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational quantum algorithms (VQAs) offer a promising approach to solving\ncomputationally demanding problems by combining parameterized quantum circuits\nwith classical optimization. Estimating probabilistic outcomes on quantum\nhardware requires repeated measurements (shots). However, in practice, the\nlimited shot budget introduces significant noise in the evaluation of the\nobjective function. Gradient estimation in VQAs often relies on the\nfinite-difference, which evaluates the noisy objective function at perturbed\ncircuit parameter values. The accuracy of this estimation is highly dependent\non the choice of step size for these perturbations. An inappropriate step size\ncan exacerbate the impact of noise, causing inaccurate gradient estimates and\nhindering the classical optimization in VQAs. This paper proposes QuGStep, an\nalgorithm that addresses the challenge of determining the appropriate step size\nfor finite-difference gradient estimation under a shot budget. QuGStep is\ngrounded in a theorem that proves the optimal step size, which accounts for the\nshot budget, minimizes the error bound in gradient estimation using finite\ndifferences. Numerical experiments approximating the ground state energy of\nseveral molecules demonstrate that QuGStep can identify the appropriate step\nsize for the given shot budget to obtain effective gradient estimation.\nNotably, the step size identified by QuGStep achieved convergence to the ground\nstate energy with over 96% fewer shots compared to using a default step size.\nThese findings highlight the potential of QuGStep to improve the practical\ndeployment and scalability of quantum computing technologies."
                },
                "authors": [
                    {
                        "name": "Senwei Liang"
                    },
                    {
                        "name": "Linghua Zhu"
                    },
                    {
                        "name": "Xiaosong Li"
                    },
                    {
                        "name": "Chao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Yang"
                },
                "author": "Chao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06946v2",
                "updated": "2025-03-18T15:36:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    36,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-11T12:54:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    54,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models"
                },
                "summary": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Updated and Final Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07933v2",
                "updated": "2025-03-18T15:30:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    30,
                    8,
                    1,
                    77,
                    0
                ],
                "published": "2024-10-10T14:00:21Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    0,
                    21,
                    3,
                    284,
                    0
                ],
                "title": "Offline Hierarchical Reinforcement Learning via Inverse Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Hierarchical Reinforcement Learning via Inverse Optimization"
                },
                "summary": "Hierarchical policies enable strong performance in many sequential\ndecision-making problems, such as those with high-dimensional action spaces,\nthose requiring long-horizon planning, and settings with sparse rewards.\nHowever, learning hierarchical policies from static offline datasets presents a\nsignificant challenge. Crucially, actions taken by higher-level policies may\nnot be directly observable within hierarchical controllers, and the offline\ndataset might have been generated using a different policy structure, hindering\nthe use of standard offline learning algorithms. In this work, we propose OHIO:\na framework for offline reinforcement learning (RL) of hierarchical policies.\nOur framework leverages knowledge of the policy structure to solve the\n\\textit{inverse problem}, recovering the unobservable high-level actions that\nlikely generated the observed data under our hierarchical policy. This approach\nconstructs a dataset suitable for off-the-shelf offline training. We\ndemonstrate our framework on robotic and network optimization problems and show\nthat it substantially outperforms end-to-end RL methods and improves\nrobustness. We investigate a variety of instantiations of our framework, both\nin direct deployment of policies trained offline and when online fine-tuning is\nperformed. Code and data are available at\nhttps://ohio-offline-hierarchical-rl.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical policies enable strong performance in many sequential\ndecision-making problems, such as those with high-dimensional action spaces,\nthose requiring long-horizon planning, and settings with sparse rewards.\nHowever, learning hierarchical policies from static offline datasets presents a\nsignificant challenge. Crucially, actions taken by higher-level policies may\nnot be directly observable within hierarchical controllers, and the offline\ndataset might have been generated using a different policy structure, hindering\nthe use of standard offline learning algorithms. In this work, we propose OHIO:\na framework for offline reinforcement learning (RL) of hierarchical policies.\nOur framework leverages knowledge of the policy structure to solve the\n\\textit{inverse problem}, recovering the unobservable high-level actions that\nlikely generated the observed data under our hierarchical policy. This approach\nconstructs a dataset suitable for off-the-shelf offline training. We\ndemonstrate our framework on robotic and network optimization problems and show\nthat it substantially outperforms end-to-end RL methods and improves\nrobustness. We investigate a variety of instantiations of our framework, both\nin direct deployment of policies trained offline and when online fine-tuning is\nperformed. Code and data are available at\nhttps://ohio-offline-hierarchical-rl.github.io"
                },
                "authors": [
                    {
                        "name": "Carolin Schmidt"
                    },
                    {
                        "name": "Daniele Gammelli"
                    },
                    {
                        "name": "James Harrison"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Filipe Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Filipe Rodrigues"
                },
                "author": "Filipe Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16657v3",
                "updated": "2025-03-18T15:19:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    19,
                    15,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-25T18:41:56Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    41,
                    56,
                    0,
                    330,
                    0
                ],
                "title": "DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation"
                },
                "summary": "Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples."
                },
                "authors": [
                    {
                        "name": "Zun Wang"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Project website: https://zunwang1.github.io/DreamRunner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14340v1",
                "updated": "2025-03-18T15:16:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    16,
                    51,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T15:16:51Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    16,
                    51,
                    1,
                    77,
                    0
                ],
                "title": "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration"
                },
                "summary": "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks."
                },
                "authors": [
                    {
                        "name": "Yisen Xu"
                    },
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Jinqiu Yang"
                    },
                    {
                        "name": "Tse-Hsun"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Nikolaos Tsantalis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Tsantalis"
                },
                "arxiv_affiliation": "Peter",
                "author": "Nikolaos Tsantalis",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16793v2",
                "updated": "2025-03-18T15:07:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    7,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-24T03:04:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    3,
                    4,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on\n  Contrastive Learning"
                },
                "summary": "Graph Neural Networks (GNNs) have gained attention for their ability to learn\nrepresentations from graph data. Due to privacy concerns and conflicts of\ninterest that prevent clients from directly sharing graph data with one\nanother, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial\nattacks that degrade performance. However, it is a common problem that client\nnodes are often unlabeled in the realm of VGFL. Consequently, the existing\nattacks, which rely on the availability of labeling information to obtain\ngradients, are inherently constrained in their applicability. This limitation\nprecludes their deployment in practical, real-world environments. To address\nthe above problems, we propose a novel graph adversarial attack against VGFL,\nreferred to as VGFL-SA, to degrade the performance of VGFL by modifying the\nlocal clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are\ntrained. VGFL-SA first accesses the graph structure and node feature\ninformation of the poisoned clients, and generates the contrastive views by\nnode-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and\nthe gradients of the adjacency matrices are obtained by the contrastive\nfunction. Finally, perturbed edges are generated using gradient modification\nrules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA\nachieves good attack effectiveness and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have gained attention for their ability to learn\nrepresentations from graph data. Due to privacy concerns and conflicts of\ninterest that prevent clients from directly sharing graph data with one\nanother, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial\nattacks that degrade performance. However, it is a common problem that client\nnodes are often unlabeled in the realm of VGFL. Consequently, the existing\nattacks, which rely on the availability of labeling information to obtain\ngradients, are inherently constrained in their applicability. This limitation\nprecludes their deployment in practical, real-world environments. To address\nthe above problems, we propose a novel graph adversarial attack against VGFL,\nreferred to as VGFL-SA, to degrade the performance of VGFL by modifying the\nlocal clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are\ntrained. VGFL-SA first accesses the graph structure and node feature\ninformation of the poisoned clients, and generates the contrastive views by\nnode-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and\nthe gradients of the adjacency matrices are obtained by the contrastive\nfunction. Finally, perturbed edges are generated using gradient modification\nrules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA\nachieves good attack effectiveness and transferability."
                },
                "authors": [
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Bin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhou"
                },
                "author": "Bin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14506v2",
                "updated": "2025-03-18T15:03:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    3,
                    47,
                    1,
                    77,
                    0
                ],
                "published": "2024-09-22T16:10:10Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    10,
                    10,
                    6,
                    266,
                    0
                ],
                "title": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy"
                },
                "summary": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot (HSR). Our method achieves a 93%\nsuccess rate in the 'fetch me' task completion with failure recovery,\nhighlighting its capability in both failure reasoning and task planning.\nInteLiPlan achieves comparable performance to state-of-the-art large-scale\nLLM-based robotics planners, while using only real-time onboard computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot (HSR). Our method achieves a 93%\nsuccess rate in the 'fetch me' task completion with failure recovery,\nhighlighting its capability in both failure reasoning and task planning.\nInteLiPlan achieves comparable performance to state-of-the-art large-scale\nLLM-based robotics planners, while using only real-time onboard computing."
                },
                "authors": [
                    {
                        "name": "Kim Tien Ly"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Ioannis Havoutis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Havoutis"
                },
                "author": "Ioannis Havoutis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13281v2",
                "updated": "2025-03-18T14:56:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    56,
                    41,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-17T15:31:55Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    55,
                    0,
                    76,
                    0
                ],
                "title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation"
                },
                "summary": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines."
                },
                "authors": [
                    {
                        "name": "Xiaodi Li"
                    },
                    {
                        "name": "Shaika Chowdhury"
                    },
                    {
                        "name": "Chung Il Wi"
                    },
                    {
                        "name": "Maria Vassilaki"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Terence T Sio"
                    },
                    {
                        "name": "Owen Garrick"
                    },
                    {
                        "name": "Young J Juhn"
                    },
                    {
                        "name": "James R Cerhan"
                    },
                    {
                        "name": "Cui Tao"
                    },
                    {
                        "name": "Nansu Zong"
                    }
                ],
                "author_detail": {
                    "name": "Nansu Zong"
                },
                "author": "Nansu Zong",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14321v1",
                "updated": "2025-03-18T14:51:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    51,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:51:42Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    51,
                    42,
                    1,
                    77,
                    0
                ],
                "title": "COPA: Comparing the Incomparable to Explore the Pareto Front",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COPA: Comparing the Incomparable to Explore the Pareto Front"
                },
                "summary": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail."
                },
                "authors": [
                    {
                        "name": "Adrián Javaloy"
                    },
                    {
                        "name": "Antonio Vergari"
                    },
                    {
                        "name": "Isabel Valera"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Valera"
                },
                "author": "Isabel Valera",
                "arxiv_comment": "19 pages, 14 figures. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14317v1",
                "updated": "2025-03-18T14:50:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    50,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:50:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    50,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "Near-Field Beam Prediction Using Far-Field Codebooks in Ultra-Massive\n  MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Field Beam Prediction Using Far-Field Codebooks in Ultra-Massive\n  MIMO Systems"
                },
                "summary": "Ultra-massive multiple-input multiple-output (UM-MIMO) technology is a key\nenabler for 6G networks, offering exceptional high data rates in\nmillimeter-wave (mmWave) and Terahertz (THz) frequency bands. The deployment of\nlarge antenna arrays at high frequencies transitions wireless communication\ninto the radiative near-field, where precise beam alignment becomes essential\nfor accurate channel estimation. Unlike far-field systems, which rely on\nangular domain only, near-field necessitates beam search across both angle and\ndistance dimensions, leading to substantially higher training overhead. To\naddress this challenge, we propose a discrete Fourier transform (DFT) based\nbeam alignment to mitigate the training overhead. We highlight that the reduced\npath loss at shorter distances can compensate for the beamforming losses\ntypically associated with using far-field codebooks in near-field scenarios.\nAdditionally, far-field beamforming in the near-field exhibits angular spread,\nwith its width determined by the user's range and angle. Leveraging this\nrelationship, we develop a correlation interferometry (CI) algorithm, termed\nCI-DFT, to efficiently estimate user angle and range parameters. Simulation\nresults demonstrate that the proposed scheme achieves performance close to\nexhaustive search in terms of achievable rate while significantly reducing the\ntraining overhead by 87.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-massive multiple-input multiple-output (UM-MIMO) technology is a key\nenabler for 6G networks, offering exceptional high data rates in\nmillimeter-wave (mmWave) and Terahertz (THz) frequency bands. The deployment of\nlarge antenna arrays at high frequencies transitions wireless communication\ninto the radiative near-field, where precise beam alignment becomes essential\nfor accurate channel estimation. Unlike far-field systems, which rely on\nangular domain only, near-field necessitates beam search across both angle and\ndistance dimensions, leading to substantially higher training overhead. To\naddress this challenge, we propose a discrete Fourier transform (DFT) based\nbeam alignment to mitigate the training overhead. We highlight that the reduced\npath loss at shorter distances can compensate for the beamforming losses\ntypically associated with using far-field codebooks in near-field scenarios.\nAdditionally, far-field beamforming in the near-field exhibits angular spread,\nwith its width determined by the user's range and angle. Leveraging this\nrelationship, we develop a correlation interferometry (CI) algorithm, termed\nCI-DFT, to efficiently estimate user angle and range parameters. Simulation\nresults demonstrate that the proposed scheme achieves performance close to\nexhaustive search in terms of achievable rate while significantly reducing the\ntraining overhead by 87.5%."
                },
                "authors": [
                    {
                        "name": "Ahmed Hussain"
                    },
                    {
                        "name": "Asmaa Abdallah"
                    },
                    {
                        "name": "Abdulkadir Celik"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Eltawil"
                },
                "author": "Ahmed M. Eltawil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09647v2",
                "updated": "2025-03-18T14:37:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    37,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T08:41:36Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    41,
                    36,
                    2,
                    71,
                    0
                ],
                "title": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading"
                },
                "summary": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level."
                },
                "authors": [
                    {
                        "name": "Ryan Quek Wei Heng"
                    },
                    {
                        "name": "Edoardo Vittori"
                    },
                    {
                        "name": "Keane Ong"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Gianmarco Mengaldo"
                    }
                ],
                "author_detail": {
                    "name": "Gianmarco Mengaldo"
                },
                "author": "Gianmarco Mengaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14287v1",
                "updated": "2025-03-18T14:24:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    24,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:24:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    24,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "Cross-Environment Transfer Learning for Location-Aided Beam Prediction\n  in 5G and Beyond Millimeter-Wave Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Environment Transfer Learning for Location-Aided Beam Prediction\n  in 5G and Beyond Millimeter-Wave Networks"
                },
                "summary": "Millimeter-wave (mm-wave) communications requirebeamforming and consequent\nprecise beam alignmentbetween the gNodeB (gNB) and the user equipment (UE)\ntoovercome high propagation losses. This beam alignment needs tobe constantly\nupdated for different UE locations based on beamsweepingradio frequency\nmeasurements, leading to significantbeam management overhead. One potential\nsolution involvesusing machine learning (ML) beam prediction algorithms\nthatleverage UE position information to select the serving beamwithout the\noverhead of beam sweeping. However, the highlysite-specific nature of mm-wave\npropagation means that MLmodels require training from scratch for each\nscenario, whichis inefficient in practice. In this paper, we propose a\nrobustcross-environment transfer learning solution for location-aidedbeam\nprediction, whereby the ML model trained on a referencegNB is transferred to a\ntarget gNB by fine-tuning with a limiteddataset. Extensive simulation results\nbased on ray-tracing in twourban environments show the effectiveness of our\nsolution forboth inter- and intra-city model transfer. Our results show thatby\ntraining the model on a reference gNB and transferring themodel by fine-tuning\nwith only 5% of the target gNB dataset,we can achieve 80% accuracy in\npredicting the best beamfor the target gNB. Importantly, our approach improves\nthepoor generalization accuracy of transferring the model to newenvironments\nwithout fine-tuning by around 75 percentage points.This demonstrates that\ntransfer learning enables high predictionaccuracy while reducing the\ncomputational and training datasetcollection burden of ML-based beam\nprediction, making itpractical for 5G-and-beyond deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter-wave (mm-wave) communications requirebeamforming and consequent\nprecise beam alignmentbetween the gNodeB (gNB) and the user equipment (UE)\ntoovercome high propagation losses. This beam alignment needs tobe constantly\nupdated for different UE locations based on beamsweepingradio frequency\nmeasurements, leading to significantbeam management overhead. One potential\nsolution involvesusing machine learning (ML) beam prediction algorithms\nthatleverage UE position information to select the serving beamwithout the\noverhead of beam sweeping. However, the highlysite-specific nature of mm-wave\npropagation means that MLmodels require training from scratch for each\nscenario, whichis inefficient in practice. In this paper, we propose a\nrobustcross-environment transfer learning solution for location-aidedbeam\nprediction, whereby the ML model trained on a referencegNB is transferred to a\ntarget gNB by fine-tuning with a limiteddataset. Extensive simulation results\nbased on ray-tracing in twourban environments show the effectiveness of our\nsolution forboth inter- and intra-city model transfer. Our results show thatby\ntraining the model on a reference gNB and transferring themodel by fine-tuning\nwith only 5% of the target gNB dataset,we can achieve 80% accuracy in\npredicting the best beamfor the target gNB. Importantly, our approach improves\nthepoor generalization accuracy of transferring the model to newenvironments\nwithout fine-tuning by around 75 percentage points.This demonstrates that\ntransfer learning enables high predictionaccuracy while reducing the\ncomputational and training datasetcollection burden of ML-based beam\nprediction, making itpractical for 5G-and-beyond deployments."
                },
                "authors": [
                    {
                        "name": "Enrico Tosi"
                    },
                    {
                        "name": "Panwei Hu"
                    },
                    {
                        "name": "Aleksandar Ichkov"
                    },
                    {
                        "name": "Marina Petrova"
                    },
                    {
                        "name": "Ljiljana Simić"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Simić"
                },
                "author": "Ljiljana Simić",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14286v1",
                "updated": "2025-03-18T14:23:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    23,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:23:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    23,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs"
                },
                "summary": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance."
                },
                "authors": [
                    {
                        "name": "Nicolas Le Roux"
                    },
                    {
                        "name": "Marc G. Bellemare"
                    },
                    {
                        "name": "Jonathan Lebensold"
                    },
                    {
                        "name": "Arnaud Bergeron"
                    },
                    {
                        "name": "Joshua Greaves"
                    },
                    {
                        "name": "Alex Fréchette"
                    },
                    {
                        "name": "Carolyne Pelletier"
                    },
                    {
                        "name": "Eric Thibodeau-Laufer Sándor Toth"
                    },
                    {
                        "name": "Samantha Work"
                    }
                ],
                "author_detail": {
                    "name": "Samantha Work"
                },
                "author": "Samantha Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14281v1",
                "updated": "2025-03-18T14:20:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    20,
                    54,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:20:54Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    20,
                    54,
                    1,
                    77,
                    0
                ],
                "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants"
                },
                "summary": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools."
                },
                "authors": [
                    {
                        "name": "Adam Štorek"
                    },
                    {
                        "name": "Mukur Gupta"
                    },
                    {
                        "name": "Noopur Bhatt"
                    },
                    {
                        "name": "Aditya Gupta"
                    },
                    {
                        "name": "Janie Kim"
                    },
                    {
                        "name": "Prashast Srivastava"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13788v2",
                "updated": "2025-03-18T14:17:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    17,
                    47,
                    1,
                    77,
                    0
                ],
                "published": "2024-10-17T17:29:04Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    29,
                    4,
                    3,
                    291,
                    0
                ],
                "title": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions"
                },
                "summary": "Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Michael J. Q. Zhang"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "arxiv_comment": "Presented at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14269v1",
                "updated": "2025-03-18T14:02:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    2,
                    59,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T14:02:59Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    14,
                    2,
                    59,
                    1,
                    77,
                    0
                ],
                "title": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks."
                },
                "authors": [
                    {
                        "name": "Vaibhav Aggarwal"
                    },
                    {
                        "name": "Ojasv Kamal"
                    },
                    {
                        "name": "Abhinav Japesh"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13501v2",
                "updated": "2025-03-18T13:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    55,
                    22,
                    1,
                    77,
                    0
                ],
                "published": "2024-03-20T10:58:58Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    10,
                    58,
                    58,
                    2,
                    80,
                    0
                ],
                "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis"
                },
                "summary": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time."
                },
                "authors": [
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "William Beluch"
                    },
                    {
                        "name": "Margret Keuper"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Anna Khoreva"
                    }
                ],
                "author_detail": {
                    "name": "Anna Khoreva"
                },
                "author": "Anna Khoreva",
                "arxiv_comment": "Accepted at ICLR 2025. Code: https://github.com/boschresearch/VSTAR\n  and project page: https://yumengli007.github.io/VSTAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14263v1",
                "updated": "2025-03-18T13:54:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    54,
                    12,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:54:12Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    54,
                    12,
                    1,
                    77,
                    0
                ],
                "title": "Conversational Agents as Catalysts for Critical Thinking: Challenging\n  Social Influence in Group Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Agents as Catalysts for Critical Thinking: Challenging\n  Social Influence in Group Decision-making"
                },
                "summary": "Group decision-making processes frequently suffer when social influence and\npower dynamics suppress minority viewpoints, leading to compliance and\ngroupthink. Conversational agents can counteract these harmful dynamics by\nencouraging critical thinking. This study investigates how LLM-powered devil's\nadvocate systems affect psychological safety, opinion expression, and\nsatisfaction in power-imbalanced group dynamics. We conducted an experiment\nwith 48 participants in 12 four-person groups, each containing three high-power\n(senior) and one low-power (junior) member. Each group completed decision tasks\nin both baseline and AI intervention conditions. Results show AI\ncounterarguments fostered a more flexible atmosphere and significantly enhanced\nboth process and outcome satisfaction for all participants, with particularly\nnotable improvements for minority members. Cognitive workload increased\nslightly, though not significantly. This research contributes empirical\nevidence on how AI systems can effectively navigate power hierarchies to foster\nmore inclusive decision-making environments, highlighting the importance of\nbalancing intervention frequency, maintaining conversational flow, and\npreserving group cohesion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group decision-making processes frequently suffer when social influence and\npower dynamics suppress minority viewpoints, leading to compliance and\ngroupthink. Conversational agents can counteract these harmful dynamics by\nencouraging critical thinking. This study investigates how LLM-powered devil's\nadvocate systems affect psychological safety, opinion expression, and\nsatisfaction in power-imbalanced group dynamics. We conducted an experiment\nwith 48 participants in 12 four-person groups, each containing three high-power\n(senior) and one low-power (junior) member. Each group completed decision tasks\nin both baseline and AI intervention conditions. Results show AI\ncounterarguments fostered a more flexible atmosphere and significantly enhanced\nboth process and outcome satisfaction for all participants, with particularly\nnotable improvements for minority members. Cognitive workload increased\nslightly, though not significantly. This research contributes empirical\nevidence on how AI systems can effectively navigate power hierarchies to foster\nmore inclusive decision-making environments, highlighting the importance of\nbalancing intervention frequency, maintaining conversational flow, and\npreserving group cohesion."
                },
                "authors": [
                    {
                        "name": "Soohwan Lee"
                    },
                    {
                        "name": "Seoyeong Hwang"
                    },
                    {
                        "name": "Dajung Kim"
                    },
                    {
                        "name": "Kyungho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyungho Lee"
                },
                "author": "Kyungho Lee",
                "arxiv_comment": "12 pages, 8 figures, 1 table, This is a preprint version of the Late\n  Breaking Work accepted to CHI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14260v1",
                "updated": "2025-03-18T13:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    50,
                    44,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:50:44Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    50,
                    44,
                    1,
                    77,
                    0
                ],
                "title": "Automating Experimental Optics with Sample Efficient Machine Learning\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Experimental Optics with Sample Efficient Machine Learning\n  Methods"
                },
                "summary": "As free-space optical systems grow in scale and complexity, troubleshooting\nbecomes increasingly time-consuming and, in the case of remote installations,\nperhaps impractical. An example of a task that is often laborious is the\nalignment of a high-finesse optical resonator, which is highly sensitive to the\nmode of the input beam. In this work, we demonstrate how machine learning can\nbe used to achieve autonomous mode-matching of a free-space optical resonator\nwith minimal supervision. Our approach leverages sample-efficient algorithms to\nreduce data requirements while maintaining a simple architecture for easy\ndeployment. The reinforcement learning scheme that we have developed shows that\nautomation is feasible even in systems prone to drift in experimental\nparameters, as may well be the case in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As free-space optical systems grow in scale and complexity, troubleshooting\nbecomes increasingly time-consuming and, in the case of remote installations,\nperhaps impractical. An example of a task that is often laborious is the\nalignment of a high-finesse optical resonator, which is highly sensitive to the\nmode of the input beam. In this work, we demonstrate how machine learning can\nbe used to achieve autonomous mode-matching of a free-space optical resonator\nwith minimal supervision. Our approach leverages sample-efficient algorithms to\nreduce data requirements while maintaining a simple architecture for easy\ndeployment. The reinforcement learning scheme that we have developed shows that\nautomation is feasible even in systems prone to drift in experimental\nparameters, as may well be the case in real-world applications."
                },
                "authors": [
                    {
                        "name": "Arindam Saha"
                    },
                    {
                        "name": "Baramee Charoensombutamon"
                    },
                    {
                        "name": "Thibault Michel"
                    },
                    {
                        "name": "V. Vijendran"
                    },
                    {
                        "name": "Lachlan Walker"
                    },
                    {
                        "name": "Akira Furusawa"
                    },
                    {
                        "name": "Syed M. Assad"
                    },
                    {
                        "name": "Ben C. Buchler"
                    },
                    {
                        "name": "Ping Koy Lam"
                    },
                    {
                        "name": "Aaron D. Tranter"
                    }
                ],
                "author_detail": {
                    "name": "Aaron D. Tranter"
                },
                "author": "Aaron D. Tranter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14258v1",
                "updated": "2025-03-18T13:48:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    48,
                    18,
                    1,
                    77,
                    0
                ],
                "title": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System"
                },
                "summary": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE."
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Baoqing Yue"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Yueyue Wu"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14251v1",
                "updated": "2025-03-18T13:39:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    39,
                    46,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:39:46Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    39,
                    46,
                    1,
                    77,
                    0
                ],
                "title": "Towards a Barrier-free GeoQA Portal: Natural Language Interaction with\n  Geospatial Data Using Multi-Agent LLMs and Semantic Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Barrier-free GeoQA Portal: Natural Language Interaction with\n  Geospatial Data Using Multi-Agent LLMs and Semantic Search"
                },
                "summary": "A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a\nMulti-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting\nopen spatial data sharing and online geo-information management. Designed with\nGIS-like interaction and layered visualization, they often challenge non-expert\nusers with complex functionalities and overlapping layers that obscure spatial\nrelationships. We propose a GeoQA Portal using a multi-agent Large Language\nModel framework for seamless natural language interaction with geospatial data.\nComplex queries are broken into subtasks handled by specialized agents,\nretrieving relevant geographic data efficiently. Task plans are shown to users,\nboosting transparency. The portal supports default and custom data inputs for\nflexibility. Semantic search via word vector similarity aids data retrieval\ndespite imperfect terms. Case studies, evaluations, and user tests confirm its\neffectiveness for non-experts, bridging GIS complexity and public access, and\noffering an intuitive solution for future geoportals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a\nMulti-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting\nopen spatial data sharing and online geo-information management. Designed with\nGIS-like interaction and layered visualization, they often challenge non-expert\nusers with complex functionalities and overlapping layers that obscure spatial\nrelationships. We propose a GeoQA Portal using a multi-agent Large Language\nModel framework for seamless natural language interaction with geospatial data.\nComplex queries are broken into subtasks handled by specialized agents,\nretrieving relevant geographic data efficiently. Task plans are shown to users,\nboosting transparency. The portal supports default and custom data inputs for\nflexibility. Semantic search via word vector similarity aids data retrieval\ndespite imperfect terms. Case studies, evaluations, and user tests confirm its\neffectiveness for non-experts, bridging GIS complexity and public access, and\noffering an intuitive solution for future geoportals."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Puzhen Zhang"
                    },
                    {
                        "name": "Guohui Xiao"
                    },
                    {
                        "name": "Linfang Ding"
                    },
                    {
                        "name": "Liqiu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Meng"
                },
                "author": "Liqiu Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14237v1",
                "updated": "2025-03-18T13:15:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    15,
                    58,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T13:15:58Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    15,
                    58,
                    1,
                    77,
                    0
                ],
                "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Your Training Flexible: Towards Deployment-Efficient Video Models"
                },
                "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT."
                },
                "authors": [
                    {
                        "name": "Chenting Wang"
                    },
                    {
                        "name": "Kunchang Li"
                    },
                    {
                        "name": "Tianxiang Jiang"
                    },
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14234v2",
                "updated": "2025-03-19T04:49:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    49,
                    29,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-18T13:11:43Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    13,
                    11,
                    43,
                    1,
                    77,
                    0
                ],
                "title": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented\n  Generation Framework for Temporal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented\n  Generation Framework for Temporal Reasoning"
                },
                "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective\nin enhancing the performance of Large Language Models (LLMs) on tasks that\nrequire external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG\nimproves information retrieval for complex reasoning tasks, providing more\nprecise and comprehensive retrieval and generating more accurate responses to\nQAs. However, most RAG methods fall short in addressing multi-step reasoning,\nparticularly when both information extraction and inference are necessary. To\naddress this limitation, this paper presents Knowledge Graph-Based Iterative\nRetrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs\nwith iterative reasoning to improve LLMs' ability to handle queries involving\ntemporal and logical dependencies. Through iterative retrieval steps, KG-IRAG\nincrementally gathers relevant data from external KGs, enabling step-by-step\nreasoning. The proposed approach is particularly suited for scenarios where\nreasoning is required alongside dynamic temporal data extraction, such as\ndetermining optimal travel times based on weather conditions or traffic\npatterns. Experimental results show that KG-IRAG improves accuracy in complex\nreasoning tasks by effectively integrating external knowledge with iterative,\nlogic-based retrieval. Additionally, three new datasets: weatherQA-Irish,\nweatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's\nperformance, demonstrating its potential beyond traditional RAG applications."
                },
                "authors": [
                    {
                        "name": "Ruiyi Yang"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Imran Razzak"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14217v1",
                "updated": "2025-03-18T12:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Decision Tree Induction Through LLMs via Semantically-Aware Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision Tree Induction Through LLMs via Semantically-Aware Evolution"
                },
                "summary": "Decision trees are a crucial class of models offering robust predictive\nperformance and inherent interpretability across various domains, including\nhealthcare, finance, and logistics. However, current tree induction methods\noften face limitations such as suboptimal solutions from greedy methods or\nprohibitive computational costs and limited applicability of exact optimization\napproaches. To address these challenges, we propose an evolutionary\noptimization method for decision tree induction based on genetic programming\n(GP). Our key innovation is the integration of semantic priors and\ndomain-specific knowledge about the search space into the optimization\nalgorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that\nincorporates semantic priors into genetic search operators through the use of\nLarge Language Models (LLMs), thereby enhancing search efficiency and targeting\nregions of the search space that yield decision trees with superior\ngeneralization performance. This is operationalized through novel genetic\noperators that work with structured natural language prompts, effectively\nutilizing LLMs as conditional generative models and sources of semantic\nknowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to\nexploit high-performing regions, and $\\textit{diversity-guided}$ mutation for\nefficient global exploration of the search space. These operators are\ncontrolled by corresponding hyperparameters that enable a more nuanced balance\nbetween exploration and exploitation across the search space. Empirically, we\ndemonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves\nsuperior-performing trees compared to existing tree induction methods, and\nexhibits significantly more efficient search performance compared to\nconventional GP approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision trees are a crucial class of models offering robust predictive\nperformance and inherent interpretability across various domains, including\nhealthcare, finance, and logistics. However, current tree induction methods\noften face limitations such as suboptimal solutions from greedy methods or\nprohibitive computational costs and limited applicability of exact optimization\napproaches. To address these challenges, we propose an evolutionary\noptimization method for decision tree induction based on genetic programming\n(GP). Our key innovation is the integration of semantic priors and\ndomain-specific knowledge about the search space into the optimization\nalgorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that\nincorporates semantic priors into genetic search operators through the use of\nLarge Language Models (LLMs), thereby enhancing search efficiency and targeting\nregions of the search space that yield decision trees with superior\ngeneralization performance. This is operationalized through novel genetic\noperators that work with structured natural language prompts, effectively\nutilizing LLMs as conditional generative models and sources of semantic\nknowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to\nexploit high-performing regions, and $\\textit{diversity-guided}$ mutation for\nefficient global exploration of the search space. These operators are\ncontrolled by corresponding hyperparameters that enable a more nuanced balance\nbetween exploration and exploitation across the search space. Empirically, we\ndemonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves\nsuperior-performing trees compared to existing tree induction methods, and\nexhibits significantly more efficient search performance compared to\nconventional GP approaches."
                },
                "authors": [
                    {
                        "name": "Tennison Liu"
                    },
                    {
                        "name": "Nicolas Huynh"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_comment": "*Liu and Huynh contributed equally. Published as a conference paper\n  at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01273v2",
                "updated": "2025-03-18T12:44:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    44,
                    59,
                    1,
                    77,
                    0
                ],
                "published": "2024-10-02T06:34:45Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    6,
                    34,
                    45,
                    2,
                    276,
                    0
                ],
                "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot\n  Interaction"
                },
                "summary": "Real-life robot navigation involves more than just reaching a destination; it\nrequires optimizing movements while addressing scenario-specific goals. An\nintuitive way for humans to express these goals is through abstract cues like\nverbal commands or rough sketches. Such human guidance may lack details or be\nnoisy. Nonetheless, we expect robots to navigate as intended. For robots to\ninterpret and execute these abstract instructions in line with human\nexpectations, they must share a common understanding of basic navigation\nconcepts with humans. To this end, we introduce CANVAS, a novel framework that\ncombines visual and linguistic instructions for commonsense-aware navigation.\nIts success is driven by imitation learning, enabling the robot to learn from\nhuman navigation behavior. We present COMMAND, a comprehensive dataset with\nhuman-annotated navigation results, spanning over 48 hours and 219 km, designed\nto train commonsense-aware navigation systems in simulated environments. Our\nexperiments show that CANVAS outperforms the strong rule-based system ROS\nNavStack across all environments, demonstrating superior performance with noisy\ninstructions. Notably, in the orchard environment, where ROS NavStack records a\n0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also\nclosely aligns with human demonstrations and commonsense constraints, even in\nunseen environments. Furthermore, real-world deployment of CANVAS showcases\nimpressive Sim2Real transfer with a total success rate of 69%, highlighting the\npotential of learning from human demonstrations in simulated environments for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-life robot navigation involves more than just reaching a destination; it\nrequires optimizing movements while addressing scenario-specific goals. An\nintuitive way for humans to express these goals is through abstract cues like\nverbal commands or rough sketches. Such human guidance may lack details or be\nnoisy. Nonetheless, we expect robots to navigate as intended. For robots to\ninterpret and execute these abstract instructions in line with human\nexpectations, they must share a common understanding of basic navigation\nconcepts with humans. To this end, we introduce CANVAS, a novel framework that\ncombines visual and linguistic instructions for commonsense-aware navigation.\nIts success is driven by imitation learning, enabling the robot to learn from\nhuman navigation behavior. We present COMMAND, a comprehensive dataset with\nhuman-annotated navigation results, spanning over 48 hours and 219 km, designed\nto train commonsense-aware navigation systems in simulated environments. Our\nexperiments show that CANVAS outperforms the strong rule-based system ROS\nNavStack across all environments, demonstrating superior performance with noisy\ninstructions. Notably, in the orchard environment, where ROS NavStack records a\n0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also\nclosely aligns with human demonstrations and commonsense constraints, even in\nunseen environments. Furthermore, real-world deployment of CANVAS showcases\nimpressive Sim2Real transfer with a total success rate of 69%, highlighting the\npotential of learning from human demonstrations in simulated environments for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Suhwan Choi"
                    },
                    {
                        "name": "Yongjun Cho"
                    },
                    {
                        "name": "Minchan Kim"
                    },
                    {
                        "name": "Jaeyoon Jung"
                    },
                    {
                        "name": "Myunchul Joe"
                    },
                    {
                        "name": "Yubeen Park"
                    },
                    {
                        "name": "Minseo Kim"
                    },
                    {
                        "name": "Sungwoong Kim"
                    },
                    {
                        "name": "Sungjae Lee"
                    },
                    {
                        "name": "Hwiseong Park"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Accepted to ICRA 2025, project page https://worv-ai.github.io/canvas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.12351v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.12351v4",
                "updated": "2025-03-18T12:39:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    39,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2023-01-29T04:10:12Z",
                "published_parsed": [
                    2023,
                    1,
                    29,
                    4,
                    10,
                    12,
                    6,
                    29,
                    0
                ],
                "title": "Emerging Synergies in Causality and Deep Generative Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Synergies in Causality and Deep Generative Models: A Survey"
                },
                "summary": "In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area."
                },
                "authors": [
                    {
                        "name": "Guanglin Zhou"
                    },
                    {
                        "name": "Shaoan Xie"
                    },
                    {
                        "name": "Guang-Yuan Hao"
                    },
                    {
                        "name": "Shiming Chen"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.12351v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.12351v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14201v1",
                "updated": "2025-03-18T12:26:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    26,
                    6,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:26:06Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    26,
                    6,
                    1,
                    77,
                    0
                ],
                "title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Personalizing Deep Learning-Based Code Completion Tools Matters"
                },
                "summary": "Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being $\\sim$10$\\times$ larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being $\\sim$10$\\times$ larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed)."
                },
                "authors": [
                    {
                        "name": "Alessandro Giagnorio"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "arxiv_comment": "Accepted for publication at ACM TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14190v1",
                "updated": "2025-03-18T12:07:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    7,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T12:07:33Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    7,
                    33,
                    1,
                    77,
                    0
                ],
                "title": "Inferring Event Descriptions from Time Series with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Event Descriptions from Time Series with Language Models"
                },
                "summary": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series data measure how environments change over time and drive\ndecision-making in critical domains like finance and healthcare. When analyzing\ntime series, we often seek to understand the underlying events occurring in the\nmeasured environment. For example, one might ask: What caused a sharp drop in\nthe stock price? Events are often described with natural language, so we\nconduct the first study of whether Large Language Models (LLMs) can infer\nnatural language events from time series. We curate a new benchmark featuring\nwin probabilities collected from 4,200 basketball and American football games,\nfeaturing 1.7M timesteps with real value data and corresponding natural\nlanguage events. Building on the recent wave of using LLMs on time series, we\nevaluate 16 LLMs and find that they demonstrate promising abilities to infer\nevents from time series data. The open-weights DeepSeek-R1 32B model\noutperforms proprietary models like GPT-4o. Despite this impressive initial\nperformance, we also find clear avenues to improve recent models, as we\nidentify failures when altering the provided context, event sequence lengths,\nand evaluation strategy. (All resources needed to reproduce our work are\navailable: https://github.com/BennyTMT/GAMETime)"
                },
                "authors": [
                    {
                        "name": "Mingtian Tan"
                    },
                    {
                        "name": "Mike A. Merrill"
                    },
                    {
                        "name": "Zack Gottesman"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "David Evans"
                    },
                    {
                        "name": "Tom Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hartvigsen"
                },
                "author": "Tom Hartvigsen",
                "arxiv_comment": "17 pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 68T07,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20963v2",
                "updated": "2025-03-18T12:00:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    12,
                    0,
                    26,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-28T11:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    25,
                    11,
                    4,
                    59,
                    0
                ],
                "title": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration"
                },
                "summary": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research."
                },
                "authors": [
                    {
                        "name": "Gerion Spielberger"
                    },
                    {
                        "name": "Florian M. Artinger"
                    },
                    {
                        "name": "Jochen Reb"
                    },
                    {
                        "name": "Rudolf Kerschreiter"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Kerschreiter"
                },
                "author": "Rudolf Kerschreiter",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14186v1",
                "updated": "2025-03-18T11:59:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    59,
                    46,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:59:46Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    59,
                    46,
                    1,
                    77,
                    0
                ],
                "title": "5G-Enabled Teleoperated Driving: An Experimental Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G-Enabled Teleoperated Driving: An Experimental Evaluation"
                },
                "summary": "Teleoperated driving enables remote human intervention in autonomous\nvehicles, addressing challenges in complex driving environments. However, its\neffectiveness depends on ultra-low latency, high-reliability communication.\nThis paper evaluates teleoperated driving over 5G networks, analyzing key\nperformance metrics such as glass-to-glass (G2G) latency, RTT and steering\ncommand delay. Using a real-world testbed with a Kia Soul EV and a remote\nteleoperation platform, we assess the feasibility and limitations of 5G-enabled\nteleoperated driving. Our system achieved an average G2G latency of 202ms and\nan RTT of 47ms highlighting the G2G latency as the critical bottleneck. The\nsteering control proved to be mostly accurate and responsive. Finally, this\npaper provides recommendations and outlines future work to improve future\nteleoperated driving deployments for safer and more reliable autonomous\nmobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperated driving enables remote human intervention in autonomous\nvehicles, addressing challenges in complex driving environments. However, its\neffectiveness depends on ultra-low latency, high-reliability communication.\nThis paper evaluates teleoperated driving over 5G networks, analyzing key\nperformance metrics such as glass-to-glass (G2G) latency, RTT and steering\ncommand delay. Using a real-world testbed with a Kia Soul EV and a remote\nteleoperation platform, we assess the feasibility and limitations of 5G-enabled\nteleoperated driving. Our system achieved an average G2G latency of 202ms and\nan RTT of 47ms highlighting the G2G latency as the critical bottleneck. The\nsteering control proved to be mostly accurate and responsive. Finally, this\npaper provides recommendations and outlines future work to improve future\nteleoperated driving deployments for safer and more reliable autonomous\nmobility."
                },
                "authors": [
                    {
                        "name": "Mehdi Testouri"
                    },
                    {
                        "name": "Gamal Elghazaly"
                    },
                    {
                        "name": "Faisal Hawlader"
                    },
                    {
                        "name": "Raphael Frank"
                    }
                ],
                "author_detail": {
                    "name": "Raphael Frank"
                },
                "author": "Raphael Frank",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14184v1",
                "updated": "2025-03-18T11:59:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    59,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:59:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    59,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic\n  Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic\n  Targets"
                },
                "summary": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering."
                },
                "authors": [
                    {
                        "name": "Atharva Ghotavadekar"
                    },
                    {
                        "name": "František Nekovář"
                    },
                    {
                        "name": "Martin Saska"
                    },
                    {
                        "name": "Jan Faigl"
                    }
                ],
                "author_detail": {
                    "name": "Jan Faigl"
                },
                "author": "Jan Faigl",
                "arxiv_doi": "10.1109/LRA.2024.3518096",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3518096",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Robotics and Automation Letters, vol. 10, no. 2, pp.\n  1249-1256, Feb. 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11139v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11139v4",
                "updated": "2025-03-18T11:58:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    58,
                    48,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-17T01:54:27Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    1,
                    54,
                    27,
                    0,
                    169,
                    0
                ],
                "title": "Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance"
                },
                "summary": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies."
                },
                "authors": [
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Avik Halder"
                    },
                    {
                        "name": "Rajarshi Mandal"
                    },
                    {
                        "name": "Sayan Layek"
                    },
                    {
                        "name": "Ian Soboroff"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "arxiv_comment": "Accepted at NAACL 2025 (Industry track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11139v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11139v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14183v1",
                "updated": "2025-03-18T11:58:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    58,
                    0,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:58:00Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    58,
                    0,
                    1,
                    77,
                    0
                ],
                "title": "Can LLMs Enable Verification in Mainstream Programming?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Enable Verification in Mainstream Programming?"
                },
                "summary": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although formal methods are capable of producing reliable software, they have\nseen minimal adoption in everyday programming. Automatic code generation using\nlarge language models is becoming increasingly widespread, but it rarely\nconsiders producing strong correctness guarantees. In this study, we explore\nthe ability of LLMs to produce verified code in three verification languages\n(Dafny, Nagini, and Verus). To do so, we use manually curated datasets derived\nfrom the state-ofthe-art Python benchmark, HumanEval. We also assess what types\nof information are sufficient to achieve good-quality results."
                },
                "authors": [
                    {
                        "name": "Aleksandr Shefer"
                    },
                    {
                        "name": "Igor Engel"
                    },
                    {
                        "name": "Stanislav Alekseev"
                    },
                    {
                        "name": "Daniil Berezun"
                    },
                    {
                        "name": "Ekaterina Verbitskaia"
                    },
                    {
                        "name": "Anton Podkopaev"
                    }
                ],
                "author_detail": {
                    "name": "Anton Podkopaev"
                },
                "author": "Anton Podkopaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16635v2",
                "updated": "2025-03-18T11:50:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    50,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-28T02:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    2,
                    16,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation"
                },
                "summary": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4o's performance\nin recognizing the majority labels of laughable contexts, achieving an F1 score\nof 43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4o's performance\nin recognizing the majority labels of laughable contexts, achieving an F1 score\nof 43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions."
                },
                "authors": [
                    {
                        "name": "Koji Inoue"
                    },
                    {
                        "name": "Mikey Elmers"
                    },
                    {
                        "name": "Divesh Lala"
                    },
                    {
                        "name": "Tatsuya Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Kawahara"
                },
                "author": "Tatsuya Kawahara",
                "arxiv_comment": "This paper has been accepted for presentation at International\n  Workshop on Spoken Dialogue Systems Technology 2025 (IWSDS 2025) and\n  represents the author's version of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05092v2",
                "updated": "2025-03-18T11:43:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    43,
                    52,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-07T17:11:23Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    11,
                    23,
                    4,
                    38,
                    0
                ],
                "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs"
                },
                "summary": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "Accepted at the ICLR 2025 Workshop on Reasoning and Planning for\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14167v1",
                "updated": "2025-03-18T11:37:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    37,
                    25,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:37:25Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    37,
                    25,
                    1,
                    77,
                    0
                ],
                "title": "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach"
                },
                "summary": "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections."
                },
                "authors": [
                    {
                        "name": "Christian Poelitz"
                    },
                    {
                        "name": "Nick McKenna"
                    }
                ],
                "author_detail": {
                    "name": "Nick McKenna"
                },
                "author": "Nick McKenna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14153v1",
                "updated": "2025-03-18T11:21:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    21,
                    53,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:21:53Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    21,
                    53,
                    1,
                    77,
                    0
                ],
                "title": "Speculative Decoding for Verilog: Speed and Quality, All in One",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding for Verilog: Speed and Quality, All in One"
                },
                "summary": "The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages."
                },
                "authors": [
                    {
                        "name": "Changran Xu"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Yunhao Zhou"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Ningyi Xu"
                    },
                    {
                        "name": "Qiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Xu"
                },
                "author": "Qiang Xu",
                "arxiv_comment": "Accepted by the 62nd Design Automation Conference (DAC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07696v2",
                "updated": "2025-03-18T11:19:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    19,
                    45,
                    1,
                    77,
                    0
                ],
                "published": "2024-04-11T12:42:18Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    12,
                    42,
                    18,
                    3,
                    102,
                    0
                ],
                "title": "Flatness Improves Backbone Generalisation in Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flatness Improves Backbone Generalisation in Few-shot Classification"
                },
                "summary": "Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. However, approaches for multi-domain FSC\ntypically result in complex pipelines aimed at information fusion and\ntask-specific adaptation without consideration of the importance of backbone\ntraining. In this work, we introduce an effective strategy for backbone\ntraining and selection in multi-domain FSC by utilizing flatness-aware training\nand fine-tuning. Our work is theoretically grounded and empirically performs on\npar or better than state-of-the-art methods despite being simpler. Further, our\nresults indicate that backbone training is crucial for good generalisation in\nFSC across different adaptation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. However, approaches for multi-domain FSC\ntypically result in complex pipelines aimed at information fusion and\ntask-specific adaptation without consideration of the importance of backbone\ntraining. In this work, we introduce an effective strategy for backbone\ntraining and selection in multi-domain FSC by utilizing flatness-aware training\nand fine-tuning. Our work is theoretically grounded and empirically performs on\npar or better than state-of-the-art methods despite being simpler. Further, our\nresults indicate that backbone training is crucial for good generalisation in\nFSC across different adaptation methods."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Martin Trapp"
                    },
                    {
                        "name": "Marcus Klasson"
                    },
                    {
                        "name": "Arno Solin"
                    }
                ],
                "author_detail": {
                    "name": "Arno Solin"
                },
                "author": "Arno Solin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14138v1",
                "updated": "2025-03-18T11:04:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    4,
                    57,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T11:04:57Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    11,
                    4,
                    57,
                    1,
                    77,
                    0
                ],
                "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The\n  Role of Datasets, Architectures, and Loss Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The\n  Role of Datasets, Architectures, and Loss Functions"
                },
                "summary": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment."
                },
                "authors": [
                    {
                        "name": "Siddharth D Jaiswal"
                    },
                    {
                        "name": "Sagnik Basu"
                    },
                    {
                        "name": "Sandipan Sikdar"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "arxiv_comment": "This work has been accepted for publication at AAAI ICWSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12687v3",
                "updated": "2025-03-18T10:50:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    50,
                    58,
                    1,
                    77,
                    0
                ],
                "published": "2024-12-17T09:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    8,
                    18,
                    1,
                    352,
                    0
                ],
                "title": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models"
                },
                "summary": "This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping."
                },
                "authors": [
                    {
                        "name": "Seungeun Oh"
                    },
                    {
                        "name": "Jinhyuk Kim"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Seung-Woo Ko"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    },
                    {
                        "name": "Seong-Lyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Lyun Kim"
                },
                "author": "Seong-Lyun Kim",
                "arxiv_comment": "7 pages, 6 figures; to be presented at IEEE International Conference\n  on Machine Learning for Communication and Networking (ICMLCN) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14130v1",
                "updated": "2025-03-18T10:49:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    49,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:49:36Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    49,
                    36,
                    1,
                    77,
                    0
                ],
                "title": "Inference-Time Intervention in Large Language Models for Reliable\n  Requirement Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Intervention in Large Language Models for Reliable\n  Requirement Verification"
                },
                "summary": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set."
                },
                "authors": [
                    {
                        "name": "Paul Darm"
                    },
                    {
                        "name": "James Xie"
                    },
                    {
                        "name": "Annalisa Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Annalisa Riccardi"
                },
                "author": "Annalisa Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.2; I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14114v1",
                "updated": "2025-03-18T10:31:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    31,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:31:29Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    31,
                    29,
                    1,
                    77,
                    0
                ],
                "title": "Enhancing Kubernetes Resilience through Anomaly Detection and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Kubernetes Resilience through Anomaly Detection and Prediction"
                },
                "summary": "Kubernetes, in recent years, has become widely used for the deployment and\nmanagement of software projects on cloud infrastructure. Due to the execution\nof these applications across numerous Nodes, each one with its unique\nspecifications, it has become a challenge to identify problems and ensure the\nsmooth operation of the application. Effective supervision of the cluster\nremains a challenging and resource intensive task. This research work focuses\non providing a novel framework system maintainer in order to overview all the\npossible resources in Kubernetes and pay the attention to specific parts of the\ncluster that may be showcasing problematic behavior. The novelty of this\ncomponent rises from the use of cluster graphical representation where\nfeatures, e.g. graph edges and neighboring nodes, are used for anomaly\ndetection. The proposed framework defines the normality in the dynamic\nenviroment of Kubernetes and the output feeds the supervised models for\nabnormaliry detection presented in user-friendly graph interface. A variety of\nmodel combinations are evaluated and tested in real-life environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kubernetes, in recent years, has become widely used for the deployment and\nmanagement of software projects on cloud infrastructure. Due to the execution\nof these applications across numerous Nodes, each one with its unique\nspecifications, it has become a challenge to identify problems and ensure the\nsmooth operation of the application. Effective supervision of the cluster\nremains a challenging and resource intensive task. This research work focuses\non providing a novel framework system maintainer in order to overview all the\npossible resources in Kubernetes and pay the attention to specific parts of the\ncluster that may be showcasing problematic behavior. The novelty of this\ncomponent rises from the use of cluster graphical representation where\nfeatures, e.g. graph edges and neighboring nodes, are used for anomaly\ndetection. The proposed framework defines the normality in the dynamic\nenviroment of Kubernetes and the output feeds the supervised models for\nabnormaliry detection presented in user-friendly graph interface. A variety of\nmodel combinations are evaluated and tested in real-life environment."
                },
                "authors": [
                    {
                        "name": "V. Anemogiannis"
                    },
                    {
                        "name": "B. Andreou"
                    },
                    {
                        "name": "K. Myrtollari"
                    },
                    {
                        "name": "K. Panagidi"
                    },
                    {
                        "name": "S. Hadjiefthymiades"
                    }
                ],
                "author_detail": {
                    "name": "S. Hadjiefthymiades"
                },
                "author": "S. Hadjiefthymiades",
                "arxiv_comment": "27 pages, 21 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09123v3",
                "updated": "2025-03-18T10:28:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    28,
                    1,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-13T13:56:55Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    13,
                    56,
                    55,
                    3,
                    165,
                    0
                ],
                "title": "Can I introduce my boyfriend to my grandmother? Evaluating Large\n  Language Models Capabilities on Iranian Social Norm Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can I introduce my boyfriend to my grandmother? Evaluating Large\n  Language Models Capabilities on Iranian Social Norm Classification"
                },
                "summary": "Creating globally inclusive AI systems demands datasets reflecting diverse\nsocial norms. Iran, with its unique cultural blend, offers an ideal case study,\nwith Farsi adding linguistic complexity. In this work, we introduce the Iranian\nSocial Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms,\nincluding environments, demographic features, and scope annotation, alongside\nEnglish translations. Our evaluation of 6 Large Language Models (LLMs) in\nclassifying Iranian social norms, using a variety of prompts, uncovered\ncritical insights into the impact of geographic and linguistic context. Results\nrevealed a substantial performance gap in LLMs' comprehension of Iranian norms.\nNotably, while the geographic context in English prompts enhanced the\nperformance, this effect was absent in Farsi, pointing to nuanced linguistic\nchallenges. Particularly, performance was significantly worse for Iran-specific\nnorms, emphasizing the importance of culturally tailored datasets. As the first\nFarsi dataset for social norm classification, ISN will facilitate crucial\ncross-cultural analyses, shedding light on how values differ across contexts\nand cultures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating globally inclusive AI systems demands datasets reflecting diverse\nsocial norms. Iran, with its unique cultural blend, offers an ideal case study,\nwith Farsi adding linguistic complexity. In this work, we introduce the Iranian\nSocial Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms,\nincluding environments, demographic features, and scope annotation, alongside\nEnglish translations. Our evaluation of 6 Large Language Models (LLMs) in\nclassifying Iranian social norms, using a variety of prompts, uncovered\ncritical insights into the impact of geographic and linguistic context. Results\nrevealed a substantial performance gap in LLMs' comprehension of Iranian norms.\nNotably, while the geographic context in English prompts enhanced the\nperformance, this effect was absent in Farsi, pointing to nuanced linguistic\nchallenges. Particularly, performance was significantly worse for Iran-specific\nnorms, emphasizing the importance of culturally tailored datasets. As the first\nFarsi dataset for social norm classification, ISN will facilitate crucial\ncross-cultural analyses, shedding light on how values differ across contexts\nand cultures."
                },
                "authors": [
                    {
                        "name": "Hamidreza Saffari"
                    },
                    {
                        "name": "Mohammadamin Shafiei"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Francesco Pierri"
                    },
                    {
                        "name": "Debora Nozza"
                    }
                ],
                "author_detail": {
                    "name": "Debora Nozza"
                },
                "author": "Debora Nozza",
                "arxiv_comment": "15 pages, 1 figure, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14103v1",
                "updated": "2025-03-18T10:18:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    18,
                    7,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:18:07Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    18,
                    7,
                    1,
                    77,
                    0
                ],
                "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model"
                },
                "summary": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research."
                },
                "authors": [
                    {
                        "name": "Jonas Oppenlaender"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Oppenlaender"
                },
                "author": "Jonas Oppenlaender",
                "arxiv_comment": "17 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14096v1",
                "updated": "2025-03-18T10:12:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    12,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T10:12:29Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    12,
                    29,
                    1,
                    77,
                    0
                ],
                "title": "GenPara: Enhancing the 3D Design Editing Process by Inferring Users'\n  Regions of Interest with Text-Conditional Shape Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenPara: Enhancing the 3D Design Editing Process by Inferring Users'\n  Regions of Interest with Text-Conditional Shape Parameters"
                },
                "summary": "In 3D design, specifying design objectives and visualizing complex shapes\nthrough text alone proves to be a significant challenge. Although advancements\nin 3D GenAI have significantly enhanced part assembly and the creation of\nhigh-quality 3D designs, many systems still to dynamically generate and edit\ndesign elements based on the shape parameters. To bridge this gap, we propose\nGenPara, an interactive 3D design editing system that leverages\ntext-conditional shape parameters of part-aware 3D designs and visualizes\ndesign space within the Exploration Map and Design Versioning Tree.\nAdditionally, among the various shape parameters generated by LLM, the system\nextracts and provides design outcomes within the user's regions of interest\nbased on Bayesian inference. A user study N = 16 revealed that \\textit{GenPara}\nenhanced the comprehension and management of designers with text-conditional\nshape parameters, streamlining design exploration and concretization. This\nimprovement boosted efficiency and creativity of the 3D design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 3D design, specifying design objectives and visualizing complex shapes\nthrough text alone proves to be a significant challenge. Although advancements\nin 3D GenAI have significantly enhanced part assembly and the creation of\nhigh-quality 3D designs, many systems still to dynamically generate and edit\ndesign elements based on the shape parameters. To bridge this gap, we propose\nGenPara, an interactive 3D design editing system that leverages\ntext-conditional shape parameters of part-aware 3D designs and visualizes\ndesign space within the Exploration Map and Design Versioning Tree.\nAdditionally, among the various shape parameters generated by LLM, the system\nextracts and provides design outcomes within the user's regions of interest\nbased on Bayesian inference. A user study N = 16 revealed that \\textit{GenPara}\nenhanced the comprehension and management of designers with text-conditional\nshape parameters, streamlining design exploration and concretization. This\nimprovement boosted efficiency and creativity of the 3D design process."
                },
                "authors": [
                    {
                        "name": "Jiin Choi"
                    },
                    {
                        "name": "Seung Won Lee"
                    },
                    {
                        "name": "Kyung Hoon Hyun"
                    }
                ],
                "author_detail": {
                    "name": "Kyung Hoon Hyun"
                },
                "author": "Kyung Hoon Hyun",
                "arxiv_doi": "10.1145/3706598.3713502",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713502",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; D.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10630v3",
                "updated": "2025-03-18T10:07:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    7,
                    7,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-13T17:59:48Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    48,
                    3,
                    72,
                    0
                ],
                "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"
                },
                "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods."
                },
                "authors": [
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Lingqing Zhao"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to CVPR 2025. Project page:\n  https://bagh2178.github.io/UniGoal/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14075v1",
                "updated": "2025-03-18T09:52:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    52,
                    45,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T09:52:45Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    52,
                    45,
                    1,
                    77,
                    0
                ],
                "title": "Growing a Twig to Accelerate Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing a Twig to Accelerate Large Vision-Language Models"
                },
                "summary": "Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods. Code will be made publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods. Code will be made publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Zhenwei Shao"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Wenwen Pan"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Ning Mao"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yu"
                },
                "author": "Jun Yu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17178v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17178v3",
                "updated": "2025-03-18T09:09:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    9,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-01-24T17:01:14Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    1,
                    14,
                    4,
                    24,
                    0
                ],
                "title": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLM Judge Design Decisions for 1/1000 of the Cost"
                },
                "summary": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility."
                },
                "authors": [
                    {
                        "name": "David Salinas"
                    },
                    {
                        "name": "Omar Swelam"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17178v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17178v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14043v1",
                "updated": "2025-03-18T09:04:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    4,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T09:04:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    4,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Learning on LLM Output Signatures for gray-box LLM Behavior Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning on LLM Output Signatures for gray-box LLM Behavior Analysis"
                },
                "summary": "Large Language Models (LLMs) have achieved widespread adoption, yet our\nunderstanding of their behavior remains limited, particularly in detecting data\ncontamination and hallucinations. While recently proposed probing techniques\nprovide insights through activation analysis, they require \"white-box\" access\nto model internals, often unavailable. Current \"gray-box\" approaches typically\nanalyze only the probability of the actual tokens in the sequence with simple\ntask-specific heuristics. Importantly, these methods overlook the rich\ninformation contained in the full token distribution at each processing step.\nTo address these limitations, we propose that gray-box analysis should leverage\nthe complete observable output of LLMs, consisting of both the previously used\ntoken probabilities as well as the complete token distribution sequences - a\nunified data type we term LOS (LLM Output Signature). To this end, we develop a\ntransformer-based approach to process LOS that theoretically guarantees\napproximation of existing techniques while enabling more nuanced analysis. Our\napproach achieves superior performance on hallucination and data contamination\ndetection in gray-box settings, significantly outperforming existing baselines.\nFurthermore, it demonstrates strong transfer capabilities across datasets and\nLLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our\ncode is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved widespread adoption, yet our\nunderstanding of their behavior remains limited, particularly in detecting data\ncontamination and hallucinations. While recently proposed probing techniques\nprovide insights through activation analysis, they require \"white-box\" access\nto model internals, often unavailable. Current \"gray-box\" approaches typically\nanalyze only the probability of the actual tokens in the sequence with simple\ntask-specific heuristics. Importantly, these methods overlook the rich\ninformation contained in the full token distribution at each processing step.\nTo address these limitations, we propose that gray-box analysis should leverage\nthe complete observable output of LLMs, consisting of both the previously used\ntoken probabilities as well as the complete token distribution sequences - a\nunified data type we term LOS (LLM Output Signature). To this end, we develop a\ntransformer-based approach to process LOS that theoretically guarantees\napproximation of existing techniques while enabling more nuanced analysis. Our\napproach achieves superior performance on hallucination and data contamination\ndetection in gray-box settings, significantly outperforming existing baselines.\nFurthermore, it demonstrates strong transfer capabilities across datasets and\nLLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our\ncode is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network."
                },
                "authors": [
                    {
                        "name": "Guy Bar-Shalom"
                    },
                    {
                        "name": "Fabrizio Frasca"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Yoav Gelberg"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18326v2",
                "updated": "2025-03-18T08:57:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    57,
                    39,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-26T13:12:40Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    13,
                    12,
                    40,
                    2,
                    178,
                    0
                ],
                "title": "PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models"
                },
                "summary": "Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods."
                },
                "authors": [
                    {
                        "name": "Huixuan Zhang"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11948v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11948v3",
                "updated": "2025-03-18T08:37:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    37,
                    47,
                    1,
                    77,
                    0
                ],
                "published": "2024-12-16T16:31:00Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    31,
                    0,
                    0,
                    351,
                    0
                ],
                "title": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews"
                },
                "summary": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top conferences. Given a PDF paper submission and\nreview template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces considerably more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top conferences. Given a PDF paper submission and\nreview template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces considerably more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool."
                },
                "authors": [
                    {
                        "name": "Maximilian Idahl"
                    },
                    {
                        "name": "Zahra Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Zahra Ahmadi"
                },
                "author": "Zahra Ahmadi",
                "arxiv_comment": "NAACL 2025 System Demonstrations Track (Camera-ready version) Demo:\n  https://huggingface.co/spaces/maxidl/openreviewer Model:\n  https://huggingface.co/maxidl/Llama-OpenReviewer-8B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11948v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11948v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14023v1",
                "updated": "2025-03-18T08:34:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    34,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T08:34:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    34,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Synthetic Data Generation Using Large Language Models: Advances in Text\n  and Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation Using Large Language Models: Advances in Text\n  and Code"
                },
                "summary": "Large language models (LLMs) have unlocked new possibilities for generating\nsynthetic training data in both natural language and code. By producing\nartificial but task-relevant examples, these models can significantly augment\nor even replace real-world datasets, especially when labeled data is scarce or\nsensitive. This paper surveys recent advances in using LLMs to create synthetic\ntext and code, emphasizing prompt-based generation, retrieval-augmented\npipelines, and iterative self-refinement. We show how these methods enrich\nlow-resource tasks such as classification and question answering, as well as\ncode-centric applications such as instruction tuning, code translation, and bug\nrepair, by enabling automated verification of functional correctness. Alongside\npotential benefits like cost-effectiveness, broad coverage, and controllable\ndiversity, we address challenges such as factual inaccuracies in generated\ntext, lack of stylistic realism, and the risk of bias amplification. Proposed\nmitigations include filtering and weighting outputs and reinforcement learning\nwith execution feedback for code. We conclude with open research directions\nlike automated prompt engineering, cross-modal data synthesis, and robust\nevaluation frameworks, highlighting the importance of LLM-generated synthetic\ndata in advancing AI while emphasizing ethical and quality safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked new possibilities for generating\nsynthetic training data in both natural language and code. By producing\nartificial but task-relevant examples, these models can significantly augment\nor even replace real-world datasets, especially when labeled data is scarce or\nsensitive. This paper surveys recent advances in using LLMs to create synthetic\ntext and code, emphasizing prompt-based generation, retrieval-augmented\npipelines, and iterative self-refinement. We show how these methods enrich\nlow-resource tasks such as classification and question answering, as well as\ncode-centric applications such as instruction tuning, code translation, and bug\nrepair, by enabling automated verification of functional correctness. Alongside\npotential benefits like cost-effectiveness, broad coverage, and controllable\ndiversity, we address challenges such as factual inaccuracies in generated\ntext, lack of stylistic realism, and the risk of bias amplification. Proposed\nmitigations include filtering and weighting outputs and reinforcement learning\nwith execution feedback for code. We conclude with open research directions\nlike automated prompt engineering, cross-modal data synthesis, and robust\nevaluation frameworks, highlighting the importance of LLM-generated synthetic\ndata in advancing AI while emphasizing ethical and quality safeguards."
                },
                "authors": [
                    {
                        "name": "Mihai Nadas"
                    },
                    {
                        "name": "Laura Diosan"
                    },
                    {
                        "name": "Andreea Tomescu"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Tomescu"
                },
                "author": "Andreea Tomescu",
                "arxiv_comment": "21 pages, 3 tables, 64 references, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05592v2",
                "updated": "2025-03-18T08:32:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    32,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-07T17:14:44Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    14,
                    44,
                    4,
                    66,
                    0
                ],
                "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14012v1",
                "updated": "2025-03-18T08:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    20,
                    24,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T08:20:24Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    20,
                    24,
                    1,
                    77,
                    0
                ],
                "title": "LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote\n  Sensing Image Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote\n  Sensing Image Object Detection"
                },
                "summary": "Remote sensing object detection (RSOD) faces formidable challenges in complex\nvisual environments. Aerial and satellite images inherently suffer from\nlimitations such as low spatial resolution, sensor noise, blurred objects,\nlow-light degradation, and partial occlusions. These degradation factors\ncollectively compromise the feature discriminability in detection models,\nresulting in three key issues: (1) reduced contrast that hampers\nforeground-background separation, (2) structural discontinuities in edge\nrepresentations, and (3) ambiguous feature responses caused by variations in\nillumination. These collectively weaken model robustness and deployment\nfeasibility. To address these challenges, we propose LEGNet, a lightweight\nnetwork that incorporates a novel edge-Gaussian aggregation (EGA) module\nspecifically designed for low-quality remote sensing images. Our key innovation\nlies in the synergistic integration of Scharr operator-based edge priors with\nuncertainty-aware Gaussian modeling: (a) The orientation-aware Scharr filters\npreserve high-frequency edge details with rotational invariance; (b) The\nuncertainty-aware Gaussian layers probabilistically refine low-confidence\nfeatures through variance estimation. This design enables precision enhancement\nwhile maintaining architectural simplicity. Comprehensive evaluations across\nfour RSOD benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0) and a UAV-view\ndataset (VisDrone2019) demonstrate significant improvements. LEGNet achieves\nstate-of-the-art performance across five benchmark datasets while ensuring\ncomputational efficiency, making it well-suited for deployment on\nresource-constrained edge devices in real-world remote sensing applications.\nThe code is available at https://github.com/lwCVer/LEGNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing object detection (RSOD) faces formidable challenges in complex\nvisual environments. Aerial and satellite images inherently suffer from\nlimitations such as low spatial resolution, sensor noise, blurred objects,\nlow-light degradation, and partial occlusions. These degradation factors\ncollectively compromise the feature discriminability in detection models,\nresulting in three key issues: (1) reduced contrast that hampers\nforeground-background separation, (2) structural discontinuities in edge\nrepresentations, and (3) ambiguous feature responses caused by variations in\nillumination. These collectively weaken model robustness and deployment\nfeasibility. To address these challenges, we propose LEGNet, a lightweight\nnetwork that incorporates a novel edge-Gaussian aggregation (EGA) module\nspecifically designed for low-quality remote sensing images. Our key innovation\nlies in the synergistic integration of Scharr operator-based edge priors with\nuncertainty-aware Gaussian modeling: (a) The orientation-aware Scharr filters\npreserve high-frequency edge details with rotational invariance; (b) The\nuncertainty-aware Gaussian layers probabilistically refine low-confidence\nfeatures through variance estimation. This design enables precision enhancement\nwhile maintaining architectural simplicity. Comprehensive evaluations across\nfour RSOD benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0) and a UAV-view\ndataset (VisDrone2019) demonstrate significant improvements. LEGNet achieves\nstate-of-the-art performance across five benchmark datasets while ensuring\ncomputational efficiency, making it well-suited for deployment on\nresource-constrained edge devices in real-world remote sensing applications.\nThe code is available at https://github.com/lwCVer/LEGNet."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Si-Bao Chen"
                    },
                    {
                        "name": "Hui-Dong Li"
                    },
                    {
                        "name": "Qing-Ling Shu"
                    },
                    {
                        "name": "Chris H. Q. Ding"
                    },
                    {
                        "name": "Jin Tang"
                    },
                    {
                        "name": "Bin Luo"
                    }
                ],
                "author_detail": {
                    "name": "Bin Luo"
                },
                "author": "Bin Luo",
                "arxiv_comment": "12 pages, 5 figures. Remote Sensing Image Object Detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16156v2",
                "updated": "2025-03-18T08:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    15,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-25T07:32:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    32,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoOrion: Tokenizing Object Dynamics in Videos"
                },
                "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos - the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos - the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks."
                },
                "authors": [
                    {
                        "name": "Yicheng Feng"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Wanpeng Zhang"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Zihao Yue"
                    },
                    {
                        "name": "Sipeng Zheng"
                    },
                    {
                        "name": "Zongqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zongqing Lu"
                },
                "author": "Zongqing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14004v1",
                "updated": "2025-03-18T08:10:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    10,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T08:10:33Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    10,
                    33,
                    1,
                    77,
                    0
                ],
                "title": "Predicting Human Choice Between Textually Described Lotteries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Human Choice Between Textually Described Lotteries"
                },
                "summary": "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap."
                },
                "authors": [
                    {
                        "name": "Eyal Marantz"
                    },
                    {
                        "name": "Ori Plonsky"
                    }
                ],
                "author_detail": {
                    "name": "Ori Plonsky"
                },
                "author": "Ori Plonsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14000v1",
                "updated": "2025-03-18T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    7,
                    17,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    8,
                    7,
                    17,
                    1,
                    77,
                    0
                ],
                "title": "LLM-based Unit Test Generation for Dynamically-Typed Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Unit Test Generation for Dynamically-Typed Programs"
                },
                "summary": "Automated unit test generation has been widely studied, but generating\neffective tests for dynamically typed programs remains a significant challenge.\nExisting approaches, including search-based software testing (SBST) and recent\nLLM-based methods, often suffer from type errors, leading to invalid inputs and\nassertion failures, ultimately reducing testing effectiveness. To address this,\nwe propose TypeTest, a novel framework that enhances type correctness in test\ngeneration through a vector-based Retrieval-Augmented Generation (RAG) system.\nTypeTest employs call instance retrieval and feature-based retrieval to infer\nparameter types accurately and construct valid test inputs. Furthermore, it\nutilizes the call graph to extract richer contextual information, enabling more\naccurate assertion generation. In addition, TypeTest incorporates a repair\nmechanism and iterative test generation, progressively refining test cases to\nimprove coverage. In an evaluation on 125 real-world Python modules, TypeTest\nachieved an average statement coverage of 86.6% and branch coverage of 76.8%,\noutperforming state-of-theart tools by 5.4% and 9.3%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated unit test generation has been widely studied, but generating\neffective tests for dynamically typed programs remains a significant challenge.\nExisting approaches, including search-based software testing (SBST) and recent\nLLM-based methods, often suffer from type errors, leading to invalid inputs and\nassertion failures, ultimately reducing testing effectiveness. To address this,\nwe propose TypeTest, a novel framework that enhances type correctness in test\ngeneration through a vector-based Retrieval-Augmented Generation (RAG) system.\nTypeTest employs call instance retrieval and feature-based retrieval to infer\nparameter types accurately and construct valid test inputs. Furthermore, it\nutilizes the call graph to extract richer contextual information, enabling more\naccurate assertion generation. In addition, TypeTest incorporates a repair\nmechanism and iterative test generation, progressively refining test cases to\nimprove coverage. In an evaluation on 125 real-world Python modules, TypeTest\nachieved an average statement coverage of 86.6% and branch coverage of 76.8%,\noutperforming state-of-theart tools by 5.4% and 9.3%, respectively."
                },
                "authors": [
                    {
                        "name": "Runlin Liu"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Yunge Hu"
                    },
                    {
                        "name": "Yuhang Lin"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Hailong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Sun"
                },
                "author": "Hailong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12918v2",
                "updated": "2025-03-18T07:55:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    55,
                    40,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T14:59:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    59,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "Query Rewriting via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query Rewriting via LLMs"
                },
                "summary": "When complex SQL queries suffer slow executions despite query optimization,\nDBAs typically invoke automated query rewriting tools to recommend ``lean''\nequivalents that are conducive to faster execution. The rewritings are usually\nachieved via transformation rules, but these rules are limited in scope and\ndifficult to update in a production system. Recently, LLM-based techniques have\nalso been suggested, but they are prone to semantic and syntactic errors.\n  We investigate here how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of logic-based and statistical tools can be\nused to check for semantic violations in the rewrites prior to DBA\nconsideration.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from standard benchmarks on contemporary\ndatabase platforms. The results show significant performance improvements for\nslow queries, with regard to both abstract costing and actual execution, over\nboth SOTA techniques and the native query optimizer. For instance, with TPC-DS\non PostgreSQL, the geometric mean of the runtime speedups for slow queries was\nas high as 18.4 over the native optimizer, whereas SOTA delivered 6 in\ncomparison.\n  Overall, LITHE is a promising step toward viable LLM-based advisory tools for\nameliorating enterprise query performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When complex SQL queries suffer slow executions despite query optimization,\nDBAs typically invoke automated query rewriting tools to recommend ``lean''\nequivalents that are conducive to faster execution. The rewritings are usually\nachieved via transformation rules, but these rules are limited in scope and\ndifficult to update in a production system. Recently, LLM-based techniques have\nalso been suggested, but they are prone to semantic and syntactic errors.\n  We investigate here how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of logic-based and statistical tools can be\nused to check for semantic violations in the rewrites prior to DBA\nconsideration.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from standard benchmarks on contemporary\ndatabase platforms. The results show significant performance improvements for\nslow queries, with regard to both abstract costing and actual execution, over\nboth SOTA techniques and the native query optimizer. For instance, with TPC-DS\non PostgreSQL, the geometric mean of the runtime speedups for slow queries was\nas high as 18.4 over the native optimizer, whereas SOTA delivered 6 in\ncomparison.\n  Overall, LITHE is a promising step toward viable LLM-based advisory tools for\nameliorating enterprise query performance."
                },
                "authors": [
                    {
                        "name": "Sriram Dharwada"
                    },
                    {
                        "name": "Himanshu Devrani"
                    },
                    {
                        "name": "Jayant Haritsa"
                    },
                    {
                        "name": "Harish Doraiswamy"
                    }
                ],
                "author_detail": {
                    "name": "Harish Doraiswamy"
                },
                "author": "Harish Doraiswamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13992v1",
                "updated": "2025-03-18T07:52:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    52,
                    4,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T07:52:04Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    52,
                    4,
                    1,
                    77,
                    0
                ],
                "title": "The KoLMogorov Test: Compression by Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KoLMogorov Test: Compression by Code Generation"
                },
                "summary": "Compression is at the heart of intelligence. A theoretically optimal way to\ncompress any sequence of data is to find the shortest program that outputs that\nsequence and then halts. However, such 'Kolmogorov compression' is\nuncomputable, and code generating LLMs struggle to approximate this theoretical\nideal, as it requires reasoning, planning and search capabilities beyond those\nof current models. In this work, we introduce the KoLMogorov-Test (KT), a\ncompression-as-intelligence test for code generating LLMs. In KT a model is\npresented with a sequence of data at inference time, and asked to generate the\nshortest program that produces the sequence. We identify several benefits of KT\nfor both evaluation and training: an essentially infinite number of problem\ninstances of varying difficulty is readily available, strong baselines already\nexist, the evaluation metric (compression) cannot be gamed, and pretraining\ndata contamination is highly unlikely. To evaluate current models, we use\naudio, text, and DNA data, as well as sequences produced by random synthetic\nprograms. Current flagship models perform poorly - both GPT4-o and\nLlama-3.1-405B struggle on our natural and synthetic sequences. On our\nsynthetic distribution, we are able to train code generation models with lower\ncompression rates than previous approaches. Moreover, we show that gains on\nsynthetic data generalize poorly to real data, suggesting that new innovations\nare necessary for additional gains on KT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression is at the heart of intelligence. A theoretically optimal way to\ncompress any sequence of data is to find the shortest program that outputs that\nsequence and then halts. However, such 'Kolmogorov compression' is\nuncomputable, and code generating LLMs struggle to approximate this theoretical\nideal, as it requires reasoning, planning and search capabilities beyond those\nof current models. In this work, we introduce the KoLMogorov-Test (KT), a\ncompression-as-intelligence test for code generating LLMs. In KT a model is\npresented with a sequence of data at inference time, and asked to generate the\nshortest program that produces the sequence. We identify several benefits of KT\nfor both evaluation and training: an essentially infinite number of problem\ninstances of varying difficulty is readily available, strong baselines already\nexist, the evaluation metric (compression) cannot be gamed, and pretraining\ndata contamination is highly unlikely. To evaluate current models, we use\naudio, text, and DNA data, as well as sequences produced by random synthetic\nprograms. Current flagship models perform poorly - both GPT4-o and\nLlama-3.1-405B struggle on our natural and synthetic sequences. On our\nsynthetic distribution, we are able to train code generation models with lower\ncompression rates than previous approaches. Moreover, we show that gains on\nsynthetic data generalize poorly to real data, suggesting that new innovations\nare necessary for additional gains on KT."
                },
                "authors": [
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Kunhao Zheng"
                    },
                    {
                        "name": "Fabian Gloeckle"
                    },
                    {
                        "name": "Jonas Gehring"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Taco Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Taco Cohen"
                },
                "author": "Taco Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13980v1",
                "updated": "2025-03-18T07:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    30,
                    29,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T07:30:29Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    30,
                    29,
                    1,
                    77,
                    0
                ],
                "title": "Empowering LLMs in Decision Games through Algorithmic Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs in Decision Games through Algorithmic Data Synthesis"
                },
                "summary": "Large Language Models (LLMs) have exhibited impressive capabilities across\nnumerous domains, yet they often struggle with complex reasoning and\ndecision-making tasks. Decision-making games, which inherently require\nmultifaceted reasoning logic, serve as ideal sandboxes for evaluating and\nenhancing the reasoning abilities of LLMs. In this work, we first explore\nwhether LLMs can master complex decision-making games through targeted\npost-training. To this end, we design data synthesis strategies and curate\nextensive offline datasets from two classic games, Doudizhu and Go. We further\ndevelop a suite of techniques to effectively incorporate this data into LLM\ntraining, resulting in two novel agents: Mastermind-Dou and Mastermind-Go. Our\nexperimental results demonstrate that these Mastermind LLMs achieve competitive\nperformance in their respective games. Additionally, we explore whether\nintegrating decision-making data can enhance the general reasoning abilities of\nLLMs. Our findings suggest that such post-training improves certain aspects of\nreasoning, providing valuable insights for optimizing LLM data collection and\nsynthesis strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited impressive capabilities across\nnumerous domains, yet they often struggle with complex reasoning and\ndecision-making tasks. Decision-making games, which inherently require\nmultifaceted reasoning logic, serve as ideal sandboxes for evaluating and\nenhancing the reasoning abilities of LLMs. In this work, we first explore\nwhether LLMs can master complex decision-making games through targeted\npost-training. To this end, we design data synthesis strategies and curate\nextensive offline datasets from two classic games, Doudizhu and Go. We further\ndevelop a suite of techniques to effectively incorporate this data into LLM\ntraining, resulting in two novel agents: Mastermind-Dou and Mastermind-Go. Our\nexperimental results demonstrate that these Mastermind LLMs achieve competitive\nperformance in their respective games. Additionally, we explore whether\nintegrating decision-making data can enhance the general reasoning abilities of\nLLMs. Our findings suggest that such post-training improves certain aspects of\nreasoning, providing valuable insights for optimizing LLM data collection and\nsynthesis strategies."
                },
                "authors": [
                    {
                        "name": "Haolin Wang"
                    },
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10118v2",
                "updated": "2025-03-18T07:28:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    28,
                    11,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-13T07:27:05Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    7,
                    27,
                    5,
                    3,
                    72,
                    0
                ],
                "title": "An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy\n  Transfer with Differentiable Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy\n  Transfer with Differentiable Simulation"
                },
                "summary": "The sim-to-real gap remains a critical challenge in robotics, hindering the\ndeployment of algorithms trained in simulation to real-world systems. This\npaper introduces a novel Real-Sim-Real (RSR) loop framework leveraging\ndifferentiable simulation to address this gap by iteratively refining\nsimulation parameters, aligning them with real-world conditions, and enabling\nrobust and efficient policy transfer. A key contribution of our work is the\ndesign of an informative cost function that encourages the collection of\ndiverse and representative real-world data, minimizing bias and maximizing the\nutility of each data point for simulation refinement. This cost function\nintegrates seamlessly into existing reinforcement learning algorithms (e.g.,\nPPO, SAC) and ensures a balanced exploration of critical regions in the real\ndomain. Furthermore, our approach is implemented on the versatile Mujoco MJX\nplatform, and our framework is compatible with a wide range of robotic systems.\nExperimental results on several robotic manipulation tasks demonstrate that our\nmethod significantly reduces the sim-to-real gap, achieving high task\nperformance and generalizability across diverse scenarios of both explicit and\nimplicit environmental uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sim-to-real gap remains a critical challenge in robotics, hindering the\ndeployment of algorithms trained in simulation to real-world systems. This\npaper introduces a novel Real-Sim-Real (RSR) loop framework leveraging\ndifferentiable simulation to address this gap by iteratively refining\nsimulation parameters, aligning them with real-world conditions, and enabling\nrobust and efficient policy transfer. A key contribution of our work is the\ndesign of an informative cost function that encourages the collection of\ndiverse and representative real-world data, minimizing bias and maximizing the\nutility of each data point for simulation refinement. This cost function\nintegrates seamlessly into existing reinforcement learning algorithms (e.g.,\nPPO, SAC) and ensures a balanced exploration of critical regions in the real\ndomain. Furthermore, our approach is implemented on the versatile Mujoco MJX\nplatform, and our framework is compatible with a wide range of robotic systems.\nExperimental results on several robotic manipulation tasks demonstrate that our\nmethod significantly reduces the sim-to-real gap, achieving high task\nperformance and generalizability across diverse scenarios of both explicit and\nimplicit environmental uncertainties."
                },
                "authors": [
                    {
                        "name": "Lu Shi"
                    },
                    {
                        "name": "Yuxuan Xu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Wenhao Zhao"
                    },
                    {
                        "name": "Yufei Jia"
                    },
                    {
                        "name": "Zike Yan"
                    },
                    {
                        "name": "Weibin Gu"
                    },
                    {
                        "name": "Guyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guyue Zhou"
                },
                "author": "Guyue Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13975v1",
                "updated": "2025-03-18T07:24:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    24,
                    5,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T07:24:05Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    24,
                    5,
                    1,
                    77,
                    0
                ],
                "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark"
                },
                "summary": "Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, early grounding failures predicted later interaction\nbreakdowns. Building on these insights, we introduce RIFTS: a benchmark derived\nfrom publicly available LLM interaction data containing situations where LLMs\nfail to initiate grounding. We note that current frontier models perform poorly\non RIFTS, highlighting the need to reconsider how we train and prompt LLMs for\nhuman interaction. To this end, we develop a preliminary intervention that\nmitigates grounding failures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, early grounding failures predicted later interaction\nbreakdowns. Building on these insights, we introduce RIFTS: a benchmark derived\nfrom publicly available LLM interaction data containing situations where LLMs\nfail to initiate grounding. We note that current frontier models perform poorly\non RIFTS, highlighting the need to reconsider how we train and prompt LLMs for\nhuman interaction. To this end, we develop a preliminary intervention that\nmitigates grounding failures."
                },
                "authors": [
                    {
                        "name": "Omar Shaikh"
                    },
                    {
                        "name": "Hussein Mozannar"
                    },
                    {
                        "name": "Gagan Bansal"
                    },
                    {
                        "name": "Adam Fourney"
                    },
                    {
                        "name": "Eric Horvitz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Horvitz"
                },
                "author": "Eric Horvitz",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09174v2",
                "updated": "2025-03-18T07:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    13,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2024-08-17T11:40:10Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    11,
                    40,
                    10,
                    5,
                    230,
                    0
                ],
                "title": "TableBench: A Comprehensive and Complex Benchmark for Table Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableBench: A Comprehensive and Complex Benchmark for Table Question\n  Answering"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have markedly enhanced\nthe interpretation and processing of tabular data, introducing previously\nunimaginable capabilities. Despite these achievements, LLMs still encounter\nsignificant challenges when applied in industrial scenarios, particularly due\nto the increased complexity of reasoning required with real-world tabular data,\nunderscoring a notable disparity between academic benchmarks and practical\napplications. To address this discrepancy, we conduct a detailed investigation\ninto the application of tabular data in industrial scenarios and propose a\ncomprehensive and complex benchmark TableBench, including 18 fields within four\nmajor categories of table question answering (TableQA) capabilities.\nFurthermore, we introduce TableLLM, trained on our meticulously constructed\ntraining set TableInstruct, achieving comparable performance with GPT-3.5.\nMassive experiments conducted on TableBench indicate that both open-source and\nproprietary LLMs still have significant room for improvement to meet real-world\ndemands, where the most advanced model, GPT-4, achieves only a modest score\ncompared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have markedly enhanced\nthe interpretation and processing of tabular data, introducing previously\nunimaginable capabilities. Despite these achievements, LLMs still encounter\nsignificant challenges when applied in industrial scenarios, particularly due\nto the increased complexity of reasoning required with real-world tabular data,\nunderscoring a notable disparity between academic benchmarks and practical\napplications. To address this discrepancy, we conduct a detailed investigation\ninto the application of tabular data in industrial scenarios and propose a\ncomprehensive and complex benchmark TableBench, including 18 fields within four\nmajor categories of table question answering (TableQA) capabilities.\nFurthermore, we introduce TableLLM, trained on our meticulously constructed\ntraining set TableInstruct, achieving comparable performance with GPT-3.5.\nMassive experiments conducted on TableBench indicate that both open-source and\nproprietary LLMs still have significant room for improvement to meet real-world\ndemands, where the most advanced model, GPT-4, achieves only a modest score\ncompared to humans."
                },
                "authors": [
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "Daixin Shu"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Tianzhen Sun"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Tongliang Li"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13966v1",
                "updated": "2025-03-18T06:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    58,
                    41,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T06:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    58,
                    41,
                    1,
                    77,
                    0
                ],
                "title": "FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation\n  Tasks"
                },
                "summary": "The aspiration of the Vision-and-Language Navigation (VLN) task has long been\nto develop an embodied agent with robust adaptability, capable of seamlessly\ntransferring its navigation capabilities across various tasks. Despite\nremarkable advancements in recent years, most methods necessitate\ndataset-specific training, thereby lacking the capability to generalize across\ndiverse datasets encompassing distinct types of instructions. Large language\nmodels (LLMs) have demonstrated exceptional reasoning and generalization\nabilities, exhibiting immense potential in robot action planning. In this\npaper, we propose FlexVLN, an innovative hierarchical approach to VLN that\nintegrates the fundamental navigation ability of a supervised-learning-based\nInstruction Follower with the robust generalization ability of the LLM Planner,\nenabling effective generalization across diverse VLN datasets. Moreover, a\nverification mechanism and a multi-model integration mechanism are proposed to\nmitigate potential hallucinations by the LLM Planner and enhance execution\naccuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as\nout-of-domain datasets for assessing generalization ability. The generalization\nperformance of FlexVLN surpasses that of all the previous methods to a large\nextent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aspiration of the Vision-and-Language Navigation (VLN) task has long been\nto develop an embodied agent with robust adaptability, capable of seamlessly\ntransferring its navigation capabilities across various tasks. Despite\nremarkable advancements in recent years, most methods necessitate\ndataset-specific training, thereby lacking the capability to generalize across\ndiverse datasets encompassing distinct types of instructions. Large language\nmodels (LLMs) have demonstrated exceptional reasoning and generalization\nabilities, exhibiting immense potential in robot action planning. In this\npaper, we propose FlexVLN, an innovative hierarchical approach to VLN that\nintegrates the fundamental navigation ability of a supervised-learning-based\nInstruction Follower with the robust generalization ability of the LLM Planner,\nenabling effective generalization across diverse VLN datasets. Moreover, a\nverification mechanism and a multi-model integration mechanism are proposed to\nmitigate potential hallucinations by the LLM Planner and enhance execution\naccuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as\nout-of-domain datasets for assessing generalization ability. The generalization\nperformance of FlexVLN surpasses that of all the previous methods to a large\nextent."
                },
                "authors": [
                    {
                        "name": "Siqi Zhang"
                    },
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Qunbo Wang"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Zhihua Wei"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13964v1",
                "updated": "2025-03-18T06:57:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    57,
                    21,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T06:57:21Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    57,
                    21,
                    1,
                    77,
                    0
                ],
                "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding"
                },
                "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent."
                },
                "authors": [
                    {
                        "name": "Siwei Han"
                    },
                    {
                        "name": "Peng Xia"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Tong Sun"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Hongtu Zhu"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13962v1",
                "updated": "2025-03-18T06:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    54,
                    59,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T06:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    54,
                    59,
                    1,
                    77,
                    0
                ],
                "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of Adversarial Robustness in Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions."
                },
                "authors": [
                    {
                        "name": "Chengze Jiang"
                    },
                    {
                        "name": "Zhuangzhuang Wang"
                    },
                    {
                        "name": "Minjing Dong"
                    },
                    {
                        "name": "Jie Gui"
                    }
                ],
                "author_detail": {
                    "name": "Jie Gui"
                },
                "author": "Jie Gui",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13956v1",
                "updated": "2025-03-18T06:48:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    48,
                    8,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T06:48:08Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    48,
                    8,
                    1,
                    77,
                    0
                ],
                "title": "Improving LLM Video Understanding with 16 Frames Per Second",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Video Understanding with 16 Frames Per Second"
                },
                "summary": "Human vision is dynamic and continuous. However, in video understanding with\nmultimodal large language models (LLMs), existing methods primarily rely on\nstatic features extracted from images sampled at a fixed low frame rate of\nframe-per-second (FPS) $\\leqslant$2, leading to critical visual information\nloss. In this paper, we introduce F-16, the first multimodal LLM designed for\nhigh-frame-rate video understanding. By increasing the frame rate to 16 FPS and\ncompressing visual tokens within each 1-second clip, F-16 efficiently captures\ndynamic visual features while preserving key semantic information. Experimental\nresults demonstrate that higher frame rates considerably enhance video\nunderstanding across multiple benchmarks, providing a new approach to improving\nvideo LLMs beyond scaling model size or training data. F-16 achieves\nstate-of-the-art performance among 7-billion-parameter video LLMs on both\ngeneral and fine-grained video understanding benchmarks, such as Video-MME and\nTemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks,\nincluding high-speed sports analysis (\\textit{e.g.}, basketball, football,\ngymnastics, and diving), outperforming SOTA proprietary visual models like\nGPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method\nfor F-16 that enables highly efficient low-frame-rate inference without\nrequiring model retraining. Upon acceptance, we will release the source code,\nmodel checkpoints, and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human vision is dynamic and continuous. However, in video understanding with\nmultimodal large language models (LLMs), existing methods primarily rely on\nstatic features extracted from images sampled at a fixed low frame rate of\nframe-per-second (FPS) $\\leqslant$2, leading to critical visual information\nloss. In this paper, we introduce F-16, the first multimodal LLM designed for\nhigh-frame-rate video understanding. By increasing the frame rate to 16 FPS and\ncompressing visual tokens within each 1-second clip, F-16 efficiently captures\ndynamic visual features while preserving key semantic information. Experimental\nresults demonstrate that higher frame rates considerably enhance video\nunderstanding across multiple benchmarks, providing a new approach to improving\nvideo LLMs beyond scaling model size or training data. F-16 achieves\nstate-of-the-art performance among 7-billion-parameter video LLMs on both\ngeneral and fine-grained video understanding benchmarks, such as Video-MME and\nTemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks,\nincluding high-speed sports analysis (\\textit{e.g.}, basketball, football,\ngymnastics, and diving), outperforming SOTA proprietary visual models like\nGPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method\nfor F-16 that enables highly efficient low-frame-rate inference without\nrequiring model retraining. Upon acceptance, we will release the source code,\nmodel checkpoints, and data."
                },
                "authors": [
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]