[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v1",
                "updated": "2025-04-08T17:59:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v1",
                "updated": "2025-04-06T15:15:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Pérez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castaño"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castaño"
                },
                "author": "Manuel Gamero-Castaño",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03708v1",
                "updated": "2025-03-27T00:10:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    10,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T00:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    10,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "Solving AI Foundational Model Latency with Telco Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving AI Foundational Model Latency with Telco Infrastructure"
                },
                "summary": "Latency remains a critical bottleneck for deploying foundational artificial\nintelligence (AI) models, such as large language models (LLMs), in\ncustomer-facing, real-time applications. While cloud-based inference offers\nscalability, it frequently introduces delays unacceptable for interactive\nexperiences, such as semantic search, personalized recommendations, or\nconversational interfaces. Telecommunications operators, historically adept at\nsolving content latency challenges through partnerships with providers like\nGoogle and Facebook, now have a unique opportunity to address similar AI\nlatency concerns. This paper presents a technical framework leveraging Telco\ninfrastructure-spanning regional data centers, existing content delivery\nnetwork (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical\n\"AI edges\" for caching and partial inference. We explore the architectural\nfeasibility of embedding semantic and vector-based AI inference caches within\nexisting Telco assets, proposing tiered caching strategies and split-inference\narchitectures that significantly reduce latency and compute costs.\nAdditionally, we address technical challenges specific to Telcos, such as cache\nsynchronization, model distribution, privacy, and hardware acceleration\nconsiderations. Finally, we discuss viable partnership models between telcos\nand AI providers, highlighting how this innovative use of telco infrastructure\ncan unlock both improved AI user experience and new revenue streams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency remains a critical bottleneck for deploying foundational artificial\nintelligence (AI) models, such as large language models (LLMs), in\ncustomer-facing, real-time applications. While cloud-based inference offers\nscalability, it frequently introduces delays unacceptable for interactive\nexperiences, such as semantic search, personalized recommendations, or\nconversational interfaces. Telecommunications operators, historically adept at\nsolving content latency challenges through partnerships with providers like\nGoogle and Facebook, now have a unique opportunity to address similar AI\nlatency concerns. This paper presents a technical framework leveraging Telco\ninfrastructure-spanning regional data centers, existing content delivery\nnetwork (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical\n\"AI edges\" for caching and partial inference. We explore the architectural\nfeasibility of embedding semantic and vector-based AI inference caches within\nexisting Telco assets, proposing tiered caching strategies and split-inference\narchitectures that significantly reduce latency and compute costs.\nAdditionally, we address technical challenges specific to Telcos, such as cache\nsynchronization, model distribution, privacy, and hardware acceleration\nconsiderations. Finally, we discuss viable partnership models between telcos\nand AI providers, highlighting how this innovative use of telco infrastructure\ncan unlock both improved AI user experience and new revenue streams."
                },
                "authors": [
                    {
                        "name": "Sebastian Barros"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Barros"
                },
                "author": "Sebastian Barros",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.06266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06266v1",
                "updated": "2025-04-08T17:59:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    59,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:59Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    59,
                    1,
                    98,
                    0
                ],
                "title": "Constraining the [CII] luminosity function from the power spectrum of\n  line intensity maps at redshift 3.6",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the [CII] luminosity function from the power spectrum of\n  line intensity maps at redshift 3.6"
                },
                "summary": "Forthcoming measurements of the line-intensity-mapping power spectrum (PS)\nare expected to set precious constraints on several quantities of astrophysical\nand cosmological interest. Our study targets the [CII] luminosity function (LF)\nat high redshift, which is still highly uncertain, in particular at the faint\nend. As an example of future opportunities, we present forecasts for the Deep\nSpectroscopic Survey (DSS) that will be conducted with the Fred Young\nSubmillimeter Telescope at $z \\simeq 3.6$ and also make predictions for\neventual $10\\times$ wider and/or $\\sqrt{10}\\times$ more sensitive surveys. The\nhalo-occupation properties of [CII] emitters in the MARIGOLD simulations\nprovide us with the motivation to abundance match two versions of the ALPINE LF\nagainst the halo mass function. We employ the resulting luminosity-mass\nrelation within the halo model to predict the expected PS signal and its\nuncertainty. Finally, we use Bayesian inference to analyse mock PS data and\nforecast what constraints could be achieved on the first two moments of the LF\nand on Schechter fits. Depending on the actual LF, the DSS will measure the\nclustering and shot-noise amplitudes of the PS with a signal-to-noise ratio of\n$\\sim 3$ or higher. However, degeneracies with the bias parameter and\nredshift-space distortions make it unfeasible to extract the first moment of\nthe LF. Even the widest and most sensitive survey we consider can only\nconstrain it with a $50\\%$ uncertainty. By jointly fitting the PS and the LF,\nwe directly constrain Schechter-function parameters. We find that the\nnormalisation and the cutoff luminosity are precisely and accurately measured\nwhile the faint-end slope remains highly uncertain (unless the true value\napproaches $-2$). Overall, increasing the survey sensitivity at fixed sky\ncoverage yields greater improvements than covering a larger area at fixed\nsensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forthcoming measurements of the line-intensity-mapping power spectrum (PS)\nare expected to set precious constraints on several quantities of astrophysical\nand cosmological interest. Our study targets the [CII] luminosity function (LF)\nat high redshift, which is still highly uncertain, in particular at the faint\nend. As an example of future opportunities, we present forecasts for the Deep\nSpectroscopic Survey (DSS) that will be conducted with the Fred Young\nSubmillimeter Telescope at $z \\simeq 3.6$ and also make predictions for\neventual $10\\times$ wider and/or $\\sqrt{10}\\times$ more sensitive surveys. The\nhalo-occupation properties of [CII] emitters in the MARIGOLD simulations\nprovide us with the motivation to abundance match two versions of the ALPINE LF\nagainst the halo mass function. We employ the resulting luminosity-mass\nrelation within the halo model to predict the expected PS signal and its\nuncertainty. Finally, we use Bayesian inference to analyse mock PS data and\nforecast what constraints could be achieved on the first two moments of the LF\nand on Schechter fits. Depending on the actual LF, the DSS will measure the\nclustering and shot-noise amplitudes of the PS with a signal-to-noise ratio of\n$\\sim 3$ or higher. However, degeneracies with the bias parameter and\nredshift-space distortions make it unfeasible to extract the first moment of\nthe LF. Even the widest and most sensitive survey we consider can only\nconstrain it with a $50\\%$ uncertainty. By jointly fitting the PS and the LF,\nwe directly constrain Schechter-function parameters. We find that the\nnormalisation and the cutoff luminosity are precisely and accurately measured\nwhile the faint-end slope remains highly uncertain (unless the true value\napproaches $-2$). Overall, increasing the survey sensitivity at fixed sky\ncoverage yields greater improvements than covering a larger area at fixed\nsensitivity."
                },
                "authors": [
                    {
                        "name": "Elena Marcuzzo"
                    },
                    {
                        "name": "Cristiano Porciani"
                    },
                    {
                        "name": "Emilio Romano-Díaz"
                    },
                    {
                        "name": "Prachi Khatri"
                    }
                ],
                "author_detail": {
                    "name": "Prachi Khatri"
                },
                "author": "Prachi Khatri",
                "arxiv_comment": "19 pages, 18 figures, 5 tables. Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06265v1",
                "updated": "2025-04-08T17:59:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    57,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning\n  through Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning\n  through Bayesian Optimization"
                },
                "summary": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization."
                },
                "authors": [
                    {
                        "name": "Bojana Ranković"
                    },
                    {
                        "name": "Philippe Schwaller"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Schwaller"
                },
                "author": "Philippe Schwaller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v1",
                "updated": "2025-04-08T17:59:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06260v1",
                "updated": "2025-04-08T17:59:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability"
                },
                "summary": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench"
                },
                "authors": [
                    {
                        "name": "Nayantara Mudur"
                    },
                    {
                        "name": "Hao Cui"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    },
                    {
                        "name": "Paul Raccuglia"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Peter Norgaard"
                    }
                ],
                "author_detail": {
                    "name": "Peter Norgaard"
                },
                "author": "Peter Norgaard",
                "arxiv_comment": "39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical\n  Reasoning and AI and Open-World Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06256v1",
                "updated": "2025-04-08T17:58:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    58,
                    47,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:58:47Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    58,
                    47,
                    1,
                    98,
                    0
                ],
                "title": "Transfer between Modalities with MetaQueries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer between Modalities with MetaQueries"
                },
                "summary": "Unified multimodal models aim to integrate understanding (text output) and\ngeneration (pixel output), but aligning these different modalities within a\nsingle architecture often demands complex training recipes and careful data\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\ndeep understanding and reasoning capabilities. Our method simplifies training,\nrequiring only paired image-caption data and standard diffusion objectives.\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\nthereby preserving its state-of-the-art multimodal understanding capabilities\nwhile achieving strong generative performance. Additionally, our method is\nflexible and can be easily instruction-tuned for advanced applications such as\nimage editing and subject-driven generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal models aim to integrate understanding (text output) and\ngeneration (pixel output), but aligning these different modalities within a\nsingle architecture often demands complex training recipes and careful data\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\ndeep understanding and reasoning capabilities. Our method simplifies training,\nrequiring only paired image-caption data and standard diffusion objectives.\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\nthereby preserving its state-of-the-art multimodal understanding capabilities\nwhile achieving strong generative performance. Additionally, our method is\nflexible and can be easily instruction-tuned for advanced applications such as\nimage editing and subject-driven generation."
                },
                "authors": [
                    {
                        "name": "Xichen Pan"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Jiuhai Chen"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Ji Hou"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Project Page: https://xichenpan.com/metaquery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05004v2",
                "updated": "2025-04-08T17:56:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    56,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T12:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    30,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Stacking Variational Bayesian Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacking Variational Bayesian Monte Carlo"
                },
                "summary": "Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method for\napproximate Bayesian inference with computationally expensive likelihoods.\nWhile VBMC's local surrogate approach provides stable approximations, its\nconservative exploration strategy and limited evaluation budget can cause it to\nmiss regions of complex posteriors. In this work, we introduce Stacking\nVariational Bayesian Monte Carlo (S-VBMC), a method that constructs global\nposterior approximations by merging independent VBMC runs through a principled\nand inexpensive post-processing step. Our approach leverages VBMC's mixture\nposterior representation and per-component evidence estimates, requiring no\nadditional likelihood evaluations while being naturally parallelizable. We\ndemonstrate S-VBMC's effectiveness on two synthetic problems designed to\nchallenge VBMC's exploration capabilities and two real-world applications from\ncomputational neuroscience, showing substantial improvements in posterior\napproximation quality across all cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method for\napproximate Bayesian inference with computationally expensive likelihoods.\nWhile VBMC's local surrogate approach provides stable approximations, its\nconservative exploration strategy and limited evaluation budget can cause it to\nmiss regions of complex posteriors. In this work, we introduce Stacking\nVariational Bayesian Monte Carlo (S-VBMC), a method that constructs global\nposterior approximations by merging independent VBMC runs through a principled\nand inexpensive post-processing step. Our approach leverages VBMC's mixture\nposterior representation and per-component evidence estimates, requiring no\nadditional likelihood evaluations while being naturally parallelizable. We\ndemonstrate S-VBMC's effectiveness on two synthetic problems designed to\nchallenge VBMC's exploration capabilities and two real-world applications from\ncomputational neuroscience, showing substantial improvements in posterior\napproximation quality across all cases."
                },
                "authors": [
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Chengkun Li"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "arxiv_comment": "Accepted at the Workshop track of the 7th Symposium in Advances in\n  Approximate Bayesian Inference (AABI 2025). 24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03601v2",
                "updated": "2025-04-08T17:46:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    46,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-04T17:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay"
                },
                "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "12 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09015v2",
                "updated": "2025-04-08T17:34:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    34,
                    30,
                    1,
                    98,
                    0
                ],
                "published": "2025-01-15T18:57:33Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    18,
                    57,
                    33,
                    2,
                    15,
                    0
                ],
                "title": "Family-wise Error Rate Control with E-values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Family-wise Error Rate Control with E-values"
                },
                "summary": "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors."
                },
                "authors": [
                    {
                        "name": "Will Hartog"
                    },
                    {
                        "name": "Lihua Lei"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Lei"
                },
                "author": "Lihua Lei",
                "arxiv_comment": "19 pages, 5 figures, 4 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62J15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06235v1",
                "updated": "2025-04-08T17:32:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    32,
                    56,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:32:56Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    32,
                    56,
                    1,
                    98,
                    0
                ],
                "title": "Decentralized Federated Domain Generalization with Style Sharing: A\n  Formal Modeling and Convergence Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Domain Generalization with Style Sharing: A\n  Formal Modeling and Convergence Analysis"
                },
                "summary": "Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much of the federated learning (FL) literature focuses on settings where\nlocal dataset statistics remain the same between training and testing time.\nRecent advances in domain generalization (DG) aim to use data from source\n(training) domains to train a model that generalizes well to data from unseen\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\nobjectives and training processes; and (2) DG research in FL being limited to\nthe conventional star-topology architecture. Addressing the second gap, we\ndevelop $\\textit{Decentralized Federated Domain Generalization with Style\nSharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\nallow devices in a peer-to-peer network to achieve DG based on sharing style\ninformation inferred from their datasets. Additionally, we fill the first gap\nby providing the first systematic approach to mathematically analyzing\nstyle-based DG training optimization. We cast existing centralized DG\nalgorithms within our framework, and employ their formalisms to model\n$\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\na sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through\nexperiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$\ncan obtain significant improvements in accuracy across target domains with\nminimal added communication overhead compared to decentralized gradient methods\nthat do not employ style sharing."
                },
                "authors": [
                    {
                        "name": "Shahryar Zehtabi"
                    },
                    {
                        "name": "Dong-Jun Han"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v3",
                "updated": "2025-04-09T08:33:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    33,
                    54,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars"
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Code: https://github.com/sprintml/privacy_attacks_against_iars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22250v2",
                "updated": "2025-04-08T17:25:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    25,
                    48,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-28T09:04:10Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    4,
                    10,
                    4,
                    87,
                    0
                ],
                "title": "Modeling Challenging Patient Interactions: LLMs for Medical\n  Communication Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Challenging Patient Interactions: LLMs for Medical\n  Communication Training"
                },
                "summary": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training."
                },
                "authors": [
                    {
                        "name": "Anna Bodonhelyi"
                    },
                    {
                        "name": "Christian Stegemann-Philipps"
                    },
                    {
                        "name": "Alessandra Sonanini"
                    },
                    {
                        "name": "Lea Herschbach"
                    },
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Anne Herrmann-Werner"
                    },
                    {
                        "name": "Teresa Festl-Wietek"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Friederike Holderried"
                    }
                ],
                "author_detail": {
                    "name": "Friederike Holderried"
                },
                "author": "Friederike Holderried",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08366v2",
                "updated": "2025-04-08T17:25:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    25,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2024-06-12T16:14:44Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    16,
                    14,
                    44,
                    2,
                    164,
                    0
                ],
                "title": "Highest Probability Density Conformal Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Highest Probability Density Conformal Regions"
                },
                "summary": "This paper proposes a new method for finding the highest predictive density\nset or region, within the heteroscedastic regression framework. This framework\nenjoys the property that any highest predictive density set is a translation of\nsome scalar multiple of a highest density set for the standardized regression\nerror, with the same prediction accuracy. The proposed method leverages this\nproperty to efficiently compute conformal prediction regions, using signed\nconformal inference, kernel density estimation, in conjunction with any\nconditional mean, and scale estimators. While most conformal prediction methods\noutput prediction intervals, this method adapts to the target. When the target\nis multi-modal, the proposed method outputs an approximation of the smallest\nmulti-modal set. When the target is uni-modal, the proposed method outputs an\napproximation of the smallest interval. Under mild regularity conditions, we\nshow that these conformal prediction sets are asymptotically close to the true\nsmallest prediction sets. Because of the conformal guarantee, even in finite\nsample sizes the method has guaranteed coverage. With simulations and a real\ndata analysis we demonstrate that the proposed method is better than existing\nmethods when the target is multi-modal, and gives similar results when the\ntarget is uni-modal. Supplementary materials, including proofs and additional\nimages, are available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a new method for finding the highest predictive density\nset or region, within the heteroscedastic regression framework. This framework\nenjoys the property that any highest predictive density set is a translation of\nsome scalar multiple of a highest density set for the standardized regression\nerror, with the same prediction accuracy. The proposed method leverages this\nproperty to efficiently compute conformal prediction regions, using signed\nconformal inference, kernel density estimation, in conjunction with any\nconditional mean, and scale estimators. While most conformal prediction methods\noutput prediction intervals, this method adapts to the target. When the target\nis multi-modal, the proposed method outputs an approximation of the smallest\nmulti-modal set. When the target is uni-modal, the proposed method outputs an\napproximation of the smallest interval. Under mild regularity conditions, we\nshow that these conformal prediction sets are asymptotically close to the true\nsmallest prediction sets. Because of the conformal guarantee, even in finite\nsample sizes the method has guaranteed coverage. With simulations and a real\ndata analysis we demonstrate that the proposed method is better than existing\nmethods when the target is multi-modal, and gives similar results when the\ntarget is uni-modal. Supplementary materials, including proofs and additional\nimages, are available online."
                },
                "authors": [
                    {
                        "name": "Max Sampson"
                    },
                    {
                        "name": "Kung-Sik Chan"
                    }
                ],
                "author_detail": {
                    "name": "Kung-Sik Chan"
                },
                "author": "Kung-Sik Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02879v2",
                "updated": "2025-04-08T17:18:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    18,
                    21,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-03T18:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    7,
                    25,
                    3,
                    277,
                    0
                ],
                "title": "Position: LLM Unlearning Benchmarks are Weak Measures of Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: LLM Unlearning Benchmarks are Weak Measures of Progress"
                },
                "summary": "Unlearning methods have the potential to improve the privacy and safety of\nlarge language models (LLMs) by removing sensitive or harmful information post\nhoc. The LLM unlearning research community has increasingly turned toward\nempirical benchmarks to assess the effectiveness of such methods. In this\npaper, we find that existing benchmarks provide an overly optimistic and\npotentially misleading view on the effectiveness of candidate unlearning\nmethods. By introducing simple, benign modifications to a number of popular\nbenchmarks, we expose instances where supposedly unlearned information remains\naccessible, or where the unlearning process has degraded the model's\nperformance on retained information to a much greater extent than indicated by\nthe original benchmark. We identify that existing benchmarks are particularly\nvulnerable to modifications that introduce even loose dependencies between the\nforget and retain information. Further, we show that ambiguity in unlearning\ntargets in existing benchmarks can easily lead to the design of methods that\noverfit to the given test queries. Based on our findings, we urge the community\nto be cautious when interpreting benchmark results as reliable measures of\nprogress, and we provide several recommendations to guide future LLM unlearning\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning methods have the potential to improve the privacy and safety of\nlarge language models (LLMs) by removing sensitive or harmful information post\nhoc. The LLM unlearning research community has increasingly turned toward\nempirical benchmarks to assess the effectiveness of such methods. In this\npaper, we find that existing benchmarks provide an overly optimistic and\npotentially misleading view on the effectiveness of candidate unlearning\nmethods. By introducing simple, benign modifications to a number of popular\nbenchmarks, we expose instances where supposedly unlearned information remains\naccessible, or where the unlearning process has degraded the model's\nperformance on retained information to a much greater extent than indicated by\nthe original benchmark. We identify that existing benchmarks are particularly\nvulnerable to modifications that introduce even loose dependencies between the\nforget and retain information. Further, we show that ambiguity in unlearning\ntargets in existing benchmarks can easily lead to the design of methods that\noverfit to the given test queries. Based on our findings, we urge the community\nto be cautious when interpreting benchmark results as reliable measures of\nprogress, and we provide several recommendations to guide future LLM unlearning\nresearch."
                },
                "authors": [
                    {
                        "name": "Pratiksha Thaker"
                    },
                    {
                        "name": "Shengyuan Hu"
                    },
                    {
                        "name": "Neil Kale"
                    },
                    {
                        "name": "Yash Maurya"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "Appears in IEEE Secure and Trustworthy Machine Learning (SaTML) '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06227v1",
                "updated": "2025-04-08T17:16:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    16,
                    52,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:16:52Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    16,
                    52,
                    1,
                    98,
                    0
                ],
                "title": "LExT: Towards Evaluating Trustworthiness of Natural Language\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LExT: Towards Evaluating Trustworthiness of Natural Language\n  Explanations"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nhigh-stakes domains, there have been several approaches proposed toward\ngenerating natural language explanations. These explanations are crucial for\nenhancing the interpretability of a model, especially in sensitive domains like\nhealthcare, where transparency and reliability are key. In light of such\nexplanations being generated by LLMs and its known concerns, there is a growing\nneed for robust evaluation frameworks to assess model-generated explanations.\nNatural Language Generation metrics like BLEU and ROUGE capture syntactic and\nsemantic accuracies but overlook other crucial aspects such as factual\naccuracy, consistency, and faithfulness. To address this gap, we propose a\ngeneral framework for quantifying trustworthiness of natural language\nexplanations, balancing Plausibility and Faithfulness, to derive a\ncomprehensive Language Explanation Trustworthiness Score (LExT) (The code and\nset up to reproduce our experiments are publicly available at\nhttps://github.com/cerai-iitm/LExT). Applying our domain-agnostic framework to\nthe healthcare domain using public medical datasets, we evaluate six models,\nincluding domain-specific and general-purpose models. Our findings demonstrate\nsignificant differences in their ability to generate trustworthy explanations.\nOn comparing these explanations, we make interesting observations such as\ninconsistencies in Faithfulness demonstrated by general-purpose models and\ntheir tendency to outperform domain-specific fine-tuned models. This work\nfurther highlights the importance of using a tailored evaluation framework to\nassess natural language explanations in sensitive fields, providing a\nfoundation for improving the trustworthiness and transparency of language\nmodels in healthcare and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nhigh-stakes domains, there have been several approaches proposed toward\ngenerating natural language explanations. These explanations are crucial for\nenhancing the interpretability of a model, especially in sensitive domains like\nhealthcare, where transparency and reliability are key. In light of such\nexplanations being generated by LLMs and its known concerns, there is a growing\nneed for robust evaluation frameworks to assess model-generated explanations.\nNatural Language Generation metrics like BLEU and ROUGE capture syntactic and\nsemantic accuracies but overlook other crucial aspects such as factual\naccuracy, consistency, and faithfulness. To address this gap, we propose a\ngeneral framework for quantifying trustworthiness of natural language\nexplanations, balancing Plausibility and Faithfulness, to derive a\ncomprehensive Language Explanation Trustworthiness Score (LExT) (The code and\nset up to reproduce our experiments are publicly available at\nhttps://github.com/cerai-iitm/LExT). Applying our domain-agnostic framework to\nthe healthcare domain using public medical datasets, we evaluate six models,\nincluding domain-specific and general-purpose models. Our findings demonstrate\nsignificant differences in their ability to generate trustworthy explanations.\nOn comparing these explanations, we make interesting observations such as\ninconsistencies in Faithfulness demonstrated by general-purpose models and\ntheir tendency to outperform domain-specific fine-tuned models. This work\nfurther highlights the importance of using a tailored evaluation framework to\nassess natural language explanations in sensitive fields, providing a\nfoundation for improving the trustworthiness and transparency of language\nmodels in healthcare and beyond."
                },
                "authors": [
                    {
                        "name": "Krithi Shailya"
                    },
                    {
                        "name": "Shreya Rajpal"
                    },
                    {
                        "name": "Gokul S Krishnan"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06225v1",
                "updated": "2025-04-08T17:13:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    13,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:13:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    13,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via\n  Adaptation"
                },
                "summary": "While decoder-only large language models (LLMs) have shown impressive\nresults, encoder-decoder models are still widely adopted in real-world\napplications for their inference efficiency and richer encoder representation.\nIn this paper, we study a novel problem: adapting pretrained decoder-only LLMs\nto encoder-decoder, with the goal of leveraging the strengths of both\napproaches to achieve a more favorable quality-efficiency trade-off. We argue\nthat adaptation not only enables inheriting the capability of decoder-only LLMs\nbut also reduces the demand for computation compared to pretraining from\nscratch. We rigorously explore different pretraining objectives and parameter\ninitialization/optimization techniques. Through extensive experiments based on\nGemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to\n1.6B), we demonstrate the effectiveness of adaptation and the advantage of\nencoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs\nachieve comparable (often better) pretraining performance but substantially\nbetter finetuning performance than their decoder-only counterpart. For example,\nGemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning.\nEncoder-decoder adaptation also allows for flexible combination of\ndifferent-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B\nby $>$3\\%. The adapted encoder representation also yields better results on\nSuperGLUE. We will release our checkpoints to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While decoder-only large language models (LLMs) have shown impressive\nresults, encoder-decoder models are still widely adopted in real-world\napplications for their inference efficiency and richer encoder representation.\nIn this paper, we study a novel problem: adapting pretrained decoder-only LLMs\nto encoder-decoder, with the goal of leveraging the strengths of both\napproaches to achieve a more favorable quality-efficiency trade-off. We argue\nthat adaptation not only enables inheriting the capability of decoder-only LLMs\nbut also reduces the demand for computation compared to pretraining from\nscratch. We rigorously explore different pretraining objectives and parameter\ninitialization/optimization techniques. Through extensive experiments based on\nGemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to\n1.6B), we demonstrate the effectiveness of adaptation and the advantage of\nencoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs\nachieve comparable (often better) pretraining performance but substantially\nbetter finetuning performance than their decoder-only counterpart. For example,\nGemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning.\nEncoder-decoder adaptation also allows for flexible combination of\ndifferent-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B\nby $>$3\\%. The adapted encoder representation also yields better results on\nSuperGLUE. We will release our checkpoints to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Fedor Moiseev"
                    },
                    {
                        "name": "Joshua Ainslie"
                    },
                    {
                        "name": "Paul Suganthan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Fede Lebron"
                    },
                    {
                        "name": "Orhan Firat"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Zhe Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Dong"
                },
                "author": "Zhe Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15341v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15341v3",
                "updated": "2025-04-08T17:09:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    9,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2024-06-21T17:55:24Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    55,
                    24,
                    4,
                    173,
                    0
                ],
                "title": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data\n  Analysis"
                },
                "summary": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides analysis code and results for solving a wide\nrange of gene-trait association problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTEX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides analysis code and results for solving a wide\nrange of gene-trait association problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTEX."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Shuyu Chen"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "31 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15341v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15341v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16520v3",
                "updated": "2025-04-08T17:08:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    8,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-21T21:21:29Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    21,
                    29,
                    0,
                    295,
                    0
                ],
                "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context"
                },
                "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
                },
                "authors": [
                    {
                        "name": "Naba Rizvi"
                    },
                    {
                        "name": "Harper Strickland"
                    },
                    {
                        "name": "Daniel Gitelman"
                    },
                    {
                        "name": "Tristan Cooper"
                    },
                    {
                        "name": "Alexis Morales-Flores"
                    },
                    {
                        "name": "Michael Golden"
                    },
                    {
                        "name": "Aekta Kallepalli"
                    },
                    {
                        "name": "Akshat Alurkar"
                    },
                    {
                        "name": "Haaset Owens"
                    },
                    {
                        "name": "Saleha Ahmedi"
                    },
                    {
                        "name": "Isha Khirwadkar"
                    },
                    {
                        "name": "Imani Munyaka"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "arxiv_comment": "9 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06219v1",
                "updated": "2025-04-08T17:08:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    8,
                    6,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:08:06Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    8,
                    6,
                    1,
                    98,
                    0
                ],
                "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling\n  Opt-Outs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling\n  Opt-Outs"
                },
                "summary": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions."
                },
                "authors": [
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Vinko Sabolčec"
                    },
                    {
                        "name": "Matin Ansaripour"
                    },
                    {
                        "name": "Ayush Kumar Tarun"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06215v1",
                "updated": "2025-04-08T17:00:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    0,
                    42,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:00:42Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    0,
                    42,
                    1,
                    98,
                    0
                ],
                "title": "Randomization Inference in Two-Sided Market Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization Inference in Two-Sided Market Experiments"
                },
                "summary": "Randomized experiments are increasingly employed in two-sided markets, such\nas buyer-seller platforms, to evaluate treatment effects from marketplace\ninterventions. These experiments must reflect the underlying two-sided market\nstructure in their design (e.g., sellers and buyers), making them particularly\nchallenging to analyze. In this paper, we propose a randomization inference\nframework to analyze outcomes from such two-sided experiments. Our approach is\nfinite-sample valid under sharp null hypotheses for any test statistic and\nmaintains asymptotic validity under weak null hypotheses through\nstudentization. Moreover, we provide heuristic guidance for choosing among\nmultiple valid randomization tests to enhance statistical power, which we\ndemonstrate empirically. Finally, we demonstrate the performance of our\nmethodology through a series of simulation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments are increasingly employed in two-sided markets, such\nas buyer-seller platforms, to evaluate treatment effects from marketplace\ninterventions. These experiments must reflect the underlying two-sided market\nstructure in their design (e.g., sellers and buyers), making them particularly\nchallenging to analyze. In this paper, we propose a randomization inference\nframework to analyze outcomes from such two-sided experiments. Our approach is\nfinite-sample valid under sharp null hypotheses for any test statistic and\nmaintains asymptotic validity under weak null hypotheses through\nstudentization. Moreover, we provide heuristic guidance for choosing among\nmultiple valid randomization tests to enhance statistical power, which we\ndemonstrate empirically. Finally, we demonstrate the performance of our\nmethodology through a series of simulation studies."
                },
                "authors": [
                    {
                        "name": "Jizhou Liu"
                    },
                    {
                        "name": "Azeem M. Shaikh"
                    },
                    {
                        "name": "Panos Toulis"
                    }
                ],
                "author_detail": {
                    "name": "Panos Toulis"
                },
                "author": "Panos Toulis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06214v1",
                "updated": "2025-04-08T16:58:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    58,
                    58,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:58:58Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    58,
                    58,
                    1,
                    98,
                    0
                ],
                "title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language\n  Models"
                },
                "summary": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/."
                },
                "authors": [
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00509v2",
                "updated": "2025-04-08T16:51:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    51,
                    11,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-01T07:57:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    57,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?"
                },
                "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs."
                },
                "authors": [
                    {
                        "name": "Kai Yan"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Xiaowen Guo"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "arxiv_comment": "23 pages, 3 figures, 10 tables. V2 refines related work and\n  acknowledgement, and adds links to chat logs for qualitative studies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06196v1",
                "updated": "2025-04-08T16:39:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    39,
                    2,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:39:02Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    39,
                    2,
                    1,
                    98,
                    0
                ],
                "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TxGemma: Efficient and Agentic LLMs for Therapeutics"
                },
                "summary": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high)."
                },
                "authors": [
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Rory Pilgrim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "David Fleet"
                    },
                    {
                        "name": "Shekoofeh Azizi"
                    }
                ],
                "author_detail": {
                    "name": "Shekoofeh Azizi"
                },
                "author": "Shekoofeh Azizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06473v3",
                "updated": "2025-04-08T16:32:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    32,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-09T02:00:37Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    0,
                    37,
                    2,
                    283,
                    0
                ],
                "title": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic\n  Guidance"
                },
                "summary": "Robot learning approaches such as behavior cloning and reinforcement learning\nhave shown great promise in synthesizing robot skills from human demonstrations\nin specific environments. However, these approaches often require task-specific\ndemonstrations or designing complex simulation environments, which limits the\ndevelopment of generalizable and robust policies for unseen real-world\nsettings. Recent advances in the use of foundation models for robotics (e.g.,\nLLMs, VLMs) have shown great potential in enabling systems to understand the\nsemantics in the world from large-scale internet data. However, it remains an\nopen challenge to use this knowledge to enable robotic systems to understand\nthe underlying dynamics of the world, to generalize policies across different\ntasks, and to adapt policies to new environments. To alleviate these\nlimitations, we propose an agentic framework for robot self-guidance and\nself-improvement, which consists of a set of role-specialized conversational\nagents, such as a high-level advisor, a grounding agent, a monitoring agent,\nand a robotic agent. Our framework iteratively grounds a base robot policy to\nrelevant objects in the environment and uses visuomotor cues to shift the\naction distribution of the policy to more desirable states, online, while\nremaining agnostic to the subjective configuration of a given robot hardware\nplatform. We demonstrate that our approach can effectively guide manipulation\npolicies to achieve significantly higher success rates, both in simulation and\nin real-world experiments, without the need for additional human demonstrations\nor extensive exploration. Code and videos available at:\nhttps://agenticrobots.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot learning approaches such as behavior cloning and reinforcement learning\nhave shown great promise in synthesizing robot skills from human demonstrations\nin specific environments. However, these approaches often require task-specific\ndemonstrations or designing complex simulation environments, which limits the\ndevelopment of generalizable and robust policies for unseen real-world\nsettings. Recent advances in the use of foundation models for robotics (e.g.,\nLLMs, VLMs) have shown great potential in enabling systems to understand the\nsemantics in the world from large-scale internet data. However, it remains an\nopen challenge to use this knowledge to enable robotic systems to understand\nthe underlying dynamics of the world, to generalize policies across different\ntasks, and to adapt policies to new environments. To alleviate these\nlimitations, we propose an agentic framework for robot self-guidance and\nself-improvement, which consists of a set of role-specialized conversational\nagents, such as a high-level advisor, a grounding agent, a monitoring agent,\nand a robotic agent. Our framework iteratively grounds a base robot policy to\nrelevant objects in the environment and uses visuomotor cues to shift the\naction distribution of the policy to more desirable states, online, while\nremaining agnostic to the subjective configuration of a given robot hardware\nplatform. We demonstrate that our approach can effectively guide manipulation\npolicies to achieve significantly higher success rates, both in simulation and\nin real-world experiments, without the need for additional human demonstrations\nor extensive exploration. Code and videos available at:\nhttps://agenticrobots.github.io"
                },
                "authors": [
                    {
                        "name": "Arthur Bucker"
                    },
                    {
                        "name": "Pablo Ortega-Kral"
                    },
                    {
                        "name": "Jonathan Francis"
                    },
                    {
                        "name": "Jean Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jean Oh"
                },
                "author": "Jean Oh",
                "arxiv_comment": "21 pages, 12 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06185v1",
                "updated": "2025-04-08T16:25:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    25,
                    59,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:25:59Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    25,
                    59,
                    1,
                    98,
                    0
                ],
                "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care"
                },
                "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication."
                },
                "authors": [
                    {
                        "name": "Vanessa Borst"
                    },
                    {
                        "name": "Timo Dittus"
                    },
                    {
                        "name": "Tassilo Dege"
                    },
                    {
                        "name": "Astrid Schmieder"
                    },
                    {
                        "name": "Samuel Kounev"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kounev"
                },
                "author": "Samuel Kounev",
                "arxiv_comment": "Main paper: 17 pages; supplementary material: 16 pages; paper\n  submitted to the application track of the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases\n  (ECML PKDD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06174v1",
                "updated": "2025-04-08T16:18:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    18,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:18:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    18,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "On Soft Clustering For Correlation Estimators: Model Uncertainty,\n  Differentiability, and Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Soft Clustering For Correlation Estimators: Model Uncertainty,\n  Differentiability, and Surrogates"
                },
                "summary": "Properly estimating correlations between objects at different spatial scales\nnecessitates $\\mathcal{O}(n^2)$ distance calculations. For this reason, most\nwidely adopted packages for estimating correlations use clustering algorithms\nto approximate local trends. However, methods for quantifying the error\nintroduced by this clustering have been understudied. In response, we present\nan algorithm for estimating correlations that is probabilistic in the way that\nit clusters objects, enabling us to quantify the uncertainty caused by\nclustering simply through model inference. These soft clustering assignments\nenable correlation estimators that are theoretically differentiable with\nrespect to their input catalogs. Thus, we also build a theoretical framework\nfor differentiable correlation functions and describe their utility in\ncomparison to existing surrogate models. Notably, we find that repeated\nnormalization and distance function calls slow gradient calculations and that\nsparse Jacobians destabilize precision, pointing towards either approximate or\nsurrogate methods as a necessary solution to exact gradients from correlation\nfunctions. To that end, we close with a discussion of surrogate models as\nproxies for correlation functions. We provide an example that demonstrates the\nefficacy of surrogate models to enable gradient-based optimization of\nastrophysical model parameters, successfully minimizing a correlation function\noutput. Our numerical experiments cover science cases across cosmology, from\npoint spread function (PSF) modeling efforts to gravitational simulations to\ngalaxy intrinsic alignment (IA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Properly estimating correlations between objects at different spatial scales\nnecessitates $\\mathcal{O}(n^2)$ distance calculations. For this reason, most\nwidely adopted packages for estimating correlations use clustering algorithms\nto approximate local trends. However, methods for quantifying the error\nintroduced by this clustering have been understudied. In response, we present\nan algorithm for estimating correlations that is probabilistic in the way that\nit clusters objects, enabling us to quantify the uncertainty caused by\nclustering simply through model inference. These soft clustering assignments\nenable correlation estimators that are theoretically differentiable with\nrespect to their input catalogs. Thus, we also build a theoretical framework\nfor differentiable correlation functions and describe their utility in\ncomparison to existing surrogate models. Notably, we find that repeated\nnormalization and distance function calls slow gradient calculations and that\nsparse Jacobians destabilize precision, pointing towards either approximate or\nsurrogate methods as a necessary solution to exact gradients from correlation\nfunctions. To that end, we close with a discussion of surrogate models as\nproxies for correlation functions. We provide an example that demonstrates the\nefficacy of surrogate models to enable gradient-based optimization of\nastrophysical model parameters, successfully minimizing a correlation function\noutput. Our numerical experiments cover science cases across cosmology, from\npoint spread function (PSF) modeling efforts to gravitational simulations to\ngalaxy intrinsic alignment (IA)."
                },
                "authors": [
                    {
                        "name": "Edward Berman"
                    },
                    {
                        "name": "Sneh Pandya"
                    },
                    {
                        "name": "Jacqueline McCleary"
                    },
                    {
                        "name": "Marko Shuntov"
                    },
                    {
                        "name": "Caitlin Casey"
                    },
                    {
                        "name": "Nicole Drakos"
                    },
                    {
                        "name": "Andreas Faisst"
                    },
                    {
                        "name": "Steven Gillman"
                    },
                    {
                        "name": "Ghassem Gozaliasl"
                    },
                    {
                        "name": "Natalie Hogg"
                    },
                    {
                        "name": "Jeyhan Kartaltepe"
                    },
                    {
                        "name": "Anton Koekemoer"
                    },
                    {
                        "name": "Wilfried Mercier"
                    },
                    {
                        "name": "Diana Scognamiglio"
                    },
                    {
                        "name": "COSMOS-Web"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "The JWST Cosmic Origins Survey"
                    }
                ],
                "author_detail": {
                    "name": "The JWST Cosmic Origins Survey"
                },
                "author": "The JWST Cosmic Origins Survey",
                "arxiv_comment": "Submitted to OjA. Code available at\n  https://github.com/EdwardBerman/cosmo-corr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04794v2",
                "updated": "2025-04-08T16:16:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    16,
                    30,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-07T15:36:05Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    5,
                    3,
                    312,
                    0
                ],
                "title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowCoder-X: Boosting Multilingual Information Extraction via Code"
                },
                "summary": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in IE,\na significant imbalance across languages persists, highlighting an underlying\ndeficiency. To address this, we propose KnowCoder-X, a powerful code LLM with\nadvanced cross-lingual and multilingual capabilities for universal information\nextraction. Firstly, it standardizes the representation of multilingual schemas\nusing Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we enhance the model's cross-lingual transferability through IE\ncross-lingual alignment instruction tuning on a translated instance prediction\ntask we proposed. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by $30.17\\%$ and SoTA by $20.03\\%$, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in IE,\na significant imbalance across languages persists, highlighting an underlying\ndeficiency. To address this, we propose KnowCoder-X, a powerful code LLM with\nadvanced cross-lingual and multilingual capabilities for universal information\nextraction. Firstly, it standardizes the representation of multilingual schemas\nusing Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we enhance the model's cross-lingual transferability through IE\ncross-lingual alignment instruction tuning on a translated instance prediction\ntask we proposed. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by $30.17\\%$ and SoTA by $20.03\\%$, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Wenxuan Jiang"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17741v2",
                "updated": "2025-04-08T16:09:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    9,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2024-03-26T14:29:51Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    14,
                    29,
                    51,
                    1,
                    86,
                    0
                ],
                "title": "The low multipoles in the Pantheon+SH0ES data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low multipoles in the Pantheon+SH0ES data"
                },
                "summary": "In previous work we have shown that the dipole in the low redshift supernovae\nof the Pantheon+SH0ES data does not agree with the one inferred from the\nvelocity of the solar system as obtained from CMB data. We interpreted this as\nthe presence of significant bulk velocities. In this paper we study the\nmonopole, dipole and quadrupole in the Pantheon+SH0ES data. We find that in\naddition to the dipole also both, the monopole and the quadrupole are detected\nwith high significance. They are of similar amplitudes as the bulk flow. While\nthe monopole is only significant at very low redshift, the quadrupole even\nincreases with redshift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In previous work we have shown that the dipole in the low redshift supernovae\nof the Pantheon+SH0ES data does not agree with the one inferred from the\nvelocity of the solar system as obtained from CMB data. We interpreted this as\nthe presence of significant bulk velocities. In this paper we study the\nmonopole, dipole and quadrupole in the Pantheon+SH0ES data. We find that in\naddition to the dipole also both, the monopole and the quadrupole are detected\nwith high significance. They are of similar amplitudes as the bulk flow. While\nthe monopole is only significant at very low redshift, the quadrupole even\nincreases with redshift."
                },
                "authors": [
                    {
                        "name": "Francesco Sorrenti"
                    },
                    {
                        "name": "Ruth Durrer"
                    },
                    {
                        "name": "Martin Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Kunz"
                },
                "author": "Martin Kunz",
                "arxiv_doi": "10.1088/1475-7516/2025/04/013",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/04/013",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.17741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "30 pages, 17 figures",
                "arxiv_journal_ref": "JCAP04(2025)013",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06160v2",
                "updated": "2025-04-09T04:24:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    24,
                    38,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T15:56:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    56,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\n  Narratives Targeting Mental Health Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\n  Narratives Targeting Mental Health Groups"
                },
                "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."
                },
                "authors": [
                    {
                        "name": "Rijul Magu"
                    },
                    {
                        "name": "Arka Dutta"
                    },
                    {
                        "name": "Sean Kim"
                    },
                    {
                        "name": "Ashiqur R. KhudaBukhsh"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; K.4.1; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16587v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16587v3",
                "updated": "2025-04-08T15:47:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    47,
                    13,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-25T17:22:10Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    22,
                    10,
                    0,
                    330,
                    0
                ],
                "title": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles"
                },
                "summary": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision."
                },
                "authors": [
                    {
                        "name": "Klinsmann Agyei"
                    },
                    {
                        "name": "Pouria Sarhadi"
                    },
                    {
                        "name": "Wasif Naeem"
                    }
                ],
                "author_detail": {
                    "name": "Wasif Naeem"
                },
                "author": "Wasif Naeem",
                "arxiv_comment": "This work has been accepted for publication at European Control\n  Conference 2025, \\c{opyright} IEEE 2025. Please cite the published version\n  when available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16587v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16587v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06144v1",
                "updated": "2025-04-08T15:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    39,
                    25,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    39,
                    25,
                    1,
                    98,
                    0
                ],
                "title": "A Training-Free Style-aligned Image Generation with Scale-wise\n  Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-Free Style-aligned Image Generation with Scale-wise\n  Autoregressive Model"
                },
                "summary": "We present a training-free style-aligned image generation method that\nleverages a scale-wise autoregressive model. While large-scale text-to-image\n(T2I) models, particularly diffusion-based methods, have demonstrated\nimpressive generation quality, they often suffer from style misalignment across\ngenerated image sets and slow inference speeds, limiting their practical\nusability. To address these issues, we propose three key components: initial\nfeature replacement to ensure consistent background appearance, pivotal feature\ninterpolation to align object placement, and dynamic style injection, which\nreinforces style consistency using a schedule function. Unlike previous methods\nrequiring fine-tuning or additional training, our approach maintains fast\ninference while preserving individual content details. Extensive experiments\nshow that our method achieves generation quality comparable to competing\napproaches, significantly improves style alignment, and delivers inference\nspeeds over six times faster than the fastest model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a training-free style-aligned image generation method that\nleverages a scale-wise autoregressive model. While large-scale text-to-image\n(T2I) models, particularly diffusion-based methods, have demonstrated\nimpressive generation quality, they often suffer from style misalignment across\ngenerated image sets and slow inference speeds, limiting their practical\nusability. To address these issues, we propose three key components: initial\nfeature replacement to ensure consistent background appearance, pivotal feature\ninterpolation to align object placement, and dynamic style injection, which\nreinforces style consistency using a schedule function. Unlike previous methods\nrequiring fine-tuning or additional training, our approach maintains fast\ninference while preserving individual content details. Extensive experiments\nshow that our method achieves generation quality comparable to competing\napproaches, significantly improves style alignment, and delivers inference\nspeeds over six times faster than the fastest model."
                },
                "authors": [
                    {
                        "name": "Jihun Park"
                    },
                    {
                        "name": "Jongmin Gim"
                    },
                    {
                        "name": "Kyoungmin Lee"
                    },
                    {
                        "name": "Minseok Oh"
                    },
                    {
                        "name": "Minwoo Choi"
                    },
                    {
                        "name": "Jaeyeul Kim"
                    },
                    {
                        "name": "Woo Chool Park"
                    },
                    {
                        "name": "Sunghoon Im"
                    }
                ],
                "author_detail": {
                    "name": "Sunghoon Im"
                },
                "author": "Sunghoon Im",
                "arxiv_comment": "17 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06143v1",
                "updated": "2025-04-08T15:38:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    38,
                    42,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:38:42Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    38,
                    42,
                    1,
                    98,
                    0
                ],
                "title": "ARLO: A Tailorable Approach for Transforming Natural Language Software\n  Requirements into Architecture using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARLO: A Tailorable Approach for Transforming Natural Language Software\n  Requirements into Architecture using LLMs"
                },
                "summary": "Software requirements expressed in natural language (NL) frequently suffer\nfrom verbosity, ambiguity, and inconsistency. This creates a range of\nchallenges, including selecting an appropriate architecture for a system and\nassessing different architectural alternatives. Relying on human expertise to\naccomplish the task of mapping NL requirements to architecture is\ntime-consuming and error-prone. This paper proposes ARLO, an approach that\nautomates this task by leveraging (1) a set of NL requirements for a system,\n(2) an existing standard that specifies architecturally relevant software\nquality attributes, and (3) a readily available Large Language Model (LLM).\nSpecifically, ARLO determines the subset of NL requirements for a given system\nthat is architecturally relevant and maps that subset to a tailorable matrix of\narchitectural choices. ARLO applies integer linear programming on the\narchitectural-choice matrix to determine the optimal architecture for the\ncurrent requirements. We demonstrate ARLO's efficacy using a set of real-world\nexamples. We highlight ARLO's ability (1) to trace the selected architectural\nchoices to the requirements and (2) to isolate NL requirements that exert a\nparticular influence on a system's architecture. This allows the\nidentification, comparative assessment, and exploration of alternative\narchitectural choices based on the requirements and constraints expressed\ntherein.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software requirements expressed in natural language (NL) frequently suffer\nfrom verbosity, ambiguity, and inconsistency. This creates a range of\nchallenges, including selecting an appropriate architecture for a system and\nassessing different architectural alternatives. Relying on human expertise to\naccomplish the task of mapping NL requirements to architecture is\ntime-consuming and error-prone. This paper proposes ARLO, an approach that\nautomates this task by leveraging (1) a set of NL requirements for a system,\n(2) an existing standard that specifies architecturally relevant software\nquality attributes, and (3) a readily available Large Language Model (LLM).\nSpecifically, ARLO determines the subset of NL requirements for a given system\nthat is architecturally relevant and maps that subset to a tailorable matrix of\narchitectural choices. ARLO applies integer linear programming on the\narchitectural-choice matrix to determine the optimal architecture for the\ncurrent requirements. We demonstrate ARLO's efficacy using a set of real-world\nexamples. We highlight ARLO's ability (1) to trace the selected architectural\nchoices to the requirements and (2) to isolate NL requirements that exert a\nparticular influence on a system's architecture. This allows the\nidentification, comparative assessment, and exploration of alternative\narchitectural choices based on the requirements and constraints expressed\ntherein."
                },
                "authors": [
                    {
                        "name": "Tooraj Helmi"
                    }
                ],
                "author_detail": {
                    "name": "Tooraj Helmi"
                },
                "author": "Tooraj Helmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06136v1",
                "updated": "2025-04-08T15:32:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    32,
                    9,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:32:09Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    32,
                    9,
                    1,
                    98,
                    0
                ],
                "title": "QGen Studio: An Adaptive Question-Answer Generation, Training and\n  Evaluation Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QGen Studio: An Adaptive Question-Answer Generation, Training and\n  Evaluation Platform"
                },
                "summary": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally."
                },
                "authors": [
                    {
                        "name": "Movina Moses"
                    },
                    {
                        "name": "Mohab Elkaref"
                    },
                    {
                        "name": "James Barry"
                    },
                    {
                        "name": "Shinnosuke Tanaka"
                    },
                    {
                        "name": "Vishnudev Kuruvanthodi"
                    },
                    {
                        "name": "Nathan Herr"
                    },
                    {
                        "name": "Campbell D Watson"
                    },
                    {
                        "name": "Geeth De Mel"
                    }
                ],
                "author_detail": {
                    "name": "Geeth De Mel"
                },
                "author": "Geeth De Mel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06134v1",
                "updated": "2025-04-08T15:28:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    28,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:28:44Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    28,
                    44,
                    1,
                    98,
                    0
                ],
                "title": "SpikeStream: Accelerating Spiking Neural Network Inference on RISC-V\n  Clusters with Sparse Computation Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikeStream: Accelerating Spiking Neural Network Inference on RISC-V\n  Clusters with Sparse Computation Extensions"
                },
                "summary": "Spiking Neural Network (SNN) inference has a clear potential for high energy\nefficiency as computation is triggered by events. However, the inherent\nsparsity of events poses challenges for conventional computing systems, driving\nthe development of specialized neuromorphic processors, which come with high\nsilicon area costs and lack the flexibility needed for running other\ncomputational kernels, limiting widespread adoption. In this paper, we explore\nthe low-level software design, parallelization, and acceleration of SNNs on\ngeneral-purpose multicore clusters with a low-overhead RISC-V ISA extension for\nstreaming sparse computations. We propose SpikeStream, an optimization\ntechnique that maps weights accesses to affine and indirect register-mapped\nmemory streams to enhance performance, utilization, and efficiency. Our results\non the end-to-end Spiking-VGG11 model demonstrate a significant 4.39x speedup\nand an increase in utilization from 9.28% to 52.3% compared to a non-streaming\nparallel baseline. Additionally, we achieve an energy efficiency gain of 3.46x\nover LSMCore and a performance gain of 2.38x over Loihi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Network (SNN) inference has a clear potential for high energy\nefficiency as computation is triggered by events. However, the inherent\nsparsity of events poses challenges for conventional computing systems, driving\nthe development of specialized neuromorphic processors, which come with high\nsilicon area costs and lack the flexibility needed for running other\ncomputational kernels, limiting widespread adoption. In this paper, we explore\nthe low-level software design, parallelization, and acceleration of SNNs on\ngeneral-purpose multicore clusters with a low-overhead RISC-V ISA extension for\nstreaming sparse computations. We propose SpikeStream, an optimization\ntechnique that maps weights accesses to affine and indirect register-mapped\nmemory streams to enhance performance, utilization, and efficiency. Our results\non the end-to-end Spiking-VGG11 model demonstrate a significant 4.39x speedup\nand an increase in utilization from 9.28% to 52.3% compared to a non-streaming\nparallel baseline. Additionally, we achieve an energy efficiency gain of 3.46x\nover LSMCore and a performance gain of 2.38x over Loihi."
                },
                "authors": [
                    {
                        "name": "Simone Manoni"
                    },
                    {
                        "name": "Paul Scheffler"
                    },
                    {
                        "name": "Luca Zanatta"
                    },
                    {
                        "name": "Andrea Acquaviva"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted as full-paper at DATE25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06131v1",
                "updated": "2025-04-08T15:23:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    23,
                    21,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:23:21Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    23,
                    21,
                    1,
                    98,
                    0
                ],
                "title": "FaceCloak: Learning to Protect Face Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceCloak: Learning to Protect Face Templates"
                },
                "summary": "Generative models can reconstruct face images from encoded representations\n(templates) bearing remarkable likeness to the original face raising security\nand privacy concerns. We present FaceCloak, a neural network framework that\nprotects face templates by generating smart, renewable binary cloaks. Our\nmethod proactively thwarts inversion attacks by cloaking face templates with\nunique disruptors synthesized from a single face template on the fly while\nprovably retaining biometric utility and unlinkability. Our cloaked templates\ncan suppress sensitive attributes while generalizing to novel feature\nextraction schemes and outperforms leading baselines in terms of biometric\nmatching and resiliency to reconstruction attacks. FaceCloak-based matching is\nextremely fast (inference time cost=0.28ms) and light-weight (0.57MB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models can reconstruct face images from encoded representations\n(templates) bearing remarkable likeness to the original face raising security\nand privacy concerns. We present FaceCloak, a neural network framework that\nprotects face templates by generating smart, renewable binary cloaks. Our\nmethod proactively thwarts inversion attacks by cloaking face templates with\nunique disruptors synthesized from a single face template on the fly while\nprovably retaining biometric utility and unlinkability. Our cloaked templates\ncan suppress sensitive attributes while generalizing to novel feature\nextraction schemes and outperforms leading baselines in terms of biometric\nmatching and resiliency to reconstruction attacks. FaceCloak-based matching is\nextremely fast (inference time cost=0.28ms) and light-weight (0.57MB)."
                },
                "authors": [
                    {
                        "name": "Sudipta Banerjee"
                    },
                    {
                        "name": "Anubhav Jain"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Nasir Memon"
                    }
                ],
                "author_detail": {
                    "name": "Nasir Memon"
                },
                "author": "Nasir Memon",
                "arxiv_comment": "Accepted in IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16260v2",
                "updated": "2025-04-08T15:19:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    19,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-25T10:23:11Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    23,
                    11,
                    0,
                    330,
                    0
                ],
                "title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures"
                },
                "summary": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks\nstepwise. However, training CoT capabilities requires detailed reasoning data,\nwhich is often scarce. The self-taught reasoner (STaR) framework addresses this\nby using reinforcement learning to automatically generate reasoning steps,\nreducing reliance on human-labeled data. Although STaR and its variants have\ndemonstrated empirical success, a theoretical foundation explaining these\nimprovements is lacking. Large language models (LLMs) have demonstrated\nremarkable mathematical capabilities, largely driven by chain-of-thought (CoT)\nprompting, which decomposes complex reasoning into step-by-step solutions.\nHowever, the mechanisms underlying LLMs' ability to perform arithmetic in a\nsingle step of CoT remain poorly understood. In this work, we propose that LLMs\nlearn arithmetic by capturing algebraic structures, such as commutativity and\nidentity properties. Since these structures are observable through input-output\nrelationships, they can generalize to unseen data. We empirically demonstrate\nthat LLMs can learn algebraic structures using a custom dataset of arithmetic\nproblems, as well as providing theoretical evidence showing that, under\nspecific configurations of weights and biases, the transformer-based LLMs can\ngenerate embeddings that remain invariant to both permutations of input tokens\nand the presence of identity elements. Our findings indicate that leveraging\nalgebraic structures can enhance the LLMs' arithmetic capabilities, offering\ninsights into improving their arithmetic performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks\nstepwise. However, training CoT capabilities requires detailed reasoning data,\nwhich is often scarce. The self-taught reasoner (STaR) framework addresses this\nby using reinforcement learning to automatically generate reasoning steps,\nreducing reliance on human-labeled data. Although STaR and its variants have\ndemonstrated empirical success, a theoretical foundation explaining these\nimprovements is lacking. Large language models (LLMs) have demonstrated\nremarkable mathematical capabilities, largely driven by chain-of-thought (CoT)\nprompting, which decomposes complex reasoning into step-by-step solutions.\nHowever, the mechanisms underlying LLMs' ability to perform arithmetic in a\nsingle step of CoT remain poorly understood. In this work, we propose that LLMs\nlearn arithmetic by capturing algebraic structures, such as commutativity and\nidentity properties. Since these structures are observable through input-output\nrelationships, they can generalize to unseen data. We empirically demonstrate\nthat LLMs can learn algebraic structures using a custom dataset of arithmetic\nproblems, as well as providing theoretical evidence showing that, under\nspecific configurations of weights and biases, the transformer-based LLMs can\ngenerate embeddings that remain invariant to both permutations of input tokens\nand the presence of identity elements. Our findings indicate that leveraging\nalgebraic structures can enhance the LLMs' arithmetic capabilities, offering\ninsights into improving their arithmetic performance."
                },
                "authors": [
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "You-Chen Lin"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "arxiv_journal_ref": "ICLR 2025 Workshop on Reasoning and Planning for Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06122v2",
                "updated": "2025-04-09T04:03:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    3,
                    0,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T15:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    15,
                    26,
                    1,
                    98,
                    0
                ],
                "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning"
                },
                "summary": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details."
                },
                "authors": [
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06108v1",
                "updated": "2025-04-08T14:55:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    55,
                    34,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    55,
                    34,
                    1,
                    98,
                    0
                ],
                "title": "Characterizing direct and indirect causal effects when outcomes are\n  dependent due to treatment spillover and outcome spillover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing direct and indirect causal effects when outcomes are\n  dependent due to treatment spillover and outcome spillover"
                },
                "summary": "We provide novel insight into causal inference when both treatment spillover\nand outcome spillover occur in connected populations, by taking advantage of\nrecent advances in statistical network analysis. Scenarios with treatment\nspillover and outcome spillover are challenging, because both forms of\nspillover affect outcomes and therefore treatment spillover and outcome\nspillover are intertwined, and outcomes are dependent conditional on treatments\nby virtue of outcome spillover. As a result, the direct and indirect causal\neffects arising from spillover have remained black boxes: While the direct and\nindirect causal effects can be identified, it is unknown how these causal\neffects explicitly depend on the effects of treatment, treatment spillover, and\noutcome spillover. We make three contributions, facilitated by low-rank random\ninterference graphs. First, we provide novel insight into direct and indirect\ncausal effects by disentangling the contributions of treatment, treatment\nspillover, and outcome spillover. Second, we provide scalable estimators of\ndirect and indirect causal effects. Third, we establish rates of convergence\nfor estimators of direct and indirect causal effects. These are the first\nconvergence rates in scenarios in which treatment spillover and outcome\nspillover are intertwined and outcomes are dependent conditional on treatments,\nand the interference graph is sparse or dense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide novel insight into causal inference when both treatment spillover\nand outcome spillover occur in connected populations, by taking advantage of\nrecent advances in statistical network analysis. Scenarios with treatment\nspillover and outcome spillover are challenging, because both forms of\nspillover affect outcomes and therefore treatment spillover and outcome\nspillover are intertwined, and outcomes are dependent conditional on treatments\nby virtue of outcome spillover. As a result, the direct and indirect causal\neffects arising from spillover have remained black boxes: While the direct and\nindirect causal effects can be identified, it is unknown how these causal\neffects explicitly depend on the effects of treatment, treatment spillover, and\noutcome spillover. We make three contributions, facilitated by low-rank random\ninterference graphs. First, we provide novel insight into direct and indirect\ncausal effects by disentangling the contributions of treatment, treatment\nspillover, and outcome spillover. Second, we provide scalable estimators of\ndirect and indirect causal effects. Third, we establish rates of convergence\nfor estimators of direct and indirect causal effects. These are the first\nconvergence rates in scenarios in which treatment spillover and outcome\nspillover are intertwined and outcomes are dependent conditional on treatments,\nand the interference graph is sparse or dense."
                },
                "authors": [
                    {
                        "name": "Subhankar Bhadra"
                    },
                    {
                        "name": "Michael Schweinberger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Schweinberger"
                },
                "author": "Michael Schweinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00936v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00936v5",
                "updated": "2025-04-08T14:47:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    47,
                    7,
                    1,
                    98,
                    0
                ],
                "published": "2024-07-01T03:37:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    3,
                    37,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey"
                },
                "summary": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Leong Hou U"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Wenbin Guo"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Guo"
                },
                "author": "Wenbin Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00936v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00936v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06095v1",
                "updated": "2025-04-08T14:35:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    35,
                    40,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:35:40Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    35,
                    40,
                    1,
                    98,
                    0
                ],
                "title": "Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for\n  Scaled-up LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for\n  Scaled-up LLM Training"
                },
                "summary": "LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and\nmodel-parallel (MP) execution. Critical to achieving efficiency is\ntensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of\nGPUs, referred to as a scale-up domain, and the larger the scale-up domain the\nbetter the performance. New datacenter architectures are emerging with more\nGPUs able to be tightly-coupled in a scale-up domain, such as moving from 8\nGPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains\nincrease the blast-radius of failures, with a failure of single GPU potentially\nimpacting TP execution on the full scale-up domain, which can degrade overall\nLLM training throughput dramatically. With as few as 0.1% of GPUs being in a\nfailed state, a high TP-degree job can experience nearly 10% reduction in LLM\ntraining throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate\nthis amplified impact of GPU failures. In NTP, a DP replica that experiences\nGPU failures operates at a reduced TP degree, contributing throughput equal to\nthe percentage of still-functional GPUs. We also propose a rack-design with\nimproved electrical and thermal capabilities in order to sustain power-boosting\nof scale-up domains that have experienced failures; combined with NTP, this can\nallow the DP replica with the reduced TP degree (i.e., with failed GPUs) to\nkeep up with the others, thereby achieving near-zero throughput loss for\nlarge-scale LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and\nmodel-parallel (MP) execution. Critical to achieving efficiency is\ntensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of\nGPUs, referred to as a scale-up domain, and the larger the scale-up domain the\nbetter the performance. New datacenter architectures are emerging with more\nGPUs able to be tightly-coupled in a scale-up domain, such as moving from 8\nGPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains\nincrease the blast-radius of failures, with a failure of single GPU potentially\nimpacting TP execution on the full scale-up domain, which can degrade overall\nLLM training throughput dramatically. With as few as 0.1% of GPUs being in a\nfailed state, a high TP-degree job can experience nearly 10% reduction in LLM\ntraining throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate\nthis amplified impact of GPU failures. In NTP, a DP replica that experiences\nGPU failures operates at a reduced TP degree, contributing throughput equal to\nthe percentage of still-functional GPUs. We also propose a rack-design with\nimproved electrical and thermal capabilities in order to sustain power-boosting\nof scale-up domains that have experienced failures; combined with NTP, this can\nallow the DP replica with the reduced TP degree (i.e., with failed GPUs) to\nkeep up with the others, thereby achieving near-zero throughput loss for\nlarge-scale LLM training."
                },
                "authors": [
                    {
                        "name": "Daiyaan Arfeen"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Bhargava Gopireddy"
                    },
                    {
                        "name": "Ahmet Inci"
                    },
                    {
                        "name": "Gregory R. Ganger"
                    }
                ],
                "author_detail": {
                    "name": "Gregory R. Ganger"
                },
                "author": "Gregory R. Ganger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09516v3",
                "updated": "2025-04-08T14:03:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    3,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T16:26:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    26,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning"
                },
                "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Hansi Zeng"
                    },
                    {
                        "name": "Zhenrui Yue"
                    },
                    {
                        "name": "Jinsung Yoon"
                    },
                    {
                        "name": "Sercan Arik"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11779v2",
                "updated": "2025-04-08T13:56:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    56,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-17T13:14:11Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    14,
                    11,
                    0,
                    48,
                    0
                ],
                "title": "Efficient Response Generation Strategy Selection for Fine-Tuning Large\n  Language Models Through Self-Aligned Perplexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Response Generation Strategy Selection for Fine-Tuning Large\n  Language Models Through Self-Aligned Perplexity"
                },
                "summary": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing 'familiarity' through theoretical analysis and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing 'familiarity' through theoretical analysis and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Xuan Ren"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06048v1",
                "updated": "2025-04-08T13:47:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    47,
                    7,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:47:07Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    47,
                    7,
                    1,
                    98,
                    0
                ],
                "title": "Trust-Region Twisted Policy Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust-Region Twisted Policy Improvement"
                },
                "summary": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\nproven challenging in practice which has motivated alternative planners like\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\npersisting design choices of these particle filters often conflict with the aim\nof online planning in RL, which is to obtain a policy improvement at the start\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\nfor RL by improving data generation within the planner through constrained\naction sampling and explicit terminal state handling, as well as improving\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\nMCTS and SMC methods in both discrete and continuous domains."
                },
                "authors": [
                    {
                        "name": "Joery A. de Vries"
                    },
                    {
                        "name": "Jinke He"
                    },
                    {
                        "name": "Yaniv Oren"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs T. J. Spaan"
                },
                "author": "Matthijs T. J. Spaan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06039v1",
                "updated": "2025-04-08T13:39:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    39,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:39:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    39,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning\n  Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning\n  Strategies"
                },
                "summary": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies."
                },
                "authors": [
                    {
                        "name": "Julia Werner"
                    },
                    {
                        "name": "Christoph Gerum"
                    },
                    {
                        "name": "Jorg Nick"
                    },
                    {
                        "name": "Maxime Le Floch"
                    },
                    {
                        "name": "Franz Brinkmann"
                    },
                    {
                        "name": "Jochen Hampe"
                    },
                    {
                        "name": "Oliver Bringmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bringmann"
                },
                "author": "Oliver Bringmann",
                "arxiv_comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBS EMBC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06036v1",
                "updated": "2025-04-08T13:36:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    36,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:36:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    36,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Sense Embeddings for Language Models and Knowledge Distillation"
                },
                "summary": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict"
                },
                "authors": [
                    {
                        "name": "Qitong Wang"
                    },
                    {
                        "name": "Mohammed J. Zaki"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Vasileios Kalantzis"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Kalantzis"
                },
                "author": "Vasileios Kalantzis",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06027v1",
                "updated": "2025-04-08T13:32:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    32,
                    56,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:32:56Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    32,
                    56,
                    1,
                    98,
                    0
                ],
                "title": "OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model"
                },
                "summary": "Multimodal remote sensing image registration aligns images from different\nsensors for data fusion and analysis. However, current methods often fail to\nextract modality-invariant features when aligning image pairs with large\nnonlinear radiometric differences. To address this issues, we propose\nOSDM-MReg, a novel multimodal image registration framework based image-to-image\ntranslation to eliminate the gap of multimodal images. Firstly, we propose a\nnovel one-step unaligned target-guided conditional denoising diffusion\nprobabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified\ndomain. In the inference stage, traditional conditional DDPM generate\ntranslated source image by a large number of iterations, which severely slows\ndown the image registration task. To address this issues, we use the unaligned\ntraget image as a condition to promote the generation of low-frequency features\nof the translated source image. Furthermore, during the training stage, we add\nthe inverse process of directly predicting the translated image to ensure that\nthe translated source image can be generated in one step during the testing\nstage. Additionally, to supervised the detail features of translated source\nimage, we propose a new perceptual loss that focuses on the high-frequency\nfeature differences between the translated and ground-truth images. Finally, a\nmultimodal multiscale image registration network (MM-Reg) fuse the multimodal\nfeature of the unimodal images and multimodal images by proposed multimodal\nfeature fusion strategy. Experiments demonstrate superior accuracy and\nefficiency across various multimodal registration tasks, particularly for\nSAR-optical image pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal remote sensing image registration aligns images from different\nsensors for data fusion and analysis. However, current methods often fail to\nextract modality-invariant features when aligning image pairs with large\nnonlinear radiometric differences. To address this issues, we propose\nOSDM-MReg, a novel multimodal image registration framework based image-to-image\ntranslation to eliminate the gap of multimodal images. Firstly, we propose a\nnovel one-step unaligned target-guided conditional denoising diffusion\nprobabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified\ndomain. In the inference stage, traditional conditional DDPM generate\ntranslated source image by a large number of iterations, which severely slows\ndown the image registration task. To address this issues, we use the unaligned\ntraget image as a condition to promote the generation of low-frequency features\nof the translated source image. Furthermore, during the training stage, we add\nthe inverse process of directly predicting the translated image to ensure that\nthe translated source image can be generated in one step during the testing\nstage. Additionally, to supervised the detail features of translated source\nimage, we propose a new perceptual loss that focuses on the high-frequency\nfeature differences between the translated and ground-truth images. Finally, a\nmultimodal multiscale image registration network (MM-Reg) fuse the multimodal\nfeature of the unimodal images and multimodal images by proposed multimodal\nfeature fusion strategy. Experiments demonstrate superior accuracy and\nefficiency across various multimodal registration tasks, particularly for\nSAR-optical image pairs."
                },
                "authors": [
                    {
                        "name": "Xiaochen Wei"
                    },
                    {
                        "name": "Weiwei Guo"
                    },
                    {
                        "name": "Wenxian Yu"
                    },
                    {
                        "name": "Feiming Wei"
                    },
                    {
                        "name": "Dongying Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongying Li"
                },
                "author": "Dongying Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06021v1",
                "updated": "2025-04-08T13:26:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    26,
                    24,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:26:24Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    26,
                    24,
                    1,
                    98,
                    0
                ],
                "title": "Memory-Modular Classification: Learning to Generalize with Memory\n  Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Modular Classification: Learning to Generalize with Memory\n  Replacement"
                },
                "summary": "We propose a novel memory-modular learner for image classification that\nseparates knowledge memorization from reasoning. Our model enables effective\ngeneralization to new classes by simply replacing the memory contents, without\nthe need for model retraining. Unlike traditional models that encode both world\nknowledge and task-specific skills into their weights during training, our\nmodel stores knowledge in the external memory of web-crawled image and text\ndata. At inference time, the model dynamically selects relevant content from\nthe memory based on the input image, allowing it to adapt to arbitrary classes\nby simply replacing the memory contents. The key differentiator that our\nlearner meta-learns to perform classification tasks with noisy web data from\nunseen classes, resulting in robust performance across various classification\nscenarios. Experimental results demonstrate the promising performance and\nversatility of our approach in handling diverse classification tasks, including\nzero-shot/few-shot classification of unseen classes, fine-grained\nclassification, and class-incremental classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel memory-modular learner for image classification that\nseparates knowledge memorization from reasoning. Our model enables effective\ngeneralization to new classes by simply replacing the memory contents, without\nthe need for model retraining. Unlike traditional models that encode both world\nknowledge and task-specific skills into their weights during training, our\nmodel stores knowledge in the external memory of web-crawled image and text\ndata. At inference time, the model dynamically selects relevant content from\nthe memory based on the input image, allowing it to adapt to arbitrary classes\nby simply replacing the memory contents. The key differentiator that our\nlearner meta-learns to perform classification tasks with noisy web data from\nunseen classes, resulting in robust performance across various classification\nscenarios. Experimental results demonstrate the promising performance and\nversatility of our approach in handling diverse classification tasks, including\nzero-shot/few-shot classification of unseen classes, fine-grained\nclassification, and class-incremental classification."
                },
                "authors": [
                    {
                        "name": "Dahyun Kang"
                    },
                    {
                        "name": "Ahmet Iscen"
                    },
                    {
                        "name": "Eunchan Jo"
                    },
                    {
                        "name": "Sua Choi"
                    },
                    {
                        "name": "Minsu Cho"
                    },
                    {
                        "name": "Cordelia Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Cordelia Schmid"
                },
                "author": "Cordelia Schmid",
                "arxiv_comment": "Accepted to TMLR. Code available: https://github.com/dahyun-kang/mml",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06017v1",
                "updated": "2025-04-08T13:22:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    22,
                    9,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:22:09Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    22,
                    9,
                    1,
                    98,
                    0
                ],
                "title": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI"
                },
                "summary": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms."
                },
                "authors": [
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Luis Javier Navarrete-Lozano"
                    },
                    {
                        "name": "María Sanz-Gómez"
                    },
                    {
                        "name": "Lidia Salas Espejo"
                    },
                    {
                        "name": "Martiño Crespo-Álvarez"
                    },
                    {
                        "name": "Francisco Oca-Gonzalez"
                    },
                    {
                        "name": "Francesco Balassone"
                    },
                    {
                        "name": "Alfonso Glera-Picón"
                    },
                    {
                        "name": "Unai Ayucar-Carbajo"
                    },
                    {
                        "name": "Endika Gil-Uriarte"
                    }
                ],
                "author_detail": {
                    "name": "Endika Gil-Uriarte"
                },
                "author": "Endika Gil-Uriarte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06015v1",
                "updated": "2025-04-08T13:21:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    21,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:21:04Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    21,
                    4,
                    1,
                    98,
                    0
                ],
                "title": "Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights\n  into Handling Faulty GNSS Measurements in Field Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights\n  into Handling Faulty GNSS Measurements in Field Robotics"
                },
                "summary": "This paper presents research findings on handling faulty measurements (i.e.,\noutliers) of global navigation satellite systems (GNSS) for robot localization\nunder adverse signal conditions in field applications, where raw GNSS data are\nfrequently corrupted due to environmental interference such as multipath,\nsignal blockage, or non-line-of-sight conditions. In this context, we\ninvestigate three strategies applied specifically to GNSS pseudorange\nobservations: robust statistics for error mitigation, machine learning for\nfaulty measurement prediction, and Bayesian inference for noise distribution\napproximation. Since previous studies have provided limited insight into the\ntheoretical foundations and practical evaluations of these three methodologies\nwithin a unified problem statement (i.e., state estimation using ranging\nsensors), we conduct extensive experiments using real-world sensor data\ncollected in diverse urban environments. Our goal is to examine both\nestablished techniques and newly proposed methods, thereby advancing the\nunderstanding of how to handle faulty range measurements, such as GNSS, for\nrobust, long-term robot localization. In addition to presenting successful\nresults, this work highlights critical observations and open questions to\nmotivate future research in robust state estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents research findings on handling faulty measurements (i.e.,\noutliers) of global navigation satellite systems (GNSS) for robot localization\nunder adverse signal conditions in field applications, where raw GNSS data are\nfrequently corrupted due to environmental interference such as multipath,\nsignal blockage, or non-line-of-sight conditions. In this context, we\ninvestigate three strategies applied specifically to GNSS pseudorange\nobservations: robust statistics for error mitigation, machine learning for\nfaulty measurement prediction, and Bayesian inference for noise distribution\napproximation. Since previous studies have provided limited insight into the\ntheoretical foundations and practical evaluations of these three methodologies\nwithin a unified problem statement (i.e., state estimation using ranging\nsensors), we conduct extensive experiments using real-world sensor data\ncollected in diverse urban environments. Our goal is to examine both\nestablished techniques and newly proposed methods, thereby advancing the\nunderstanding of how to handle faulty range measurements, such as GNSS, for\nrobust, long-term robot localization. In addition to presenting successful\nresults, this work highlights critical observations and open questions to\nmotivate future research in robust state estimation."
                },
                "authors": [
                    {
                        "name": "Haoming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haoming Zhang"
                },
                "author": "Haoming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06011v1",
                "updated": "2025-04-08T13:16:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    16,
                    54,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:16:54Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    16,
                    54,
                    1,
                    98,
                    0
                ],
                "title": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for\n  Hindi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for\n  Hindi"
                },
                "summary": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services."
                },
                "authors": [
                    {
                        "name": "Monojit Choudhury"
                    },
                    {
                        "name": "Shivam Chauhan"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Dhruv Sahnan"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Aaryamonvikram Singh"
                    },
                    {
                        "name": "Alok Anil Jadhav"
                    },
                    {
                        "name": "Utkarsh Agarwal"
                    },
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "Debopriyo Banerjee"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Junaid Bhat"
                    },
                    {
                        "name": "Awantika Shukla"
                    },
                    {
                        "name": "Samujjwal Ghosh"
                    },
                    {
                        "name": "Samta Kamboj"
                    },
                    {
                        "name": "Onkar Pandit"
                    },
                    {
                        "name": "Lalit Pradhan"
                    },
                    {
                        "name": "Rahul Pal"
                    },
                    {
                        "name": "Sunil Sahu"
                    },
                    {
                        "name": "Soundar Doraiswamy"
                    },
                    {
                        "name": "Parvez Mullah"
                    },
                    {
                        "name": "Ali El Filali"
                    },
                    {
                        "name": "Neha Sengupta"
                    },
                    {
                        "name": "Gokul Ramakrishnan"
                    },
                    {
                        "name": "Rituraj Joshi"
                    },
                    {
                        "name": "Gurpreet Gosal"
                    },
                    {
                        "name": "Avraham Sheinin"
                    },
                    {
                        "name": "Natalia Vassilieva"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06006v1",
                "updated": "2025-04-08T13:15:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    15,
                    47,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:15:47Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    15,
                    47,
                    1,
                    98,
                    0
                ],
                "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?"
                },
                "summary": "Optimal hyperparameter selection is critical for maximizing neural network\nperformance, especially as models grow in complexity. This work investigates\nthe viability of using large language models (LLMs) for hyperparameter\noptimization by employing a fine-tuned version of Code Llama. Through\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\naccurate and efficient hyperparameter recommendations tailored to diverse\nneural network architectures. Unlike traditional methods such as Optuna, which\nrely on exhaustive trials, the proposed approach achieves competitive or\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\nreducing computational overhead. Our approach highlights that LLM-based\noptimization not only matches state-of-the-art methods like Tree-structured\nParzen Estimators but also accelerates the tuning process. This positions LLMs\nas a promising alternative to conventional optimization techniques,\nparticularly for rapid experimentation. Furthermore, the ability to generate\nhyperparameters in a single inference step makes this method particularly\nwell-suited for resource-constrained environments such as edge devices and\nmobile applications, where computational efficiency is paramount. The results\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\ncomparable stability, underscoring their value in advancing machine learning\nworkflows. All generated hyperparameters are included in the LEMUR Neural\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\nbenchmark for hyperparameter optimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal hyperparameter selection is critical for maximizing neural network\nperformance, especially as models grow in complexity. This work investigates\nthe viability of using large language models (LLMs) for hyperparameter\noptimization by employing a fine-tuned version of Code Llama. Through\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\naccurate and efficient hyperparameter recommendations tailored to diverse\nneural network architectures. Unlike traditional methods such as Optuna, which\nrely on exhaustive trials, the proposed approach achieves competitive or\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\nreducing computational overhead. Our approach highlights that LLM-based\noptimization not only matches state-of-the-art methods like Tree-structured\nParzen Estimators but also accelerates the tuning process. This positions LLMs\nas a promising alternative to conventional optimization techniques,\nparticularly for rapid experimentation. Furthermore, the ability to generate\nhyperparameters in a single inference step makes this method particularly\nwell-suited for resource-constrained environments such as edge devices and\nmobile applications, where computational efficiency is paramount. The results\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\ncomparable stability, underscoring their value in advancing machine learning\nworkflows. All generated hyperparameters are included in the LEMUR Neural\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\nbenchmark for hyperparameter optimization research."
                },
                "authors": [
                    {
                        "name": "Roman Kochnev"
                    },
                    {
                        "name": "Arash Torabi Goodarzi"
                    },
                    {
                        "name": "Zofia Antonina Bentyn"
                    },
                    {
                        "name": "Dmitry Ignatov"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06001v1",
                "updated": "2025-04-08T13:09:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    9,
                    56,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:09:56Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    9,
                    56,
                    1,
                    98,
                    0
                ],
                "title": "Optical and X-ray timing analysis of the 2018-2020 outburst and\n  rebrightening of the black-hole transient MAXI J1820+070",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical and X-ray timing analysis of the 2018-2020 outburst and\n  rebrightening of the black-hole transient MAXI J1820+070"
                },
                "summary": "We report the results of a comprehensive analysis of the multiwavelength (in\noptical and X-rays) and multitimescale (from months to tenths of a second)\nvariability of the 2018-2020 outburst of the black hole transient MAXI\nJ1820+070. During the first outburst episode, a detailed analysis of the\noptical photometry shows a periodicity that evolves over time and stabilises at\na frequency of $1.4517(1)$ $1/d$ ($\\sim0.5\\%$ longer than the orbital period).\nThis super-orbital modulation is also seen in the X-rays for a few days soon\nafter the transition to the high-soft state. We also observed optical\nQuasi-Periodic Oscillations (QPOs), which correspond to some of the QPOs\nobserved in X-rays at three different epochs when the source was in the\nlow-hard state. In two epochs, optical QPOs with a centroid consistent with\nhalf the frequency of the most prominent X-ray QPO can be seen. If the lowest\nmodulation frequency is the one observed in the optical, the characteristic\nprecession frequency of MAXI J1820+070 is lower than that inferred from the\n`fundamental' QPO in the X-rays. Assuming that QPOs can be generated by\nLense-Thirring precession, we calculate the spin of the black hole in the case\nwhere the fundamental precession frequency is tracked by the optical emission.\nWe find a relatively slowly spinning black hole with a spin parameter $\\lesssim\n0.15$. The super-orbital optical and X-ray modulations observed after the\ndisappearance of the QPOs may be triggered by the self-irradiation of the outer\ndisc by a standard inner disc truncated at a few gravitational radii.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the results of a comprehensive analysis of the multiwavelength (in\noptical and X-rays) and multitimescale (from months to tenths of a second)\nvariability of the 2018-2020 outburst of the black hole transient MAXI\nJ1820+070. During the first outburst episode, a detailed analysis of the\noptical photometry shows a periodicity that evolves over time and stabilises at\na frequency of $1.4517(1)$ $1/d$ ($\\sim0.5\\%$ longer than the orbital period).\nThis super-orbital modulation is also seen in the X-rays for a few days soon\nafter the transition to the high-soft state. We also observed optical\nQuasi-Periodic Oscillations (QPOs), which correspond to some of the QPOs\nobserved in X-rays at three different epochs when the source was in the\nlow-hard state. In two epochs, optical QPOs with a centroid consistent with\nhalf the frequency of the most prominent X-ray QPO can be seen. If the lowest\nmodulation frequency is the one observed in the optical, the characteristic\nprecession frequency of MAXI J1820+070 is lower than that inferred from the\n`fundamental' QPO in the X-rays. Assuming that QPOs can be generated by\nLense-Thirring precession, we calculate the spin of the black hole in the case\nwhere the fundamental precession frequency is tracked by the optical emission.\nWe find a relatively slowly spinning black hole with a spin parameter $\\lesssim\n0.15$. The super-orbital optical and X-ray modulations observed after the\ndisappearance of the QPOs may be triggered by the self-irradiation of the outer\ndisc by a standard inner disc truncated at a few gravitational radii."
                },
                "authors": [
                    {
                        "name": "M. Fiori"
                    },
                    {
                        "name": "L. Zampieri"
                    },
                    {
                        "name": "A. Burtovoi"
                    },
                    {
                        "name": "G. Naletto"
                    },
                    {
                        "name": "P. Ochner"
                    },
                    {
                        "name": "U. Munari"
                    },
                    {
                        "name": "F. Manzini"
                    },
                    {
                        "name": "A. Vagnozzi"
                    },
                    {
                        "name": "E. A. Barsukova"
                    },
                    {
                        "name": "M. A. Burlak"
                    },
                    {
                        "name": "V. P. Goranski"
                    },
                    {
                        "name": "N. P. Ikonnikova"
                    },
                    {
                        "name": "N. A. Katysheva"
                    },
                    {
                        "name": "E. G. Sheyanov"
                    },
                    {
                        "name": "S. Yu. Shugarov"
                    },
                    {
                        "name": "A. V. Zharova"
                    },
                    {
                        "name": "A. M. Zubareva"
                    },
                    {
                        "name": "S. E. Motta"
                    }
                ],
                "author_detail": {
                    "name": "S. E. Motta"
                },
                "author": "S. E. Motta",
                "arxiv_comment": "18 pages, 18 figures, 6 tables. Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05995v1",
                "updated": "2025-04-08T13:01:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    1,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:01:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    1,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge"
                },
                "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework)."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Sahinur Rahman Laskar"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shammur Absar Chowdhury"
                },
                "author": "Shammur Absar Chowdhury",
                "arxiv_comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05989v1",
                "updated": "2025-04-08T12:51:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    51,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T12:51:49Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    51,
                    49,
                    1,
                    98,
                    0
                ],
                "title": "Comparative Analysis of Classical and Quantum-Inspired Solvers: A\n  Preliminary Study on the Weighted Max-Cut Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Classical and Quantum-Inspired Solvers: A\n  Preliminary Study on the Weighted Max-Cut Problem"
                },
                "summary": "Combinatorial optimization is essential across numerous disciplines.\nTraditional metaheuristics excel at exploring complex solution spaces\nefficiently, yet they often struggle with scalability. Deep learning has become\na viable alternative for quickly generating high-quality solutions,\nparticularly when metaheuristics underperform. In recent years,\nquantum-inspired approaches such as tensor networks have shown promise in\naddressing these challenges. Despite these advancements, a thorough comparison\nof the different paradigms is missing. This study evaluates eight algorithms on\nWeighted Max-Cut graphs ranging from 10 to 250 nodes. Specifically, we compare\na Genetic Algorithm representing metaheuristics, a Graph Neural Network for\ndeep learning, and the Density Matrix Renormalization Group as a tensor network\napproach. Our analysis focuses on solution quality and computational efficiency\n(i.e., time and memory usage). Numerical results show that the Genetic\nAlgorithm achieves near-optimal results for small graphs, although its\ncomputation time grows significantly with problem size. The Graph Neural\nNetwork offers a balanced solution for medium-sized instances with low memory\ndemands and rapid inference, yet it exhibits more significant variability on\nlarger graphs. Meanwhile, the Tensor Network approach consistently yields high\napproximation ratios and efficient execution on larger graphs, albeit with\nincreased memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial optimization is essential across numerous disciplines.\nTraditional metaheuristics excel at exploring complex solution spaces\nefficiently, yet they often struggle with scalability. Deep learning has become\na viable alternative for quickly generating high-quality solutions,\nparticularly when metaheuristics underperform. In recent years,\nquantum-inspired approaches such as tensor networks have shown promise in\naddressing these challenges. Despite these advancements, a thorough comparison\nof the different paradigms is missing. This study evaluates eight algorithms on\nWeighted Max-Cut graphs ranging from 10 to 250 nodes. Specifically, we compare\na Genetic Algorithm representing metaheuristics, a Graph Neural Network for\ndeep learning, and the Density Matrix Renormalization Group as a tensor network\napproach. Our analysis focuses on solution quality and computational efficiency\n(i.e., time and memory usage). Numerical results show that the Genetic\nAlgorithm achieves near-optimal results for small graphs, although its\ncomputation time grows significantly with problem size. The Graph Neural\nNetwork offers a balanced solution for medium-sized instances with low memory\ndemands and rapid inference, yet it exhibits more significant variability on\nlarger graphs. Meanwhile, the Tensor Network approach consistently yields high\napproximation ratios and efficient execution on larger graphs, albeit with\nincreased memory consumption."
                },
                "authors": [
                    {
                        "name": "Aitor Morais"
                    },
                    {
                        "name": "Eneko Osaba"
                    },
                    {
                        "name": "Iker Pastor"
                    },
                    {
                        "name": "Izaskun Oregui"
                    }
                ],
                "author_detail": {
                    "name": "Izaskun Oregui"
                },
                "author": "Izaskun Oregui",
                "arxiv_comment": "9 pages, 3 figures, 4 tables, paper Submitted to GECCO '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00597v2",
                "updated": "2025-04-08T12:40:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    40,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-01T09:55:23Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    55,
                    23,
                    1,
                    91,
                    0
                ],
                "title": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from out-language passages, but a much weaker ability to formulate\na full answer in the correct language. Our analysis, based on both accuracy and\nfeature attribution techniques, further shows that distracting passages\nnegatively impact answer quality regardless of their language. However,\ndistractors in the query language exert a slightly stronger influence. Taken\ntogether, our findings deepen the understanding of how LLMs utilize context in\nmRAG systems, providing directions for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from out-language passages, but a much weaker ability to formulate\na full answer in the correct language. Our analysis, based on both accuracy and\nfeature attribution techniques, further shows that distracting passages\nnegatively impact answer quality regardless of their language. However,\ndistractors in the query language exert a slightly stronger influence. Taken\ntogether, our findings deepen the understanding of how LLMs utilize context in\nmRAG systems, providing directions for future improvements."
                },
                "authors": [
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "Under review at COLM2025. All codes and data are released at\n  https://github.com/Betswish/mRAG-Context-Consistency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05982v1",
                "updated": "2025-04-08T12:39:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    39,
                    14,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T12:39:14Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    39,
                    14,
                    1,
                    98,
                    0
                ],
                "title": "Have you tried turning it off and on again? Stochastic resetting for\n  enhanced sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Have you tried turning it off and on again? Stochastic resetting for\n  enhanced sampling"
                },
                "summary": "Molecular dynamics simulations are widely used across chemistry, physics, and\nbiology, providing quantitative insight into complex processes with atomic\ndetail. However, their limited timescale of a few microseconds is a significant\nobstacle in describing phenomena such as conformational transitions of\nbiomolecules and polymorphism in molecular crystals. Recently, stochastic\nresetting, i.e., randomly stopping and restarting the simulations, emerged as a\npowerful enhanced sampling approach, which is collective variable-free, highly\nparallelized, and easily implemented in existing molecular dynamics codes.\nResetting expedites sampling rare events while enabling the inference of\nkinetic observables of the underlying process. It can be employed as a\nstandalone tool or in combination with other enhanced sampling methods, such as\nMetadynamics, with each technique compensating for the drawbacks of the other.\nHere, we comprehensively describe resetting and its theoretical background,\nreview recent developments in stochastic resetting for enhanced sampling, and\nprovide instructive guidelines for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular dynamics simulations are widely used across chemistry, physics, and\nbiology, providing quantitative insight into complex processes with atomic\ndetail. However, their limited timescale of a few microseconds is a significant\nobstacle in describing phenomena such as conformational transitions of\nbiomolecules and polymorphism in molecular crystals. Recently, stochastic\nresetting, i.e., randomly stopping and restarting the simulations, emerged as a\npowerful enhanced sampling approach, which is collective variable-free, highly\nparallelized, and easily implemented in existing molecular dynamics codes.\nResetting expedites sampling rare events while enabling the inference of\nkinetic observables of the underlying process. It can be employed as a\nstandalone tool or in combination with other enhanced sampling methods, such as\nMetadynamics, with each technique compensating for the drawbacks of the other.\nHere, we comprehensively describe resetting and its theoretical background,\nreview recent developments in stochastic resetting for enhanced sampling, and\nprovide instructive guidelines for practitioners."
                },
                "authors": [
                    {
                        "name": "Ofir Blumer"
                    },
                    {
                        "name": "Barak Hirshberg"
                    }
                ],
                "author_detail": {
                    "name": "Barak Hirshberg"
                },
                "author": "Barak Hirshberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23803v2",
                "updated": "2025-04-08T12:36:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    36,
                    8,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-31T07:31:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    31,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via\n  Scaling Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via\n  Scaling Test-Time Compute"
                },
                "summary": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner"
                },
                "authors": [
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Binhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Binhua Li"
                },
                "author": "Binhua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05946v1",
                "updated": "2025-04-08T11:59:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T11:59:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control"
                },
                "summary": "Model Predictive Control~(MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose \\IMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model~(LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution~(L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, \\IMPC enables dynamic human-LLM interaction and\nfine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization~(DPO) using a tailored loss function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Control~(MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose \\IMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model~(LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution~(L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, \\IMPC enables dynamic human-LLM interaction and\nfine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization~(DPO) using a tailored loss function."
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08574v2",
                "updated": "2025-04-08T11:57:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    57,
                    2,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-13T12:44:41Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    12,
                    44,
                    41,
                    2,
                    318,
                    0
                ],
                "title": "Themes of Building LLM-based Applications for Production: A\n  Practitioner's View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Themes of Building LLM-based Applications for Production: A\n  Practitioner's View"
                },
                "summary": "Background: Large language models (LLMs) have become a paramount interest of\nresearchers and practitioners alike, yet a comprehensive overview of key\nconsiderations for those developing LLM-based systems is lacking. This study\naddresses this gap by collecting and mapping the topics practitioners discuss\nonline, offering practical insights into where priorities lie in developing\nLLM-based applications. Method: We collected 189 videos from 2022 to 2024 from\npractitioners actively developing such systems and discussing various aspects\nthey encounter during development and deployment of LLMs in production. We\nanalyzed the transcripts using BERTopic, then manually sorted and merged the\ngenerated topics into themes, leading to a total of 20 topics in 8 themes.\nResults: The most prevalent topics fall within the theme Design & Architecture,\nwith a strong focus on retrieval-augmented generation (RAG) systems. Other\nfrequently discussed topics include model capabilities and enhancement\ntechniques (e.g., fine-tuning, prompt engineering), infrastructure and tooling,\nand risks and ethical challenges. Implications: Our results highlight current\ndiscussions and challenges in deploying LLMs in production. This way, we\nprovide a systematic overview of key aspects practitioners should be aware of\nwhen developing LLM-based applications. We further pale off topics of interest\nfor academics where further research is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) have become a paramount interest of\nresearchers and practitioners alike, yet a comprehensive overview of key\nconsiderations for those developing LLM-based systems is lacking. This study\naddresses this gap by collecting and mapping the topics practitioners discuss\nonline, offering practical insights into where priorities lie in developing\nLLM-based applications. Method: We collected 189 videos from 2022 to 2024 from\npractitioners actively developing such systems and discussing various aspects\nthey encounter during development and deployment of LLMs in production. We\nanalyzed the transcripts using BERTopic, then manually sorted and merged the\ngenerated topics into themes, leading to a total of 20 topics in 8 themes.\nResults: The most prevalent topics fall within the theme Design & Architecture,\nwith a strong focus on retrieval-augmented generation (RAG) systems. Other\nfrequently discussed topics include model capabilities and enhancement\ntechniques (e.g., fine-tuning, prompt engineering), infrastructure and tooling,\nand risks and ethical challenges. Implications: Our results highlight current\ndiscussions and challenges in deploying LLMs in production. This way, we\nprovide a systematic overview of key aspects practitioners should be aware of\nwhen developing LLM-based applications. We further pale off topics of interest\nfor academics where further research is needed."
                },
                "authors": [
                    {
                        "name": "Alina Mailach"
                    },
                    {
                        "name": "Sebastian Simon"
                    },
                    {
                        "name": "Johannes Dorn"
                    },
                    {
                        "name": "Norbert Siegmund"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Siegmund"
                },
                "author": "Norbert Siegmund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01759v2",
                "updated": "2025-04-08T11:42:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    42,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2023-11-03T07:34:47Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    7,
                    34,
                    47,
                    4,
                    307,
                    0
                ],
                "title": "TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices"
                },
                "summary": "Developing deep learning models on tiny devices (e.g. Microcontroller units,\nMCUs) has attracted much attention in various embedded IoT applications.\nHowever, it is challenging to efficiently design and deploy recent advanced\nmodels (e.g. transformers) on tiny devices due to their severe hardware\nresource constraints. In this work, we propose TinyFormer, a framework\nspecifically designed to develop and deploy resource-efficient transformers on\nMCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine.\nSeparately, SuperNAS aims to search for an appropriate supernet from a vast\nsearch space. SparseNAS evaluates the best sparse single-path model including\ntransformer architecture from the identified supernet. Finally, SparseEngine\nefficiently deploys the searched sparse models onto MCUs. To the best of our\nknowledge, SparseEngine is the first deployment framework capable of performing\ninference of sparse models with transformer on MCUs. Evaluation results on the\nCIFAR-10 dataset demonstrate that TinyFormer can develop efficient transformers\nwith an accuracy of 96.1% while adhering to hardware constraints of 1MB storage\nand $320$KB memory. Additionally, TinyFormer achieves significant speedups in\nsparse inference, up to 12.2x, when compared to the CMSIS-NN library.\nTinyFormer is believed to bring powerful transformers into TinyML scenarios and\ngreatly expand the scope of deep learning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing deep learning models on tiny devices (e.g. Microcontroller units,\nMCUs) has attracted much attention in various embedded IoT applications.\nHowever, it is challenging to efficiently design and deploy recent advanced\nmodels (e.g. transformers) on tiny devices due to their severe hardware\nresource constraints. In this work, we propose TinyFormer, a framework\nspecifically designed to develop and deploy resource-efficient transformers on\nMCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine.\nSeparately, SuperNAS aims to search for an appropriate supernet from a vast\nsearch space. SparseNAS evaluates the best sparse single-path model including\ntransformer architecture from the identified supernet. Finally, SparseEngine\nefficiently deploys the searched sparse models onto MCUs. To the best of our\nknowledge, SparseEngine is the first deployment framework capable of performing\ninference of sparse models with transformer on MCUs. Evaluation results on the\nCIFAR-10 dataset demonstrate that TinyFormer can develop efficient transformers\nwith an accuracy of 96.1% while adhering to hardware constraints of 1MB storage\nand $320$KB memory. Additionally, TinyFormer achieves significant speedups in\nsparse inference, up to 12.2x, when compared to the CMSIS-NN library.\nTinyFormer is believed to bring powerful transformers into TinyML scenarios and\ngreatly expand the scope of deep learning applications."
                },
                "authors": [
                    {
                        "name": "Jianlei Yang"
                    },
                    {
                        "name": "Jiacheng Liao"
                    },
                    {
                        "name": "Fanding Lei"
                    },
                    {
                        "name": "Meichen Liu"
                    },
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Han Wan"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Weisheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weisheng Zhao"
                },
                "author": "Weisheng Zhao",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13453v2",
                "updated": "2025-04-08T11:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    5,
                    1,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-17T11:26:10Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    11,
                    26,
                    10,
                    3,
                    291,
                    0
                ],
                "title": "Adaptive Augmentation Policy Optimization with LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Augmentation Policy Optimization with LLM Feedback"
                },
                "summary": "Data augmentation is a critical component of deep learning pipelines,\nenhancing model generalization by increasing dataset diversity. Traditional\naugmentation strategies rely on manually designed transformations, stochastic\nsampling, or automated search-based approaches. Although automated methods\nimprove performance, they often require extensive computational resources and\nare tailored to specific datasets. In this work, we propose a Large Language\nModel (LLM)-guided augmentation optimization strategy that refines augmentation\npolicies based on model performance feedback. We introduce two approaches: (1)\nLLM-Guided Augmentation Policy Optimization, where augmentation policies are\nselected by an LLM prior to training and iteratively refined across multiple\ntraining cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization,\nwhere policies adapt in real-time based on performance metrics. This\nin-training approach eliminates the need for full model retraining before\nreceiving LLM feedback, thereby reducing computational costs while improving\nperformance. Our methodology employs an LLM to dynamically select augmentation\ntransformations based on dataset characteristics, model architecture, and prior\ntraining outcomes. Unlike traditional search-based methods, our approach\nleverages the contextual knowledge of LLMs, particularly in specialized domains\nlike medical imaging, to recommend augmentation strategies tailored to\ndomain-specific data. We evaluate our approach on multiple domain-specific\nimage classification datasets where augmentation is key to model robustness.\nResults show that LLM-guided augmentation optimization outperforms traditional\nmethods, improving model accuracy. These findings highlight the potential of\nLLMs in automating and adapting deep learning training workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is a critical component of deep learning pipelines,\nenhancing model generalization by increasing dataset diversity. Traditional\naugmentation strategies rely on manually designed transformations, stochastic\nsampling, or automated search-based approaches. Although automated methods\nimprove performance, they often require extensive computational resources and\nare tailored to specific datasets. In this work, we propose a Large Language\nModel (LLM)-guided augmentation optimization strategy that refines augmentation\npolicies based on model performance feedback. We introduce two approaches: (1)\nLLM-Guided Augmentation Policy Optimization, where augmentation policies are\nselected by an LLM prior to training and iteratively refined across multiple\ntraining cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization,\nwhere policies adapt in real-time based on performance metrics. This\nin-training approach eliminates the need for full model retraining before\nreceiving LLM feedback, thereby reducing computational costs while improving\nperformance. Our methodology employs an LLM to dynamically select augmentation\ntransformations based on dataset characteristics, model architecture, and prior\ntraining outcomes. Unlike traditional search-based methods, our approach\nleverages the contextual knowledge of LLMs, particularly in specialized domains\nlike medical imaging, to recommend augmentation strategies tailored to\ndomain-specific data. We evaluate our approach on multiple domain-specific\nimage classification datasets where augmentation is key to model robustness.\nResults show that LLM-guided augmentation optimization outperforms traditional\nmethods, improving model accuracy. These findings highlight the potential of\nLLMs in automating and adapting deep learning training workflows."
                },
                "authors": [
                    {
                        "name": "Ant Duru"
                    },
                    {
                        "name": "Alptekin Temizel"
                    }
                ],
                "author_detail": {
                    "name": "Alptekin Temizel"
                },
                "author": "Alptekin Temizel",
                "arxiv_comment": "15 pages, 4 tables, 3 figures submitted for consideration to 2025\n  Medical Image Understanding and Analysis Conference (MIUA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15313v2",
                "updated": "2025-04-08T11:04:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    4,
                    33,
                    1,
                    98,
                    0
                ],
                "published": "2024-08-27T17:31:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nsupervised optimization, a labeling function is used to capture the global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark that includes comprehensive discriminative and\ngenerative tasks for helpfulness and harmlessness. The results indicate that\nour method significantly outperforms existing approaches in both safety and\nhelpfulness. Moreover, BFPO achieves the same level of safety as methods that\nheavily rely on human labor with less than 10\\% of the computational resources\nand human prompting and annotation process. The training recipes can be found\nhere: https://github.com/wx-zhang/bfpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nsupervised optimization, a labeling function is used to capture the global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark that includes comprehensive discriminative and\ngenerative tasks for helpfulness and harmlessness. The results indicate that\nour method significantly outperforms existing approaches in both safety and\nhelpfulness. Moreover, BFPO achieves the same level of safety as methods that\nheavily rely on human labor with less than 10\\% of the computational resources\nand human prompting and annotation process. The training recipes can be found\nhere: https://github.com/wx-zhang/bfpo."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "arxiv_comment": "The paper has been accepted in ICLR 2025 as spotlight presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10666v2",
                "updated": "2025-04-08T10:56:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    56,
                    7,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-09T19:49:31Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    19,
                    49,
                    31,
                    6,
                    68,
                    0
                ],
                "title": "Green Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Prompting"
                },
                "summary": "Large Language Models (LLMs) have become widely used across various domains\nspanning search engines, code generation, and text creation. However, a major\nconcern associated with their adoption is the high cost of inference, impacting\nboth their sustainability and financial feasibility. In this study, we\nempirically study how different prompt and response characteristics directly\nimpact LLM inference energy cost. We conduct experiments leveraging three\nopen-source transformer-based LLMs across three task types$-$question\nanswering, sentiment analysis, and text generation. For each inference, we\nanalyzed prompt and response characteristics (length, semantic meaning, time\ntaken, energy consumption). Our results demonstrate that even when presented\nwith identical tasks, models generate responses with varying characteristics\nand subsequently exhibit distinct energy consumption patterns. We found that\nprompt length is less significant than the semantic meaning of the task itself.\nIn addition, we identified specific keywords associated with higher or lower\nenergy usage that vary between associated tasks. These findings highlight the\nimportance of prompt design in optimizing inference efficiency. We conclude\nthat the semantic meaning of prompts and certain task-related keywords\nsignificantly impact inference costs, leading the way for deeper exploration\ntowards creating energy-adaptive LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become widely used across various domains\nspanning search engines, code generation, and text creation. However, a major\nconcern associated with their adoption is the high cost of inference, impacting\nboth their sustainability and financial feasibility. In this study, we\nempirically study how different prompt and response characteristics directly\nimpact LLM inference energy cost. We conduct experiments leveraging three\nopen-source transformer-based LLMs across three task types$-$question\nanswering, sentiment analysis, and text generation. For each inference, we\nanalyzed prompt and response characteristics (length, semantic meaning, time\ntaken, energy consumption). Our results demonstrate that even when presented\nwith identical tasks, models generate responses with varying characteristics\nand subsequently exhibit distinct energy consumption patterns. We found that\nprompt length is less significant than the semantic meaning of the task itself.\nIn addition, we identified specific keywords associated with higher or lower\nenergy usage that vary between associated tasks. These findings highlight the\nimportance of prompt design in optimizing inference efficiency. We conclude\nthat the semantic meaning of prompts and certain task-related keywords\nsignificantly impact inference costs, leading the way for deeper exploration\ntowards creating energy-adaptive LLMs."
                },
                "authors": [
                    {
                        "name": "Marta Adamska"
                    },
                    {
                        "name": "Daria Smirnova"
                    },
                    {
                        "name": "Hamid Nasiri"
                    },
                    {
                        "name": "Zhengxin Yu"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05898v1",
                "updated": "2025-04-08T10:49:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    49,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:49:45Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    49,
                    45,
                    1,
                    98,
                    0
                ],
                "title": "Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and\n  Human Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and\n  Human Evaluation"
                },
                "summary": "Large language models show promising results in various NLP tasks. Despite\nthese successes, the robustness and consistency of LLMs in underrepresented\nlanguages remain largely unexplored, especially concerning local dialects.\nExisting benchmarks also focus on main dialects, neglecting LLMs' ability on\nlocal dialect texts. In this paper, we introduce a Thai local dialect benchmark\ncovering Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai,\nevaluating LLMs on five NLP tasks: summarization, question answering,\ntranslation, conversation, and food-related tasks. Furthermore, we propose a\nhuman evaluation guideline and metric for Thai local dialects to assess\ngeneration fluency and dialect-specific accuracy. Results show that LLM\nperformance declines significantly in local Thai dialects compared to standard\nThai, with only proprietary models like GPT-4o and Gemini2 demonstrating some\nfluency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models show promising results in various NLP tasks. Despite\nthese successes, the robustness and consistency of LLMs in underrepresented\nlanguages remain largely unexplored, especially concerning local dialects.\nExisting benchmarks also focus on main dialects, neglecting LLMs' ability on\nlocal dialect texts. In this paper, we introduce a Thai local dialect benchmark\ncovering Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai,\nevaluating LLMs on five NLP tasks: summarization, question answering,\ntranslation, conversation, and food-related tasks. Furthermore, we propose a\nhuman evaluation guideline and metric for Thai local dialects to assess\ngeneration fluency and dialect-specific accuracy. Results show that LLM\nperformance declines significantly in local Thai dialects compared to standard\nThai, with only proprietary models like GPT-4o and Gemini2 demonstrating some\nfluency"
                },
                "authors": [
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Kanruethai Masuk"
                    },
                    {
                        "name": "Surapon Nonesung"
                    },
                    {
                        "name": "Chalermpun Mai-On"
                    },
                    {
                        "name": "Sarana Nutanong"
                    },
                    {
                        "name": "Wuttikorn Ponwitayarat"
                    },
                    {
                        "name": "Potsawee Manakul"
                    }
                ],
                "author_detail": {
                    "name": "Potsawee Manakul"
                },
                "author": "Potsawee Manakul",
                "arxiv_comment": "Datasets and codes are available at\n  https://github.com/mrpeerat/Thai_local_benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08424v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08424v3",
                "updated": "2025-04-08T10:48:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    48,
                    19,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-12T12:15:14Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    12,
                    15,
                    14,
                    4,
                    103,
                    0
                ],
                "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task"
                },
                "summary": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot. Video:\nhttps://youtu.be/tBJHfAuzohI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot. Video:\nhttps://youtu.be/tBJHfAuzohI"
                },
                "authors": [
                    {
                        "name": "Hassan Ali"
                    },
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_doi": "10.1007/978-981-96-3525-2_25",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-3525-2_25",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.08424v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08424v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 16th International Conference on\n  Social Robotics (ICSR) 2024,15 pages,5 figures,2 tables; work was co-funded\n  by Horizon Europe project TERAIS under Grant agreement number 101079338",
                "arxiv_journal_ref": "In: Palinko, O., et al. Social Robotics. ICSR + AI 2024. vol\n  15563. Springer (2025)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13717v2",
                "updated": "2025-04-08T10:43:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    43,
                    0,
                    1,
                    98,
                    0
                ],
                "published": "2024-09-07T18:47:38Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    18,
                    47,
                    38,
                    5,
                    251,
                    0
                ],
                "title": "DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level\n  Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level\n  Relation Extraction"
                },
                "summary": "The remarkable capabilities of Large Language Models (LLMs) in text\ncomprehension and generation have revolutionized Information Extraction (IE).\nOne such advancement is in Document-level Relation Triplet Extraction (DocRTE),\na critical task in information systems that aims to extract entities and their\nsemantic relationships from documents. However, existing methods are primarily\ndesigned for Sentence level Relation Triplet Extraction (SentRTE), which\ntypically handles a limited set of relations and triplet facts within a single\nsentence. Additionally, some approaches treat relations as candidate choices\nintegrated into prompt templates, resulting in inefficient processing and\nsuboptimal performance when determining the relation elements in triplets. To\naddress these limitations, we introduce a Discriminative and Voice Aware\nParadigm DiVA. DiVA involves only two steps: performing document-level relation\nextraction (DocRE) and then identifying the subject object entities based on\nthe relation. No additional processing is required simply input the document to\ndirectly obtain the triplets. This streamlined process more accurately reflects\nreal-world scenarios for triplet extraction. Our innovation lies in\ntransforming DocRE into a discriminative task, where the model pays attention\nto each relation and to the often overlooked issue of active vs. passive voice\nwithin the triplet. Our experiments on the Re-DocRED and DocRED datasets\ndemonstrate state-of-the-art results for the DocRTE task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable capabilities of Large Language Models (LLMs) in text\ncomprehension and generation have revolutionized Information Extraction (IE).\nOne such advancement is in Document-level Relation Triplet Extraction (DocRTE),\na critical task in information systems that aims to extract entities and their\nsemantic relationships from documents. However, existing methods are primarily\ndesigned for Sentence level Relation Triplet Extraction (SentRTE), which\ntypically handles a limited set of relations and triplet facts within a single\nsentence. Additionally, some approaches treat relations as candidate choices\nintegrated into prompt templates, resulting in inefficient processing and\nsuboptimal performance when determining the relation elements in triplets. To\naddress these limitations, we introduce a Discriminative and Voice Aware\nParadigm DiVA. DiVA involves only two steps: performing document-level relation\nextraction (DocRE) and then identifying the subject object entities based on\nthe relation. No additional processing is required simply input the document to\ndirectly obtain the triplets. This streamlined process more accurately reflects\nreal-world scenarios for triplet extraction. Our innovation lies in\ntransforming DocRE into a discriminative task, where the model pays attention\nto each relation and to the often overlooked issue of active vs. passive voice\nwithin the triplet. Our experiments on the Re-DocRED and DocRED datasets\ndemonstrate state-of-the-art results for the DocRTE task."
                },
                "authors": [
                    {
                        "name": "Yiheng Wu"
                    },
                    {
                        "name": "Roman Yangarber"
                    },
                    {
                        "name": "Xian Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xian Mao"
                },
                "author": "Xian Mao",
                "arxiv_comment": "After internal discussions among the co-authors, we have decided to\n  withdraw the manuscript due to a change in research direction and a lack of\n  unanimous agreement to proceed with publication at this time",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06252v2",
                "updated": "2025-04-08T10:32:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    32,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-10T08:33:47Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    33,
                    47,
                    0,
                    41,
                    0
                ],
                "title": "CliniQ: A Multi-faceted Benchmark for Electronic Health Record Retrieval\n  with Semantic Match Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CliniQ: A Multi-faceted Benchmark for Electronic Health Record Retrieval\n  with Semantic Match Assessment"
                },
                "summary": "Electronic Health Record (EHR) retrieval plays a pivotal role in various\nclinical tasks, but its development has been severely impeded by the lack of\npublicly available benchmarks. In this paper, we introduce a novel public EHR\nretrieval benchmark, CliniQ, to address this gap. We consider two retrieval\nsettings: Single-Patient Retrieval and Multi-Patient Retrieval, reflecting\nvarious real-world scenarios. Single-Patient Retrieval focuses on finding\nrelevant parts within a patient note, while Multi-Patient Retrieval involves\nretrieving EHRs from multiple patients. We build our benchmark upon 1,000\ndischarge summary notes along with the ICD codes and prescription labels from\nMIMIC-III, and collect 1,246 unique queries with 77,206 relevance judgments by\nfurther leveraging powerful LLMs as annotators. Additionally, we include a\nnovel assessment of the semantic gap issue in EHR retrieval by categorizing\nmatching types into string match and four types of semantic matches. On our\nproposed benchmark, we conduct a comprehensive evaluation of various retrieval\nmethods, ranging from conventional exact match to popular dense retrievers. Our\nexperiments find that BM25 sets a strong baseline and performs competitively to\nthe dense retrievers, and general domain dense retrievers surprisingly\noutperform those designed for the medical domain. In-depth analyses on various\nmatching types reveal the strengths and drawbacks of different methods,\nenlightening the potential for targeted improvement. We believe that our\nbenchmark will stimulate the research communities to advance EHR retrieval\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Record (EHR) retrieval plays a pivotal role in various\nclinical tasks, but its development has been severely impeded by the lack of\npublicly available benchmarks. In this paper, we introduce a novel public EHR\nretrieval benchmark, CliniQ, to address this gap. We consider two retrieval\nsettings: Single-Patient Retrieval and Multi-Patient Retrieval, reflecting\nvarious real-world scenarios. Single-Patient Retrieval focuses on finding\nrelevant parts within a patient note, while Multi-Patient Retrieval involves\nretrieving EHRs from multiple patients. We build our benchmark upon 1,000\ndischarge summary notes along with the ICD codes and prescription labels from\nMIMIC-III, and collect 1,246 unique queries with 77,206 relevance judgments by\nfurther leveraging powerful LLMs as annotators. Additionally, we include a\nnovel assessment of the semantic gap issue in EHR retrieval by categorizing\nmatching types into string match and four types of semantic matches. On our\nproposed benchmark, we conduct a comprehensive evaluation of various retrieval\nmethods, ranging from conventional exact match to popular dense retrievers. Our\nexperiments find that BM25 sets a strong baseline and performs competitively to\nthe dense retrievers, and general domain dense retrievers surprisingly\noutperform those designed for the medical domain. In-depth analyses on various\nmatching types reveal the strengths and drawbacks of different methods,\nenlightening the potential for targeted improvement. We believe that our\nbenchmark will stimulate the research communities to advance EHR retrieval\nsystems."
                },
                "authors": [
                    {
                        "name": "Zhengyun Zhao"
                    },
                    {
                        "name": "Hongyi Yuan"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Haichao Chen"
                    },
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Songchi Zhou"
                    },
                    {
                        "name": "Yue Zhong"
                    },
                    {
                        "name": "Sheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Yu"
                },
                "author": "Sheng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15429v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15429v4",
                "updated": "2025-04-08T10:27:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    27,
                    59,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-21T12:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations"
                },
                "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Shuojie Fu"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Cemre Zor"
                    },
                    {
                        "name": "Guy Martin"
                    },
                    {
                        "name": "James Kinross"
                    },
                    {
                        "name": "Uddhav Vaghela"
                    },
                    {
                        "name": "Ovidiu Serban"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "long paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15429v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15429v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13212v2",
                "updated": "2025-04-08T10:06:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    6,
                    2,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-20T11:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "title": "Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation"
                },
                "summary": "Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment."
                },
                "authors": [
                    {
                        "name": "David Otero"
                    },
                    {
                        "name": "Javier Parapar"
                    },
                    {
                        "name": "Álvaro Barreiro"
                    }
                ],
                "author_detail": {
                    "name": "Álvaro Barreiro"
                },
                "author": "Álvaro Barreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05871v1",
                "updated": "2025-04-08T09:54:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    54,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:54:49Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    54,
                    49,
                    1,
                    98,
                    0
                ],
                "title": "Agent Guide: A Simple Agent Behavioral Watermarking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Guide: A Simple Agent Behavioral Watermarking Framework"
                },
                "summary": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems."
                },
                "authors": [
                    {
                        "name": "Kaibo Huang"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05866v1",
                "updated": "2025-04-08T09:47:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    47,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:47:15Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    47,
                    15,
                    1,
                    98,
                    0
                ],
                "title": "CTI-HAL: A Human-Annotated Dataset for Cyber Threat Intelligence\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTI-HAL: A Human-Annotated Dataset for Cyber Threat Intelligence\n  Analysis"
                },
                "summary": "Organizations are increasingly targeted by Advanced Persistent Threats\n(APTs), which involve complex, multi-stage tactics and diverse techniques.\nCyber Threat Intelligence (CTI) sources, such as incident reports and security\nblogs, provide valuable insights, but are often unstructured and in natural\nlanguage, making it difficult to automatically extract information. Recent\nstudies have explored the use of AI to perform automatic extraction from CTI\ndata, leveraging existing CTI datasets for performance evaluation and\nfine-tuning. However, they present challenges and limitations that impact their\neffectiveness. To overcome these issues, we introduce a novel dataset manually\nconstructed from CTI reports and structured according to the MITRE ATT&CK\nframework. To assess its quality, we conducted an inter-annotator agreement\nstudy using Krippendorff alpha, confirming its reliability. Furthermore, the\ndataset was used to evaluate a Large Language Model (LLM) in a real-world\nbusiness context, showing promising generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations are increasingly targeted by Advanced Persistent Threats\n(APTs), which involve complex, multi-stage tactics and diverse techniques.\nCyber Threat Intelligence (CTI) sources, such as incident reports and security\nblogs, provide valuable insights, but are often unstructured and in natural\nlanguage, making it difficult to automatically extract information. Recent\nstudies have explored the use of AI to perform automatic extraction from CTI\ndata, leveraging existing CTI datasets for performance evaluation and\nfine-tuning. However, they present challenges and limitations that impact their\neffectiveness. To overcome these issues, we introduce a novel dataset manually\nconstructed from CTI reports and structured according to the MITRE ATT&CK\nframework. To assess its quality, we conducted an inter-annotator agreement\nstudy using Krippendorff alpha, confirming its reliability. Furthermore, the\ndataset was used to evaluate a Large Language Model (LLM) in a real-world\nbusiness context, showing promising generalizability."
                },
                "authors": [
                    {
                        "name": "Sofia Della Penna"
                    },
                    {
                        "name": "Roberto Natella"
                    },
                    {
                        "name": "Vittorio Orbinato"
                    },
                    {
                        "name": "Lorenzo Parracino"
                    },
                    {
                        "name": "Luciano Pianese"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Pianese"
                },
                "author": "Luciano Pianese",
                "arxiv_comment": "Accepted for publication in the Workshop on Attackers and Cybercrime\n  Operations (WACCO 2025), co-located with IEEE European Symposium on Security\n  and Privacy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17875v3",
                "updated": "2025-04-08T09:44:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    44,
                    28,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-23T13:47:05Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    47,
                    5,
                    2,
                    297,
                    0
                ],
                "title": "Understanding Layer Significance in LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Layer Significance in LLM Alignment"
                },
                "summary": "Aligning large language models (LLMs) through supervised fine-tuning is\nessential for tailoring them to specific applications. Recent studies suggest\nthat alignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To uncover how alignment affects model behavior at\na granular level, we propose identifying which layers within LLMs are most\ncritical to the alignment process. Our approach, named ILA, involves learning a\nbinary mask for the parameter changes in each layer during alignment, as an\nindicator of layer significance. Experimental results reveal that, despite\nsubstantial differences in alignment datasets, the important layers of a model\nidentified by ILA exhibit nearly 90\\% overlap, highlighting fundamental\npatterns in LLM alignment. The results also indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss. Finally, we discuss how these findings extend\nfrom LLM alignment to reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) through supervised fine-tuning is\nessential for tailoring them to specific applications. Recent studies suggest\nthat alignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To uncover how alignment affects model behavior at\na granular level, we propose identifying which layers within LLMs are most\ncritical to the alignment process. Our approach, named ILA, involves learning a\nbinary mask for the parameter changes in each layer during alignment, as an\nindicator of layer significance. Experimental results reveal that, despite\nsubstantial differences in alignment datasets, the important layers of a model\nidentified by ILA exhibit nearly 90\\% overlap, highlighting fundamental\npatterns in LLM alignment. The results also indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss. Finally, we discuss how these findings extend\nfrom LLM alignment to reasoning."
                },
                "authors": [
                    {
                        "name": "Guangyuan Shi"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09702v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09702v3",
                "updated": "2025-04-08T09:44:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    44,
                    22,
                    1,
                    98,
                    0
                ],
                "published": "2023-10-15T01:41:42Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    1,
                    41,
                    42,
                    6,
                    288,
                    0
                ],
                "title": "Inference with Mondrian Random Forests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Mondrian Random Forests"
                },
                "summary": "Random forests are popular methods for regression and classification\nanalysis, and many different variants have been proposed in recent years. One\ninteresting example is the Mondrian random forest, in which the underlying\nconstituent trees are constructed via a Mondrian process. We give precise bias\nand variance characterizations, along with a Berry-Esseen-type central limit\ntheorem, for the Mondrian random forest regression estimator. By combining\nthese results with a carefully crafted debiasing approach and an accurate\nvariance estimator, we present valid statistical inference methods for the\nunknown regression function. These methods come with explicitly characterized\nerror bounds in terms of the sample size, tree complexity parameter, and number\nof trees in the forest, and include coverage error rates for feasible\nconfidence interval estimators. Our novel debiasing procedure for the Mondrian\nrandom forest also allows it to achieve the minimax-optimal point estimation\nconvergence rate in mean squared error for multivariate $\\beta$-H\\\"older\nregression functions, for all $\\beta > 0$, provided that the underlying tuning\nparameters are chosen appropriately. Efficient and implementable algorithms are\ndevised for both batch and online learning settings, and we study the\ncomputational complexity of different Mondrian random forest implementations.\nFinally, simulations with synthetic data validate our theory and methodology,\ndemonstrating their excellent finite-sample properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random forests are popular methods for regression and classification\nanalysis, and many different variants have been proposed in recent years. One\ninteresting example is the Mondrian random forest, in which the underlying\nconstituent trees are constructed via a Mondrian process. We give precise bias\nand variance characterizations, along with a Berry-Esseen-type central limit\ntheorem, for the Mondrian random forest regression estimator. By combining\nthese results with a carefully crafted debiasing approach and an accurate\nvariance estimator, we present valid statistical inference methods for the\nunknown regression function. These methods come with explicitly characterized\nerror bounds in terms of the sample size, tree complexity parameter, and number\nof trees in the forest, and include coverage error rates for feasible\nconfidence interval estimators. Our novel debiasing procedure for the Mondrian\nrandom forest also allows it to achieve the minimax-optimal point estimation\nconvergence rate in mean squared error for multivariate $\\beta$-H\\\"older\nregression functions, for all $\\beta > 0$, provided that the underlying tuning\nparameters are chosen appropriately. Efficient and implementable algorithms are\ndevised for both batch and online learning settings, and we study the\ncomputational complexity of different Mondrian random forest implementations.\nFinally, simulations with synthetic data validate our theory and methodology,\ndemonstrating their excellent finite-sample properties."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Jason M. Klusowski"
                    },
                    {
                        "name": "William G. Underwood"
                    }
                ],
                "author_detail": {
                    "name": "William G. Underwood"
                },
                "author": "William G. Underwood",
                "arxiv_comment": "64 pages, 1 figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09702v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09702v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08 (Primary), 62G05, 62G20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05863v1",
                "updated": "2025-04-08T09:43:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    43,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:43:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    43,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Extending Parametric Model Embedding with Physical Information for\n  Design-space Dimensionality Reduction in Shape Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending Parametric Model Embedding with Physical Information for\n  Design-space Dimensionality Reduction in Shape Optimization"
                },
                "summary": "In this work, an extension of the parametric model embedding (PME) approach\nis presented, aiming to achieve more effective design-space dimensionality\nreduction for shape optimization in vehicle design. PME, rooted in principal\ncomponent analysis (PCA), not only identifies a reduced set of critical modes\nbut also re-parameterizes the original design space, enabling direct and\ninterpretable manipulations of shape modifications within the reduced space.\nAlongside the \"physics-informed\" version (PI-PME), which enriches geometry with\nlow-fidelity distributed and lumped physical quantities, a \"physics-driven\"\nvariant (PD-PME) is introduced that focuses exclusively on physical parameters.\nBoth formulations employ PCA to capture the principal modes of variability yet\ndiffer in their balance between geometric and physical information, through the\nad-hoc definition of a weighted inner product. Through test cases involving the\nRAE-2822 airfoil, a bio-inspired underwater glider, a naval propeller, and the\nDTMB-5415 destroyer-type vessel, it is shown how the resulting frameworks\nprovide a first-level assessment of design variability, offer interpretability\nregarding which original variables most strongly affect performance, and\nefficiently bridge geometric and physical parameters. Furthermore, lumped\nphysical parameters can serve as a low-fidelity foundation for multi-fidelity\noptimization, directly leveraging the linear re-parameterization to drive the\nreduced design variables. Meanwhile, distributed physical parameters enable the\nconstruction of machine-learning-based reduced-order models to infer integral\nquantities of interest. By allowing the user to embed these insights early in\nthe design process, PI-PME and PD-PME facilitate more robust, cost-effective\nexploration, paving the way for subsequent high-fidelity optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, an extension of the parametric model embedding (PME) approach\nis presented, aiming to achieve more effective design-space dimensionality\nreduction for shape optimization in vehicle design. PME, rooted in principal\ncomponent analysis (PCA), not only identifies a reduced set of critical modes\nbut also re-parameterizes the original design space, enabling direct and\ninterpretable manipulations of shape modifications within the reduced space.\nAlongside the \"physics-informed\" version (PI-PME), which enriches geometry with\nlow-fidelity distributed and lumped physical quantities, a \"physics-driven\"\nvariant (PD-PME) is introduced that focuses exclusively on physical parameters.\nBoth formulations employ PCA to capture the principal modes of variability yet\ndiffer in their balance between geometric and physical information, through the\nad-hoc definition of a weighted inner product. Through test cases involving the\nRAE-2822 airfoil, a bio-inspired underwater glider, a naval propeller, and the\nDTMB-5415 destroyer-type vessel, it is shown how the resulting frameworks\nprovide a first-level assessment of design variability, offer interpretability\nregarding which original variables most strongly affect performance, and\nefficiently bridge geometric and physical parameters. Furthermore, lumped\nphysical parameters can serve as a low-fidelity foundation for multi-fidelity\noptimization, directly leveraging the linear re-parameterization to drive the\nreduced design variables. Meanwhile, distributed physical parameters enable the\nconstruction of machine-learning-based reduced-order models to infer integral\nquantities of interest. By allowing the user to embed these insights early in\nthe design process, PI-PME and PD-PME facilitate more robust, cost-effective\nexploration, paving the way for subsequent high-fidelity optimization."
                },
                "authors": [
                    {
                        "name": "Andrea Serani"
                    },
                    {
                        "name": "Giorgio Palma"
                    },
                    {
                        "name": "Jeroen Wackers"
                    },
                    {
                        "name": "Domenico Quagliarella"
                    },
                    {
                        "name": "Stefano Gaggero"
                    },
                    {
                        "name": "Matteo Diez"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Diez"
                },
                "author": "Matteo Diez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05862v1",
                "updated": "2025-04-08T09:41:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    41,
                    3,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:41:03Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    41,
                    3,
                    1,
                    98,
                    0
                ],
                "title": "Are Generative AI Agents Effective Personalized Financial Advisors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Generative AI Agents Effective Personalized Financial Advisors?"
                },
                "summary": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice."
                },
                "authors": [
                    {
                        "name": "Takehiro Takayanagi"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    },
                    {
                        "name": "Javier Sanz-Cruzado"
                    },
                    {
                        "name": "Richard McCreadie"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03543v3",
                "updated": "2025-04-08T09:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    39,
                    25,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-04T15:49:49Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    15,
                    49,
                    49,
                    3,
                    95,
                    0
                ],
                "title": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Xueling Liu"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Ding Pan"
                    },
                    {
                        "name": "Yizhi LI"
                    },
                    {
                        "name": "Ruibo Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05846v1",
                "updated": "2025-04-08T09:25:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    25,
                    21,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:25:21Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    25,
                    21,
                    1,
                    98,
                    0
                ],
                "title": "PathGPT: Leveraging Large Language Models for Personalized Route\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathGPT: Leveraging Large Language Models for Personalized Route\n  Generation"
                },
                "summary": "The proliferation of GPS enabled devices has led to the accumulation of a\nsubstantial corpus of historical trajectory data. By leveraging these data for\ntraining machine learning models,researchers have devised novel data-driven\nmethodologies that address the personalized route recommendation (PRR) problem.\nIn contrast to conventional algorithms such as Dijkstra shortest path\nalgorithm,these novel algorithms possess the capacity to discern and learn\npatterns within the data,thereby facilitating the generation of more\npersonalized paths. However,once these models have been trained,their\napplication is constrained to the generation of routes that align with their\ntraining patterns. This limitation renders them less adaptable to novel\nscenarios and the deployment of multiple machine learning models might be\nnecessary to address new possible scenarios,which can be costly as each model\nmust be trained separately. Inspired by recent advances in the field of Large\nLanguage Models (LLMs),we leveraged their natural language understanding\ncapabilities to develop a unified model to solve the PRR problem while being\nseamlessly adaptable to new scenarios without additional training. To\naccomplish this,we combined the extensive knowledge LLMs acquired during\ntraining with further access to external hand-crafted context\ninformation,similar to RAG (Retrieved Augmented Generation) systems,to enhance\ntheir ability to generate paths according to user-defined requirements.\nExtensive experiments on different datasets show a considerable uplift in LLM\nperformance on the PRR problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of GPS enabled devices has led to the accumulation of a\nsubstantial corpus of historical trajectory data. By leveraging these data for\ntraining machine learning models,researchers have devised novel data-driven\nmethodologies that address the personalized route recommendation (PRR) problem.\nIn contrast to conventional algorithms such as Dijkstra shortest path\nalgorithm,these novel algorithms possess the capacity to discern and learn\npatterns within the data,thereby facilitating the generation of more\npersonalized paths. However,once these models have been trained,their\napplication is constrained to the generation of routes that align with their\ntraining patterns. This limitation renders them less adaptable to novel\nscenarios and the deployment of multiple machine learning models might be\nnecessary to address new possible scenarios,which can be costly as each model\nmust be trained separately. Inspired by recent advances in the field of Large\nLanguage Models (LLMs),we leveraged their natural language understanding\ncapabilities to develop a unified model to solve the PRR problem while being\nseamlessly adaptable to new scenarios without additional training. To\naccomplish this,we combined the extensive knowledge LLMs acquired during\ntraining with further access to external hand-crafted context\ninformation,similar to RAG (Retrieved Augmented Generation) systems,to enhance\ntheir ability to generate paths according to user-defined requirements.\nExtensive experiments on different datasets show a considerable uplift in LLM\nperformance on the PRR problem."
                },
                "authors": [
                    {
                        "name": "Steeve Cuthbert Marcelyn"
                    },
                    {
                        "name": "Yucen Gao"
                    },
                    {
                        "name": "Yuzhe Zhang"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05831v1",
                "updated": "2025-04-08T09:14:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    14,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:14:38Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    14,
                    38,
                    1,
                    98,
                    0
                ],
                "title": "Leveraging Robust Optimization for LLM Alignment under Distribution\n  Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Robust Optimization for LLM Alignment under Distribution\n  Shifts"
                },
                "summary": "Large language models (LLMs) increasingly rely on preference alignment\nmethods to steer outputs toward human values, yet these methods are often\nconstrained by the scarcity of high-quality human-annotated data. To tackle\nthis, recent approaches have turned to synthetic data generated by LLMs as a\nscalable alternative. However, synthetic data can introduce distribution\nshifts, compromising the nuanced human preferences that are essential for\ndesirable outputs. In this paper, we propose a novel distribution-aware\noptimization framework that improves preference alignment in the presence of\nsuch shifts. Our approach first estimates the likelihood ratios between the\ntarget and training distributions leveraging a learned classifier, then it\nminimizes the worst-case loss over data regions that reflect the target\nhuman-preferred distribution. By explicitly prioritizing the target\ndistribution during optimization, our method mitigates the adverse effects of\ndistributional variation and enhances the generation of responses that\nfaithfully reflect human values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on preference alignment\nmethods to steer outputs toward human values, yet these methods are often\nconstrained by the scarcity of high-quality human-annotated data. To tackle\nthis, recent approaches have turned to synthetic data generated by LLMs as a\nscalable alternative. However, synthetic data can introduce distribution\nshifts, compromising the nuanced human preferences that are essential for\ndesirable outputs. In this paper, we propose a novel distribution-aware\noptimization framework that improves preference alignment in the presence of\nsuch shifts. Our approach first estimates the likelihood ratios between the\ntarget and training distributions leveraging a learned classifier, then it\nminimizes the worst-case loss over data regions that reflect the target\nhuman-preferred distribution. By explicitly prioritizing the target\ndistribution during optimization, our method mitigates the adverse effects of\ndistributional variation and enhances the generation of responses that\nfaithfully reflect human values."
                },
                "authors": [
                    {
                        "name": "Mingye Zhu"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Junbo Guo"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07908v2",
                "updated": "2025-04-08T09:13:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    13,
                    46,
                    1,
                    98,
                    0
                ],
                "published": "2024-09-12T10:24:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    24,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "Parameter constraints for accreting millisecond pulsars with synthetic\n  NICER data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter constraints for accreting millisecond pulsars with synthetic\n  NICER data"
                },
                "summary": "Pulse profile modelling (PPM) is a technique for inferring mass, radius and\nhotspot properties of millisecond pulsars. PPM is now regularly used for\nanalysis of rotation-powered millisecond pulsars (RMPs) with data from the\nNeutron Star Interior Composition ExploreR (NICER). Extending PPM to accreting\nmillisecond pulsars (AMPs) is attractive, because they are a different source\nclass featuring bright X-ray radiation from hotspots powered by accretion. In\nthis paper, we present a modification of one of the PPM codes, X-PSI, so that\nit can be used for AMPs. In particular, we implement a model of an accretion\ndisc and atmosphere model appropriate for the hotspots of AMPs, and improve the\noverall computational efficiency. We then test parameter recovery with\nsynthetic NICER data in two scenarios with reasonable parameters for AMPs. We\nfind in the first scenario, where the hotspot is large, that we are able to\ntightly and accurately constrain all parameters including mass and radius. In\nthe second scenario, which is a high inclination system with a smaller hotspot,\nwe find degeneracy between a subset of model parameters and a slight bias in\nthe inferred mass and radius. This analysis of synthetic data lays the ground\nwork for future analysis of AMPs with NICER data. Such an analysis could be\ncomplemented by future (joint) analysis of polarization data from the Imaging\nX-ray Polarimetry Explorer (IXPE).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulse profile modelling (PPM) is a technique for inferring mass, radius and\nhotspot properties of millisecond pulsars. PPM is now regularly used for\nanalysis of rotation-powered millisecond pulsars (RMPs) with data from the\nNeutron Star Interior Composition ExploreR (NICER). Extending PPM to accreting\nmillisecond pulsars (AMPs) is attractive, because they are a different source\nclass featuring bright X-ray radiation from hotspots powered by accretion. In\nthis paper, we present a modification of one of the PPM codes, X-PSI, so that\nit can be used for AMPs. In particular, we implement a model of an accretion\ndisc and atmosphere model appropriate for the hotspots of AMPs, and improve the\noverall computational efficiency. We then test parameter recovery with\nsynthetic NICER data in two scenarios with reasonable parameters for AMPs. We\nfind in the first scenario, where the hotspot is large, that we are able to\ntightly and accurately constrain all parameters including mass and radius. In\nthe second scenario, which is a high inclination system with a smaller hotspot,\nwe find degeneracy between a subset of model parameters and a slight bias in\nthe inferred mass and radius. This analysis of synthetic data lays the ground\nwork for future analysis of AMPs with NICER data. Such an analysis could be\ncomplemented by future (joint) analysis of polarization data from the Imaging\nX-ray Polarimetry Explorer (IXPE)."
                },
                "authors": [
                    {
                        "name": "Bas Dorsman"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Mason Ng"
                    },
                    {
                        "name": "Satish Kamath"
                    },
                    {
                        "name": "Anna Bobrikova"
                    },
                    {
                        "name": "Juri Poutanen"
                    },
                    {
                        "name": "Vladislav Loktev"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Devarshi Choudhury"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Slavko Bogdanov"
                    },
                    {
                        "name": "Deepto Chakrabarty"
                    }
                ],
                "author_detail": {
                    "name": "Deepto Chakrabarty"
                },
                "author": "Deepto Chakrabarty",
                "arxiv_doi": "10.1093/mnras/staf438",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf438",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 6 figures, 3 tables. Published in MNRAS",
                "arxiv_journal_ref": "MNRAS, 538, 2853 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23494v2",
                "updated": "2025-04-08T09:12:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    12,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-30T16:03:02Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    16,
                    3,
                    2,
                    6,
                    89,
                    0
                ],
                "title": "DNA and Human Language: Epigenetic Memory and Redundancy in Linear\n  Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNA and Human Language: Epigenetic Memory and Redundancy in Linear\n  Sequence"
                },
                "summary": "DNA is often described as the 'language of life', but whether it possesses\nformal linguistic properties remains unresolved. Here, we present the first\nempirical evidence that DNA sequences exhibit core linguistic features,\nspecifically, functional and information redundancy, through comprehensive\nanalysis of genomic and epigenetic datasets. By mapping DNA sequences into a\nlinguistic feature space, we demonstrate that fixed-length (41 bp) DNA segments\nencode information analogously to human language, with redundancy contributing\nto signal stability in aqueous intracellular environments. Moreover, we provide\nthe first evidence of one-dimensional epigenetic memory, showing that linear\nDNA sequences can maintain epigenetic marks such as 6mA methylation,\ncontrasting with models focusing on epigenetic memory transmission via 3D\nchromatin organization[1]. Our tailored linguistic mapping strategy also\naddresses persistent challenges in genomic data processing, significantly\nimproving data cleaning and feature extraction. Together, these findings\nestablish a conceptual paradigm that bridges molecular information encoding and\nlinguistic theory, laying the foundation for next-generation large language\nmodels (LLMs) specifically tailored to DNA, marking a shift at the interface of\nmolecular biology, information theory, and artificial intelligence (AI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNA is often described as the 'language of life', but whether it possesses\nformal linguistic properties remains unresolved. Here, we present the first\nempirical evidence that DNA sequences exhibit core linguistic features,\nspecifically, functional and information redundancy, through comprehensive\nanalysis of genomic and epigenetic datasets. By mapping DNA sequences into a\nlinguistic feature space, we demonstrate that fixed-length (41 bp) DNA segments\nencode information analogously to human language, with redundancy contributing\nto signal stability in aqueous intracellular environments. Moreover, we provide\nthe first evidence of one-dimensional epigenetic memory, showing that linear\nDNA sequences can maintain epigenetic marks such as 6mA methylation,\ncontrasting with models focusing on epigenetic memory transmission via 3D\nchromatin organization[1]. Our tailored linguistic mapping strategy also\naddresses persistent challenges in genomic data processing, significantly\nimproving data cleaning and feature extraction. Together, these findings\nestablish a conceptual paradigm that bridges molecular information encoding and\nlinguistic theory, laying the foundation for next-generation large language\nmodels (LLMs) specifically tailored to DNA, marking a shift at the interface of\nmolecular biology, information theory, and artificial intelligence (AI)."
                },
                "authors": [
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Dongbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Wang"
                },
                "author": "Dongbo Wang",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05824v1",
                "updated": "2025-04-08T09:06:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    6,
                    52,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:06:52Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    6,
                    52,
                    1,
                    98,
                    0
                ],
                "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency\n  and Accuracy in Large-Scale Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency\n  and Accuracy in Large-Scale Systems"
                },
                "summary": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield."
                },
                "authors": [
                    {
                        "name": "Zhang Dong"
                    },
                    {
                        "name": "Songhang deng"
                    },
                    {
                        "name": "Mingbang Wang"
                    },
                    {
                        "name": "Le Dai"
                    },
                    {
                        "name": "Jiyuan Li"
                    },
                    {
                        "name": "Xingzu Liu"
                    },
                    {
                        "name": "Ruilin Nong"
                    }
                ],
                "author_detail": {
                    "name": "Ruilin Nong"
                },
                "author": "Ruilin Nong",
                "arxiv_comment": "submission of acl 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04650v2",
                "updated": "2025-04-08T09:06:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    6,
                    1,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T00:45:10Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    0,
                    45,
                    10,
                    0,
                    97,
                    0
                ],
                "title": "Autono: A ReAct-Based Highly Robust Autonomous Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autono: A ReAct-Based Highly Robust Autonomous Agent Framework"
                },
                "summary": "This paper proposes a highly robust autonomous agent framework based on the\nReAct paradigm, designed to solve complex tasks through adaptive decision\nmaking and multi-agent collaboration. Unlike traditional frameworks that rely\non fixed workflows generated by LLM-based planners, this framework dynamically\ngenerates next actions during agent execution based on prior trajectories,\nthereby enhancing its robustness. To address potential termination issues\ncaused by adaptive execution paths, I propose a timely abandonment strategy\nincorporating a probabilistic penalty mechanism. For multi-agent collaboration,\nI introduce a memory transfer mechanism that enables shared and dynamically\nupdated memory among agents. The framework's innovative timely abandonment\nstrategy dynamically adjusts the probability of task abandonment via\nprobabilistic penalties, allowing developers to balance conservative and\nexploratory tendencies in agent execution strategies by tuning hyperparameters.\nThis significantly improves adaptability and task execution efficiency in\ncomplex environments. Additionally, agents can be extended through external\ntool integration, supported by modular design and MCP protocol compatibility,\nwhich enables flexible action space expansion. Through explicit division of\nlabor, the multi-agent collaboration mechanism enables agents to focus on\nspecific task components, thereby significantly improving execution efficiency\nand quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a highly robust autonomous agent framework based on the\nReAct paradigm, designed to solve complex tasks through adaptive decision\nmaking and multi-agent collaboration. Unlike traditional frameworks that rely\non fixed workflows generated by LLM-based planners, this framework dynamically\ngenerates next actions during agent execution based on prior trajectories,\nthereby enhancing its robustness. To address potential termination issues\ncaused by adaptive execution paths, I propose a timely abandonment strategy\nincorporating a probabilistic penalty mechanism. For multi-agent collaboration,\nI introduce a memory transfer mechanism that enables shared and dynamically\nupdated memory among agents. The framework's innovative timely abandonment\nstrategy dynamically adjusts the probability of task abandonment via\nprobabilistic penalties, allowing developers to balance conservative and\nexploratory tendencies in agent execution strategies by tuning hyperparameters.\nThis significantly improves adaptability and task execution efficiency in\ncomplex environments. Additionally, agents can be extended through external\ntool integration, supported by modular design and MCP protocol compatibility,\nwhich enables flexible action space expansion. Through explicit division of\nlabor, the multi-agent collaboration mechanism enables agents to focus on\nspecific task components, thereby significantly improving execution efficiency\nand quality."
                },
                "authors": [
                    {
                        "name": "Zihao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Wu"
                },
                "author": "Zihao Wu",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.8; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13595v2",
                "updated": "2025-04-08T08:57:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    57,
                    22,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-19T10:13:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    13,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMTEB: Massive Multilingual Text Embedding Benchmark"
                },
                "summary": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost."
                },
                "authors": [
                    {
                        "name": "Kenneth Enevoldsen"
                    },
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Imene Kerboua"
                    },
                    {
                        "name": "Márton Kardos"
                    },
                    {
                        "name": "Ashwin Mathur"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Jay Gala"
                    },
                    {
                        "name": "Wissam Siblini"
                    },
                    {
                        "name": "Dominik Krzemiński"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Saba Sturua"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "Gabriel Sequeira"
                    },
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Shreeya Dhakal"
                    },
                    {
                        "name": "Jonathan Rystrøm"
                    },
                    {
                        "name": "Roman Solomatin"
                    },
                    {
                        "name": "Ömer Çağatan"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Martin Bernstorff"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Akshita Sukhlecha"
                    },
                    {
                        "name": "Bhavish Pahwa"
                    },
                    {
                        "name": "Rafał Poświata"
                    },
                    {
                        "name": "Kranthi Kiran GV"
                    },
                    {
                        "name": "Shawon Ashraf"
                    },
                    {
                        "name": "Daniel Auras"
                    },
                    {
                        "name": "Björn Plüster"
                    },
                    {
                        "name": "Jan Philipp Harries"
                    },
                    {
                        "name": "Loïc Magne"
                    },
                    {
                        "name": "Isabelle Mohr"
                    },
                    {
                        "name": "Mariya Hendriksen"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Hippolyte Gisserot-Boukhlef"
                    },
                    {
                        "name": "Tom Aarsen"
                    },
                    {
                        "name": "Jan Kostkan"
                    },
                    {
                        "name": "Konrad Wojtasik"
                    },
                    {
                        "name": "Taemin Lee"
                    },
                    {
                        "name": "Marek Šuppa"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Roberta Rocca"
                    },
                    {
                        "name": "Mohammed Hamdy"
                    },
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "John Yang"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Aleksei Vatolin"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Manan Dey"
                    },
                    {
                        "name": "Dipam Vasani"
                    },
                    {
                        "name": "Pranjal Chitale"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Nguyen Tai"
                    },
                    {
                        "name": "Artem Snegirev"
                    },
                    {
                        "name": "Michael Günther"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xing Han Lù"
                    },
                    {
                        "name": "Jordan Clive"
                    },
                    {
                        "name": "Gayatri Krishnakumar"
                    },
                    {
                        "name": "Anna Maksimova"
                    },
                    {
                        "name": "Silvan Wehrli"
                    },
                    {
                        "name": "Maria Tikhonova"
                    },
                    {
                        "name": "Henil Panchal"
                    },
                    {
                        "name": "Aleksandr Abramov"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Lester James Miranda"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Ruqiya Bin Safi"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Alessia Borghini"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Hongjin Su"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Howard Yen"
                    },
                    {
                        "name": "Lasse Hansen"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Muennighoff"
                },
                "author": "Niklas Muennighoff",
                "arxiv_comment": "Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10778v2",
                "updated": "2025-04-08T08:54:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    54,
                    33,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-14T10:12:22Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    10,
                    12,
                    22,
                    5,
                    349,
                    0
                ],
                "title": "Sample-efficient Unsupervised Policy Cloning from Ensemble\n  Self-supervised Labeled Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-efficient Unsupervised Policy Cloning from Ensemble\n  Self-supervised Labeled Videos"
                },
                "summary": "Current advanced policy learning methodologies have demonstrated the ability\nto develop expert-level strategies when provided enough information. However,\ntheir requirements, including task-specific rewards, action-labeled expert\ntrajectories, and huge environmental interactions, can be expensive or even\nunavailable in many scenarios. In contrast, humans can efficiently acquire\nskills within a few trials and errors by imitating easily accessible internet\nvideos, in the absence of any other supervision. In this paper, we try to let\nmachines replicate this efficient watching-and-learning process through\nUnsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a\nnovel framework to efficiently learn policies from action-free videos without\nrewards and any other expert supervision. UPESV trains a video labeling model\nto infer the expert actions in expert videos through several organically\ncombined self-supervised tasks. Each task performs its duties, and they\ntogether enable the model to make full use of both action-free videos and\nreward-free interactions for robust dynamics understanding and advanced action\nprediction. Simultaneously, UPESV clones a policy from the labeled expert\nvideos, in turn collecting environmental interactions for self-supervised\ntasks. After a sample-efficient, unsupervised, and iterative training process,\nUPESV obtains an advanced policy based on a robust video labeling model.\nExtensive experiments in sixteen challenging procedurally generated\nenvironments demonstrate that the proposed UPESV achieves state-of-the-art\ninteraction-limited policy learning performance (outperforming five current\nadvanced baselines on 12/16 tasks) without exposure to any other supervision\nexcept for videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current advanced policy learning methodologies have demonstrated the ability\nto develop expert-level strategies when provided enough information. However,\ntheir requirements, including task-specific rewards, action-labeled expert\ntrajectories, and huge environmental interactions, can be expensive or even\nunavailable in many scenarios. In contrast, humans can efficiently acquire\nskills within a few trials and errors by imitating easily accessible internet\nvideos, in the absence of any other supervision. In this paper, we try to let\nmachines replicate this efficient watching-and-learning process through\nUnsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a\nnovel framework to efficiently learn policies from action-free videos without\nrewards and any other expert supervision. UPESV trains a video labeling model\nto infer the expert actions in expert videos through several organically\ncombined self-supervised tasks. Each task performs its duties, and they\ntogether enable the model to make full use of both action-free videos and\nreward-free interactions for robust dynamics understanding and advanced action\nprediction. Simultaneously, UPESV clones a policy from the labeled expert\nvideos, in turn collecting environmental interactions for self-supervised\ntasks. After a sample-efficient, unsupervised, and iterative training process,\nUPESV obtains an advanced policy based on a robust video labeling model.\nExtensive experiments in sixteen challenging procedurally generated\nenvironments demonstrate that the proposed UPESV achieves state-of-the-art\ninteraction-limited policy learning performance (outperforming five current\nadvanced baselines on 12/16 tasks) without exposure to any other supervision\nexcept for videos."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yaran Chen"
                    },
                    {
                        "name": "Haoran Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoran Li"
                },
                "author": "Haoran Li",
                "arxiv_comment": "ICRA 2025, 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05813v1",
                "updated": "2025-04-08T08:53:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    53,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:53:38Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    53,
                    38,
                    1,
                    98,
                    0
                ],
                "title": "A new approach for simulating PBH formation from generic curvature\n  fluctuations with the Misner-Sharp formalism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new approach for simulating PBH formation from generic curvature\n  fluctuations with the Misner-Sharp formalism"
                },
                "summary": "Primordial Black Holes (PBHs) may have formed in the early Universe due to\nthe collapse of super-horizon curvature fluctuations. Simulations of PBH\nformation have been essential for inferring the initial conditions that lead to\nblack hole formation and for studying their properties and impact on our\nUniverse. The Misner-Sharp formalism is commonly used as a standard approach\nfor these simulations. Recently, type-II fluctuations, characterized by a\nnon-monotonic areal radius, have gained interest. In the standard Misner-Sharp\napproach for simulating PBH formation with these fluctuations, the evolution\nequations suffer from divergent terms (0/0), which complicate and prevent the\nsimulations. We formulate a new approach to overcome this issue in a simple\nmanner by using the trace of the extrinsic curvature as an auxiliary variable,\nallowing simulations of type-II fluctuations within the Misner-Sharp formalism.\nUsing a set of standard exponential-shaped curvature profiles, we apply our new\napproach and numerical code based on pseudospectral methods to study the time\nevolution of the gravitational collapse, threshold values of type A/B PBHs and\nPBH mass. Interestingly, we identify cases of type-II fluctuations that do not\nnecessarily result in PBH formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Primordial Black Holes (PBHs) may have formed in the early Universe due to\nthe collapse of super-horizon curvature fluctuations. Simulations of PBH\nformation have been essential for inferring the initial conditions that lead to\nblack hole formation and for studying their properties and impact on our\nUniverse. The Misner-Sharp formalism is commonly used as a standard approach\nfor these simulations. Recently, type-II fluctuations, characterized by a\nnon-monotonic areal radius, have gained interest. In the standard Misner-Sharp\napproach for simulating PBH formation with these fluctuations, the evolution\nequations suffer from divergent terms (0/0), which complicate and prevent the\nsimulations. We formulate a new approach to overcome this issue in a simple\nmanner by using the trace of the extrinsic curvature as an auxiliary variable,\nallowing simulations of type-II fluctuations within the Misner-Sharp formalism.\nUsing a set of standard exponential-shaped curvature profiles, we apply our new\napproach and numerical code based on pseudospectral methods to study the time\nevolution of the gravitational collapse, threshold values of type A/B PBHs and\nPBH mass. Interestingly, we identify cases of type-II fluctuations that do not\nnecessarily result in PBH formation."
                },
                "authors": [
                    {
                        "name": "Albert Escrivà"
                    }
                ],
                "author_detail": {
                    "name": "Albert Escrivà"
                },
                "author": "Albert Escrivà",
                "arxiv_comment": "27 pages and 7 figures. Basic version of the numerical code publicly\n  available here: https://github.com/albert-escriva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05812v1",
                "updated": "2025-04-08T08:48:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    48,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:48:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    48,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization"
                },
                "summary": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervisions--such as human labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form commonsense reasoning tasks. Specifically,\nwithout any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves truthfulness\naccuracy of Qwen2.5-7B Instruct from 87.16\\% to 97.25\\% on TruthfulQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervisions--such as human labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form commonsense reasoning tasks. Specifically,\nwithout any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves truthfulness\naccuracy of Qwen2.5-7B Instruct from 87.16\\% to 97.25\\% on TruthfulQA."
                },
                "authors": [
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Haitao Wu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03814v2",
                "updated": "2025-04-08T08:45:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    45,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?"
                },
                "summary": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties."
                },
                "authors": [
                    {
                        "name": "Grgur Kovač"
                    },
                    {
                        "name": "Jérémy Perez"
                    },
                    {
                        "name": "Rémy Portelas"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05805v1",
                "updated": "2025-04-08T08:37:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    37,
                    32,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:37:32Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    37,
                    32,
                    1,
                    98,
                    0
                ],
                "title": "Why is Normalization Necessary for Linear Recommenders?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why is Normalization Necessary for Linear Recommenders?"
                },
                "summary": "Despite their simplicity, linear autoencoder (LAE)-based models have shown\ncomparable or even better performance with faster inference speed than neural\nrecommender models. However, LAEs face two critical challenges: (i) popularity\nbias, which tends to recommend popular items, and (ii) neighborhood bias, which\noverly focuses on capturing local item correlations. To address these issues,\nthis paper first analyzes the effect of two existing normalization methods for\nLAEs, i.e., random-walk and symmetric normalization. Our theoretical analysis\nreveals that normalization highly affects the degree of popularity and\nneighborhood biases among items. Inspired by this analysis, we propose a\nversatile normalization solution, called Data-Adaptive Normalization (DAN),\nwhich flexibly controls the popularity and neighborhood biases by adjusting\nitem- and user-side normalization to align with unique dataset characteristics.\nOwing to its model-agnostic property, DAN can be easily applied to various\nLAE-based models. Experimental results show that DAN-equipped LAEs consistently\nimprove existing LAE-based models across six benchmark datasets, with\nsignificant gains of up to 128.57% and 12.36% for long-tail items and unbiased\nevaluations, respectively. Refer to our code in https://github.com/psm1206/DAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their simplicity, linear autoencoder (LAE)-based models have shown\ncomparable or even better performance with faster inference speed than neural\nrecommender models. However, LAEs face two critical challenges: (i) popularity\nbias, which tends to recommend popular items, and (ii) neighborhood bias, which\noverly focuses on capturing local item correlations. To address these issues,\nthis paper first analyzes the effect of two existing normalization methods for\nLAEs, i.e., random-walk and symmetric normalization. Our theoretical analysis\nreveals that normalization highly affects the degree of popularity and\nneighborhood biases among items. Inspired by this analysis, we propose a\nversatile normalization solution, called Data-Adaptive Normalization (DAN),\nwhich flexibly controls the popularity and neighborhood biases by adjusting\nitem- and user-side normalization to align with unique dataset characteristics.\nOwing to its model-agnostic property, DAN can be easily applied to various\nLAE-based models. Experimental results show that DAN-equipped LAEs consistently\nimprove existing LAE-based models across six benchmark datasets, with\nsignificant gains of up to 128.57% and 12.36% for long-tail items and unbiased\nevaluations, respectively. Refer to our code in https://github.com/psm1206/DAN."
                },
                "authors": [
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Mincheol Yoon"
                    },
                    {
                        "name": "Hye-young Kim"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_comment": "Accepted by SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05804v1",
                "updated": "2025-04-08T08:36:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    36,
                    18,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:36:18Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    36,
                    18,
                    1,
                    98,
                    0
                ],
                "title": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization"
                },
                "summary": "The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present StealthRank, a novel adversarial ranking attack that\nmanipulates LLM-driven product recommendation systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within product descriptions that subtly yet effectively\ninfluence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs,\ndemonstrating its ability to covertly boost the ranking of target products\nwhile avoiding explicit manipulation traces that can be easily detected. Our\nresults show that StealthRank consistently outperforms state-of-the-art\nadversarial ranking baselines in both effectiveness and stealth, highlighting\ncritical vulnerabilities in LLM-driven recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present StealthRank, a novel adversarial ranking attack that\nmanipulates LLM-driven product recommendation systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within product descriptions that subtly yet effectively\ninfluence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs,\ndemonstrating its ability to covertly boost the ranking of target products\nwhile avoiding explicit manipulation traces that can be easily detected. Our\nresults show that StealthRank consistently outperforms state-of-the-art\nadversarial ranking baselines in both effectiveness and stealth, highlighting\ncritical vulnerabilities in LLM-driven recommendation systems."
                },
                "authors": [
                    {
                        "name": "Yiming Tang"
                    },
                    {
                        "name": "Yi Fan"
                    },
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Tiankai Yang"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Xiyang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiyang Hu"
                },
                "author": "Xiyang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04582v2",
                "updated": "2025-04-08T08:35:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    35,
                    53,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-06T18:46:08Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    18,
                    46,
                    8,
                    6,
                    96,
                    0
                ],
                "title": "Your Image Generator Is Your New Private Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Image Generator Is Your New Private Dataset"
                },
                "summary": "Generative diffusion models have emerged as powerful tools to synthetically\nproduce training data, offering potential solutions to data scarcity and\nreducing labelling costs for downstream supervised deep learning applications.\nHowever, effectively leveraging text-conditioned image generation for building\nclassifier training sets requires addressing key issues: constructing\ninformative textual prompts, adapting generative models to specific domains,\nand ensuring robust performance. This paper proposes the Text-Conditioned\nKnowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines\ndynamic image captioning, parameter-efficient diffusion model fine-tuning, and\nGenerative Knowledge Distillation techniques to create synthetic datasets\ntailored for image classification. The pipeline is rigorously evaluated on ten\ndiverse image classification benchmarks. The results demonstrate that models\ntrained solely on TCKR-generated data achieve classification accuracies on par\nwith (and in several cases exceeding) models trained on real images.\nFurthermore, the evaluation reveals that these synthetic-data-trained models\nexhibit substantially enhanced privacy characteristics: their vulnerability to\nMembership Inference Attacks is significantly reduced, with the membership\ninference AUC lowered by 5.49 points on average compared to using real training\ndata, demonstrating a substantial improvement in the performance-privacy\ntrade-off. These findings indicate that high-fidelity synthetic data can\neffectively replace real data for training classifiers, yielding strong\nperformance whilst simultaneously providing improved privacy protection as a\nvaluable emergent property. The code and trained models are available in the\naccompanying open-source repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative diffusion models have emerged as powerful tools to synthetically\nproduce training data, offering potential solutions to data scarcity and\nreducing labelling costs for downstream supervised deep learning applications.\nHowever, effectively leveraging text-conditioned image generation for building\nclassifier training sets requires addressing key issues: constructing\ninformative textual prompts, adapting generative models to specific domains,\nand ensuring robust performance. This paper proposes the Text-Conditioned\nKnowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines\ndynamic image captioning, parameter-efficient diffusion model fine-tuning, and\nGenerative Knowledge Distillation techniques to create synthetic datasets\ntailored for image classification. The pipeline is rigorously evaluated on ten\ndiverse image classification benchmarks. The results demonstrate that models\ntrained solely on TCKR-generated data achieve classification accuracies on par\nwith (and in several cases exceeding) models trained on real images.\nFurthermore, the evaluation reveals that these synthetic-data-trained models\nexhibit substantially enhanced privacy characteristics: their vulnerability to\nMembership Inference Attacks is significantly reduced, with the membership\ninference AUC lowered by 5.49 points on average compared to using real training\ndata, demonstrating a substantial improvement in the performance-privacy\ntrade-off. These findings indicate that high-fidelity synthetic data can\neffectively replace real data for training classifiers, yielding strong\nperformance whilst simultaneously providing improved privacy protection as a\nvaluable emergent property. The code and trained models are available in the\naccompanying open-source repository."
                },
                "authors": [
                    {
                        "name": "Nicolo Resmini"
                    },
                    {
                        "name": "Eugenio Lomurno"
                    },
                    {
                        "name": "Cristian Sbrolli"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07766v2",
                "updated": "2025-04-08T08:33:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    33,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-01-14T00:47:24Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    0,
                    47,
                    24,
                    1,
                    14,
                    0
                ],
                "title": "Large Language Models for Knowledge Graph Embedding: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Knowledge Graph Embedding: A Survey"
                },
                "summary": "Large language models (LLMs) have garnered significant attention for their\nsuperior performance in many knowledge-driven applications on the world wide\nweb.These models are designed to train hundreds of millions or more parameters\non large amounts of text data, enabling them to understand and generate\nnaturallanguage effectively. As the superior performance of LLMs becomes\napparent,they are increasingly being applied to knowledge graph embedding (KGE)\nrelated tasks to improve the processing results. Traditional KGE representation\nlearning methods map entities and relations into a low-dimensional vector\nspace, enablingthe triples in the knowledge graph to satisfy a specific scoring\nfunction in thevector space. However, based on the powerful language\nunderstanding and seman-tic modeling capabilities of LLMs, that have recently\nbeen invoked to varying degrees in different types of KGE related scenarios\nsuch as multi-modal KGE andopen KGE according to their task characteristics. In\nthis paper, we investigate awide range of approaches for performing\nLLMs-related tasks in different types of KGE scenarios. To better compare the\nvarious approaches, we summarize each KGE scenario in a classification.\nFinally, we discuss the applications in which the methods are mainly used and\nsuggest several forward-looking directions for the development of this new\nresearch area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention for their\nsuperior performance in many knowledge-driven applications on the world wide\nweb.These models are designed to train hundreds of millions or more parameters\non large amounts of text data, enabling them to understand and generate\nnaturallanguage effectively. As the superior performance of LLMs becomes\napparent,they are increasingly being applied to knowledge graph embedding (KGE)\nrelated tasks to improve the processing results. Traditional KGE representation\nlearning methods map entities and relations into a low-dimensional vector\nspace, enablingthe triples in the knowledge graph to satisfy a specific scoring\nfunction in thevector space. However, based on the powerful language\nunderstanding and seman-tic modeling capabilities of LLMs, that have recently\nbeen invoked to varying degrees in different types of KGE related scenarios\nsuch as multi-modal KGE andopen KGE according to their task characteristics. In\nthis paper, we investigate awide range of approaches for performing\nLLMs-related tasks in different types of KGE scenarios. To better compare the\nvarious approaches, we summarize each KGE scenario in a classification.\nFinally, we discuss the applications in which the methods are mainly used and\nsuggest several forward-looking directions for the development of this new\nresearch area."
                },
                "authors": [
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Yuanyuan Fang"
                    },
                    {
                        "name": "Naixing Xu"
                    },
                    {
                        "name": "Shihao Hou"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05801v1",
                "updated": "2025-04-08T08:31:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    31,
                    3,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:31:03Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    31,
                    3,
                    1,
                    98,
                    0
                ],
                "title": "From Superficial to Deep: Integrating External Knowledge for Follow-up\n  Question Generation Using Knowledge Graph and LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Superficial to Deep: Integrating External Knowledge for Follow-up\n  Question Generation Using Knowledge Graph and LLM"
                },
                "summary": "In a conversational system, dynamically generating follow-up questions based\non context can help users explore information and provide a better user\nexperience. Humans are usually able to ask questions that involve some general\nlife knowledge and demonstrate higher order cognitive skills. However, the\nquestions generated by existing methods are often limited to shallow contextual\nquestions that are uninspiring and have a large gap to the human level. In this\npaper, we propose a three-stage external knowledge-enhanced follow-up question\ngeneration method, which generates questions by identifying contextual topics,\nconstructing a knowledge graph (KG) online, and finally combining these with a\nlarge language model to generate the final question. The model generates\ninformation-rich and exploratory follow-up questions by introducing external\ncommon sense knowledge and performing a knowledge fusion operation. Experiments\nshow that compared to baseline models, our method generates questions that are\nmore informative and closer to human questioning levels while maintaining\ncontextual relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a conversational system, dynamically generating follow-up questions based\non context can help users explore information and provide a better user\nexperience. Humans are usually able to ask questions that involve some general\nlife knowledge and demonstrate higher order cognitive skills. However, the\nquestions generated by existing methods are often limited to shallow contextual\nquestions that are uninspiring and have a large gap to the human level. In this\npaper, we propose a three-stage external knowledge-enhanced follow-up question\ngeneration method, which generates questions by identifying contextual topics,\nconstructing a knowledge graph (KG) online, and finally combining these with a\nlarge language model to generate the final question. The model generates\ninformation-rich and exploratory follow-up questions by introducing external\ncommon sense knowledge and performing a knowledge fusion operation. Experiments\nshow that compared to baseline models, our method generates questions that are\nmore informative and closer to human questioning levels while maintaining\ncontextual relevance."
                },
                "authors": [
                    {
                        "name": "Jianyu Liu"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Sheng Bi"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Guilin Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Qi"
                },
                "author": "Guilin Qi",
                "arxiv_comment": "Proceedings of the 31st International Conference on Computational\n  Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16310v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16310v4",
                "updated": "2025-04-08T08:30:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    30,
                    11,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-25T11:57:48Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    57,
                    48,
                    0,
                    330,
                    0
                ],
                "title": "Functionality understanding and segmentation in 3D scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functionality understanding and segmentation in 3D scenes"
                },
                "summary": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://tev-fbk.github.io/fun3du/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://tev-fbk.github.io/fun3du/"
                },
                "authors": [
                    {
                        "name": "Jaime Corsetti"
                    },
                    {
                        "name": "Francesco Giuliari"
                    },
                    {
                        "name": "Alice Fasoli"
                    },
                    {
                        "name": "Davide Boscaini"
                    },
                    {
                        "name": "Fabio Poiesi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Poiesi"
                },
                "author": "Fabio Poiesi",
                "arxiv_comment": "CVPR 2025 Highlight. Camera ready version. 20 pages, 12 figures, 7\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16310v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16310v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v2",
                "updated": "2025-04-08T08:18:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    18,
                    14,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13622v2",
                "updated": "2025-04-08T08:17:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    17,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-19T10:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    59,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality\n  Hallucination Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality\n  Hallucination Detection in Large Language Models"
                },
                "summary": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages. Our code is available at\nhttps://github.com/oneonlee/REFIND.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages. Our code is available at\nhttps://github.com/oneonlee/REFIND."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_comment": "Accepted to SemEval@ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05786v1",
                "updated": "2025-04-08T08:11:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    11,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:11:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    11,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM"
                },
                "summary": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05712v3",
                "updated": "2025-04-08T08:10:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    10,
                    7,
                    1,
                    98,
                    0
                ],
                "published": "2024-07-08T08:12:57Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    8,
                    12,
                    57,
                    0,
                    190,
                    0
                ],
                "title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices"
                },
                "summary": "Existing neural head avatars methods have achieved significant progress in\nthe image quality and motion range of portrait animation. However, these\nmethods neglect the computational overhead, and to the best of our knowledge,\nnone is designed to run on mobile devices. This paper presents MobilePortrait,\na lightweight one-shot neural head avatars method that reduces learning\ncomplexity by integrating external knowledge into both the motion modeling and\nimage synthesis, enabling real-time inference on mobile devices. Specifically,\nwe introduce a mixed representation of explicit and implicit keypoints for\nprecise motion modeling and precomputed visual features for enhanced foreground\nand background synthesis. With these two key designs and using simple U-Nets as\nbackbones, our method achieves state-of-the-art performance with less than\none-tenth the computational demand. It has been validated to reach speeds of\nover 100 FPS on mobile devices and support both video and audio-driven inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing neural head avatars methods have achieved significant progress in\nthe image quality and motion range of portrait animation. However, these\nmethods neglect the computational overhead, and to the best of our knowledge,\nnone is designed to run on mobile devices. This paper presents MobilePortrait,\na lightweight one-shot neural head avatars method that reduces learning\ncomplexity by integrating external knowledge into both the motion modeling and\nimage synthesis, enabling real-time inference on mobile devices. Specifically,\nwe introduce a mixed representation of explicit and implicit keypoints for\nprecise motion modeling and precomputed visual features for enhanced foreground\nand background synthesis. With these two key designs and using simple U-Nets as\nbackbones, our method achieves state-of-the-art performance with less than\none-tenth the computational demand. It has been validated to reach speeds of\nover 100 FPS on mobile devices and support both video and audio-driven inputs."
                },
                "authors": [
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Gaojie Lin"
                    },
                    {
                        "name": "Zhengkun Rong"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Yongming Zhu"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Tianyun Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tianyun Zhong"
                },
                "author": "Tianyun Zhong",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05775v1",
                "updated": "2025-04-08T07:57:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    57,
                    29,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:57:29Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    57,
                    29,
                    1,
                    98,
                    0
                ],
                "title": "Survey of the Edge Radial Electric Field in L-mode TCV Plasmas using\n  Doppler Backscattering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of the Edge Radial Electric Field in L-mode TCV Plasmas using\n  Doppler Backscattering"
                },
                "summary": "A Doppler backscattering (DBS) diagnostic has recently been installed on the\nTokamak \\`a Configuration Variable (TCV) to facilitate the study of edge\nturbulence and flow shear in a versatile experimental environment. The dual\nchannel V-band DBS system is coupled to TCV's quasi-optical diagnostic\nlauncher, providing access to the upper low-field side region of the plasma\ncross-section. Verifications of the DBS measurements are presented. The DBS\nequilibrium $v_\\perp$ profiles are found to compare favorably with gas puff\nimaging (GPI) measurements and to the $E_r$ inferred from the radial force\nbalance of the carbon impurity. The radial structure of the edge $E_r \\times B$\nequilibrium flow and its dependencies are investigated across a representative\nset of L-mode TCV discharges, by varying density, auxiliary heating and\nmagnetic configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Doppler backscattering (DBS) diagnostic has recently been installed on the\nTokamak \\`a Configuration Variable (TCV) to facilitate the study of edge\nturbulence and flow shear in a versatile experimental environment. The dual\nchannel V-band DBS system is coupled to TCV's quasi-optical diagnostic\nlauncher, providing access to the upper low-field side region of the plasma\ncross-section. Verifications of the DBS measurements are presented. The DBS\nequilibrium $v_\\perp$ profiles are found to compare favorably with gas puff\nimaging (GPI) measurements and to the $E_r$ inferred from the radial force\nbalance of the carbon impurity. The radial structure of the edge $E_r \\times B$\nequilibrium flow and its dependencies are investigated across a representative\nset of L-mode TCV discharges, by varying density, auxiliary heating and\nmagnetic configuration."
                },
                "authors": [
                    {
                        "name": "Sascha Rienäcker"
                    },
                    {
                        "name": "Pascale Hennequin"
                    },
                    {
                        "name": "Laure Vermare"
                    },
                    {
                        "name": "Cyrille Honoré"
                    },
                    {
                        "name": "Stefano Coda"
                    },
                    {
                        "name": "Benoit Labit"
                    },
                    {
                        "name": "Benjamin Vincent"
                    },
                    {
                        "name": "Yinghan Wang"
                    },
                    {
                        "name": "Lorenzo Frassinetti"
                    },
                    {
                        "name": "Olivier Panico"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Panico"
                },
                "author": "Olivier Panico",
                "arxiv_comment": "Submitted to Plasma Phys. Control. Fusion, Feb 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.06265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06265v1",
                "updated": "2025-04-08T17:59:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    57,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning\n  through Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning\n  through Bayesian Optimization"
                },
                "summary": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization."
                },
                "authors": [
                    {
                        "name": "Bojana Ranković"
                    },
                    {
                        "name": "Philippe Schwaller"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Schwaller"
                },
                "author": "Philippe Schwaller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v1",
                "updated": "2025-04-08T17:59:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06260v1",
                "updated": "2025-04-08T17:59:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:59:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability"
                },
                "summary": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench"
                },
                "authors": [
                    {
                        "name": "Nayantara Mudur"
                    },
                    {
                        "name": "Hao Cui"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    },
                    {
                        "name": "Paul Raccuglia"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Peter Norgaard"
                    }
                ],
                "author_detail": {
                    "name": "Peter Norgaard"
                },
                "author": "Peter Norgaard",
                "arxiv_comment": "39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical\n  Reasoning and AI and Open-World Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06256v1",
                "updated": "2025-04-08T17:58:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    58,
                    47,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:58:47Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    58,
                    47,
                    1,
                    98,
                    0
                ],
                "title": "Transfer between Modalities with MetaQueries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer between Modalities with MetaQueries"
                },
                "summary": "Unified multimodal models aim to integrate understanding (text output) and\ngeneration (pixel output), but aligning these different modalities within a\nsingle architecture often demands complex training recipes and careful data\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\ndeep understanding and reasoning capabilities. Our method simplifies training,\nrequiring only paired image-caption data and standard diffusion objectives.\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\nthereby preserving its state-of-the-art multimodal understanding capabilities\nwhile achieving strong generative performance. Additionally, our method is\nflexible and can be easily instruction-tuned for advanced applications such as\nimage editing and subject-driven generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified multimodal models aim to integrate understanding (text output) and\ngeneration (pixel output), but aligning these different modalities within a\nsingle architecture often demands complex training recipes and careful data\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\ndeep understanding and reasoning capabilities. Our method simplifies training,\nrequiring only paired image-caption data and standard diffusion objectives.\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\nthereby preserving its state-of-the-art multimodal understanding capabilities\nwhile achieving strong generative performance. Additionally, our method is\nflexible and can be easily instruction-tuned for advanced applications such as\nimage editing and subject-driven generation."
                },
                "authors": [
                    {
                        "name": "Xichen Pan"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Jiuhai Chen"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Ji Hou"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Project Page: https://xichenpan.com/metaquery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03601v2",
                "updated": "2025-04-08T17:46:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    46,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-04T17:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay"
                },
                "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "12 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v3",
                "updated": "2025-04-09T08:33:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    33,
                    54,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars"
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Code: https://github.com/sprintml/privacy_attacks_against_iars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22250v2",
                "updated": "2025-04-08T17:25:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    25,
                    48,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-28T09:04:10Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    9,
                    4,
                    10,
                    4,
                    87,
                    0
                ],
                "title": "Modeling Challenging Patient Interactions: LLMs for Medical\n  Communication Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Challenging Patient Interactions: LLMs for Medical\n  Communication Training"
                },
                "summary": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective patient communication is pivotal in healthcare, yet traditional\nmedical training often lacks exposure to diverse, challenging interpersonal\ndynamics. To bridge this gap, this study proposes the use of Large Language\nModels (LLMs) to simulate authentic patient communication styles, specifically\nthe \"accuser\" and \"rationalizer\" personas derived from the Satir model, while\nalso ensuring multilingual applicability to accommodate diverse cultural\ncontexts and enhance accessibility for medical professionals. Leveraging\nadvanced prompt engineering, including behavioral prompts, author's notes, and\nstubbornness mechanisms, we developed virtual patients (VPs) that embody\nnuanced emotional and conversational traits. Medical professionals evaluated\nthese VPs, rating their authenticity (accuser: $3.8 \\pm 1.0$; rationalizer:\n$3.7 \\pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly\nidentifying their styles. Emotion analysis revealed distinct profiles: the\naccuser exhibited pain, anger, and distress, while the rationalizer displayed\ncontemplation and calmness, aligning with predefined, detailed patient\ndescription including medical history. Sentiment scores (on a scale from zero\nto nine) further validated these differences in the communication styles, with\nthe accuser adopting negative ($3.1 \\pm 0.6$) and the rationalizer more neutral\n($4.0 \\pm 0.4$) tone. These results underscore LLMs' capability to replicate\ncomplex communication styles, offering transformative potential for medical\neducation. This approach equips trainees to navigate challenging clinical\nscenarios by providing realistic, adaptable patient interactions, enhancing\nempathy and diagnostic acumen. Our findings advocate for AI-driven tools as\nscalable, cost-effective solutions to cultivate nuanced communication skills,\nsetting a foundation for future innovations in healthcare training."
                },
                "authors": [
                    {
                        "name": "Anna Bodonhelyi"
                    },
                    {
                        "name": "Christian Stegemann-Philipps"
                    },
                    {
                        "name": "Alessandra Sonanini"
                    },
                    {
                        "name": "Lea Herschbach"
                    },
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Anne Herrmann-Werner"
                    },
                    {
                        "name": "Teresa Festl-Wietek"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Friederike Holderried"
                    }
                ],
                "author_detail": {
                    "name": "Friederike Holderried"
                },
                "author": "Friederike Holderried",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02879v2",
                "updated": "2025-04-08T17:18:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    18,
                    21,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-03T18:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    7,
                    25,
                    3,
                    277,
                    0
                ],
                "title": "Position: LLM Unlearning Benchmarks are Weak Measures of Progress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: LLM Unlearning Benchmarks are Weak Measures of Progress"
                },
                "summary": "Unlearning methods have the potential to improve the privacy and safety of\nlarge language models (LLMs) by removing sensitive or harmful information post\nhoc. The LLM unlearning research community has increasingly turned toward\nempirical benchmarks to assess the effectiveness of such methods. In this\npaper, we find that existing benchmarks provide an overly optimistic and\npotentially misleading view on the effectiveness of candidate unlearning\nmethods. By introducing simple, benign modifications to a number of popular\nbenchmarks, we expose instances where supposedly unlearned information remains\naccessible, or where the unlearning process has degraded the model's\nperformance on retained information to a much greater extent than indicated by\nthe original benchmark. We identify that existing benchmarks are particularly\nvulnerable to modifications that introduce even loose dependencies between the\nforget and retain information. Further, we show that ambiguity in unlearning\ntargets in existing benchmarks can easily lead to the design of methods that\noverfit to the given test queries. Based on our findings, we urge the community\nto be cautious when interpreting benchmark results as reliable measures of\nprogress, and we provide several recommendations to guide future LLM unlearning\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning methods have the potential to improve the privacy and safety of\nlarge language models (LLMs) by removing sensitive or harmful information post\nhoc. The LLM unlearning research community has increasingly turned toward\nempirical benchmarks to assess the effectiveness of such methods. In this\npaper, we find that existing benchmarks provide an overly optimistic and\npotentially misleading view on the effectiveness of candidate unlearning\nmethods. By introducing simple, benign modifications to a number of popular\nbenchmarks, we expose instances where supposedly unlearned information remains\naccessible, or where the unlearning process has degraded the model's\nperformance on retained information to a much greater extent than indicated by\nthe original benchmark. We identify that existing benchmarks are particularly\nvulnerable to modifications that introduce even loose dependencies between the\nforget and retain information. Further, we show that ambiguity in unlearning\ntargets in existing benchmarks can easily lead to the design of methods that\noverfit to the given test queries. Based on our findings, we urge the community\nto be cautious when interpreting benchmark results as reliable measures of\nprogress, and we provide several recommendations to guide future LLM unlearning\nresearch."
                },
                "authors": [
                    {
                        "name": "Pratiksha Thaker"
                    },
                    {
                        "name": "Shengyuan Hu"
                    },
                    {
                        "name": "Neil Kale"
                    },
                    {
                        "name": "Yash Maurya"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "Appears in IEEE Secure and Trustworthy Machine Learning (SaTML) '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06227v1",
                "updated": "2025-04-08T17:16:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    16,
                    52,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:16:52Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    16,
                    52,
                    1,
                    98,
                    0
                ],
                "title": "LExT: Towards Evaluating Trustworthiness of Natural Language\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LExT: Towards Evaluating Trustworthiness of Natural Language\n  Explanations"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nhigh-stakes domains, there have been several approaches proposed toward\ngenerating natural language explanations. These explanations are crucial for\nenhancing the interpretability of a model, especially in sensitive domains like\nhealthcare, where transparency and reliability are key. In light of such\nexplanations being generated by LLMs and its known concerns, there is a growing\nneed for robust evaluation frameworks to assess model-generated explanations.\nNatural Language Generation metrics like BLEU and ROUGE capture syntactic and\nsemantic accuracies but overlook other crucial aspects such as factual\naccuracy, consistency, and faithfulness. To address this gap, we propose a\ngeneral framework for quantifying trustworthiness of natural language\nexplanations, balancing Plausibility and Faithfulness, to derive a\ncomprehensive Language Explanation Trustworthiness Score (LExT) (The code and\nset up to reproduce our experiments are publicly available at\nhttps://github.com/cerai-iitm/LExT). Applying our domain-agnostic framework to\nthe healthcare domain using public medical datasets, we evaluate six models,\nincluding domain-specific and general-purpose models. Our findings demonstrate\nsignificant differences in their ability to generate trustworthy explanations.\nOn comparing these explanations, we make interesting observations such as\ninconsistencies in Faithfulness demonstrated by general-purpose models and\ntheir tendency to outperform domain-specific fine-tuned models. This work\nfurther highlights the importance of using a tailored evaluation framework to\nassess natural language explanations in sensitive fields, providing a\nfoundation for improving the trustworthiness and transparency of language\nmodels in healthcare and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nhigh-stakes domains, there have been several approaches proposed toward\ngenerating natural language explanations. These explanations are crucial for\nenhancing the interpretability of a model, especially in sensitive domains like\nhealthcare, where transparency and reliability are key. In light of such\nexplanations being generated by LLMs and its known concerns, there is a growing\nneed for robust evaluation frameworks to assess model-generated explanations.\nNatural Language Generation metrics like BLEU and ROUGE capture syntactic and\nsemantic accuracies but overlook other crucial aspects such as factual\naccuracy, consistency, and faithfulness. To address this gap, we propose a\ngeneral framework for quantifying trustworthiness of natural language\nexplanations, balancing Plausibility and Faithfulness, to derive a\ncomprehensive Language Explanation Trustworthiness Score (LExT) (The code and\nset up to reproduce our experiments are publicly available at\nhttps://github.com/cerai-iitm/LExT). Applying our domain-agnostic framework to\nthe healthcare domain using public medical datasets, we evaluate six models,\nincluding domain-specific and general-purpose models. Our findings demonstrate\nsignificant differences in their ability to generate trustworthy explanations.\nOn comparing these explanations, we make interesting observations such as\ninconsistencies in Faithfulness demonstrated by general-purpose models and\ntheir tendency to outperform domain-specific fine-tuned models. This work\nfurther highlights the importance of using a tailored evaluation framework to\nassess natural language explanations in sensitive fields, providing a\nfoundation for improving the trustworthiness and transparency of language\nmodels in healthcare and beyond."
                },
                "authors": [
                    {
                        "name": "Krithi Shailya"
                    },
                    {
                        "name": "Shreya Rajpal"
                    },
                    {
                        "name": "Gokul S Krishnan"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06225v1",
                "updated": "2025-04-08T17:13:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    13,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:13:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    13,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via\n  Adaptation"
                },
                "summary": "While decoder-only large language models (LLMs) have shown impressive\nresults, encoder-decoder models are still widely adopted in real-world\napplications for their inference efficiency and richer encoder representation.\nIn this paper, we study a novel problem: adapting pretrained decoder-only LLMs\nto encoder-decoder, with the goal of leveraging the strengths of both\napproaches to achieve a more favorable quality-efficiency trade-off. We argue\nthat adaptation not only enables inheriting the capability of decoder-only LLMs\nbut also reduces the demand for computation compared to pretraining from\nscratch. We rigorously explore different pretraining objectives and parameter\ninitialization/optimization techniques. Through extensive experiments based on\nGemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to\n1.6B), we demonstrate the effectiveness of adaptation and the advantage of\nencoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs\nachieve comparable (often better) pretraining performance but substantially\nbetter finetuning performance than their decoder-only counterpart. For example,\nGemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning.\nEncoder-decoder adaptation also allows for flexible combination of\ndifferent-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B\nby $>$3\\%. The adapted encoder representation also yields better results on\nSuperGLUE. We will release our checkpoints to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While decoder-only large language models (LLMs) have shown impressive\nresults, encoder-decoder models are still widely adopted in real-world\napplications for their inference efficiency and richer encoder representation.\nIn this paper, we study a novel problem: adapting pretrained decoder-only LLMs\nto encoder-decoder, with the goal of leveraging the strengths of both\napproaches to achieve a more favorable quality-efficiency trade-off. We argue\nthat adaptation not only enables inheriting the capability of decoder-only LLMs\nbut also reduces the demand for computation compared to pretraining from\nscratch. We rigorously explore different pretraining objectives and parameter\ninitialization/optimization techniques. Through extensive experiments based on\nGemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to\n1.6B), we demonstrate the effectiveness of adaptation and the advantage of\nencoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs\nachieve comparable (often better) pretraining performance but substantially\nbetter finetuning performance than their decoder-only counterpart. For example,\nGemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning.\nEncoder-decoder adaptation also allows for flexible combination of\ndifferent-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B\nby $>$3\\%. The adapted encoder representation also yields better results on\nSuperGLUE. We will release our checkpoints to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Fedor Moiseev"
                    },
                    {
                        "name": "Joshua Ainslie"
                    },
                    {
                        "name": "Paul Suganthan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Fede Lebron"
                    },
                    {
                        "name": "Orhan Firat"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Zhe Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Dong"
                },
                "author": "Zhe Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15341v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15341v3",
                "updated": "2025-04-08T17:09:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    9,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2024-06-21T17:55:24Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    17,
                    55,
                    24,
                    4,
                    173,
                    0
                ],
                "title": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenoTEX: An LLM Agent Benchmark for Automated Gene Expression Data\n  Analysis"
                },
                "summary": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides analysis code and results for solving a wide\nrange of gene-trait association problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTEX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning have significantly improved the\nidentification of disease-associated genes from gene expression datasets.\nHowever, these processes often require extensive expertise and manual effort,\nlimiting their scalability. Large Language Model (LLM)-based agents have shown\npromise in automating these tasks due to their increasing problem-solving\nabilities. To support the evaluation and development of such methods, we\nintroduce GenoTEX, a benchmark dataset for the automated analysis of gene\nexpression data. GenoTEX provides analysis code and results for solving a wide\nrange of gene-trait association problems, encompassing dataset selection,\npreprocessing, and statistical analysis, in a pipeline that follows\ncomputational genomics standards. The benchmark includes expert-curated\nannotations from bioinformaticians to ensure accuracy and reliability. To\nprovide baselines for these tasks, we present GenoAgent, a team of LLM-based\nagents that adopt a multi-step programming workflow with flexible\nself-correction, to collaboratively analyze gene expression datasets. Our\nexperiments demonstrate the potential of LLM-based methods in analyzing genomic\ndata, while error analysis highlights the challenges and areas for future\nimprovement. We propose GenoTEX as a promising resource for benchmarking and\nenhancing automated methods for gene expression data analysis. The benchmark is\navailable at https://github.com/Liu-Hy/GenoTEX."
                },
                "authors": [
                    {
                        "name": "Haoyang Liu"
                    },
                    {
                        "name": "Shuyu Chen"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "31 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15341v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15341v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16520v3",
                "updated": "2025-04-08T17:08:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    8,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-21T21:21:29Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    21,
                    29,
                    0,
                    295,
                    0
                ],
                "title": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context"
                },
                "summary": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives."
                },
                "authors": [
                    {
                        "name": "Naba Rizvi"
                    },
                    {
                        "name": "Harper Strickland"
                    },
                    {
                        "name": "Daniel Gitelman"
                    },
                    {
                        "name": "Tristan Cooper"
                    },
                    {
                        "name": "Alexis Morales-Flores"
                    },
                    {
                        "name": "Michael Golden"
                    },
                    {
                        "name": "Aekta Kallepalli"
                    },
                    {
                        "name": "Akshat Alurkar"
                    },
                    {
                        "name": "Haaset Owens"
                    },
                    {
                        "name": "Saleha Ahmedi"
                    },
                    {
                        "name": "Isha Khirwadkar"
                    },
                    {
                        "name": "Imani Munyaka"
                    },
                    {
                        "name": "Nedjma Ousidhoum"
                    }
                ],
                "author_detail": {
                    "name": "Nedjma Ousidhoum"
                },
                "author": "Nedjma Ousidhoum",
                "arxiv_comment": "9 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06219v1",
                "updated": "2025-04-08T17:08:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    8,
                    6,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T17:08:06Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    8,
                    6,
                    1,
                    98,
                    0
                ],
                "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling\n  Opt-Outs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling\n  Opt-Outs"
                },
                "summary": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of web crawling opt-outs by copyright holders of\nonline content raises critical questions about the impact of data compliance on\nlarge language model (LLM) performance. However, little is known about how\nthese restrictions (and the resultant filtering of pretraining datasets) affect\nthe capabilities of models trained using these corpora. In this work, we\nconceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which\nquantifies the performance difference between models trained on datasets that\ncomply with web crawling opt-outs, and those that do not. We measure the data\ncompliance gap in two settings: pretraining models from scratch and continual\npretraining from existing compliant models (simulating a setting where\ncopyrighted data could be integrated later in pretraining). Our experiments\nwith 1.5B models show that, as of January 2025, compliance with web data\nopt-outs does not degrade general knowledge acquisition (close to 0\\% DCG).\nHowever, in specialized domains such as biomedical research, excluding major\npublishers leads to performance declines. These findings suggest that while\ngeneral-purpose LLMs can be trained to perform equally well using fully open\ndata, performance in specialized domains may benefit from access to\nhigh-quality copyrighted sources later in training. Our study provides\nempirical insights into the long-debated trade-off between data compliance and\ndownstream model performance, informing future discussions on AI training\npractices and policy decisions."
                },
                "authors": [
                    {
                        "name": "Dongyang Fan"
                    },
                    {
                        "name": "Vinko Sabolčec"
                    },
                    {
                        "name": "Matin Ansaripour"
                    },
                    {
                        "name": "Ayush Kumar Tarun"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Imanol Schlag"
                    }
                ],
                "author_detail": {
                    "name": "Imanol Schlag"
                },
                "author": "Imanol Schlag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17560v2",
                "updated": "2025-04-08T16:59:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    59,
                    9,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-24T19:00:02Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    0,
                    2,
                    0,
                    55,
                    0
                ],
                "title": "Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet\n  EXplorer (UVEX)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet\n  EXplorer (UVEX)"
                },
                "summary": "The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope\nselected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.\nUVEX will undertake deep, cadenced surveys of the entire sky to probe low mass\ngalaxies and explore the ultraviolet (UV) time-domain sky, and it will carry\nthe first rapidly deployable UV spectroscopic capability for a broad range of\nscience applications. One of UVEX's prime objectives is to follow up\ngravitational wave (GW) binary neutron star mergers as targets of opportunity\n(ToOs), rapidly scanning across their localization regions to search for their\nkilonova (KN) counterparts. Early-time multiband ultraviolet light curves of\nKNe are key to explaining the interplay between jet and ejecta in binary\nneutron star mergers. Owing to high Galactic extinction in the ultraviolet and\nthe variation of GW distance estimates over the sky, the sensitivity to\nkilonovae can vary significantly across the GW localization and even across the\nfootprint of a single image given UVEX's large field of view. Good ToO\nobserving strategies to trade off between area and depth are neither simple nor\nobvious. We present an optimal strategy for GW follow-up with UVEX in which\nexposure time is adjusted dynamically for each field individually to maximize\nthe overall probability of detection. We model the scheduling problem using the\nexpressive and powerful mathematical framework of mixed integer linear\nprogramming (MILP), and employ a state-of-the-art MILP solver to automatically\ngenerate observing plan timelines that achieve high probabilities of kilonova\ndetection. We have implemented this strategy in an open-source astronomical\nscheduling software package called the Multi-Mission Multi-Messenger\nObservation Planning Toolkit (M4OPT), on GitHub at\nhttps://github.com/m4opt/m4opt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope\nselected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.\nUVEX will undertake deep, cadenced surveys of the entire sky to probe low mass\ngalaxies and explore the ultraviolet (UV) time-domain sky, and it will carry\nthe first rapidly deployable UV spectroscopic capability for a broad range of\nscience applications. One of UVEX's prime objectives is to follow up\ngravitational wave (GW) binary neutron star mergers as targets of opportunity\n(ToOs), rapidly scanning across their localization regions to search for their\nkilonova (KN) counterparts. Early-time multiband ultraviolet light curves of\nKNe are key to explaining the interplay between jet and ejecta in binary\nneutron star mergers. Owing to high Galactic extinction in the ultraviolet and\nthe variation of GW distance estimates over the sky, the sensitivity to\nkilonovae can vary significantly across the GW localization and even across the\nfootprint of a single image given UVEX's large field of view. Good ToO\nobserving strategies to trade off between area and depth are neither simple nor\nobvious. We present an optimal strategy for GW follow-up with UVEX in which\nexposure time is adjusted dynamically for each field individually to maximize\nthe overall probability of detection. We model the scheduling problem using the\nexpressive and powerful mathematical framework of mixed integer linear\nprogramming (MILP), and employ a state-of-the-art MILP solver to automatically\ngenerate observing plan timelines that achieve high probabilities of kilonova\ndetection. We have implemented this strategy in an open-source astronomical\nscheduling software package called the Multi-Mission Multi-Messenger\nObservation Planning Toolkit (M4OPT), on GitHub at\nhttps://github.com/m4opt/m4opt."
                },
                "authors": [
                    {
                        "name": "Leo P. Singer"
                    },
                    {
                        "name": "Alexander W. Criswell"
                    },
                    {
                        "name": "Sydney C. Leggio"
                    },
                    {
                        "name": "R. Weizmann Kiendrebeogo"
                    },
                    {
                        "name": "Michael W. Coughlin"
                    },
                    {
                        "name": "Hannah P. Earnshaw"
                    },
                    {
                        "name": "Suvi Gezari"
                    },
                    {
                        "name": "Brian W. Grefenstette"
                    },
                    {
                        "name": "Fiona A. Harrison"
                    },
                    {
                        "name": "Mansi M. Kasliwal"
                    },
                    {
                        "name": "Brett M. Morris"
                    },
                    {
                        "name": "Erik Tollerud"
                    },
                    {
                        "name": "S. Bradley Cenko"
                    }
                ],
                "author_detail": {
                    "name": "S. Bradley Cenko"
                },
                "author": "S. Bradley Cenko",
                "arxiv_comment": "For data release, see https://zenodo.org/records/15176276",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06214v1",
                "updated": "2025-04-08T16:58:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    58,
                    58,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:58:58Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    58,
                    58,
                    1,
                    98,
                    0
                ],
                "title": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 128K to 4M: Efficient Training of Ultra-Long Context Large Language\n  Models"
                },
                "summary": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context capabilities are essential for a wide range of applications,\nincluding document and video understanding, in-context learning, and\ninference-time scaling, all of which require models to process and reason over\nlong sequences of text and multimodal data. In this work, we introduce a\nefficient training recipe for building ultra-long context LLMs from aligned\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\nand 4M tokens. Our approach leverages efficient continued pretraining\nstrategies to extend the context window and employs effective instruction\ntuning to maintain the instruction-following and reasoning abilities. Our\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\nstate-of-the-art performance across a diverse set of long-context benchmarks.\nImportantly, models trained with our approach maintain competitive performance\non standard benchmarks, demonstrating balanced improvements for both long and\nshort context tasks. We further provide an in-depth analysis of key design\nchoices, highlighting the impacts of scaling strategies and data composition.\nOur findings establish a robust framework for efficiently scaling context\nlengths while preserving general model capabilities. We release all model\nweights at: https://ultralong.github.io/."
                },
                "authors": [
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00509v2",
                "updated": "2025-04-08T16:51:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    51,
                    11,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-01T07:57:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    57,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?"
                },
                "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs."
                },
                "authors": [
                    {
                        "name": "Kai Yan"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Xiaowen Guo"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "arxiv_comment": "23 pages, 3 figures, 10 tables. V2 refines related work and\n  acknowledgement, and adds links to chat logs for qualitative studies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06196v1",
                "updated": "2025-04-08T16:39:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    39,
                    2,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:39:02Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    39,
                    2,
                    1,
                    98,
                    0
                ],
                "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TxGemma: Efficient and Agentic LLMs for Therapeutics"
                },
                "summary": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high)."
                },
                "authors": [
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Paul F. Jaeger"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Rory Pilgrim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "David Fleet"
                    },
                    {
                        "name": "Shekoofeh Azizi"
                    }
                ],
                "author_detail": {
                    "name": "Shekoofeh Azizi"
                },
                "author": "Shekoofeh Azizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06473v3",
                "updated": "2025-04-08T16:32:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    32,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-09T02:00:37Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    0,
                    37,
                    2,
                    283,
                    0
                ],
                "title": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAPPA: Generalizing and Adapting Robot Policies via Online Agentic\n  Guidance"
                },
                "summary": "Robot learning approaches such as behavior cloning and reinforcement learning\nhave shown great promise in synthesizing robot skills from human demonstrations\nin specific environments. However, these approaches often require task-specific\ndemonstrations or designing complex simulation environments, which limits the\ndevelopment of generalizable and robust policies for unseen real-world\nsettings. Recent advances in the use of foundation models for robotics (e.g.,\nLLMs, VLMs) have shown great potential in enabling systems to understand the\nsemantics in the world from large-scale internet data. However, it remains an\nopen challenge to use this knowledge to enable robotic systems to understand\nthe underlying dynamics of the world, to generalize policies across different\ntasks, and to adapt policies to new environments. To alleviate these\nlimitations, we propose an agentic framework for robot self-guidance and\nself-improvement, which consists of a set of role-specialized conversational\nagents, such as a high-level advisor, a grounding agent, a monitoring agent,\nand a robotic agent. Our framework iteratively grounds a base robot policy to\nrelevant objects in the environment and uses visuomotor cues to shift the\naction distribution of the policy to more desirable states, online, while\nremaining agnostic to the subjective configuration of a given robot hardware\nplatform. We demonstrate that our approach can effectively guide manipulation\npolicies to achieve significantly higher success rates, both in simulation and\nin real-world experiments, without the need for additional human demonstrations\nor extensive exploration. Code and videos available at:\nhttps://agenticrobots.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot learning approaches such as behavior cloning and reinforcement learning\nhave shown great promise in synthesizing robot skills from human demonstrations\nin specific environments. However, these approaches often require task-specific\ndemonstrations or designing complex simulation environments, which limits the\ndevelopment of generalizable and robust policies for unseen real-world\nsettings. Recent advances in the use of foundation models for robotics (e.g.,\nLLMs, VLMs) have shown great potential in enabling systems to understand the\nsemantics in the world from large-scale internet data. However, it remains an\nopen challenge to use this knowledge to enable robotic systems to understand\nthe underlying dynamics of the world, to generalize policies across different\ntasks, and to adapt policies to new environments. To alleviate these\nlimitations, we propose an agentic framework for robot self-guidance and\nself-improvement, which consists of a set of role-specialized conversational\nagents, such as a high-level advisor, a grounding agent, a monitoring agent,\nand a robotic agent. Our framework iteratively grounds a base robot policy to\nrelevant objects in the environment and uses visuomotor cues to shift the\naction distribution of the policy to more desirable states, online, while\nremaining agnostic to the subjective configuration of a given robot hardware\nplatform. We demonstrate that our approach can effectively guide manipulation\npolicies to achieve significantly higher success rates, both in simulation and\nin real-world experiments, without the need for additional human demonstrations\nor extensive exploration. Code and videos available at:\nhttps://agenticrobots.github.io"
                },
                "authors": [
                    {
                        "name": "Arthur Bucker"
                    },
                    {
                        "name": "Pablo Ortega-Kral"
                    },
                    {
                        "name": "Jonathan Francis"
                    },
                    {
                        "name": "Jean Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jean Oh"
                },
                "author": "Jean Oh",
                "arxiv_comment": "21 pages, 12 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06185v1",
                "updated": "2025-04-08T16:25:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    25,
                    59,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T16:25:59Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    25,
                    59,
                    1,
                    98,
                    0
                ],
                "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and\n  Real-World Wound Care"
                },
                "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication."
                },
                "authors": [
                    {
                        "name": "Vanessa Borst"
                    },
                    {
                        "name": "Timo Dittus"
                    },
                    {
                        "name": "Tassilo Dege"
                    },
                    {
                        "name": "Astrid Schmieder"
                    },
                    {
                        "name": "Samuel Kounev"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kounev"
                },
                "author": "Samuel Kounev",
                "arxiv_comment": "Main paper: 17 pages; supplementary material: 16 pages; paper\n  submitted to the application track of the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases\n  (ECML PKDD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04794v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04794v2",
                "updated": "2025-04-08T16:16:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    16,
                    16,
                    30,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-07T15:36:05Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    15,
                    36,
                    5,
                    3,
                    312,
                    0
                ],
                "title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowCoder-X: Boosting Multilingual Information Extraction via Code"
                },
                "summary": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in IE,\na significant imbalance across languages persists, highlighting an underlying\ndeficiency. To address this, we propose KnowCoder-X, a powerful code LLM with\nadvanced cross-lingual and multilingual capabilities for universal information\nextraction. Firstly, it standardizes the representation of multilingual schemas\nusing Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we enhance the model's cross-lingual transferability through IE\ncross-lingual alignment instruction tuning on a translated instance prediction\ntask we proposed. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by $30.17\\%$ and SoTA by $20.03\\%$, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in IE,\na significant imbalance across languages persists, highlighting an underlying\ndeficiency. To address this, we propose KnowCoder-X, a powerful code LLM with\nadvanced cross-lingual and multilingual capabilities for universal information\nextraction. Firstly, it standardizes the representation of multilingual schemas\nusing Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we enhance the model's cross-lingual transferability through IE\ncross-lingual alignment instruction tuning on a translated instance prediction\ntask we proposed. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by $30.17\\%$ and SoTA by $20.03\\%$, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Wenxuan Jiang"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04794v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04794v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06160v2",
                "updated": "2025-04-09T04:24:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    24,
                    38,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T15:56:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    56,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\n  Narratives Targeting Mental Health Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\n  Narratives Targeting Mental Health Groups"
                },
                "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."
                },
                "authors": [
                    {
                        "name": "Rijul Magu"
                    },
                    {
                        "name": "Arka Dutta"
                    },
                    {
                        "name": "Sean Kim"
                    },
                    {
                        "name": "Ashiqur R. KhudaBukhsh"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; K.4.1; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06154v1",
                "updated": "2025-04-08T15:48:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    48,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    48,
                    26,
                    1,
                    98,
                    0
                ],
                "title": "Exploring Adversarial Obstacle Attacks in Search-based Path Planning for\n  Autonomous Mobile Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Adversarial Obstacle Attacks in Search-based Path Planning for\n  Autonomous Mobile Robots"
                },
                "summary": "Path planning algorithms, such as the search-based A*, are a critical\ncomponent of autonomous mobile robotics, enabling robots to navigate from a\nstarting point to a destination efficiently and safely. We investigated the\nresilience of the A* algorithm in the face of potential adversarial\ninterventions known as obstacle attacks. The adversary's goal is to delay the\nrobot's timely arrival at its destination by introducing obstacles along its\noriginal path.\n  We developed malicious software to execute the attacks and conducted\nexperiments to assess their impact, both in simulation using TurtleBot in\nGazebo and in real-world deployment with the Unitree Go1 robot. In simulation,\nthe attacks resulted in an average delay of 36\\%, with the most significant\ndelays occurring in scenarios where the robot was forced to take substantially\nlonger alternative paths. In real-world experiments, the delays were even more\npronounced, with all attacks successfully rerouting the robot and causing\nmeasurable disruptions. These results highlight that the algorithm's robustness\nis not solely an attribute of its design but is significantly influenced by the\noperational environment. For example, in constrained environments like tunnels,\nthe delays were maximized due to the limited availability of alternative\nroutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path planning algorithms, such as the search-based A*, are a critical\ncomponent of autonomous mobile robotics, enabling robots to navigate from a\nstarting point to a destination efficiently and safely. We investigated the\nresilience of the A* algorithm in the face of potential adversarial\ninterventions known as obstacle attacks. The adversary's goal is to delay the\nrobot's timely arrival at its destination by introducing obstacles along its\noriginal path.\n  We developed malicious software to execute the attacks and conducted\nexperiments to assess their impact, both in simulation using TurtleBot in\nGazebo and in real-world deployment with the Unitree Go1 robot. In simulation,\nthe attacks resulted in an average delay of 36\\%, with the most significant\ndelays occurring in scenarios where the robot was forced to take substantially\nlonger alternative paths. In real-world experiments, the delays were even more\npronounced, with all attacks successfully rerouting the robot and causing\nmeasurable disruptions. These results highlight that the algorithm's robustness\nis not solely an attribute of its design but is significantly influenced by the\noperational environment. For example, in constrained environments like tunnels,\nthe delays were maximized due to the limited availability of alternative\nroutes."
                },
                "authors": [
                    {
                        "name": "Adrian Szvoren"
                    },
                    {
                        "name": "Jianwei Liu"
                    },
                    {
                        "name": "Dimitrios Kanoulas"
                    },
                    {
                        "name": "Nilufer Tuptuk"
                    }
                ],
                "author_detail": {
                    "name": "Nilufer Tuptuk"
                },
                "author": "Nilufer Tuptuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16587v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16587v3",
                "updated": "2025-04-08T15:47:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    47,
                    13,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-25T17:22:10Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    17,
                    22,
                    10,
                    0,
                    330,
                    0
                ],
                "title": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Decision-making for COLREGs and the Control\n  of Autonomous Surface Vehicles"
                },
                "summary": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of autonomous surface vehicles (ASVs), devising decision-making\nand obstacle avoidance solutions that address maritime COLREGs (Collision\nRegulations), primarily defined for human operators, has long been a pressing\nchallenge. Recent advancements in explainable Artificial Intelligence (AI) and\nmachine learning have shown promise in enabling human-like decision-making.\nNotably, significant developments have occurred in the application of Large\nLanguage Models (LLMs) to the decision-making of complex systems, such as\nself-driving cars. The textual and somewhat ambiguous nature of COLREGs (from\nan algorithmic perspective), however, poses challenges that align well with the\ncapabilities of LLMs, suggesting that LLMs may become increasingly suitable for\nthis application soon. This paper presents and demonstrates the first\napplication of LLM-based decision-making and control for ASVs. The proposed\nmethod establishes a high-level decision-maker that uses online collision risk\nindices and key measurements to make decisions for safe manoeuvres. A tailored\ndesign and runtime structure is developed to support training and real-time\naction generation on a realistic ASV model. Local planning and control\nalgorithms are integrated to execute the commands for waypoint following and\ncollision avoidance at a lower level. To the authors' knowledge, this study\nrepresents the first attempt to apply explainable AI to the dynamic control\nproblem of maritime systems recognising the COLREGs rules, opening new avenues\nfor research in this challenging area. Results obtained across multiple test\nscenarios demonstrate the system's ability to maintain online COLREGs\ncompliance, accurate waypoint tracking, and feasible control, while providing\nhuman-interpretable reasoning for each decision."
                },
                "authors": [
                    {
                        "name": "Klinsmann Agyei"
                    },
                    {
                        "name": "Pouria Sarhadi"
                    },
                    {
                        "name": "Wasif Naeem"
                    }
                ],
                "author_detail": {
                    "name": "Wasif Naeem"
                },
                "author": "Wasif Naeem",
                "arxiv_comment": "This work has been accepted for publication at European Control\n  Conference 2025, \\c{opyright} IEEE 2025. Please cite the published version\n  when available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16587v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16587v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05173v2",
                "updated": "2025-04-08T15:41:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    41,
                    5,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T15:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    17,
                    35,
                    0,
                    97,
                    0
                ],
                "title": "PRDTs: Composable Knowledge-Based Consensus Protocols with Replicated\n  Data Types",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRDTs: Composable Knowledge-Based Consensus Protocols with Replicated\n  Data Types"
                },
                "summary": "Consensus protocols are fundamental in distributed systems as they enable\nsoftware with strong consistency properties. However, designing optimized\nprotocols for specific use-cases under certain system assumptions is typically\na laborious and error-prone process requiring expert knowledge. While most\nrecent optimized protocols are variations of well-known algorithms like Paxos\nor Raft, they often necessitate complete re-implementations, potentially\nintroducing new bugs and complicating the application of existing verification\nresults. This approach stands in the way of application-specific consistency\nprotocols that can easily be amended or swapped out, depending on the given\napplication and deployment scenario.\n  We propose Protocol Replicated Data Types (PRDTs), a novel programming model\nfor implementing consensus protocols using replicated data types (RDTs).\nInspired by the knowledge-based view of consensus, PRDTs employ RDTs to\nmonotonically accumulate knowledge until agreement is reached. This approach\nallows for implementations focusing on high-level protocol logic with minimal\nnetwork environment assumptions. Moreover, by applying existing algebraic\ncomposition techniques for RDTs in the PRDT context, we enable composable\nprotocol building-blocks for implementing complex protocols. We present a\nformal model of our approach, demonstrate its application in PRDT-based\nimplementations of existing protocols, and report empirical evaluation results.\nOur findings indicate that the PRDT approach offers enhanced flexibility and\ncomposability in protocol design, facilitates reasoning about correctness, and\ndoes not suffer from inherent performance limitations that would prevent its\nuse in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus protocols are fundamental in distributed systems as they enable\nsoftware with strong consistency properties. However, designing optimized\nprotocols for specific use-cases under certain system assumptions is typically\na laborious and error-prone process requiring expert knowledge. While most\nrecent optimized protocols are variations of well-known algorithms like Paxos\nor Raft, they often necessitate complete re-implementations, potentially\nintroducing new bugs and complicating the application of existing verification\nresults. This approach stands in the way of application-specific consistency\nprotocols that can easily be amended or swapped out, depending on the given\napplication and deployment scenario.\n  We propose Protocol Replicated Data Types (PRDTs), a novel programming model\nfor implementing consensus protocols using replicated data types (RDTs).\nInspired by the knowledge-based view of consensus, PRDTs employ RDTs to\nmonotonically accumulate knowledge until agreement is reached. This approach\nallows for implementations focusing on high-level protocol logic with minimal\nnetwork environment assumptions. Moreover, by applying existing algebraic\ncomposition techniques for RDTs in the PRDT context, we enable composable\nprotocol building-blocks for implementing complex protocols. We present a\nformal model of our approach, demonstrate its application in PRDT-based\nimplementations of existing protocols, and report empirical evaluation results.\nOur findings indicate that the PRDT approach offers enhanced flexibility and\ncomposability in protocol design, facilitates reasoning about correctness, and\ndoes not suffer from inherent performance limitations that would prevent its\nuse in real-world applications."
                },
                "authors": [
                    {
                        "name": "Julian Haas"
                    },
                    {
                        "name": "Ragnar Mogk"
                    },
                    {
                        "name": "Annette Bieniusa"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06143v1",
                "updated": "2025-04-08T15:38:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    38,
                    42,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:38:42Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    38,
                    42,
                    1,
                    98,
                    0
                ],
                "title": "ARLO: A Tailorable Approach for Transforming Natural Language Software\n  Requirements into Architecture using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARLO: A Tailorable Approach for Transforming Natural Language Software\n  Requirements into Architecture using LLMs"
                },
                "summary": "Software requirements expressed in natural language (NL) frequently suffer\nfrom verbosity, ambiguity, and inconsistency. This creates a range of\nchallenges, including selecting an appropriate architecture for a system and\nassessing different architectural alternatives. Relying on human expertise to\naccomplish the task of mapping NL requirements to architecture is\ntime-consuming and error-prone. This paper proposes ARLO, an approach that\nautomates this task by leveraging (1) a set of NL requirements for a system,\n(2) an existing standard that specifies architecturally relevant software\nquality attributes, and (3) a readily available Large Language Model (LLM).\nSpecifically, ARLO determines the subset of NL requirements for a given system\nthat is architecturally relevant and maps that subset to a tailorable matrix of\narchitectural choices. ARLO applies integer linear programming on the\narchitectural-choice matrix to determine the optimal architecture for the\ncurrent requirements. We demonstrate ARLO's efficacy using a set of real-world\nexamples. We highlight ARLO's ability (1) to trace the selected architectural\nchoices to the requirements and (2) to isolate NL requirements that exert a\nparticular influence on a system's architecture. This allows the\nidentification, comparative assessment, and exploration of alternative\narchitectural choices based on the requirements and constraints expressed\ntherein.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software requirements expressed in natural language (NL) frequently suffer\nfrom verbosity, ambiguity, and inconsistency. This creates a range of\nchallenges, including selecting an appropriate architecture for a system and\nassessing different architectural alternatives. Relying on human expertise to\naccomplish the task of mapping NL requirements to architecture is\ntime-consuming and error-prone. This paper proposes ARLO, an approach that\nautomates this task by leveraging (1) a set of NL requirements for a system,\n(2) an existing standard that specifies architecturally relevant software\nquality attributes, and (3) a readily available Large Language Model (LLM).\nSpecifically, ARLO determines the subset of NL requirements for a given system\nthat is architecturally relevant and maps that subset to a tailorable matrix of\narchitectural choices. ARLO applies integer linear programming on the\narchitectural-choice matrix to determine the optimal architecture for the\ncurrent requirements. We demonstrate ARLO's efficacy using a set of real-world\nexamples. We highlight ARLO's ability (1) to trace the selected architectural\nchoices to the requirements and (2) to isolate NL requirements that exert a\nparticular influence on a system's architecture. This allows the\nidentification, comparative assessment, and exploration of alternative\narchitectural choices based on the requirements and constraints expressed\ntherein."
                },
                "authors": [
                    {
                        "name": "Tooraj Helmi"
                    }
                ],
                "author_detail": {
                    "name": "Tooraj Helmi"
                },
                "author": "Tooraj Helmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06136v1",
                "updated": "2025-04-08T15:32:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    32,
                    9,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:32:09Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    32,
                    9,
                    1,
                    98,
                    0
                ],
                "title": "QGen Studio: An Adaptive Question-Answer Generation, Training and\n  Evaluation Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QGen Studio: An Adaptive Question-Answer Generation, Training and\n  Evaluation Platform"
                },
                "summary": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally."
                },
                "authors": [
                    {
                        "name": "Movina Moses"
                    },
                    {
                        "name": "Mohab Elkaref"
                    },
                    {
                        "name": "James Barry"
                    },
                    {
                        "name": "Shinnosuke Tanaka"
                    },
                    {
                        "name": "Vishnudev Kuruvanthodi"
                    },
                    {
                        "name": "Nathan Herr"
                    },
                    {
                        "name": "Campbell D Watson"
                    },
                    {
                        "name": "Geeth De Mel"
                    }
                ],
                "author_detail": {
                    "name": "Geeth De Mel"
                },
                "author": "Geeth De Mel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16260v2",
                "updated": "2025-04-08T15:19:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    19,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-25T10:23:11Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    10,
                    23,
                    11,
                    0,
                    330,
                    0
                ],
                "title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures"
                },
                "summary": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks\nstepwise. However, training CoT capabilities requires detailed reasoning data,\nwhich is often scarce. The self-taught reasoner (STaR) framework addresses this\nby using reinforcement learning to automatically generate reasoning steps,\nreducing reliance on human-labeled data. Although STaR and its variants have\ndemonstrated empirical success, a theoretical foundation explaining these\nimprovements is lacking. Large language models (LLMs) have demonstrated\nremarkable mathematical capabilities, largely driven by chain-of-thought (CoT)\nprompting, which decomposes complex reasoning into step-by-step solutions.\nHowever, the mechanisms underlying LLMs' ability to perform arithmetic in a\nsingle step of CoT remain poorly understood. In this work, we propose that LLMs\nlearn arithmetic by capturing algebraic structures, such as commutativity and\nidentity properties. Since these structures are observable through input-output\nrelationships, they can generalize to unseen data. We empirically demonstrate\nthat LLMs can learn algebraic structures using a custom dataset of arithmetic\nproblems, as well as providing theoretical evidence showing that, under\nspecific configurations of weights and biases, the transformer-based LLMs can\ngenerate embeddings that remain invariant to both permutations of input tokens\nand the presence of identity elements. Our findings indicate that leveraging\nalgebraic structures can enhance the LLMs' arithmetic capabilities, offering\ninsights into improving their arithmetic performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks\nstepwise. However, training CoT capabilities requires detailed reasoning data,\nwhich is often scarce. The self-taught reasoner (STaR) framework addresses this\nby using reinforcement learning to automatically generate reasoning steps,\nreducing reliance on human-labeled data. Although STaR and its variants have\ndemonstrated empirical success, a theoretical foundation explaining these\nimprovements is lacking. Large language models (LLMs) have demonstrated\nremarkable mathematical capabilities, largely driven by chain-of-thought (CoT)\nprompting, which decomposes complex reasoning into step-by-step solutions.\nHowever, the mechanisms underlying LLMs' ability to perform arithmetic in a\nsingle step of CoT remain poorly understood. In this work, we propose that LLMs\nlearn arithmetic by capturing algebraic structures, such as commutativity and\nidentity properties. Since these structures are observable through input-output\nrelationships, they can generalize to unseen data. We empirically demonstrate\nthat LLMs can learn algebraic structures using a custom dataset of arithmetic\nproblems, as well as providing theoretical evidence showing that, under\nspecific configurations of weights and biases, the transformer-based LLMs can\ngenerate embeddings that remain invariant to both permutations of input tokens\nand the presence of identity elements. Our findings indicate that leveraging\nalgebraic structures can enhance the LLMs' arithmetic capabilities, offering\ninsights into improving their arithmetic performance."
                },
                "authors": [
                    {
                        "name": "Fu-Chieh Chang"
                    },
                    {
                        "name": "You-Chen Lin"
                    },
                    {
                        "name": "Pei-Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yuan Wu"
                },
                "author": "Pei-Yuan Wu",
                "arxiv_journal_ref": "ICLR 2025 Workshop on Reasoning and Planning for Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06122v2",
                "updated": "2025-04-09T04:03:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    3,
                    0,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T15:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    15,
                    26,
                    1,
                    98,
                    0
                ],
                "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning"
                },
                "summary": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details."
                },
                "authors": [
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06114v1",
                "updated": "2025-04-08T15:06:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    6,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T15:06:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    6,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Computing for Community-Based Economies: A Sociotechnical Ecosystem for\n  Democratic, Egalitarian and Sustainable Futures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing for Community-Based Economies: A Sociotechnical Ecosystem for\n  Democratic, Egalitarian and Sustainable Futures"
                },
                "summary": "Automation and industrial mass production, particularly in sectors with low\nwages, have harmful consequences that contribute to widening wealth\ndisparities, excessive pollution, and worsened working conditions. Coupled with\na mass consumption society, there is a risk of detrimental social outcomes and\nthreats to democracy, such as misinformation and political polarization. But\nAI, robotics and other emerging technologies could also provide a transition to\ncommunity-based economies, in which more democratic, egalitarian, and\nsustainable value circulations can be established. Based on both a review of\ncase studies, and our own experiments in Detroit, we derive three core\nprinciples for the use of computing in community-based economies. The\nprefigurative principle requires that the development process itself\nincorporates equity goals, rather than viewing equity as something to be\nachieved in the future. The generative principle requires the prevention of\nvalue extraction, and its replacement by circulations in which value is\nreturned back to the aspects of labor, nature, and society by which it is\ngenerated. And third, the solidarity principle requires that deployments at all\nscales and across all domains support both individual freedoms and\nopportunities for mutual aid. Thus we propose the use of computational\ntechnologies to develop a specifically generative form of community-based\neconomy: one that is egalitarian regarding race, class and gender; sustainable\nboth environmentally and socially; and democratic in the deep sense of putting\npeople in control of their own lives and livelihoods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation and industrial mass production, particularly in sectors with low\nwages, have harmful consequences that contribute to widening wealth\ndisparities, excessive pollution, and worsened working conditions. Coupled with\na mass consumption society, there is a risk of detrimental social outcomes and\nthreats to democracy, such as misinformation and political polarization. But\nAI, robotics and other emerging technologies could also provide a transition to\ncommunity-based economies, in which more democratic, egalitarian, and\nsustainable value circulations can be established. Based on both a review of\ncase studies, and our own experiments in Detroit, we derive three core\nprinciples for the use of computing in community-based economies. The\nprefigurative principle requires that the development process itself\nincorporates equity goals, rather than viewing equity as something to be\nachieved in the future. The generative principle requires the prevention of\nvalue extraction, and its replacement by circulations in which value is\nreturned back to the aspects of labor, nature, and society by which it is\ngenerated. And third, the solidarity principle requires that deployments at all\nscales and across all domains support both individual freedoms and\nopportunities for mutual aid. Thus we propose the use of computational\ntechnologies to develop a specifically generative form of community-based\neconomy: one that is egalitarian regarding race, class and gender; sustainable\nboth environmentally and socially; and democratic in the deep sense of putting\npeople in control of their own lives and livelihoods."
                },
                "authors": [
                    {
                        "name": "Kwame Porter Robinson"
                    },
                    {
                        "name": "Ron Eglash"
                    },
                    {
                        "name": "Lionel Robert"
                    },
                    {
                        "name": "Audrey Bennett"
                    },
                    {
                        "name": "Mark Guzdial"
                    },
                    {
                        "name": "Michael Nayebare"
                    }
                ],
                "author_detail": {
                    "name": "Michael Nayebare"
                },
                "author": "Michael Nayebare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00936v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00936v5",
                "updated": "2025-04-08T14:47:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    47,
                    7,
                    1,
                    98,
                    0
                ],
                "published": "2024-07-01T03:37:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    3,
                    37,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey"
                },
                "summary": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Leong Hou U"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Wenbin Guo"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Guo"
                },
                "author": "Wenbin Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00936v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00936v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06095v1",
                "updated": "2025-04-08T14:35:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    35,
                    40,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:35:40Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    35,
                    40,
                    1,
                    98,
                    0
                ],
                "title": "Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for\n  Scaled-up LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for\n  Scaled-up LLM Training"
                },
                "summary": "LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and\nmodel-parallel (MP) execution. Critical to achieving efficiency is\ntensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of\nGPUs, referred to as a scale-up domain, and the larger the scale-up domain the\nbetter the performance. New datacenter architectures are emerging with more\nGPUs able to be tightly-coupled in a scale-up domain, such as moving from 8\nGPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains\nincrease the blast-radius of failures, with a failure of single GPU potentially\nimpacting TP execution on the full scale-up domain, which can degrade overall\nLLM training throughput dramatically. With as few as 0.1% of GPUs being in a\nfailed state, a high TP-degree job can experience nearly 10% reduction in LLM\ntraining throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate\nthis amplified impact of GPU failures. In NTP, a DP replica that experiences\nGPU failures operates at a reduced TP degree, contributing throughput equal to\nthe percentage of still-functional GPUs. We also propose a rack-design with\nimproved electrical and thermal capabilities in order to sustain power-boosting\nof scale-up domains that have experienced failures; combined with NTP, this can\nallow the DP replica with the reduced TP degree (i.e., with failed GPUs) to\nkeep up with the others, thereby achieving near-zero throughput loss for\nlarge-scale LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and\nmodel-parallel (MP) execution. Critical to achieving efficiency is\ntensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of\nGPUs, referred to as a scale-up domain, and the larger the scale-up domain the\nbetter the performance. New datacenter architectures are emerging with more\nGPUs able to be tightly-coupled in a scale-up domain, such as moving from 8\nGPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains\nincrease the blast-radius of failures, with a failure of single GPU potentially\nimpacting TP execution on the full scale-up domain, which can degrade overall\nLLM training throughput dramatically. With as few as 0.1% of GPUs being in a\nfailed state, a high TP-degree job can experience nearly 10% reduction in LLM\ntraining throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate\nthis amplified impact of GPU failures. In NTP, a DP replica that experiences\nGPU failures operates at a reduced TP degree, contributing throughput equal to\nthe percentage of still-functional GPUs. We also propose a rack-design with\nimproved electrical and thermal capabilities in order to sustain power-boosting\nof scale-up domains that have experienced failures; combined with NTP, this can\nallow the DP replica with the reduced TP degree (i.e., with failed GPUs) to\nkeep up with the others, thereby achieving near-zero throughput loss for\nlarge-scale LLM training."
                },
                "authors": [
                    {
                        "name": "Daiyaan Arfeen"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Bhargava Gopireddy"
                    },
                    {
                        "name": "Ahmet Inci"
                    },
                    {
                        "name": "Gregory R. Ganger"
                    }
                ],
                "author_detail": {
                    "name": "Gregory R. Ganger"
                },
                "author": "Gregory R. Ganger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11670v2",
                "updated": "2025-04-08T14:29:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    29,
                    21,
                    1,
                    98,
                    0
                ],
                "published": "2024-03-18T11:18:33Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    11,
                    18,
                    33,
                    0,
                    78,
                    0
                ],
                "title": "Advancing Quantum Software Engineering: A Vision of Hybrid Full-Stack\n  Iterative Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Quantum Software Engineering: A Vision of Hybrid Full-Stack\n  Iterative Model"
                },
                "summary": "This paper introduces a vision for Quantum Software Development lifecycle,\nproposing a hybrid full-stack iterative model that integrates quantum and\nclassical computing. Addressing the current challenges in Quantum Computing\n(QC) such as the need for integrating diverse programming languages and\nmanaging the complexities of quantum-classical systems, this model is rooted in\nthe principles of DevOps and continuous software engineering. It presents a\ncomprehensive lifecycle for quantum software development, encompassing\nquantum-agnostic coding, testing, deployment, cloud computing services,\norchestration, translation, execution, and interpretation phases. Each phase is\ndesigned to accommodate the unique demands of QC, enabling traditional software\ndevelopers to engage with QC environments without needing in-depth QC\nexpertise. The paper presents a detailed implementation roadmap, utilizing a\nrange of existing tools and frameworks, thereby making quantum software\ndevelopment more accessible and efficient. The proposed model not only\naddresses current challenges in quantum software development but also makes a\nsubstantial contribution to the field of Quantum Software Engineering (QSE). By\nproposing a structured and accessible model, it sets the stage for further\nadvancements and research in QSE, enhancing its practicality and relevance in a\nwide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a vision for Quantum Software Development lifecycle,\nproposing a hybrid full-stack iterative model that integrates quantum and\nclassical computing. Addressing the current challenges in Quantum Computing\n(QC) such as the need for integrating diverse programming languages and\nmanaging the complexities of quantum-classical systems, this model is rooted in\nthe principles of DevOps and continuous software engineering. It presents a\ncomprehensive lifecycle for quantum software development, encompassing\nquantum-agnostic coding, testing, deployment, cloud computing services,\norchestration, translation, execution, and interpretation phases. Each phase is\ndesigned to accommodate the unique demands of QC, enabling traditional software\ndevelopers to engage with QC environments without needing in-depth QC\nexpertise. The paper presents a detailed implementation roadmap, utilizing a\nrange of existing tools and frameworks, thereby making quantum software\ndevelopment more accessible and efficient. The proposed model not only\naddresses current challenges in quantum software development but also makes a\nsubstantial contribution to the field of Quantum Software Engineering (QSE). By\nproposing a structured and accessible model, it sets the stage for further\nadvancements and research in QSE, enhancing its practicality and relevance in a\nwide range of applications."
                },
                "authors": [
                    {
                        "name": "Arif Ali Khan"
                    },
                    {
                        "name": "Davide Taibi"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Azeem Akbar"
                },
                "author": "Muhammad Azeem Akbar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09516v3",
                "updated": "2025-04-08T14:03:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    3,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T16:26:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    26,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning"
                },
                "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities to use search\nengines during inference is often suboptimal, as the LLM might not fully\npossess the capability on how to interact optimally with the search engine.\nThis paper introduces Search-R1, an extension of reinforcement learning (RL)\nfor reasoning frameworks where the LLM learns to autonomously generate\n(multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn\nsearch interactions, leveraging retrieved token masking for stable RL training\nand a simple outcome-based reward function. Experiments on seven\nquestion-answering datasets show that Search-R1 improves performance by 41%\n(Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same\nsetting. This paper further provides empirical insights into RL optimization\nmethods, LLM choices, and response length dynamics in retrieval-augmented\nreasoning. The code and model checkpoints are available at\nhttps://github.com/PeterGriffinJin/Search-R1."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Hansi Zeng"
                    },
                    {
                        "name": "Zhenrui Yue"
                    },
                    {
                        "name": "Jinsung Yoon"
                    },
                    {
                        "name": "Sercan Arik"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11779v2",
                "updated": "2025-04-08T13:56:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    56,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-17T13:14:11Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    14,
                    11,
                    0,
                    48,
                    0
                ],
                "title": "Efficient Response Generation Strategy Selection for Fine-Tuning Large\n  Language Models Through Self-Aligned Perplexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Response Generation Strategy Selection for Fine-Tuning Large\n  Language Models Through Self-Aligned Perplexity"
                },
                "summary": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing 'familiarity' through theoretical analysis and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing 'familiarity' through theoretical analysis and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Xuan Ren"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06036v1",
                "updated": "2025-04-08T13:36:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    36,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:36:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    36,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Multi-Sense Embeddings for Language Models and Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Sense Embeddings for Language Models and Knowledge Distillation"
                },
                "summary": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) rely on contextual embeddings\nwhich generate different (continuous) representations for the same token\ndepending on its surrounding context. Nonetheless, words and tokens typically\nhave a limited number of senses (or meanings). We propose multi-sense\nembeddings as a drop-in replacement for each token in order to capture the\nrange of their uses in a language. To construct a sense embedding dictionary,\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\nthe cluster centers as representative sense embeddings. In addition, we propose\na novel knowledge distillation method that leverages the sense dictionary to\nlearn a smaller student model that mimics the senses from the much larger base\nLLM model, offering significant space and inference time savings, while\nmaintaining competitive performance. Via thorough experiments on various\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\ndistillation approach. We share our code at\nhttps://github.com/Qitong-Wang/SenseDict"
                },
                "authors": [
                    {
                        "name": "Qitong Wang"
                    },
                    {
                        "name": "Mohammed J. Zaki"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Vasileios Kalantzis"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Kalantzis"
                },
                "author": "Vasileios Kalantzis",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06017v1",
                "updated": "2025-04-08T13:22:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    22,
                    9,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:22:09Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    22,
                    9,
                    1,
                    98,
                    0
                ],
                "title": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI"
                },
                "summary": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms."
                },
                "authors": [
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Luis Javier Navarrete-Lozano"
                    },
                    {
                        "name": "María Sanz-Gómez"
                    },
                    {
                        "name": "Lidia Salas Espejo"
                    },
                    {
                        "name": "Martiño Crespo-Álvarez"
                    },
                    {
                        "name": "Francisco Oca-Gonzalez"
                    },
                    {
                        "name": "Francesco Balassone"
                    },
                    {
                        "name": "Alfonso Glera-Picón"
                    },
                    {
                        "name": "Unai Ayucar-Carbajo"
                    },
                    {
                        "name": "Endika Gil-Uriarte"
                    }
                ],
                "author_detail": {
                    "name": "Endika Gil-Uriarte"
                },
                "author": "Endika Gil-Uriarte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06011v1",
                "updated": "2025-04-08T13:16:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    16,
                    54,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:16:54Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    16,
                    54,
                    1,
                    98,
                    0
                ],
                "title": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for\n  Hindi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for\n  Hindi"
                },
                "summary": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services."
                },
                "authors": [
                    {
                        "name": "Monojit Choudhury"
                    },
                    {
                        "name": "Shivam Chauhan"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Dhruv Sahnan"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Aaryamonvikram Singh"
                    },
                    {
                        "name": "Alok Anil Jadhav"
                    },
                    {
                        "name": "Utkarsh Agarwal"
                    },
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "Debopriyo Banerjee"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Junaid Bhat"
                    },
                    {
                        "name": "Awantika Shukla"
                    },
                    {
                        "name": "Samujjwal Ghosh"
                    },
                    {
                        "name": "Samta Kamboj"
                    },
                    {
                        "name": "Onkar Pandit"
                    },
                    {
                        "name": "Lalit Pradhan"
                    },
                    {
                        "name": "Rahul Pal"
                    },
                    {
                        "name": "Sunil Sahu"
                    },
                    {
                        "name": "Soundar Doraiswamy"
                    },
                    {
                        "name": "Parvez Mullah"
                    },
                    {
                        "name": "Ali El Filali"
                    },
                    {
                        "name": "Neha Sengupta"
                    },
                    {
                        "name": "Gokul Ramakrishnan"
                    },
                    {
                        "name": "Rituraj Joshi"
                    },
                    {
                        "name": "Gurpreet Gosal"
                    },
                    {
                        "name": "Avraham Sheinin"
                    },
                    {
                        "name": "Natalia Vassilieva"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06006v1",
                "updated": "2025-04-08T13:15:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    15,
                    47,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:15:47Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    15,
                    47,
                    1,
                    98,
                    0
                ],
                "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?"
                },
                "summary": "Optimal hyperparameter selection is critical for maximizing neural network\nperformance, especially as models grow in complexity. This work investigates\nthe viability of using large language models (LLMs) for hyperparameter\noptimization by employing a fine-tuned version of Code Llama. Through\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\naccurate and efficient hyperparameter recommendations tailored to diverse\nneural network architectures. Unlike traditional methods such as Optuna, which\nrely on exhaustive trials, the proposed approach achieves competitive or\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\nreducing computational overhead. Our approach highlights that LLM-based\noptimization not only matches state-of-the-art methods like Tree-structured\nParzen Estimators but also accelerates the tuning process. This positions LLMs\nas a promising alternative to conventional optimization techniques,\nparticularly for rapid experimentation. Furthermore, the ability to generate\nhyperparameters in a single inference step makes this method particularly\nwell-suited for resource-constrained environments such as edge devices and\nmobile applications, where computational efficiency is paramount. The results\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\ncomparable stability, underscoring their value in advancing machine learning\nworkflows. All generated hyperparameters are included in the LEMUR Neural\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\nbenchmark for hyperparameter optimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal hyperparameter selection is critical for maximizing neural network\nperformance, especially as models grow in complexity. This work investigates\nthe viability of using large language models (LLMs) for hyperparameter\noptimization by employing a fine-tuned version of Code Llama. Through\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\naccurate and efficient hyperparameter recommendations tailored to diverse\nneural network architectures. Unlike traditional methods such as Optuna, which\nrely on exhaustive trials, the proposed approach achieves competitive or\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\nreducing computational overhead. Our approach highlights that LLM-based\noptimization not only matches state-of-the-art methods like Tree-structured\nParzen Estimators but also accelerates the tuning process. This positions LLMs\nas a promising alternative to conventional optimization techniques,\nparticularly for rapid experimentation. Furthermore, the ability to generate\nhyperparameters in a single inference step makes this method particularly\nwell-suited for resource-constrained environments such as edge devices and\nmobile applications, where computational efficiency is paramount. The results\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\ncomparable stability, underscoring their value in advancing machine learning\nworkflows. All generated hyperparameters are included in the LEMUR Neural\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\nbenchmark for hyperparameter optimization research."
                },
                "authors": [
                    {
                        "name": "Roman Kochnev"
                    },
                    {
                        "name": "Arash Torabi Goodarzi"
                    },
                    {
                        "name": "Zofia Antonina Bentyn"
                    },
                    {
                        "name": "Dmitry Ignatov"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05995v1",
                "updated": "2025-04-08T13:01:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    1,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T13:01:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    1,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday\n  Knowledge"
                },
                "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework)."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Sahinur Rahman Laskar"
                    },
                    {
                        "name": "Mucahid Kutlu"
                    },
                    {
                        "name": "Shammur Absar Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Shammur Absar Chowdhury"
                },
                "author": "Shammur Absar Chowdhury",
                "arxiv_comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00597v2",
                "updated": "2025-04-08T12:40:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    40,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-01T09:55:23Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    55,
                    23,
                    1,
                    91,
                    0
                ],
                "title": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from out-language passages, but a much weaker ability to formulate\na full answer in the correct language. Our analysis, based on both accuracy and\nfeature attribution techniques, further shows that distracting passages\nnegatively impact answer quality regardless of their language. However,\ndistractors in the query language exert a slightly stronger influence. Taken\ntogether, our findings deepen the understanding of how LLMs utilize context in\nmRAG systems, providing directions for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from out-language passages, but a much weaker ability to formulate\na full answer in the correct language. Our analysis, based on both accuracy and\nfeature attribution techniques, further shows that distracting passages\nnegatively impact answer quality regardless of their language. However,\ndistractors in the query language exert a slightly stronger influence. Taken\ntogether, our findings deepen the understanding of how LLMs utilize context in\nmRAG systems, providing directions for future improvements."
                },
                "authors": [
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Raquel Fernández"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "Under review at COLM2025. All codes and data are released at\n  https://github.com/Betswish/mRAG-Context-Consistency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23803v2",
                "updated": "2025-04-08T12:36:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    36,
                    8,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-31T07:31:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    7,
                    31,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via\n  Scaling Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via\n  Scaling Test-Time Compute"
                },
                "summary": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner"
                },
                "authors": [
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Jue Chen"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Binhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Binhua Li"
                },
                "author": "Binhua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17486v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17486v3",
                "updated": "2025-04-08T12:19:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    19,
                    1,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-21T18:55:14Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    18,
                    55,
                    14,
                    4,
                    80,
                    0
                ],
                "title": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes"
                },
                "summary": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity."
                },
                "authors": [
                    {
                        "name": "Zhengqing Gao"
                    },
                    {
                        "name": "Dongting Hu"
                    },
                    {
                        "name": "Jia-Wang Bian"
                    },
                    {
                        "name": "Huan Fu"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17486v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17486v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05946v1",
                "updated": "2025-04-08T11:59:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T11:59:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control"
                },
                "summary": "Model Predictive Control~(MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose \\IMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model~(LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution~(L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, \\IMPC enables dynamic human-LLM interaction and\nfine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization~(DPO) using a tailored loss function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Control~(MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose \\IMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model~(LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution~(L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, \\IMPC enables dynamic human-LLM interaction and\nfine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization~(DPO) using a tailored loss function."
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08574v2",
                "updated": "2025-04-08T11:57:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    57,
                    2,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-13T12:44:41Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    12,
                    44,
                    41,
                    2,
                    318,
                    0
                ],
                "title": "Themes of Building LLM-based Applications for Production: A\n  Practitioner's View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Themes of Building LLM-based Applications for Production: A\n  Practitioner's View"
                },
                "summary": "Background: Large language models (LLMs) have become a paramount interest of\nresearchers and practitioners alike, yet a comprehensive overview of key\nconsiderations for those developing LLM-based systems is lacking. This study\naddresses this gap by collecting and mapping the topics practitioners discuss\nonline, offering practical insights into where priorities lie in developing\nLLM-based applications. Method: We collected 189 videos from 2022 to 2024 from\npractitioners actively developing such systems and discussing various aspects\nthey encounter during development and deployment of LLMs in production. We\nanalyzed the transcripts using BERTopic, then manually sorted and merged the\ngenerated topics into themes, leading to a total of 20 topics in 8 themes.\nResults: The most prevalent topics fall within the theme Design & Architecture,\nwith a strong focus on retrieval-augmented generation (RAG) systems. Other\nfrequently discussed topics include model capabilities and enhancement\ntechniques (e.g., fine-tuning, prompt engineering), infrastructure and tooling,\nand risks and ethical challenges. Implications: Our results highlight current\ndiscussions and challenges in deploying LLMs in production. This way, we\nprovide a systematic overview of key aspects practitioners should be aware of\nwhen developing LLM-based applications. We further pale off topics of interest\nfor academics where further research is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) have become a paramount interest of\nresearchers and practitioners alike, yet a comprehensive overview of key\nconsiderations for those developing LLM-based systems is lacking. This study\naddresses this gap by collecting and mapping the topics practitioners discuss\nonline, offering practical insights into where priorities lie in developing\nLLM-based applications. Method: We collected 189 videos from 2022 to 2024 from\npractitioners actively developing such systems and discussing various aspects\nthey encounter during development and deployment of LLMs in production. We\nanalyzed the transcripts using BERTopic, then manually sorted and merged the\ngenerated topics into themes, leading to a total of 20 topics in 8 themes.\nResults: The most prevalent topics fall within the theme Design & Architecture,\nwith a strong focus on retrieval-augmented generation (RAG) systems. Other\nfrequently discussed topics include model capabilities and enhancement\ntechniques (e.g., fine-tuning, prompt engineering), infrastructure and tooling,\nand risks and ethical challenges. Implications: Our results highlight current\ndiscussions and challenges in deploying LLMs in production. This way, we\nprovide a systematic overview of key aspects practitioners should be aware of\nwhen developing LLM-based applications. We further pale off topics of interest\nfor academics where further research is needed."
                },
                "authors": [
                    {
                        "name": "Alina Mailach"
                    },
                    {
                        "name": "Sebastian Simon"
                    },
                    {
                        "name": "Johannes Dorn"
                    },
                    {
                        "name": "Norbert Siegmund"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Siegmund"
                },
                "author": "Norbert Siegmund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01759v2",
                "updated": "2025-04-08T11:42:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    42,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2023-11-03T07:34:47Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    7,
                    34,
                    47,
                    4,
                    307,
                    0
                ],
                "title": "TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices"
                },
                "summary": "Developing deep learning models on tiny devices (e.g. Microcontroller units,\nMCUs) has attracted much attention in various embedded IoT applications.\nHowever, it is challenging to efficiently design and deploy recent advanced\nmodels (e.g. transformers) on tiny devices due to their severe hardware\nresource constraints. In this work, we propose TinyFormer, a framework\nspecifically designed to develop and deploy resource-efficient transformers on\nMCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine.\nSeparately, SuperNAS aims to search for an appropriate supernet from a vast\nsearch space. SparseNAS evaluates the best sparse single-path model including\ntransformer architecture from the identified supernet. Finally, SparseEngine\nefficiently deploys the searched sparse models onto MCUs. To the best of our\nknowledge, SparseEngine is the first deployment framework capable of performing\ninference of sparse models with transformer on MCUs. Evaluation results on the\nCIFAR-10 dataset demonstrate that TinyFormer can develop efficient transformers\nwith an accuracy of 96.1% while adhering to hardware constraints of 1MB storage\nand $320$KB memory. Additionally, TinyFormer achieves significant speedups in\nsparse inference, up to 12.2x, when compared to the CMSIS-NN library.\nTinyFormer is believed to bring powerful transformers into TinyML scenarios and\ngreatly expand the scope of deep learning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing deep learning models on tiny devices (e.g. Microcontroller units,\nMCUs) has attracted much attention in various embedded IoT applications.\nHowever, it is challenging to efficiently design and deploy recent advanced\nmodels (e.g. transformers) on tiny devices due to their severe hardware\nresource constraints. In this work, we propose TinyFormer, a framework\nspecifically designed to develop and deploy resource-efficient transformers on\nMCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine.\nSeparately, SuperNAS aims to search for an appropriate supernet from a vast\nsearch space. SparseNAS evaluates the best sparse single-path model including\ntransformer architecture from the identified supernet. Finally, SparseEngine\nefficiently deploys the searched sparse models onto MCUs. To the best of our\nknowledge, SparseEngine is the first deployment framework capable of performing\ninference of sparse models with transformer on MCUs. Evaluation results on the\nCIFAR-10 dataset demonstrate that TinyFormer can develop efficient transformers\nwith an accuracy of 96.1% while adhering to hardware constraints of 1MB storage\nand $320$KB memory. Additionally, TinyFormer achieves significant speedups in\nsparse inference, up to 12.2x, when compared to the CMSIS-NN library.\nTinyFormer is believed to bring powerful transformers into TinyML scenarios and\ngreatly expand the scope of deep learning applications."
                },
                "authors": [
                    {
                        "name": "Jianlei Yang"
                    },
                    {
                        "name": "Jiacheng Liao"
                    },
                    {
                        "name": "Fanding Lei"
                    },
                    {
                        "name": "Meichen Liu"
                    },
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Han Wan"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Weisheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weisheng Zhao"
                },
                "author": "Weisheng Zhao",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13453v2",
                "updated": "2025-04-08T11:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    5,
                    1,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-17T11:26:10Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    11,
                    26,
                    10,
                    3,
                    291,
                    0
                ],
                "title": "Adaptive Augmentation Policy Optimization with LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Augmentation Policy Optimization with LLM Feedback"
                },
                "summary": "Data augmentation is a critical component of deep learning pipelines,\nenhancing model generalization by increasing dataset diversity. Traditional\naugmentation strategies rely on manually designed transformations, stochastic\nsampling, or automated search-based approaches. Although automated methods\nimprove performance, they often require extensive computational resources and\nare tailored to specific datasets. In this work, we propose a Large Language\nModel (LLM)-guided augmentation optimization strategy that refines augmentation\npolicies based on model performance feedback. We introduce two approaches: (1)\nLLM-Guided Augmentation Policy Optimization, where augmentation policies are\nselected by an LLM prior to training and iteratively refined across multiple\ntraining cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization,\nwhere policies adapt in real-time based on performance metrics. This\nin-training approach eliminates the need for full model retraining before\nreceiving LLM feedback, thereby reducing computational costs while improving\nperformance. Our methodology employs an LLM to dynamically select augmentation\ntransformations based on dataset characteristics, model architecture, and prior\ntraining outcomes. Unlike traditional search-based methods, our approach\nleverages the contextual knowledge of LLMs, particularly in specialized domains\nlike medical imaging, to recommend augmentation strategies tailored to\ndomain-specific data. We evaluate our approach on multiple domain-specific\nimage classification datasets where augmentation is key to model robustness.\nResults show that LLM-guided augmentation optimization outperforms traditional\nmethods, improving model accuracy. These findings highlight the potential of\nLLMs in automating and adapting deep learning training workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is a critical component of deep learning pipelines,\nenhancing model generalization by increasing dataset diversity. Traditional\naugmentation strategies rely on manually designed transformations, stochastic\nsampling, or automated search-based approaches. Although automated methods\nimprove performance, they often require extensive computational resources and\nare tailored to specific datasets. In this work, we propose a Large Language\nModel (LLM)-guided augmentation optimization strategy that refines augmentation\npolicies based on model performance feedback. We introduce two approaches: (1)\nLLM-Guided Augmentation Policy Optimization, where augmentation policies are\nselected by an LLM prior to training and iteratively refined across multiple\ntraining cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization,\nwhere policies adapt in real-time based on performance metrics. This\nin-training approach eliminates the need for full model retraining before\nreceiving LLM feedback, thereby reducing computational costs while improving\nperformance. Our methodology employs an LLM to dynamically select augmentation\ntransformations based on dataset characteristics, model architecture, and prior\ntraining outcomes. Unlike traditional search-based methods, our approach\nleverages the contextual knowledge of LLMs, particularly in specialized domains\nlike medical imaging, to recommend augmentation strategies tailored to\ndomain-specific data. We evaluate our approach on multiple domain-specific\nimage classification datasets where augmentation is key to model robustness.\nResults show that LLM-guided augmentation optimization outperforms traditional\nmethods, improving model accuracy. These findings highlight the potential of\nLLMs in automating and adapting deep learning training workflows."
                },
                "authors": [
                    {
                        "name": "Ant Duru"
                    },
                    {
                        "name": "Alptekin Temizel"
                    }
                ],
                "author_detail": {
                    "name": "Alptekin Temizel"
                },
                "author": "Alptekin Temizel",
                "arxiv_comment": "15 pages, 4 tables, 3 figures submitted for consideration to 2025\n  Medical Image Understanding and Analysis Conference (MIUA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15313v2",
                "updated": "2025-04-08T11:04:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    4,
                    33,
                    1,
                    98,
                    0
                ],
                "published": "2024-08-27T17:31:21Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    31,
                    21,
                    1,
                    240,
                    0
                ],
                "title": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nsupervised optimization, a labeling function is used to capture the global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark that includes comprehensive discriminative and\ngenerative tasks for helpfulness and harmlessness. The results indicate that\nour method significantly outperforms existing approaches in both safety and\nhelpfulness. Moreover, BFPO achieves the same level of safety as methods that\nheavily rely on human labor with less than 10\\% of the computational resources\nand human prompting and annotation process. The training recipes can be found\nhere: https://github.com/wx-zhang/bfpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on human preferences, typically\nthrough reinforcement learning from human feedback (RLHF), has proven\nsuccessful in enhancing their capabilities. However, ensuring the safety of\nLLMs during fine-tuning remains a critical concern, and mitigating the\npotential conflicts in safety and helpfulness is costly in RLHF. To address\nthis issue, we propose a supervised learning framework called Bi-Factorial\nPreference Optimization (BFPO), which re-parameterizes a joint RLHF objective\nof both safety and helpfulness into a single supervised learning objective. In\nsupervised optimization, a labeling function is used to capture the global\npreferences ranking to balance both safety and helpfulness. To evaluate BFPO,\nwe develop a benchmark that includes comprehensive discriminative and\ngenerative tasks for helpfulness and harmlessness. The results indicate that\nour method significantly outperforms existing approaches in both safety and\nhelpfulness. Moreover, BFPO achieves the same level of safety as methods that\nheavily rely on human labor with less than 10\\% of the computational resources\nand human prompting and annotation process. The training recipes can be found\nhere: https://github.com/wx-zhang/bfpo."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "arxiv_comment": "The paper has been accepted in ICLR 2025 as spotlight presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10666v2",
                "updated": "2025-04-08T10:56:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    56,
                    7,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-09T19:49:31Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    19,
                    49,
                    31,
                    6,
                    68,
                    0
                ],
                "title": "Green Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Prompting"
                },
                "summary": "Large Language Models (LLMs) have become widely used across various domains\nspanning search engines, code generation, and text creation. However, a major\nconcern associated with their adoption is the high cost of inference, impacting\nboth their sustainability and financial feasibility. In this study, we\nempirically study how different prompt and response characteristics directly\nimpact LLM inference energy cost. We conduct experiments leveraging three\nopen-source transformer-based LLMs across three task types$-$question\nanswering, sentiment analysis, and text generation. For each inference, we\nanalyzed prompt and response characteristics (length, semantic meaning, time\ntaken, energy consumption). Our results demonstrate that even when presented\nwith identical tasks, models generate responses with varying characteristics\nand subsequently exhibit distinct energy consumption patterns. We found that\nprompt length is less significant than the semantic meaning of the task itself.\nIn addition, we identified specific keywords associated with higher or lower\nenergy usage that vary between associated tasks. These findings highlight the\nimportance of prompt design in optimizing inference efficiency. We conclude\nthat the semantic meaning of prompts and certain task-related keywords\nsignificantly impact inference costs, leading the way for deeper exploration\ntowards creating energy-adaptive LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become widely used across various domains\nspanning search engines, code generation, and text creation. However, a major\nconcern associated with their adoption is the high cost of inference, impacting\nboth their sustainability and financial feasibility. In this study, we\nempirically study how different prompt and response characteristics directly\nimpact LLM inference energy cost. We conduct experiments leveraging three\nopen-source transformer-based LLMs across three task types$-$question\nanswering, sentiment analysis, and text generation. For each inference, we\nanalyzed prompt and response characteristics (length, semantic meaning, time\ntaken, energy consumption). Our results demonstrate that even when presented\nwith identical tasks, models generate responses with varying characteristics\nand subsequently exhibit distinct energy consumption patterns. We found that\nprompt length is less significant than the semantic meaning of the task itself.\nIn addition, we identified specific keywords associated with higher or lower\nenergy usage that vary between associated tasks. These findings highlight the\nimportance of prompt design in optimizing inference efficiency. We conclude\nthat the semantic meaning of prompts and certain task-related keywords\nsignificantly impact inference costs, leading the way for deeper exploration\ntowards creating energy-adaptive LLMs."
                },
                "authors": [
                    {
                        "name": "Marta Adamska"
                    },
                    {
                        "name": "Daria Smirnova"
                    },
                    {
                        "name": "Hamid Nasiri"
                    },
                    {
                        "name": "Zhengxin Yu"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05898v1",
                "updated": "2025-04-08T10:49:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    49,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:49:45Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    49,
                    45,
                    1,
                    98,
                    0
                ],
                "title": "Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and\n  Human Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and\n  Human Evaluation"
                },
                "summary": "Large language models show promising results in various NLP tasks. Despite\nthese successes, the robustness and consistency of LLMs in underrepresented\nlanguages remain largely unexplored, especially concerning local dialects.\nExisting benchmarks also focus on main dialects, neglecting LLMs' ability on\nlocal dialect texts. In this paper, we introduce a Thai local dialect benchmark\ncovering Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai,\nevaluating LLMs on five NLP tasks: summarization, question answering,\ntranslation, conversation, and food-related tasks. Furthermore, we propose a\nhuman evaluation guideline and metric for Thai local dialects to assess\ngeneration fluency and dialect-specific accuracy. Results show that LLM\nperformance declines significantly in local Thai dialects compared to standard\nThai, with only proprietary models like GPT-4o and Gemini2 demonstrating some\nfluency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models show promising results in various NLP tasks. Despite\nthese successes, the robustness and consistency of LLMs in underrepresented\nlanguages remain largely unexplored, especially concerning local dialects.\nExisting benchmarks also focus on main dialects, neglecting LLMs' ability on\nlocal dialect texts. In this paper, we introduce a Thai local dialect benchmark\ncovering Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai,\nevaluating LLMs on five NLP tasks: summarization, question answering,\ntranslation, conversation, and food-related tasks. Furthermore, we propose a\nhuman evaluation guideline and metric for Thai local dialects to assess\ngeneration fluency and dialect-specific accuracy. Results show that LLM\nperformance declines significantly in local Thai dialects compared to standard\nThai, with only proprietary models like GPT-4o and Gemini2 demonstrating some\nfluency"
                },
                "authors": [
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Kanruethai Masuk"
                    },
                    {
                        "name": "Surapon Nonesung"
                    },
                    {
                        "name": "Chalermpun Mai-On"
                    },
                    {
                        "name": "Sarana Nutanong"
                    },
                    {
                        "name": "Wuttikorn Ponwitayarat"
                    },
                    {
                        "name": "Potsawee Manakul"
                    }
                ],
                "author_detail": {
                    "name": "Potsawee Manakul"
                },
                "author": "Potsawee Manakul",
                "arxiv_comment": "Datasets and codes are available at\n  https://github.com/mrpeerat/Thai_local_benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08424v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08424v3",
                "updated": "2025-04-08T10:48:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    48,
                    19,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-12T12:15:14Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    12,
                    15,
                    14,
                    4,
                    103,
                    0
                ],
                "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction\n  in an Object Categorization Task"
                },
                "summary": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot. Video:\nhttps://youtu.be/tBJHfAuzohI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human intention-based systems enable robots to perceive and interpret user\nactions to interact with humans and adapt to their behavior proactively.\nTherefore, intention prediction is pivotal in creating a natural interaction\nwith social robots in human-designed environments. In this paper, we examine\nusing Large Language Models (LLMs) to infer human intention in a collaborative\nobject categorization task with a physical robot. We propose a novel multimodal\napproach that integrates user non-verbal cues, like hand gestures, body poses,\nand facial expressions, with environment states and user verbal cues to predict\nuser intentions in a hierarchical architecture. Our evaluation of five LLMs\nshows the potential for reasoning about verbal and non-verbal user cues,\nleveraging their context-understanding and real-world knowledge to support\nintention prediction while collaborating on a task with a social robot. Video:\nhttps://youtu.be/tBJHfAuzohI"
                },
                "authors": [
                    {
                        "name": "Hassan Ali"
                    },
                    {
                        "name": "Philipp Allgeuer"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_doi": "10.1007/978-981-96-3525-2_25",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-3525-2_25",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.08424v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08424v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 16th International Conference on\n  Social Robotics (ICSR) 2024,15 pages,5 figures,2 tables; work was co-funded\n  by Horizon Europe project TERAIS under Grant agreement number 101079338",
                "arxiv_journal_ref": "In: Palinko, O., et al. Social Robotics. ICSR + AI 2024. vol\n  15563. Springer (2025)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13717v2",
                "updated": "2025-04-08T10:43:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    43,
                    0,
                    1,
                    98,
                    0
                ],
                "published": "2024-09-07T18:47:38Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    18,
                    47,
                    38,
                    5,
                    251,
                    0
                ],
                "title": "DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level\n  Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level\n  Relation Extraction"
                },
                "summary": "The remarkable capabilities of Large Language Models (LLMs) in text\ncomprehension and generation have revolutionized Information Extraction (IE).\nOne such advancement is in Document-level Relation Triplet Extraction (DocRTE),\na critical task in information systems that aims to extract entities and their\nsemantic relationships from documents. However, existing methods are primarily\ndesigned for Sentence level Relation Triplet Extraction (SentRTE), which\ntypically handles a limited set of relations and triplet facts within a single\nsentence. Additionally, some approaches treat relations as candidate choices\nintegrated into prompt templates, resulting in inefficient processing and\nsuboptimal performance when determining the relation elements in triplets. To\naddress these limitations, we introduce a Discriminative and Voice Aware\nParadigm DiVA. DiVA involves only two steps: performing document-level relation\nextraction (DocRE) and then identifying the subject object entities based on\nthe relation. No additional processing is required simply input the document to\ndirectly obtain the triplets. This streamlined process more accurately reflects\nreal-world scenarios for triplet extraction. Our innovation lies in\ntransforming DocRE into a discriminative task, where the model pays attention\nto each relation and to the often overlooked issue of active vs. passive voice\nwithin the triplet. Our experiments on the Re-DocRED and DocRED datasets\ndemonstrate state-of-the-art results for the DocRTE task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable capabilities of Large Language Models (LLMs) in text\ncomprehension and generation have revolutionized Information Extraction (IE).\nOne such advancement is in Document-level Relation Triplet Extraction (DocRTE),\na critical task in information systems that aims to extract entities and their\nsemantic relationships from documents. However, existing methods are primarily\ndesigned for Sentence level Relation Triplet Extraction (SentRTE), which\ntypically handles a limited set of relations and triplet facts within a single\nsentence. Additionally, some approaches treat relations as candidate choices\nintegrated into prompt templates, resulting in inefficient processing and\nsuboptimal performance when determining the relation elements in triplets. To\naddress these limitations, we introduce a Discriminative and Voice Aware\nParadigm DiVA. DiVA involves only two steps: performing document-level relation\nextraction (DocRE) and then identifying the subject object entities based on\nthe relation. No additional processing is required simply input the document to\ndirectly obtain the triplets. This streamlined process more accurately reflects\nreal-world scenarios for triplet extraction. Our innovation lies in\ntransforming DocRE into a discriminative task, where the model pays attention\nto each relation and to the often overlooked issue of active vs. passive voice\nwithin the triplet. Our experiments on the Re-DocRED and DocRED datasets\ndemonstrate state-of-the-art results for the DocRTE task."
                },
                "authors": [
                    {
                        "name": "Yiheng Wu"
                    },
                    {
                        "name": "Roman Yangarber"
                    },
                    {
                        "name": "Xian Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xian Mao"
                },
                "author": "Xian Mao",
                "arxiv_comment": "After internal discussions among the co-authors, we have decided to\n  withdraw the manuscript due to a change in research direction and a lack of\n  unanimous agreement to proceed with publication at this time",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06252v2",
                "updated": "2025-04-08T10:32:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    32,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-10T08:33:47Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    8,
                    33,
                    47,
                    0,
                    41,
                    0
                ],
                "title": "CliniQ: A Multi-faceted Benchmark for Electronic Health Record Retrieval\n  with Semantic Match Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CliniQ: A Multi-faceted Benchmark for Electronic Health Record Retrieval\n  with Semantic Match Assessment"
                },
                "summary": "Electronic Health Record (EHR) retrieval plays a pivotal role in various\nclinical tasks, but its development has been severely impeded by the lack of\npublicly available benchmarks. In this paper, we introduce a novel public EHR\nretrieval benchmark, CliniQ, to address this gap. We consider two retrieval\nsettings: Single-Patient Retrieval and Multi-Patient Retrieval, reflecting\nvarious real-world scenarios. Single-Patient Retrieval focuses on finding\nrelevant parts within a patient note, while Multi-Patient Retrieval involves\nretrieving EHRs from multiple patients. We build our benchmark upon 1,000\ndischarge summary notes along with the ICD codes and prescription labels from\nMIMIC-III, and collect 1,246 unique queries with 77,206 relevance judgments by\nfurther leveraging powerful LLMs as annotators. Additionally, we include a\nnovel assessment of the semantic gap issue in EHR retrieval by categorizing\nmatching types into string match and four types of semantic matches. On our\nproposed benchmark, we conduct a comprehensive evaluation of various retrieval\nmethods, ranging from conventional exact match to popular dense retrievers. Our\nexperiments find that BM25 sets a strong baseline and performs competitively to\nthe dense retrievers, and general domain dense retrievers surprisingly\noutperform those designed for the medical domain. In-depth analyses on various\nmatching types reveal the strengths and drawbacks of different methods,\nenlightening the potential for targeted improvement. We believe that our\nbenchmark will stimulate the research communities to advance EHR retrieval\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Record (EHR) retrieval plays a pivotal role in various\nclinical tasks, but its development has been severely impeded by the lack of\npublicly available benchmarks. In this paper, we introduce a novel public EHR\nretrieval benchmark, CliniQ, to address this gap. We consider two retrieval\nsettings: Single-Patient Retrieval and Multi-Patient Retrieval, reflecting\nvarious real-world scenarios. Single-Patient Retrieval focuses on finding\nrelevant parts within a patient note, while Multi-Patient Retrieval involves\nretrieving EHRs from multiple patients. We build our benchmark upon 1,000\ndischarge summary notes along with the ICD codes and prescription labels from\nMIMIC-III, and collect 1,246 unique queries with 77,206 relevance judgments by\nfurther leveraging powerful LLMs as annotators. Additionally, we include a\nnovel assessment of the semantic gap issue in EHR retrieval by categorizing\nmatching types into string match and four types of semantic matches. On our\nproposed benchmark, we conduct a comprehensive evaluation of various retrieval\nmethods, ranging from conventional exact match to popular dense retrievers. Our\nexperiments find that BM25 sets a strong baseline and performs competitively to\nthe dense retrievers, and general domain dense retrievers surprisingly\noutperform those designed for the medical domain. In-depth analyses on various\nmatching types reveal the strengths and drawbacks of different methods,\nenlightening the potential for targeted improvement. We believe that our\nbenchmark will stimulate the research communities to advance EHR retrieval\nsystems."
                },
                "authors": [
                    {
                        "name": "Zhengyun Zhao"
                    },
                    {
                        "name": "Hongyi Yuan"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Haichao Chen"
                    },
                    {
                        "name": "Huaiyuan Ying"
                    },
                    {
                        "name": "Songchi Zhou"
                    },
                    {
                        "name": "Yue Zhong"
                    },
                    {
                        "name": "Sheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Yu"
                },
                "author": "Sheng Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15429v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15429v4",
                "updated": "2025-04-08T10:27:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    27,
                    59,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-21T12:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations"
                },
                "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Shuojie Fu"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Cemre Zor"
                    },
                    {
                        "name": "Guy Martin"
                    },
                    {
                        "name": "James Kinross"
                    },
                    {
                        "name": "Uddhav Vaghela"
                    },
                    {
                        "name": "Ovidiu Serban"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "long paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15429v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15429v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13212v2",
                "updated": "2025-04-08T10:06:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    6,
                    2,
                    1,
                    98,
                    0
                ],
                "published": "2024-11-20T11:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "title": "Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation"
                },
                "summary": "Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment."
                },
                "authors": [
                    {
                        "name": "David Otero"
                    },
                    {
                        "name": "Javier Parapar"
                    },
                    {
                        "name": "Álvaro Barreiro"
                    }
                ],
                "author_detail": {
                    "name": "Álvaro Barreiro"
                },
                "author": "Álvaro Barreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05871v1",
                "updated": "2025-04-08T09:54:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    54,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:54:49Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    54,
                    49,
                    1,
                    98,
                    0
                ],
                "title": "Agent Guide: A Simple Agent Behavioral Watermarking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Guide: A Simple Agent Behavioral Watermarking Framework"
                },
                "summary": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems."
                },
                "authors": [
                    {
                        "name": "Kaibo Huang"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05866v1",
                "updated": "2025-04-08T09:47:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    47,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:47:15Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    47,
                    15,
                    1,
                    98,
                    0
                ],
                "title": "CTI-HAL: A Human-Annotated Dataset for Cyber Threat Intelligence\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTI-HAL: A Human-Annotated Dataset for Cyber Threat Intelligence\n  Analysis"
                },
                "summary": "Organizations are increasingly targeted by Advanced Persistent Threats\n(APTs), which involve complex, multi-stage tactics and diverse techniques.\nCyber Threat Intelligence (CTI) sources, such as incident reports and security\nblogs, provide valuable insights, but are often unstructured and in natural\nlanguage, making it difficult to automatically extract information. Recent\nstudies have explored the use of AI to perform automatic extraction from CTI\ndata, leveraging existing CTI datasets for performance evaluation and\nfine-tuning. However, they present challenges and limitations that impact their\neffectiveness. To overcome these issues, we introduce a novel dataset manually\nconstructed from CTI reports and structured according to the MITRE ATT&CK\nframework. To assess its quality, we conducted an inter-annotator agreement\nstudy using Krippendorff alpha, confirming its reliability. Furthermore, the\ndataset was used to evaluate a Large Language Model (LLM) in a real-world\nbusiness context, showing promising generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations are increasingly targeted by Advanced Persistent Threats\n(APTs), which involve complex, multi-stage tactics and diverse techniques.\nCyber Threat Intelligence (CTI) sources, such as incident reports and security\nblogs, provide valuable insights, but are often unstructured and in natural\nlanguage, making it difficult to automatically extract information. Recent\nstudies have explored the use of AI to perform automatic extraction from CTI\ndata, leveraging existing CTI datasets for performance evaluation and\nfine-tuning. However, they present challenges and limitations that impact their\neffectiveness. To overcome these issues, we introduce a novel dataset manually\nconstructed from CTI reports and structured according to the MITRE ATT&CK\nframework. To assess its quality, we conducted an inter-annotator agreement\nstudy using Krippendorff alpha, confirming its reliability. Furthermore, the\ndataset was used to evaluate a Large Language Model (LLM) in a real-world\nbusiness context, showing promising generalizability."
                },
                "authors": [
                    {
                        "name": "Sofia Della Penna"
                    },
                    {
                        "name": "Roberto Natella"
                    },
                    {
                        "name": "Vittorio Orbinato"
                    },
                    {
                        "name": "Lorenzo Parracino"
                    },
                    {
                        "name": "Luciano Pianese"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Pianese"
                },
                "author": "Luciano Pianese",
                "arxiv_comment": "Accepted for publication in the Workshop on Attackers and Cybercrime\n  Operations (WACCO 2025), co-located with IEEE European Symposium on Security\n  and Privacy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17875v3",
                "updated": "2025-04-08T09:44:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    44,
                    28,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-23T13:47:05Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    47,
                    5,
                    2,
                    297,
                    0
                ],
                "title": "Understanding Layer Significance in LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Layer Significance in LLM Alignment"
                },
                "summary": "Aligning large language models (LLMs) through supervised fine-tuning is\nessential for tailoring them to specific applications. Recent studies suggest\nthat alignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To uncover how alignment affects model behavior at\na granular level, we propose identifying which layers within LLMs are most\ncritical to the alignment process. Our approach, named ILA, involves learning a\nbinary mask for the parameter changes in each layer during alignment, as an\nindicator of layer significance. Experimental results reveal that, despite\nsubstantial differences in alignment datasets, the important layers of a model\nidentified by ILA exhibit nearly 90\\% overlap, highlighting fundamental\npatterns in LLM alignment. The results also indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss. Finally, we discuss how these findings extend\nfrom LLM alignment to reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) through supervised fine-tuning is\nessential for tailoring them to specific applications. Recent studies suggest\nthat alignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To uncover how alignment affects model behavior at\na granular level, we propose identifying which layers within LLMs are most\ncritical to the alignment process. Our approach, named ILA, involves learning a\nbinary mask for the parameter changes in each layer during alignment, as an\nindicator of layer significance. Experimental results reveal that, despite\nsubstantial differences in alignment datasets, the important layers of a model\nidentified by ILA exhibit nearly 90\\% overlap, highlighting fundamental\npatterns in LLM alignment. The results also indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss. Finally, we discuss how these findings extend\nfrom LLM alignment to reasoning."
                },
                "authors": [
                    {
                        "name": "Guangyuan Shi"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05862v1",
                "updated": "2025-04-08T09:41:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    41,
                    3,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:41:03Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    41,
                    3,
                    1,
                    98,
                    0
                ],
                "title": "Are Generative AI Agents Effective Personalized Financial Advisors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Generative AI Agents Effective Personalized Financial Advisors?"
                },
                "summary": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice."
                },
                "authors": [
                    {
                        "name": "Takehiro Takayanagi"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    },
                    {
                        "name": "Javier Sanz-Cruzado"
                    },
                    {
                        "name": "Richard McCreadie"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03543v3",
                "updated": "2025-04-08T09:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    39,
                    25,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-04T15:49:49Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    15,
                    49,
                    49,
                    3,
                    95,
                    0
                ],
                "title": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Xueling Liu"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Ding Pan"
                    },
                    {
                        "name": "Yizhi LI"
                    },
                    {
                        "name": "Ruibo Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05846v1",
                "updated": "2025-04-08T09:25:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    25,
                    21,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:25:21Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    25,
                    21,
                    1,
                    98,
                    0
                ],
                "title": "PathGPT: Leveraging Large Language Models for Personalized Route\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathGPT: Leveraging Large Language Models for Personalized Route\n  Generation"
                },
                "summary": "The proliferation of GPS enabled devices has led to the accumulation of a\nsubstantial corpus of historical trajectory data. By leveraging these data for\ntraining machine learning models,researchers have devised novel data-driven\nmethodologies that address the personalized route recommendation (PRR) problem.\nIn contrast to conventional algorithms such as Dijkstra shortest path\nalgorithm,these novel algorithms possess the capacity to discern and learn\npatterns within the data,thereby facilitating the generation of more\npersonalized paths. However,once these models have been trained,their\napplication is constrained to the generation of routes that align with their\ntraining patterns. This limitation renders them less adaptable to novel\nscenarios and the deployment of multiple machine learning models might be\nnecessary to address new possible scenarios,which can be costly as each model\nmust be trained separately. Inspired by recent advances in the field of Large\nLanguage Models (LLMs),we leveraged their natural language understanding\ncapabilities to develop a unified model to solve the PRR problem while being\nseamlessly adaptable to new scenarios without additional training. To\naccomplish this,we combined the extensive knowledge LLMs acquired during\ntraining with further access to external hand-crafted context\ninformation,similar to RAG (Retrieved Augmented Generation) systems,to enhance\ntheir ability to generate paths according to user-defined requirements.\nExtensive experiments on different datasets show a considerable uplift in LLM\nperformance on the PRR problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of GPS enabled devices has led to the accumulation of a\nsubstantial corpus of historical trajectory data. By leveraging these data for\ntraining machine learning models,researchers have devised novel data-driven\nmethodologies that address the personalized route recommendation (PRR) problem.\nIn contrast to conventional algorithms such as Dijkstra shortest path\nalgorithm,these novel algorithms possess the capacity to discern and learn\npatterns within the data,thereby facilitating the generation of more\npersonalized paths. However,once these models have been trained,their\napplication is constrained to the generation of routes that align with their\ntraining patterns. This limitation renders them less adaptable to novel\nscenarios and the deployment of multiple machine learning models might be\nnecessary to address new possible scenarios,which can be costly as each model\nmust be trained separately. Inspired by recent advances in the field of Large\nLanguage Models (LLMs),we leveraged their natural language understanding\ncapabilities to develop a unified model to solve the PRR problem while being\nseamlessly adaptable to new scenarios without additional training. To\naccomplish this,we combined the extensive knowledge LLMs acquired during\ntraining with further access to external hand-crafted context\ninformation,similar to RAG (Retrieved Augmented Generation) systems,to enhance\ntheir ability to generate paths according to user-defined requirements.\nExtensive experiments on different datasets show a considerable uplift in LLM\nperformance on the PRR problem."
                },
                "authors": [
                    {
                        "name": "Steeve Cuthbert Marcelyn"
                    },
                    {
                        "name": "Yucen Gao"
                    },
                    {
                        "name": "Yuzhe Zhang"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05831v1",
                "updated": "2025-04-08T09:14:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    14,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:14:38Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    14,
                    38,
                    1,
                    98,
                    0
                ],
                "title": "Leveraging Robust Optimization for LLM Alignment under Distribution\n  Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Robust Optimization for LLM Alignment under Distribution\n  Shifts"
                },
                "summary": "Large language models (LLMs) increasingly rely on preference alignment\nmethods to steer outputs toward human values, yet these methods are often\nconstrained by the scarcity of high-quality human-annotated data. To tackle\nthis, recent approaches have turned to synthetic data generated by LLMs as a\nscalable alternative. However, synthetic data can introduce distribution\nshifts, compromising the nuanced human preferences that are essential for\ndesirable outputs. In this paper, we propose a novel distribution-aware\noptimization framework that improves preference alignment in the presence of\nsuch shifts. Our approach first estimates the likelihood ratios between the\ntarget and training distributions leveraging a learned classifier, then it\nminimizes the worst-case loss over data regions that reflect the target\nhuman-preferred distribution. By explicitly prioritizing the target\ndistribution during optimization, our method mitigates the adverse effects of\ndistributional variation and enhances the generation of responses that\nfaithfully reflect human values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on preference alignment\nmethods to steer outputs toward human values, yet these methods are often\nconstrained by the scarcity of high-quality human-annotated data. To tackle\nthis, recent approaches have turned to synthetic data generated by LLMs as a\nscalable alternative. However, synthetic data can introduce distribution\nshifts, compromising the nuanced human preferences that are essential for\ndesirable outputs. In this paper, we propose a novel distribution-aware\noptimization framework that improves preference alignment in the presence of\nsuch shifts. Our approach first estimates the likelihood ratios between the\ntarget and training distributions leveraging a learned classifier, then it\nminimizes the worst-case loss over data regions that reflect the target\nhuman-preferred distribution. By explicitly prioritizing the target\ndistribution during optimization, our method mitigates the adverse effects of\ndistributional variation and enhances the generation of responses that\nfaithfully reflect human values."
                },
                "authors": [
                    {
                        "name": "Mingye Zhu"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Junbo Guo"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23494v2",
                "updated": "2025-04-08T09:12:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    12,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-30T16:03:02Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    16,
                    3,
                    2,
                    6,
                    89,
                    0
                ],
                "title": "DNA and Human Language: Epigenetic Memory and Redundancy in Linear\n  Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNA and Human Language: Epigenetic Memory and Redundancy in Linear\n  Sequence"
                },
                "summary": "DNA is often described as the 'language of life', but whether it possesses\nformal linguistic properties remains unresolved. Here, we present the first\nempirical evidence that DNA sequences exhibit core linguistic features,\nspecifically, functional and information redundancy, through comprehensive\nanalysis of genomic and epigenetic datasets. By mapping DNA sequences into a\nlinguistic feature space, we demonstrate that fixed-length (41 bp) DNA segments\nencode information analogously to human language, with redundancy contributing\nto signal stability in aqueous intracellular environments. Moreover, we provide\nthe first evidence of one-dimensional epigenetic memory, showing that linear\nDNA sequences can maintain epigenetic marks such as 6mA methylation,\ncontrasting with models focusing on epigenetic memory transmission via 3D\nchromatin organization[1]. Our tailored linguistic mapping strategy also\naddresses persistent challenges in genomic data processing, significantly\nimproving data cleaning and feature extraction. Together, these findings\nestablish a conceptual paradigm that bridges molecular information encoding and\nlinguistic theory, laying the foundation for next-generation large language\nmodels (LLMs) specifically tailored to DNA, marking a shift at the interface of\nmolecular biology, information theory, and artificial intelligence (AI).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNA is often described as the 'language of life', but whether it possesses\nformal linguistic properties remains unresolved. Here, we present the first\nempirical evidence that DNA sequences exhibit core linguistic features,\nspecifically, functional and information redundancy, through comprehensive\nanalysis of genomic and epigenetic datasets. By mapping DNA sequences into a\nlinguistic feature space, we demonstrate that fixed-length (41 bp) DNA segments\nencode information analogously to human language, with redundancy contributing\nto signal stability in aqueous intracellular environments. Moreover, we provide\nthe first evidence of one-dimensional epigenetic memory, showing that linear\nDNA sequences can maintain epigenetic marks such as 6mA methylation,\ncontrasting with models focusing on epigenetic memory transmission via 3D\nchromatin organization[1]. Our tailored linguistic mapping strategy also\naddresses persistent challenges in genomic data processing, significantly\nimproving data cleaning and feature extraction. Together, these findings\nestablish a conceptual paradigm that bridges molecular information encoding and\nlinguistic theory, laying the foundation for next-generation large language\nmodels (LLMs) specifically tailored to DNA, marking a shift at the interface of\nmolecular biology, information theory, and artificial intelligence (AI)."
                },
                "authors": [
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Dongbo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongbo Wang"
                },
                "author": "Dongbo Wang",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05824v1",
                "updated": "2025-04-08T09:06:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    6,
                    52,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:06:52Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    6,
                    52,
                    1,
                    98,
                    0
                ],
                "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency\n  and Accuracy in Large-Scale Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency\n  and Accuracy in Large-Scale Systems"
                },
                "summary": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield."
                },
                "authors": [
                    {
                        "name": "Zhang Dong"
                    },
                    {
                        "name": "Songhang deng"
                    },
                    {
                        "name": "Mingbang Wang"
                    },
                    {
                        "name": "Le Dai"
                    },
                    {
                        "name": "Jiyuan Li"
                    },
                    {
                        "name": "Xingzu Liu"
                    },
                    {
                        "name": "Ruilin Nong"
                    }
                ],
                "author_detail": {
                    "name": "Ruilin Nong"
                },
                "author": "Ruilin Nong",
                "arxiv_comment": "submission of acl 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04650v2",
                "updated": "2025-04-08T09:06:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    6,
                    1,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T00:45:10Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    0,
                    45,
                    10,
                    0,
                    97,
                    0
                ],
                "title": "Autono: A ReAct-Based Highly Robust Autonomous Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autono: A ReAct-Based Highly Robust Autonomous Agent Framework"
                },
                "summary": "This paper proposes a highly robust autonomous agent framework based on the\nReAct paradigm, designed to solve complex tasks through adaptive decision\nmaking and multi-agent collaboration. Unlike traditional frameworks that rely\non fixed workflows generated by LLM-based planners, this framework dynamically\ngenerates next actions during agent execution based on prior trajectories,\nthereby enhancing its robustness. To address potential termination issues\ncaused by adaptive execution paths, I propose a timely abandonment strategy\nincorporating a probabilistic penalty mechanism. For multi-agent collaboration,\nI introduce a memory transfer mechanism that enables shared and dynamically\nupdated memory among agents. The framework's innovative timely abandonment\nstrategy dynamically adjusts the probability of task abandonment via\nprobabilistic penalties, allowing developers to balance conservative and\nexploratory tendencies in agent execution strategies by tuning hyperparameters.\nThis significantly improves adaptability and task execution efficiency in\ncomplex environments. Additionally, agents can be extended through external\ntool integration, supported by modular design and MCP protocol compatibility,\nwhich enables flexible action space expansion. Through explicit division of\nlabor, the multi-agent collaboration mechanism enables agents to focus on\nspecific task components, thereby significantly improving execution efficiency\nand quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a highly robust autonomous agent framework based on the\nReAct paradigm, designed to solve complex tasks through adaptive decision\nmaking and multi-agent collaboration. Unlike traditional frameworks that rely\non fixed workflows generated by LLM-based planners, this framework dynamically\ngenerates next actions during agent execution based on prior trajectories,\nthereby enhancing its robustness. To address potential termination issues\ncaused by adaptive execution paths, I propose a timely abandonment strategy\nincorporating a probabilistic penalty mechanism. For multi-agent collaboration,\nI introduce a memory transfer mechanism that enables shared and dynamically\nupdated memory among agents. The framework's innovative timely abandonment\nstrategy dynamically adjusts the probability of task abandonment via\nprobabilistic penalties, allowing developers to balance conservative and\nexploratory tendencies in agent execution strategies by tuning hyperparameters.\nThis significantly improves adaptability and task execution efficiency in\ncomplex environments. Additionally, agents can be extended through external\ntool integration, supported by modular design and MCP protocol compatibility,\nwhich enables flexible action space expansion. Through explicit division of\nlabor, the multi-agent collaboration mechanism enables agents to focus on\nspecific task components, thereby significantly improving execution efficiency\nand quality."
                },
                "authors": [
                    {
                        "name": "Zihao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Wu"
                },
                "author": "Zihao Wu",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.8; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05822v1",
                "updated": "2025-04-08T09:05:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    5,
                    33,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:05:33Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    5,
                    33,
                    1,
                    98,
                    0
                ],
                "title": "Federated Unlearning Made Practical: Seamless Integration via Negated\n  Pseudo-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Unlearning Made Practical: Seamless Integration via Negated\n  Pseudo-Gradients"
                },
                "summary": "The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, anyway employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negated of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions, thus underscoring its practical\napplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The right to be forgotten is a fundamental principle of privacy-preserving\nregulations and extends to Machine Learning (ML) paradigms such as Federated\nLearning (FL). While FL enhances privacy by enabling collaborative model\ntraining without sharing private data, trained models still retain the\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\noften rely on impractical assumptions for real-world FL deployments, such as\nstoring client update histories or requiring access to a publicly available\ndataset. To address these constraints, this paper introduces a novel method\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\nOur approach only uses standard client model updates, anyway employed during\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\nto be forgotten, we apply the negated of their pseudo-gradients, appropriately\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\nintegrates with FL workflows, incurs no additional computational and\ncommunication overhead beyond standard FL rounds, and supports concurrent\nunlearning requests. We extensively evaluated the proposed method on two\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\na real-world medical imaging dataset for segmentation (ProstateMRI), using\nthree different neural architectures: two residual networks and a vision\ntransformer. The experimental results across various settings demonstrate that\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\nwithout relying on any additional assumptions, thus underscoring its practical\napplicability."
                },
                "authors": [
                    {
                        "name": "Alessio Mora"
                    },
                    {
                        "name": "Carlo Mazzocca"
                    },
                    {
                        "name": "Rebecca Montanari"
                    },
                    {
                        "name": "Paolo Bellavista"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Bellavista"
                },
                "author": "Paolo Bellavista",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13595v2",
                "updated": "2025-04-08T08:57:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    57,
                    22,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-19T10:13:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    13,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMTEB: Massive Multilingual Text Embedding Benchmark"
                },
                "summary": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost."
                },
                "authors": [
                    {
                        "name": "Kenneth Enevoldsen"
                    },
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Imene Kerboua"
                    },
                    {
                        "name": "Márton Kardos"
                    },
                    {
                        "name": "Ashwin Mathur"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Jay Gala"
                    },
                    {
                        "name": "Wissam Siblini"
                    },
                    {
                        "name": "Dominik Krzemiński"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Saba Sturua"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "Gabriel Sequeira"
                    },
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Shreeya Dhakal"
                    },
                    {
                        "name": "Jonathan Rystrøm"
                    },
                    {
                        "name": "Roman Solomatin"
                    },
                    {
                        "name": "Ömer Çağatan"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Martin Bernstorff"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Akshita Sukhlecha"
                    },
                    {
                        "name": "Bhavish Pahwa"
                    },
                    {
                        "name": "Rafał Poświata"
                    },
                    {
                        "name": "Kranthi Kiran GV"
                    },
                    {
                        "name": "Shawon Ashraf"
                    },
                    {
                        "name": "Daniel Auras"
                    },
                    {
                        "name": "Björn Plüster"
                    },
                    {
                        "name": "Jan Philipp Harries"
                    },
                    {
                        "name": "Loïc Magne"
                    },
                    {
                        "name": "Isabelle Mohr"
                    },
                    {
                        "name": "Mariya Hendriksen"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Hippolyte Gisserot-Boukhlef"
                    },
                    {
                        "name": "Tom Aarsen"
                    },
                    {
                        "name": "Jan Kostkan"
                    },
                    {
                        "name": "Konrad Wojtasik"
                    },
                    {
                        "name": "Taemin Lee"
                    },
                    {
                        "name": "Marek Šuppa"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Roberta Rocca"
                    },
                    {
                        "name": "Mohammed Hamdy"
                    },
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "John Yang"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Aleksei Vatolin"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Manan Dey"
                    },
                    {
                        "name": "Dipam Vasani"
                    },
                    {
                        "name": "Pranjal Chitale"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Nguyen Tai"
                    },
                    {
                        "name": "Artem Snegirev"
                    },
                    {
                        "name": "Michael Günther"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xing Han Lù"
                    },
                    {
                        "name": "Jordan Clive"
                    },
                    {
                        "name": "Gayatri Krishnakumar"
                    },
                    {
                        "name": "Anna Maksimova"
                    },
                    {
                        "name": "Silvan Wehrli"
                    },
                    {
                        "name": "Maria Tikhonova"
                    },
                    {
                        "name": "Henil Panchal"
                    },
                    {
                        "name": "Aleksandr Abramov"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Lester James Miranda"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Ruqiya Bin Safi"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Alessia Borghini"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Hongjin Su"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Howard Yen"
                    },
                    {
                        "name": "Lasse Hansen"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Muennighoff"
                },
                "author": "Niklas Muennighoff",
                "arxiv_comment": "Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05812v1",
                "updated": "2025-04-08T08:48:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    48,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:48:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    48,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Question is Already Half the Answer: Fully Unsupervised LLM\n  Reasoning Incentivization"
                },
                "summary": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervisions--such as human labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form commonsense reasoning tasks. Specifically,\nwithout any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves truthfulness\naccuracy of Qwen2.5-7B Instruct from 87.16\\% to 97.25\\% on TruthfulQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervisions--such as human labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form commonsense reasoning tasks. Specifically,\nwithout any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves truthfulness\naccuracy of Qwen2.5-7B Instruct from 87.16\\% to 97.25\\% on TruthfulQA."
                },
                "authors": [
                    {
                        "name": "Qingyang Zhang"
                    },
                    {
                        "name": "Haitao Wu"
                    },
                    {
                        "name": "Changqing Zhang"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03814v2",
                "updated": "2025-04-08T08:45:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    45,
                    26,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Training Loops in LLMs: How training data properties modulate\n  distribution shift in generated data?"
                },
                "summary": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties."
                },
                "authors": [
                    {
                        "name": "Grgur Kovač"
                    },
                    {
                        "name": "Jérémy Perez"
                    },
                    {
                        "name": "Rémy Portelas"
                    },
                    {
                        "name": "Peter Ford Dominey"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05804v1",
                "updated": "2025-04-08T08:36:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    36,
                    18,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:36:18Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    36,
                    18,
                    1,
                    98,
                    0
                ],
                "title": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization"
                },
                "summary": "The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present StealthRank, a novel adversarial ranking attack that\nmanipulates LLM-driven product recommendation systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within product descriptions that subtly yet effectively\ninfluence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs,\ndemonstrating its ability to covertly boost the ranking of target products\nwhile avoiding explicit manipulation traces that can be easily detected. Our\nresults show that StealthRank consistently outperforms state-of-the-art\nadversarial ranking baselines in both effectiveness and stealth, highlighting\ncritical vulnerabilities in LLM-driven recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present StealthRank, a novel adversarial ranking attack that\nmanipulates LLM-driven product recommendation systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within product descriptions that subtly yet effectively\ninfluence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs,\ndemonstrating its ability to covertly boost the ranking of target products\nwhile avoiding explicit manipulation traces that can be easily detected. Our\nresults show that StealthRank consistently outperforms state-of-the-art\nadversarial ranking baselines in both effectiveness and stealth, highlighting\ncritical vulnerabilities in LLM-driven recommendation systems."
                },
                "authors": [
                    {
                        "name": "Yiming Tang"
                    },
                    {
                        "name": "Yi Fan"
                    },
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Tiankai Yang"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Xiyang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiyang Hu"
                },
                "author": "Xiyang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07766v2",
                "updated": "2025-04-08T08:33:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    33,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-01-14T00:47:24Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    0,
                    47,
                    24,
                    1,
                    14,
                    0
                ],
                "title": "Large Language Models for Knowledge Graph Embedding: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Knowledge Graph Embedding: A Survey"
                },
                "summary": "Large language models (LLMs) have garnered significant attention for their\nsuperior performance in many knowledge-driven applications on the world wide\nweb.These models are designed to train hundreds of millions or more parameters\non large amounts of text data, enabling them to understand and generate\nnaturallanguage effectively. As the superior performance of LLMs becomes\napparent,they are increasingly being applied to knowledge graph embedding (KGE)\nrelated tasks to improve the processing results. Traditional KGE representation\nlearning methods map entities and relations into a low-dimensional vector\nspace, enablingthe triples in the knowledge graph to satisfy a specific scoring\nfunction in thevector space. However, based on the powerful language\nunderstanding and seman-tic modeling capabilities of LLMs, that have recently\nbeen invoked to varying degrees in different types of KGE related scenarios\nsuch as multi-modal KGE andopen KGE according to their task characteristics. In\nthis paper, we investigate awide range of approaches for performing\nLLMs-related tasks in different types of KGE scenarios. To better compare the\nvarious approaches, we summarize each KGE scenario in a classification.\nFinally, we discuss the applications in which the methods are mainly used and\nsuggest several forward-looking directions for the development of this new\nresearch area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered significant attention for their\nsuperior performance in many knowledge-driven applications on the world wide\nweb.These models are designed to train hundreds of millions or more parameters\non large amounts of text data, enabling them to understand and generate\nnaturallanguage effectively. As the superior performance of LLMs becomes\napparent,they are increasingly being applied to knowledge graph embedding (KGE)\nrelated tasks to improve the processing results. Traditional KGE representation\nlearning methods map entities and relations into a low-dimensional vector\nspace, enablingthe triples in the knowledge graph to satisfy a specific scoring\nfunction in thevector space. However, based on the powerful language\nunderstanding and seman-tic modeling capabilities of LLMs, that have recently\nbeen invoked to varying degrees in different types of KGE related scenarios\nsuch as multi-modal KGE andopen KGE according to their task characteristics. In\nthis paper, we investigate awide range of approaches for performing\nLLMs-related tasks in different types of KGE scenarios. To better compare the\nvarious approaches, we summarize each KGE scenario in a classification.\nFinally, we discuss the applications in which the methods are mainly used and\nsuggest several forward-looking directions for the development of this new\nresearch area."
                },
                "authors": [
                    {
                        "name": "Bingchen Liu"
                    },
                    {
                        "name": "Yuanyuan Fang"
                    },
                    {
                        "name": "Naixing Xu"
                    },
                    {
                        "name": "Shihao Hou"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05801v1",
                "updated": "2025-04-08T08:31:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    31,
                    3,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:31:03Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    31,
                    3,
                    1,
                    98,
                    0
                ],
                "title": "From Superficial to Deep: Integrating External Knowledge for Follow-up\n  Question Generation Using Knowledge Graph and LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Superficial to Deep: Integrating External Knowledge for Follow-up\n  Question Generation Using Knowledge Graph and LLM"
                },
                "summary": "In a conversational system, dynamically generating follow-up questions based\non context can help users explore information and provide a better user\nexperience. Humans are usually able to ask questions that involve some general\nlife knowledge and demonstrate higher order cognitive skills. However, the\nquestions generated by existing methods are often limited to shallow contextual\nquestions that are uninspiring and have a large gap to the human level. In this\npaper, we propose a three-stage external knowledge-enhanced follow-up question\ngeneration method, which generates questions by identifying contextual topics,\nconstructing a knowledge graph (KG) online, and finally combining these with a\nlarge language model to generate the final question. The model generates\ninformation-rich and exploratory follow-up questions by introducing external\ncommon sense knowledge and performing a knowledge fusion operation. Experiments\nshow that compared to baseline models, our method generates questions that are\nmore informative and closer to human questioning levels while maintaining\ncontextual relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a conversational system, dynamically generating follow-up questions based\non context can help users explore information and provide a better user\nexperience. Humans are usually able to ask questions that involve some general\nlife knowledge and demonstrate higher order cognitive skills. However, the\nquestions generated by existing methods are often limited to shallow contextual\nquestions that are uninspiring and have a large gap to the human level. In this\npaper, we propose a three-stage external knowledge-enhanced follow-up question\ngeneration method, which generates questions by identifying contextual topics,\nconstructing a knowledge graph (KG) online, and finally combining these with a\nlarge language model to generate the final question. The model generates\ninformation-rich and exploratory follow-up questions by introducing external\ncommon sense knowledge and performing a knowledge fusion operation. Experiments\nshow that compared to baseline models, our method generates questions that are\nmore informative and closer to human questioning levels while maintaining\ncontextual relevance."
                },
                "authors": [
                    {
                        "name": "Jianyu Liu"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Sheng Bi"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Guilin Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guilin Qi"
                },
                "author": "Guilin Qi",
                "arxiv_comment": "Proceedings of the 31st International Conference on Computational\n  Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05793v1",
                "updated": "2025-04-08T08:22:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    22,
                    32,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:22:32Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    22,
                    32,
                    1,
                    98,
                    0
                ],
                "title": "Negotiating Strict Latency Limits for Dynamic Real-Time Services in\n  Vehicular Time-Sensitive Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negotiating Strict Latency Limits for Dynamic Real-Time Services in\n  Vehicular Time-Sensitive Networks"
                },
                "summary": "Future vehicles are expected to dynamically deploy in-vehicle applications\nwithin a Service-Oriented Architecture (SOA). Critical services operate under\nhard real-time constraints, which Time-Sensitive Networking (TSN) complements\non the in-vehicle Ethernet layer. TSN ensures deterministic communication\nbetween critical services and its Credit-Based Shaper (CBS) supports dynamic\nresource reservations. However, the dynamic nature of service deployment\nchallenges network resource configuration, since any new reservation may change\nthe latency of already validated flows. In addition, standard methods of\nworst-case latency analysis for CBS have been found incorrect, and current TSN\nstream reservation procedures lack mechanisms to signal application layer\nQuality-of-Service (QoS) requirements or verify deadlines. In this paper, we\npropose a QoS negotiation scheme within the automotive SOA that interacts with\nthe TSN network controller to reserve resources while ensuring latency bounds.\nWe comparatively evaluate reservation schemes using worst-case analysis and\nsimulations of a realistic In-Vehicle Network (IVN) for demonstrating their\nimpact on QoS guarantees, resource utilization, and setup times. We find that\nonly a reservation scheme utilizing per-queue delay budgets and network\ncalculus provides valid configurations and guarantees acceptable latency bounds\nthroughout the IVN. The proposed service negotiation mechanism efficiently\nestablishes 450 vehicular network reservations in just 11ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future vehicles are expected to dynamically deploy in-vehicle applications\nwithin a Service-Oriented Architecture (SOA). Critical services operate under\nhard real-time constraints, which Time-Sensitive Networking (TSN) complements\non the in-vehicle Ethernet layer. TSN ensures deterministic communication\nbetween critical services and its Credit-Based Shaper (CBS) supports dynamic\nresource reservations. However, the dynamic nature of service deployment\nchallenges network resource configuration, since any new reservation may change\nthe latency of already validated flows. In addition, standard methods of\nworst-case latency analysis for CBS have been found incorrect, and current TSN\nstream reservation procedures lack mechanisms to signal application layer\nQuality-of-Service (QoS) requirements or verify deadlines. In this paper, we\npropose a QoS negotiation scheme within the automotive SOA that interacts with\nthe TSN network controller to reserve resources while ensuring latency bounds.\nWe comparatively evaluate reservation schemes using worst-case analysis and\nsimulations of a realistic In-Vehicle Network (IVN) for demonstrating their\nimpact on QoS guarantees, resource utilization, and setup times. We find that\nonly a reservation scheme utilizing per-queue delay budgets and network\ncalculus provides valid configurations and guarantees acceptable latency bounds\nthroughout the IVN. The proposed service negotiation mechanism efficiently\nestablishes 450 vehicular network reservations in just 11ms."
                },
                "authors": [
                    {
                        "name": "Timo Häckel"
                    },
                    {
                        "name": "Lisa Maile"
                    },
                    {
                        "name": "Philipp Meyer"
                    },
                    {
                        "name": "Franz Korf"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Thomas C. Schmidt"
                },
                "author": "Thomas C. Schmidt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v2",
                "updated": "2025-04-08T08:18:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    18,
                    14,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13622v2",
                "updated": "2025-04-08T08:17:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    17,
                    49,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-19T10:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    59,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality\n  Hallucination Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality\n  Hallucination Detection in Large Language Models"
                },
                "summary": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages. Our code is available at\nhttps://github.com/oneonlee/REFIND.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages. Our code is available at\nhttps://github.com/oneonlee/REFIND."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_comment": "Accepted to SemEval@ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05786v1",
                "updated": "2025-04-08T08:11:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    11,
                    39,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:11:39Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    11,
                    39,
                    1,
                    98,
                    0
                ],
                "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM"
                },
                "summary": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Yuxuan Fan"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05770v1",
                "updated": "2025-04-08T07:50:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    50,
                    19,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:50:19Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    50,
                    19,
                    1,
                    98,
                    0
                ],
                "title": "A Lightweight Multi-Module Fusion Approach for Korean Character\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Multi-Module Fusion Approach for Korean Character\n  Recognition"
                },
                "summary": "Optical Character Recognition (OCR) is essential in applications such as\ndocument processing, license plate recognition, and intelligent surveillance.\nHowever, existing OCR models often underperform in real-world scenarios due to\nirregular text layouts, poor image quality, character variability, and high\ncomputational costs.\n  This paper introduces SDA-Net (Stroke-Sensitive Attention and Dynamic Context\nEncoding Network), a lightweight and efficient architecture designed for robust\nsingle-character recognition. SDA-Net incorporates: (1) a Dual Attention\nMechanism to enhance stroke-level and spatial feature extraction; (2) a Dynamic\nContext Encoding module that adaptively refines semantic information using a\nlearnable gating mechanism; (3) a U-Net-inspired Feature Fusion Strategy for\ncombining low-level and high-level features; and (4) a highly optimized\nlightweight backbone that reduces memory and computational demands.\n  Experimental results show that SDA-Net achieves state-of-the-art accuracy on\nchallenging OCR benchmarks, with significantly faster inference, making it\nwell-suited for deployment in real-time and edge-based OCR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Character Recognition (OCR) is essential in applications such as\ndocument processing, license plate recognition, and intelligent surveillance.\nHowever, existing OCR models often underperform in real-world scenarios due to\nirregular text layouts, poor image quality, character variability, and high\ncomputational costs.\n  This paper introduces SDA-Net (Stroke-Sensitive Attention and Dynamic Context\nEncoding Network), a lightweight and efficient architecture designed for robust\nsingle-character recognition. SDA-Net incorporates: (1) a Dual Attention\nMechanism to enhance stroke-level and spatial feature extraction; (2) a Dynamic\nContext Encoding module that adaptively refines semantic information using a\nlearnable gating mechanism; (3) a U-Net-inspired Feature Fusion Strategy for\ncombining low-level and high-level features; and (4) a highly optimized\nlightweight backbone that reduces memory and computational demands.\n  Experimental results show that SDA-Net achieves state-of-the-art accuracy on\nchallenging OCR benchmarks, with significantly faster inference, making it\nwell-suited for deployment in real-time and edge-based OCR systems."
                },
                "authors": [
                    {
                        "name": "Inho Jake Park"
                    },
                    {
                        "name": "Jaehoon Jay Jeong"
                    },
                    {
                        "name": "Ho-Sang Jo"
                    }
                ],
                "author_detail": {
                    "name": "Ho-Sang Jo"
                },
                "author": "Ho-Sang Jo",
                "arxiv_comment": "12 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05764v1",
                "updated": "2025-04-08T07:45:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    45,
                    50,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:45:50Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    45,
                    50,
                    1,
                    98,
                    0
                ],
                "title": "Layer-Aware Embedding Fusion for LLMs in Text Classifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Aware Embedding Fusion for LLMs in Text Classifications"
                },
                "summary": "Embedding fusion has emerged as an effective approach for enhancing\nperformance across various NLP tasks. However, systematic guidelines for\nselecting optimal layers and developing effective fusion strategies for the\nintegration of LLMs remain underexplored. In this study, we propose a\nlayer-aware embedding selection method and investigate how to quantitatively\nevaluate different layers to identify the most important ones for downstream\nNLP tasks, showing that the critical layers vary depending on the dataset. We\nalso explore how combining embeddings from multiple LLMs, without requiring\nmodel fine-tuning, can improve performance. Experiments on four English text\nclassification datasets (SST-2, MR, R8, and R52) demonstrate that different\nlayers in LLMs exhibit varying degrees of representational strength for\nclassification, and that combining embeddings from different models can enhance\nperformance if the models exhibit complementary characteristics. Additionally,\nwe discuss resources overhead (memory and inference time) to provide a balanced\nperspective on the real world feasibility of embedding fusion. Future work will\nexplore multilingual and domain specific datasets, as well as techniques for\nautomating layer selection, to improve both performance and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding fusion has emerged as an effective approach for enhancing\nperformance across various NLP tasks. However, systematic guidelines for\nselecting optimal layers and developing effective fusion strategies for the\nintegration of LLMs remain underexplored. In this study, we propose a\nlayer-aware embedding selection method and investigate how to quantitatively\nevaluate different layers to identify the most important ones for downstream\nNLP tasks, showing that the critical layers vary depending on the dataset. We\nalso explore how combining embeddings from multiple LLMs, without requiring\nmodel fine-tuning, can improve performance. Experiments on four English text\nclassification datasets (SST-2, MR, R8, and R52) demonstrate that different\nlayers in LLMs exhibit varying degrees of representational strength for\nclassification, and that combining embeddings from different models can enhance\nperformance if the models exhibit complementary characteristics. Additionally,\nwe discuss resources overhead (memory and inference time) to provide a balanced\nperspective on the real world feasibility of embedding fusion. Future work will\nexplore multilingual and domain specific datasets, as well as techniques for\nautomating layer selection, to improve both performance and scalability."
                },
                "authors": [
                    {
                        "name": "Jiho Gwak"
                    },
                    {
                        "name": "Yuchul Jung"
                    }
                ],
                "author_detail": {
                    "name": "Yuchul Jung"
                },
                "author": "Yuchul Jung",
                "arxiv_comment": "11 pages, 3 figures, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18681v2",
                "updated": "2025-04-08T07:32:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    32,
                    34,
                    1,
                    98,
                    0
                ],
                "published": "2023-11-30T16:28:40Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    28,
                    40,
                    3,
                    334,
                    0
                ],
                "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance"
                },
                "summary": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog."
                },
                "authors": [
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Özsoy"
                    },
                    {
                        "name": "Benjamin Busam"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Matthias Keicher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Keicher"
                },
                "author": "Matthias Keicher",
                "arxiv_comment": "improved version accepted at MIDL 2025:\n  https://openreview.net/pdf?id=trUvr1gSNI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05747v1",
                "updated": "2025-04-08T07:24:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    24,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:24:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    24,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "SEA-LION: Southeast Asian Languages in One Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA-LION: Southeast Asian Languages in One Network"
                },
                "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity."
                },
                "authors": [
                    {
                        "name": "Raymond Ng"
                    },
                    {
                        "name": "Thanh Ngan Nguyen"
                    },
                    {
                        "name": "Yuli Huang"
                    },
                    {
                        "name": "Ngee Chia Tai"
                    },
                    {
                        "name": "Wai Yi Leong"
                    },
                    {
                        "name": "Wei Qi Leong"
                    },
                    {
                        "name": "Xianbin Yong"
                    },
                    {
                        "name": "Jian Gang Ngui"
                    },
                    {
                        "name": "Yosephine Susanto"
                    },
                    {
                        "name": "Nicholas Cheng"
                    },
                    {
                        "name": "Hamsawardhini Rengarajan"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Adithya Venkatadri Hulagadri"
                    },
                    {
                        "name": "Kok Wai Teng"
                    },
                    {
                        "name": "Yeo Yeow Tong"
                    },
                    {
                        "name": "Bryan Siow"
                    },
                    {
                        "name": "Wei Yi Teo"
                    },
                    {
                        "name": "Wayne Lau"
                    },
                    {
                        "name": "Choon Meng Tan"
                    },
                    {
                        "name": "Brandon Ong"
                    },
                    {
                        "name": "Zhi Hao Ong"
                    },
                    {
                        "name": "Jann Railey Montalan"
                    },
                    {
                        "name": "Adwin Chan"
                    },
                    {
                        "name": "Sajeban Antonyrex"
                    },
                    {
                        "name": "Ren Lee"
                    },
                    {
                        "name": "Esther Choa"
                    },
                    {
                        "name": "David Ong Tat-Wee"
                    },
                    {
                        "name": "Bing Jie Darius Liu"
                    },
                    {
                        "name": "William Chandra Tjhi"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Leslie Teo"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Teo"
                },
                "author": "Leslie Teo",
                "arxiv_comment": "We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07355v2",
                "updated": "2025-04-08T07:15:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    15,
                    16,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-10T09:48:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    48,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models"
                },
                "summary": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology."
                },
                "authors": [
                    {
                        "name": "Andrea Caria"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caria"
                },
                "author": "Andrea Caria",
                "arxiv_comment": "Needs a major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05738v1",
                "updated": "2025-04-08T07:14:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    14,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:14:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    14,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "LLM-assisted Mutation for Whitebox API Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Mutation for Whitebox API Testing"
                },
                "summary": "Cloud applications heavily rely on APIs to communicate with each other and\nexchange data. To ensure the reliability of cloud applications, cloud providers\nwidely adopt API testing techniques. Unfortunately, existing API testing\napproaches are insufficient to reach strict conditions, a problem known as\nfitness plateaus, due to the lack of gradient provided by coverage metrics. To\naddress this issue, we propose MioHint, a novel white-box API testing approach\nthat leverages the code comprehension capabilities of Large Language Model\n(LLM) to boost API testing. The key challenge of LLM-based API testing lies in\nsystem-level testing, which emphasizes the dependencies between requests and\ntargets across functions and files, thereby making the entire codebase the\nobject of analysis. However, feeding the entire codebase to an LLM is\nimpractical due to its limited context length and short memory. MioHint\naddresses this challenge by synergizing static analysis with LLMs. We retrieve\nrelevant code with data-dependency analysis at the statement level, including\ndef-use analysis for variables used in the target and function expansion for\nsubfunctions called by the target.\n  To evaluate the effectiveness of our method, we conducted experiments across\n16 real-world REST API services. The findings reveal that MioHint achieves an\naverage increase of 4.95% absolute in line coverage compared to the baseline,\nEvoMaster, alongside a remarkable factor of 67x improvement in mutation\naccuracy. Furthermore, our method successfully covers over 57% of hard-to-cover\ntargets while in baseline the coverage is less than 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud applications heavily rely on APIs to communicate with each other and\nexchange data. To ensure the reliability of cloud applications, cloud providers\nwidely adopt API testing techniques. Unfortunately, existing API testing\napproaches are insufficient to reach strict conditions, a problem known as\nfitness plateaus, due to the lack of gradient provided by coverage metrics. To\naddress this issue, we propose MioHint, a novel white-box API testing approach\nthat leverages the code comprehension capabilities of Large Language Model\n(LLM) to boost API testing. The key challenge of LLM-based API testing lies in\nsystem-level testing, which emphasizes the dependencies between requests and\ntargets across functions and files, thereby making the entire codebase the\nobject of analysis. However, feeding the entire codebase to an LLM is\nimpractical due to its limited context length and short memory. MioHint\naddresses this challenge by synergizing static analysis with LLMs. We retrieve\nrelevant code with data-dependency analysis at the statement level, including\ndef-use analysis for variables used in the target and function expansion for\nsubfunctions called by the target.\n  To evaluate the effectiveness of our method, we conducted experiments across\n16 real-world REST API services. The findings reveal that MioHint achieves an\naverage increase of 4.95% absolute in line coverage compared to the baseline,\nEvoMaster, alongside a remarkable factor of 67x improvement in mutation\naccuracy. Furthermore, our method successfully covers over 57% of hard-to-cover\ntargets while in baseline the coverage is less than 10%."
                },
                "authors": [
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jiacheng Shen"
                    },
                    {
                        "name": "Yuxin Su"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05736v1",
                "updated": "2025-04-08T07:10:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    10,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:10:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    10,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "Rank-Then-Score: Enhancing Large Language Models for Automated Essay\n  Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank-Then-Score: Enhancing Large Language Models for Automated Essay\n  Scoring"
                },
                "summary": "In recent years, large language models (LLMs) achieve remarkable success\nacross a variety of tasks. However, their potential in the domain of Automated\nEssay Scoring (AES) remains largely underexplored. Moreover, compared to\nEnglish data, the methods for Chinese AES is not well developed. In this paper,\nwe propose Rank-Then-Score (RTS), a fine-tuning framework based on large\nlanguage models to enhance their essay scoring capabilities. Specifically, we\nfine-tune the ranking model (Ranker) with feature-enriched data, and then feed\nthe output of the ranking model, in the form of a candidate score set, with the\nessay content into the scoring model (Scorer) to produce the final score.\nExperimental results on two benchmark datasets, HSK and ASAP, demonstrate that\nRTS consistently outperforms the direct prompting (Vanilla) method in terms of\naverage QWK across all LLMs and datasets, and achieves the best performance on\nChinese essay scoring using the HSK dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) achieve remarkable success\nacross a variety of tasks. However, their potential in the domain of Automated\nEssay Scoring (AES) remains largely underexplored. Moreover, compared to\nEnglish data, the methods for Chinese AES is not well developed. In this paper,\nwe propose Rank-Then-Score (RTS), a fine-tuning framework based on large\nlanguage models to enhance their essay scoring capabilities. Specifically, we\nfine-tune the ranking model (Ranker) with feature-enriched data, and then feed\nthe output of the ranking model, in the form of a candidate score set, with the\nessay content into the scoring model (Scorer) to produce the final score.\nExperimental results on two benchmark datasets, HSK and ASAP, demonstrate that\nRTS consistently outperforms the direct prompting (Vanilla) method in terms of\naverage QWK across all LLMs and datasets, and achieves the best performance on\nChinese essay scoring using the HSK dataset."
                },
                "authors": [
                    {
                        "name": "Yida Cai"
                    },
                    {
                        "name": "Kun Liang"
                    },
                    {
                        "name": "Sanwoo Lee"
                    },
                    {
                        "name": "Qinghan Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05732v1",
                "updated": "2025-04-08T07:03:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    48,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:03:48Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    48,
                    1,
                    98,
                    0
                ],
                "title": "LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling\n  for Generating Long-Form Articles from Extremely Long Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling\n  for Generating Long-Form Articles from Extremely Long Resources"
                },
                "summary": "Long-form generation is crucial for a wide range of practical applications,\ntypically categorized into short-to-long and long-to-long generation. While\nshort-to-long generations have received considerable attention, generating long\ntexts from extremely long resources remains relatively underexplored. The\nprimary challenge in long-to-long generation lies in effectively integrating\nand analyzing relevant information from extensive inputs, which remains\ndifficult for current large language models (LLMs). In this paper, we propose\nLLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\nconvolutional neural networks, which iteratively integrate local features into\nhigher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked\nconvolutional scaling layers to progressively expand the understanding of input\nmaterials. Both quantitative and qualitative experimental results demonstrate\nthat our approach substantially enhances the ability of LLMs to process long\ninputs and generate coherent, informative long-form articles, outperforming\nseveral representative baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form generation is crucial for a wide range of practical applications,\ntypically categorized into short-to-long and long-to-long generation. While\nshort-to-long generations have received considerable attention, generating long\ntexts from extremely long resources remains relatively underexplored. The\nprimary challenge in long-to-long generation lies in effectively integrating\nand analyzing relevant information from extensive inputs, which remains\ndifficult for current large language models (LLMs). In this paper, we propose\nLLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\nconvolutional neural networks, which iteratively integrate local features into\nhigher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked\nconvolutional scaling layers to progressively expand the understanding of input\nmaterials. Both quantitative and qualitative experimental results demonstrate\nthat our approach substantially enhances the ability of LLMs to process long\ninputs and generate coherent, informative long-form articles, outperforming\nseveral representative baselines."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Zhu Zhang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Zirui Ren"
                    },
                    {
                        "name": "Xiaorong Wang"
                    },
                    {
                        "name": "Zhili Li"
                    },
                    {
                        "name": "Chaoqun He"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05731v1",
                "updated": "2025-04-08T07:03:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:03:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Retrieval Augmented Generation with Collaborative Filtering for\n  Personalized Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation with Collaborative Filtering for\n  Personalized Text Generation"
                },
                "summary": "Recently, the personalization of Large Language Models (LLMs) to generate\ncontent that aligns with individual user preferences has garnered widespread\nattention. Personalized Retrieval-Augmented Generation (RAG), which retrieves\nrelevant documents from the user's history to reflect their preferences and\nenhance LLM generation, is one commonly used approach for personalization.\nHowever, existing personalized RAG methods do not consider that the histories\nof similar users can also assist in personalized generation for the current\nuser, meaning that collaborative information between users can also benefit\npersonalized generation. Inspired by the application of collaborative filtering\nin recommender systems, we propose a method called CFRAG, which adapts\nCollaborative Filtering to RAG for personalized text generation. However, this\npresents two challenges: (1)~how to incorporate collaborative information\nwithout explicit user similarity labels? (2)~how to retrieve documents that\nsupport personalized LLM generation? For Challenge 1, we use contrastive\nlearning to train user embeddings to retrieve similar users and introduce\ncollaborative information. For Challenge 2, we design a personalized retriever\nand reranker to retrieve the top-$k$ documents from these users' histories. We\ntake into account the user's preference during retrieval and reranking. Then we\nleverage feedback from the LLM to fine-tune the personalized retriever and\nreranker, enabling them to retrieve documents that meet the personalized\ngeneration needs of the LLM. Experimental results on the Language Model\nPersonalization (LaMP) benchmark validate the effectiveness of CFRAG. Further\nanalysis confirms the importance of incorporating collaborative information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the personalization of Large Language Models (LLMs) to generate\ncontent that aligns with individual user preferences has garnered widespread\nattention. Personalized Retrieval-Augmented Generation (RAG), which retrieves\nrelevant documents from the user's history to reflect their preferences and\nenhance LLM generation, is one commonly used approach for personalization.\nHowever, existing personalized RAG methods do not consider that the histories\nof similar users can also assist in personalized generation for the current\nuser, meaning that collaborative information between users can also benefit\npersonalized generation. Inspired by the application of collaborative filtering\nin recommender systems, we propose a method called CFRAG, which adapts\nCollaborative Filtering to RAG for personalized text generation. However, this\npresents two challenges: (1)~how to incorporate collaborative information\nwithout explicit user similarity labels? (2)~how to retrieve documents that\nsupport personalized LLM generation? For Challenge 1, we use contrastive\nlearning to train user embeddings to retrieve similar users and introduce\ncollaborative information. For Challenge 2, we design a personalized retriever\nand reranker to retrieve the top-$k$ documents from these users' histories. We\ntake into account the user's preference during retrieval and reranking. Then we\nleverage feedback from the LLM to fine-tune the personalized retriever and\nreranker, enabling them to retrieve documents that meet the personalized\ngeneration needs of the LLM. Experimental results on the Language Model\nPersonalization (LaMP) benchmark validate the effectiveness of CFRAG. Further\nanalysis confirms the importance of incorporating collaborative information."
                },
                "authors": [
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "Accepted by SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05730v1",
                "updated": "2025-04-08T07:03:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    8,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T07:03:08Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    7,
                    3,
                    8,
                    1,
                    98,
                    0
                ],
                "title": "Unified Generative Search and Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Generative Search and Recommendation"
                },
                "summary": "Modern commercial platforms typically offer both search and recommendation\nfunctionalities to serve diverse user needs, making joint modeling of these\ntasks an appealing direction. While prior work has shown that integrating\nsearch and recommendation can be mutually beneficial, it also reveals a\nperformance trade-off: enhancements in one task often come at the expense of\nthe other. This challenge arises from their distinct information requirements:\nsearch emphasizes semantic relevance between queries and items, whereas\nrecommendation depends more on collaborative signals among users and items.\nEffectively addressing this trade-off requires tackling two key problems: (1)\nintegrating both semantic and collaborative signals into item representations,\nand (2) guiding the model to distinguish and adapt to the unique demands of\nsearch and recommendation. The emergence of generative retrieval with Large\nLanguage Models (LLMs) presents new possibilities. This paradigm encodes items\nas identifiers and frames both search and recommendation as sequential\ngeneration tasks, offering the flexibility to leverage multiple identifiers and\ntask-specific prompts. In light of this, we introduce GenSAR, a unified\ngenerative framework for balanced search and recommendation. Our approach\ndesigns dual-purpose identifiers and tailored training strategies to\nincorporate complementary signals and align with task-specific objectives.\nExperiments on both public and commercial datasets demonstrate that GenSAR\neffectively reduces the trade-off and achieves state-of-the-art performance on\nboth tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial platforms typically offer both search and recommendation\nfunctionalities to serve diverse user needs, making joint modeling of these\ntasks an appealing direction. While prior work has shown that integrating\nsearch and recommendation can be mutually beneficial, it also reveals a\nperformance trade-off: enhancements in one task often come at the expense of\nthe other. This challenge arises from their distinct information requirements:\nsearch emphasizes semantic relevance between queries and items, whereas\nrecommendation depends more on collaborative signals among users and items.\nEffectively addressing this trade-off requires tackling two key problems: (1)\nintegrating both semantic and collaborative signals into item representations,\nand (2) guiding the model to distinguish and adapt to the unique demands of\nsearch and recommendation. The emergence of generative retrieval with Large\nLanguage Models (LLMs) presents new possibilities. This paradigm encodes items\nas identifiers and frames both search and recommendation as sequential\ngeneration tasks, offering the flexibility to leverage multiple identifiers and\ntask-specific prompts. In light of this, we introduce GenSAR, a unified\ngenerative framework for balanced search and recommendation. Our approach\ndesigns dual-purpose identifiers and tailored training strategies to\nincorporate complementary signals and align with task-specific objectives.\nExperiments on both public and commercial datasets demonstrate that GenSAR\neffectively reduces the trade-off and achieves state-of-the-art performance on\nboth tasks."
                },
                "authors": [
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Enyun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Enyun Yu"
                },
                "author": "Enyun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19679v3",
                "updated": "2025-04-08T06:48:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    48,
                    4,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-27T01:42:10Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    42,
                    10,
                    3,
                    58,
                    0
                ],
                "title": "Old Experience Helps: Leveraging Survey Methodology to Improve AI Text\n  Annotation Reliability in Social Sciences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old Experience Helps: Leveraging Survey Methodology to Improve AI Text\n  Annotation Reliability in Social Sciences"
                },
                "summary": "This paper introduces a framework for assessing the reliability of Large\nLanguage Model (LLM) text annotations in social science research by adapting\nestablished survey methodology principles. Drawing parallels between survey\nrespondent behavior and LLM outputs, the study implements three key\ninterventions: option randomization, position randomization, and reverse\nvalidation. While traditional accuracy metrics may mask model instabilities,\nparticularly in edge cases, the framework provides a more comprehensive\nreliability assessment. Using the F1000 dataset in biomedical science and three\nsizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates\nthat these survey-inspired interventions can effectively identify unreliable\nannotations that might otherwise go undetected through accuracy metrics alone.\nThe results show that 5-25% of LLM annotations change under these\ninterventions, with larger models exhibiting greater stability. Notably, for\nrare categories approximately 50% of \"correct\" annotations demonstrate low\nreliability when subjected to this framework. The paper then introduce an\ninformation-theoretic reliability score (R-score) based on Kullback-Leibler\ndivergence that quantifies annotation confidence and distinguishes between\nrandom guessing and meaningful annotations at the case level. This approach\ncomplements existing expert validation methods by providing a scalable way to\nassess internal annotation reliability and offers practical guidance for prompt\ndesign and downstream analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a framework for assessing the reliability of Large\nLanguage Model (LLM) text annotations in social science research by adapting\nestablished survey methodology principles. Drawing parallels between survey\nrespondent behavior and LLM outputs, the study implements three key\ninterventions: option randomization, position randomization, and reverse\nvalidation. While traditional accuracy metrics may mask model instabilities,\nparticularly in edge cases, the framework provides a more comprehensive\nreliability assessment. Using the F1000 dataset in biomedical science and three\nsizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates\nthat these survey-inspired interventions can effectively identify unreliable\nannotations that might otherwise go undetected through accuracy metrics alone.\nThe results show that 5-25% of LLM annotations change under these\ninterventions, with larger models exhibiting greater stability. Notably, for\nrare categories approximately 50% of \"correct\" annotations demonstrate low\nreliability when subjected to this framework. The paper then introduce an\ninformation-theoretic reliability score (R-score) based on Kullback-Leibler\ndivergence that quantifies annotation confidence and distinguishes between\nrandom guessing and meaningful annotations at the case level. This approach\ncomplements existing expert validation methods by providing a scalable way to\nassess internal annotation reliability and offers practical guidance for prompt\ndesign and downstream analysis."
                },
                "authors": [
                    {
                        "name": "Linzhuo li"
                    }
                ],
                "author_detail": {
                    "name": "Linzhuo li"
                },
                "author": "Linzhuo li",
                "arxiv_comment": "7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02810v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02810v3",
                "updated": "2025-04-08T06:37:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    37,
                    51,
                    1,
                    98,
                    0
                ],
                "published": "2024-09-21T05:54:35Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    5,
                    54,
                    35,
                    5,
                    265,
                    0
                ],
                "title": "StateAct: Enhancing LLM Base Agents via Self-prompting and\n  State-tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StateAct: Enhancing LLM Base Agents via Self-prompting and\n  State-tracking"
                },
                "summary": "Large language models (LLMs) are increasingly used as autonomous agents,\ntackling tasks from robotics to web navigation. Their performance depends on\nthe underlying base agent. Existing methods, however, struggle with\nlong-context reasoning and goal adherence. We introduce StateAct, a novel and\nefficient base agent that enhances decision-making through (1) self-prompting,\nwhich reinforces task goals at every step, and (2) chain-of-states, an\nextension of chain-of-thought that tracks state information over time. StateAct\noutperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30%\non Textcraft, and 7% on Webshop across multiple frontier LLMs. We also\ndemonstrate that StateAct can be used as a drop-in replacement for ReAct with\nadvanced LLM agent methods such as test-time scaling, yielding an additional\n12% gain on Textcraft. By improving efficiency and long-range reasoning without\nrequiring additional training or retrieval, StateAct provides a scalable\nfoundation for LLM agents. We open source our code to support further research\nat https://github.com/ai-nikolai/stateact .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as autonomous agents,\ntackling tasks from robotics to web navigation. Their performance depends on\nthe underlying base agent. Existing methods, however, struggle with\nlong-context reasoning and goal adherence. We introduce StateAct, a novel and\nefficient base agent that enhances decision-making through (1) self-prompting,\nwhich reinforces task goals at every step, and (2) chain-of-states, an\nextension of chain-of-thought that tracks state information over time. StateAct\noutperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30%\non Textcraft, and 7% on Webshop across multiple frontier LLMs. We also\ndemonstrate that StateAct can be used as a drop-in replacement for ReAct with\nadvanced LLM agent methods such as test-time scaling, yielding an additional\n12% gain on Textcraft. By improving efficiency and long-range reasoning without\nrequiring additional training or retrieval, StateAct provides a scalable\nfoundation for LLM agents. We open source our code to support further research\nat https://github.com/ai-nikolai/stateact ."
                },
                "authors": [
                    {
                        "name": "Nikolai Rozanov"
                    },
                    {
                        "name": "Marek Rei"
                    }
                ],
                "author_detail": {
                    "name": "Marek Rei"
                },
                "author": "Marek Rei",
                "arxiv_comment": "9 pages, 5 pages appendix, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02810v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02810v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05716v1",
                "updated": "2025-04-08T06:34:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    34,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:34:15Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    34,
                    15,
                    1,
                    98,
                    0
                ],
                "title": "Single-Agent vs. Multi-Agent LLM Strategies for Automated Student\n  Reflection Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-Agent vs. Multi-Agent LLM Strategies for Automated Student\n  Reflection Assessment"
                },
                "summary": "We explore the use of Large Language Models (LLMs) for automated assessment\nof open-text student reflections and prediction of academic performance.\nTraditional methods for evaluating reflections are time-consuming and may not\nscale effectively in educational settings. In this work, we employ LLMs to\ntransform student reflections into quantitative scores using two assessment\nstrategies (single-agent and multi-agent) and two prompting techniques\n(zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278\nreflections from 377 students over three academic terms, demonstrate that the\nsingle-agent with few-shot strategy achieves the highest match rate with human\nevaluations. Furthermore, models utilizing LLM-assessed reflection scores\noutperform baselines in both at-risk student identification and grade\nprediction tasks. These findings suggest that LLMs can effectively automate\nreflection assessment, reduce educators' workload, and enable timely support\nfor students who may need additional assistance. Our work emphasizes the\npotential of integrating advanced generative AI technologies into educational\npractices to enhance student engagement and academic success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the use of Large Language Models (LLMs) for automated assessment\nof open-text student reflections and prediction of academic performance.\nTraditional methods for evaluating reflections are time-consuming and may not\nscale effectively in educational settings. In this work, we employ LLMs to\ntransform student reflections into quantitative scores using two assessment\nstrategies (single-agent and multi-agent) and two prompting techniques\n(zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278\nreflections from 377 students over three academic terms, demonstrate that the\nsingle-agent with few-shot strategy achieves the highest match rate with human\nevaluations. Furthermore, models utilizing LLM-assessed reflection scores\noutperform baselines in both at-risk student identification and grade\nprediction tasks. These findings suggest that LLMs can effectively automate\nreflection assessment, reduce educators' workload, and enable timely support\nfor students who may need additional assistance. Our work emphasizes the\npotential of integrating advanced generative AI technologies into educational\npractices to enhance student engagement and academic success."
                },
                "authors": [
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Cheng Tang"
                    },
                    {
                        "name": "Valdemar Švábenský"
                    },
                    {
                        "name": "Daisuke Deguchi"
                    },
                    {
                        "name": "Takayoshi Yamashita"
                    },
                    {
                        "name": "Atsushi Shimada"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Shimada"
                },
                "author": "Atsushi Shimada",
                "arxiv_comment": "To be published in Proceedings of the 29th Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.6; K.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07300v3",
                "updated": "2025-04-08T06:21:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    21,
                    53,
                    1,
                    98,
                    0
                ],
                "published": "2024-03-12T04:04:38Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    4,
                    4,
                    38,
                    1,
                    72,
                    0
                ],
                "title": "CALF: Aligning LLMs for Time Series Forecasting via Cross-modal\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALF: Aligning LLMs for Time Series Forecasting via Cross-modal\n  Fine-Tuning"
                },
                "summary": "Deep learning (e.g., Transformer) has been widely and successfully used in\nmultivariate time series forecasting (MTSF). Unlike existing methods that focus\non training models from a single modal of time series input, large language\nmodels (LLMs) based MTSF methods with cross-modal text and time series input\nhave recently shown great superiority, especially with limited temporal data.\nHowever, current LLM-based MTSF methods usually focus on adapting and\nfine-tuning LLMs, while neglecting the distribution discrepancy between textual\nand temporal input tokens, thus leading to sub-optimal performance. To address\nthis issue, we propose a novel Cross-Modal LLM Fine-Tuning (CALF) framework for\nMTSF by reducing the distribution discrepancy between textual and temporal\ndata, which mainly consists of the temporal target branch with temporal input\nand the textual source branch with aligned textual input. To reduce the\ndistribution discrepancy, we develop the cross-modal match module to first\nalign cross-modal input distributions. Additionally, to minimize the modality\ndistribution gap in both feature and output spaces, feature regularization loss\nis developed to align the intermediate features between the two branches for\nbetter weight updates, while output consistency loss is introduced to allow the\noutput representations of both branches to correspond effectively. Thanks to\nthe modality alignment, CALF establishes state-of-the-art performance for both\nlong-term and short-term forecasting tasks with low computational complexity,\nand exhibiting favorable few-shot and zero-shot abilities similar to that in\nLLMs. Code is available at https://github.com/Hank0626/LLaTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (e.g., Transformer) has been widely and successfully used in\nmultivariate time series forecasting (MTSF). Unlike existing methods that focus\non training models from a single modal of time series input, large language\nmodels (LLMs) based MTSF methods with cross-modal text and time series input\nhave recently shown great superiority, especially with limited temporal data.\nHowever, current LLM-based MTSF methods usually focus on adapting and\nfine-tuning LLMs, while neglecting the distribution discrepancy between textual\nand temporal input tokens, thus leading to sub-optimal performance. To address\nthis issue, we propose a novel Cross-Modal LLM Fine-Tuning (CALF) framework for\nMTSF by reducing the distribution discrepancy between textual and temporal\ndata, which mainly consists of the temporal target branch with temporal input\nand the textual source branch with aligned textual input. To reduce the\ndistribution discrepancy, we develop the cross-modal match module to first\nalign cross-modal input distributions. Additionally, to minimize the modality\ndistribution gap in both feature and output spaces, feature regularization loss\nis developed to align the intermediate features between the two branches for\nbetter weight updates, while output consistency loss is introduced to allow the\noutput representations of both branches to correspond effectively. Thanks to\nthe modality alignment, CALF establishes state-of-the-art performance for both\nlong-term and short-term forecasting tasks with low computational complexity,\nand exhibiting favorable few-shot and zero-shot abilities similar to that in\nLLMs. Code is available at https://github.com/Hank0626/LLaTA."
                },
                "authors": [
                    {
                        "name": "Peiyuan Liu"
                    },
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Naiqi Li"
                    },
                    {
                        "name": "Jigang Bao"
                    },
                    {
                        "name": "Xudong Ren"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    }
                ],
                "author_detail": {
                    "name": "Shu-Tao Xia"
                },
                "author": "Shu-Tao Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05711v1",
                "updated": "2025-04-08T06:11:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    11,
                    5,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:11:05Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    11,
                    5,
                    1,
                    98,
                    0
                ],
                "title": "Automated Archival Descriptions with Federated Intelligence of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Archival Descriptions with Federated Intelligence of LLMs"
                },
                "summary": "Enforcing archival standards requires specialized expertise, and manually\ncreating metadata descriptions for archival materials is a tedious and\nerror-prone task. This work aims at exploring the potential of agentic AI and\nlarge language models (LLMs) in addressing the challenges of implementing a\nstandardized archival description process. To this end, we introduce an agentic\nAI-driven system for automated generation of high-quality metadata descriptions\nof archival materials. We develop a federated optimization approach that unites\nthe intelligence of multiple LLMs to construct optimal archival metadata. We\nalso suggest methods to overcome the challenges associated with using LLMs for\nconsistent metadata generation. To evaluate the feasibility and effectiveness\nof our techniques, we conducted extensive experiments using a real-world\ndataset of archival materials, which covers a variety of document types and\ndata formats. The evaluation results demonstrate the feasibility of our\ntechniques and highlight the superior performance of the federated optimization\napproach compared to single-model solutions in metadata quality and\nreliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enforcing archival standards requires specialized expertise, and manually\ncreating metadata descriptions for archival materials is a tedious and\nerror-prone task. This work aims at exploring the potential of agentic AI and\nlarge language models (LLMs) in addressing the challenges of implementing a\nstandardized archival description process. To this end, we introduce an agentic\nAI-driven system for automated generation of high-quality metadata descriptions\nof archival materials. We develop a federated optimization approach that unites\nthe intelligence of multiple LLMs to construct optimal archival metadata. We\nalso suggest methods to overcome the challenges associated with using LLMs for\nconsistent metadata generation. To evaluate the feasibility and effectiveness\nof our techniques, we conducted extensive experiments using a real-world\ndataset of archival materials, which covers a variety of document types and\ndata formats. The evaluation results demonstrate the feasibility of our\ntechniques and highlight the superior performance of the federated optimization\napproach compared to single-model solutions in metadata quality and\nreliability."
                },
                "authors": [
                    {
                        "name": "Jinghua Groppe"
                    },
                    {
                        "name": "Andreas Marquet"
                    },
                    {
                        "name": "Annabel Walz"
                    },
                    {
                        "name": "Sven Groppe"
                    }
                ],
                "author_detail": {
                    "name": "Sven Groppe"
                },
                "author": "Sven Groppe",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05694v1",
                "updated": "2025-04-08T05:35:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    35,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T05:35:38Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    35,
                    38,
                    1,
                    98,
                    0
                ],
                "title": "Large Language Models Enhanced Hyperbolic Space Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Enhanced Hyperbolic Space Recommender Systems"
                },
                "summary": "Large Language Models (LLMs) have attracted significant attention in\nrecommender systems for their excellent world knowledge capabilities. However,\nexisting methods that rely on Euclidean space struggle to capture the rich\nhierarchical information inherent in textual and semantic data, which is\nessential for capturing user preferences. The geometric properties of\nhyperbolic space offer a promising solution to address this issue.\nNevertheless, integrating LLMs-based methods with hyperbolic space to\neffectively extract and incorporate diverse hierarchical information is\nnon-trivial. To this end, we propose a model-agnostic framework, named\nHyperLLM, which extracts and integrates hierarchical information from both\nstructural and semantic perspectives. Structurally, HyperLLM uses LLMs to\ngenerate multi-level classification tags with hierarchical parent-child\nrelationships for each item. Then, tag-item and user-item interactions are\njointly learned and aligned through contrastive learning, thereby providing the\nmodel with clear hierarchical information. Semantically, HyperLLM introduces a\nnovel meta-optimized strategy to extract hierarchical information from semantic\nembeddings and bridge the gap between the semantic and collaborative spaces for\nseamless integration. Extensive experiments show that HyperLLM significantly\noutperforms recommender systems based on hyperbolic space and LLMs, achieving\nperformance improvements of over 40%. Furthermore, HyperLLM not only improves\nrecommender performance but also enhances training stability, highlighting the\ncritical role of hierarchical information in recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have attracted significant attention in\nrecommender systems for their excellent world knowledge capabilities. However,\nexisting methods that rely on Euclidean space struggle to capture the rich\nhierarchical information inherent in textual and semantic data, which is\nessential for capturing user preferences. The geometric properties of\nhyperbolic space offer a promising solution to address this issue.\nNevertheless, integrating LLMs-based methods with hyperbolic space to\neffectively extract and incorporate diverse hierarchical information is\nnon-trivial. To this end, we propose a model-agnostic framework, named\nHyperLLM, which extracts and integrates hierarchical information from both\nstructural and semantic perspectives. Structurally, HyperLLM uses LLMs to\ngenerate multi-level classification tags with hierarchical parent-child\nrelationships for each item. Then, tag-item and user-item interactions are\njointly learned and aligned through contrastive learning, thereby providing the\nmodel with clear hierarchical information. Semantically, HyperLLM introduces a\nnovel meta-optimized strategy to extract hierarchical information from semantic\nembeddings and bridge the gap between the semantic and collaborative spaces for\nseamless integration. Extensive experiments show that HyperLLM significantly\noutperforms recommender systems based on hyperbolic space and LLMs, achieving\nperformance improvements of over 40%. Furthermore, HyperLLM not only improves\nrecommender performance but also enhances training stability, highlighting the\ncritical role of hierarchical information in recommender systems."
                },
                "authors": [
                    {
                        "name": "Wentao Cheng"
                    },
                    {
                        "name": "Zhida Qin"
                    },
                    {
                        "name": "Zexue Wu"
                    },
                    {
                        "name": "Pengzhan Zhou"
                    },
                    {
                        "name": "Tianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Huang"
                },
                "author": "Tianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05693v1",
                "updated": "2025-04-08T05:34:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    34,
                    38,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T05:34:38Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    34,
                    38,
                    1,
                    98,
                    0
                ],
                "title": "STRIVE: A Think & Improve Approach with Iterative Refinement for\n  Enhancing Question Quality Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRIVE: A Think & Improve Approach with Iterative Refinement for\n  Enhancing Question Quality Estimation"
                },
                "summary": "Automatically assessing question quality is crucial for educators as it saves\ntime, ensures consistency, and provides immediate feedback for refining\nteaching materials. We propose a novel methodology called STRIVE (Structured\nThinking and Refinement with multiLLMs for Improving Verified Question\nEstimation) using a series of Large Language Models (LLMs) for automatic\nquestion evaluation. This approach aims to improve the accuracy and depth of\nquestion quality assessment, ultimately supporting diverse learners and\nenhancing educational practices. The method estimates question quality in an\nautomated manner by generating multiple evaluations based on the strengths and\nweaknesses of the provided question and then choosing the best solution\ngenerated by the LLM. Then the process is improved by iterative review and\nresponse with another LLM until the evaluation metric values converge. This\nsophisticated method of evaluating question quality improves the estimation of\nquestion quality by automating the task of question quality evaluation.\nCorrelation scores show that using this proposed method helps to improve\ncorrelation with human judgments compared to the baseline method. Error\nanalysis shows that metrics like relevance and appropriateness improve\nsignificantly relative to human judgments by using STRIVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically assessing question quality is crucial for educators as it saves\ntime, ensures consistency, and provides immediate feedback for refining\nteaching materials. We propose a novel methodology called STRIVE (Structured\nThinking and Refinement with multiLLMs for Improving Verified Question\nEstimation) using a series of Large Language Models (LLMs) for automatic\nquestion evaluation. This approach aims to improve the accuracy and depth of\nquestion quality assessment, ultimately supporting diverse learners and\nenhancing educational practices. The method estimates question quality in an\nautomated manner by generating multiple evaluations based on the strengths and\nweaknesses of the provided question and then choosing the best solution\ngenerated by the LLM. Then the process is improved by iterative review and\nresponse with another LLM until the evaluation metric values converge. This\nsophisticated method of evaluating question quality improves the estimation of\nquestion quality by automating the task of question quality evaluation.\nCorrelation scores show that using this proposed method helps to improve\ncorrelation with human judgments compared to the baseline method. Error\nanalysis shows that metrics like relevance and appropriateness improve\nsignificantly relative to human judgments by using STRIVE."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05689v1",
                "updated": "2025-04-08T05:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    20,
                    56,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T05:20:56Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    20,
                    56,
                    1,
                    98,
                    0
                ],
                "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language\n  Models Caused by Role Separators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language\n  Models Caused by Role Separators"
                },
                "summary": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods."
                },
                "authors": [
                    {
                        "name": "Xitao Li"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00092v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00092v3",
                "updated": "2025-04-08T05:07:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    7,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2024-08-26T12:00:29Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    12,
                    0,
                    29,
                    0,
                    239,
                    0
                ],
                "title": "Large Language Model for Patent Concept Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Patent Concept Generation"
                },
                "summary": "In traditional innovation practices, concept and IP generation are often\niteratively integrated. Both processes demand an intricate understanding of\nadvanced technical domain knowledge. Existing large language models (LLMs),\nwhile possessing massive pre-trained knowledge, often fall short in the\ninnovative concept generation due to a lack of specialized knowledge necessary\nfor the generation. To bridge this critical gap, we propose a novel knowledge\nfinetuning (KFT) framework to endow LLM-based AI with the ability to\nautonomously mine, understand, and apply domain-specific knowledge and concepts\nfor invention generation, i.e., concept and patent generation together. Our\nproposed PatentGPT integrates knowledge injection pre-training (KPT),\ndomain-specific supervised finetuning (SFT), and reinforcement learning from\nhuman feedback (RLHF). Extensive evaluation shows that PatentGPT significantly\noutperforms the state-of-the-art models on patent-related benchmark tests. Our\nmethod not only provides new insights into data-driven innovation but also\npaves a new path to fine-tune LLMs for applications in the context of\ntechnology. We also discuss the managerial and policy implications of\nAI-generating inventions in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional innovation practices, concept and IP generation are often\niteratively integrated. Both processes demand an intricate understanding of\nadvanced technical domain knowledge. Existing large language models (LLMs),\nwhile possessing massive pre-trained knowledge, often fall short in the\ninnovative concept generation due to a lack of specialized knowledge necessary\nfor the generation. To bridge this critical gap, we propose a novel knowledge\nfinetuning (KFT) framework to endow LLM-based AI with the ability to\nautonomously mine, understand, and apply domain-specific knowledge and concepts\nfor invention generation, i.e., concept and patent generation together. Our\nproposed PatentGPT integrates knowledge injection pre-training (KPT),\ndomain-specific supervised finetuning (SFT), and reinforcement learning from\nhuman feedback (RLHF). Extensive evaluation shows that PatentGPT significantly\noutperforms the state-of-the-art models on patent-related benchmark tests. Our\nmethod not only provides new insights into data-driven innovation but also\npaves a new path to fine-tune LLMs for applications in the context of\ntechnology. We also discuss the managerial and policy implications of\nAI-generating inventions in the future."
                },
                "authors": [
                    {
                        "name": "Runtao Ren"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Jianxi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Luo"
                },
                "author": "Jianxi Luo",
                "arxiv_doi": "10.1016/j.aei.2025.103301",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.aei.2025.103301",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.00092v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00092v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in Advanced Engineering Informatics, Link:\n  https://doi.org/10.1016/j.aei.2025.103301",
                "arxiv_journal_ref": "Advanced Engineering Informatics 65 (2025): 103301",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12485v2",
                "updated": "2025-04-08T04:50:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    50,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-18T03:11:06Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    11,
                    6,
                    1,
                    49,
                    0
                ],
                "title": "Safe at the Margins: A General Approach to Safety Alignment in\n  Low-Resource English Languages -- A Singlish Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe at the Margins: A General Approach to Safety Alignment in\n  Low-Resource English Languages -- A Singlish Case Study"
                },
                "summary": "Ensuring the safety of Large Language Models (LLMs) in diverse linguistic\nsettings remains challenging, particularly for low-resource languages. Existing\nsafety alignment methods are English-centric, limiting their effectiveness. We\nsystematically compare Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Kahneman-Tversky Optimization (KTO) for aligning\nSEA-Lion-v2.1-Instruct, a Llama 3-8B variant, to reduce toxicity in Singlish.\nOur results show that SFT+KTO achieves superior safety alignment with higher\nsample efficiency than DPO. Additionally, we introduce KTO-S, which enhances\nstability via improved KL divergence regularization. Our approach reduces\nSinglish toxicity by 99\\%, generalizes to TOXIGEN, and maintains strong\nperformance on standard LLM benchmarks, providing a scalable framework for\nsafer AI deployment in multilingual contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of Large Language Models (LLMs) in diverse linguistic\nsettings remains challenging, particularly for low-resource languages. Existing\nsafety alignment methods are English-centric, limiting their effectiveness. We\nsystematically compare Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Kahneman-Tversky Optimization (KTO) for aligning\nSEA-Lion-v2.1-Instruct, a Llama 3-8B variant, to reduce toxicity in Singlish.\nOur results show that SFT+KTO achieves superior safety alignment with higher\nsample efficiency than DPO. Additionally, we introduce KTO-S, which enhances\nstability via improved KL divergence regularization. Our approach reduces\nSinglish toxicity by 99\\%, generalizes to TOXIGEN, and maintains strong\nperformance on standard LLM benchmarks, providing a scalable framework for\nsafer AI deployment in multilingual contexts."
                },
                "authors": [
                    {
                        "name": "Isaac Lim"
                    },
                    {
                        "name": "Shaun Khoo"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    },
                    {
                        "name": "Watson Chua"
                    },
                    {
                        "name": "Jia Yi Goh"
                    },
                    {
                        "name": "Jessica Foo"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Foo"
                },
                "author": "Jessica Foo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05683v1",
                "updated": "2025-04-08T04:46:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    46,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T04:46:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    46,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs\n  Ready for HR Spoken Interview Transcript Analysis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs\n  Ready for HR Spoken Interview Transcript Analysis?"
                },
                "summary": "This research paper presents a comprehensive analysis of the performance of\nprominent pre-trained large language models (LLMs), including GPT-4 Turbo,\nGPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,\ntext-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in\ncomparison to expert human evaluators in providing scores, identifying errors,\nand offering feedback and improvement suggestions to candidates during mock HR\n(Human Resources) interviews. We introduce a dataset called HURIT (Human\nResource Interview Transcripts), which comprises 3,890 HR interview transcripts\nsourced from real-world HR interview scenarios. Our findings reveal that\npre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit\ncommendable performance and are capable of producing evaluations comparable to\nthose of expert human evaluators. Although these LLMs demonstrate proficiency\nin providing scores comparable to human experts in terms of human evaluation\nmetrics, they frequently fail to identify errors and offer specific actionable\nadvice for candidate performance improvement in HR interviews. Our research\nsuggests that the current state-of-the-art pre-trained LLMs are not fully\nconducive for automatic deployment in an HR interview assessment. Instead, our\nfindings advocate for a human-in-the-loop approach, to incorporate manual\nchecks for inconsistencies and provisions for improving feedback quality as a\nmore suitable strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research paper presents a comprehensive analysis of the performance of\nprominent pre-trained large language models (LLMs), including GPT-4 Turbo,\nGPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,\ntext-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in\ncomparison to expert human evaluators in providing scores, identifying errors,\nand offering feedback and improvement suggestions to candidates during mock HR\n(Human Resources) interviews. We introduce a dataset called HURIT (Human\nResource Interview Transcripts), which comprises 3,890 HR interview transcripts\nsourced from real-world HR interview scenarios. Our findings reveal that\npre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit\ncommendable performance and are capable of producing evaluations comparable to\nthose of expert human evaluators. Although these LLMs demonstrate proficiency\nin providing scores comparable to human experts in terms of human evaluation\nmetrics, they frequently fail to identify errors and offer specific actionable\nadvice for candidate performance improvement in HR interviews. Our research\nsuggests that the current state-of-the-art pre-trained LLMs are not fully\nconducive for automatic deployment in an HR interview assessment. Instead, our\nfindings advocate for a human-in-the-loop approach, to incorporate manual\nchecks for inconsistencies and provisions for improving feedback quality as a\nmore suitable strategy."
                },
                "authors": [
                    {
                        "name": "Subhankar Maity"
                    },
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Sudeshna Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Sudeshna Sarkar"
                },
                "author": "Sudeshna Sarkar",
                "arxiv_comment": "32 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05682v1",
                "updated": "2025-04-08T04:45:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    45,
                    0,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T04:45:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    45,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "On the Suitability of Reinforcement Fine-Tuning to Visual Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Suitability of Reinforcement Fine-Tuning to Visual Tasks"
                },
                "summary": "Reinforcement Fine-Tuning (RFT) is proved to be greatly valuable for\nenhancing the reasoning ability of LLMs. Researchers have been starting to\napply RFT to MLLMs, hoping it will also enhance the capabilities of visual\nunderstanding. However, these works are at a very early stage and have not\nexamined how suitable RFT actually is for visual tasks. In this work, we\nendeavor to understand the suitabilities and limitations of RFT for visual\ntasks, through experimental analysis and observations. We start by quantitative\ncomparisons on various tasks, which shows RFT is generally better than SFT on\nvisual tasks. %especially when the number of training samples are limited. To\ncheck whether such advantages are brought up by the reasoning process, we\ndesign a new reward that encourages the model to ``think'' more, whose results\nshow more thinking can be beneficial for complicated tasks but harmful for\nsimple tasks. We hope this study can provide more insight for the rapid\nadvancements on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Fine-Tuning (RFT) is proved to be greatly valuable for\nenhancing the reasoning ability of LLMs. Researchers have been starting to\napply RFT to MLLMs, hoping it will also enhance the capabilities of visual\nunderstanding. However, these works are at a very early stage and have not\nexamined how suitable RFT actually is for visual tasks. In this work, we\nendeavor to understand the suitabilities and limitations of RFT for visual\ntasks, through experimental analysis and observations. We start by quantitative\ncomparisons on various tasks, which shows RFT is generally better than SFT on\nvisual tasks. %especially when the number of training samples are limited. To\ncheck whether such advantages are brought up by the reasoning process, we\ndesign a new reward that encourages the model to ``think'' more, whose results\nshow more thinking can be beneficial for complicated tasks but harmful for\nsimple tasks. We hope this study can provide more insight for the rapid\nadvancements on this topic."
                },
                "authors": [
                    {
                        "name": "Xiaxu Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Chunxu Liu"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Xiaoyan Hu"
                    },
                    {
                        "name": "Chengqian Ma"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]