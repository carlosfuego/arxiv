[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernández"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivanković Nizić"
                    },
                    {
                        "name": "Milko Jaksić"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.04328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04328v1",
                "updated": "2025-02-06T18:59:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    55,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:59:55Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    55,
                    3,
                    37,
                    0
                ],
                "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment"
                },
                "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola."
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Yongming Rao"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Rao"
                },
                "author": "Yongming Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04326v1",
                "updated": "2025-02-06T18:59:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    40,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:59:40Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    40,
                    3,
                    37,
                    0
                ],
                "title": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal\n  LLMs"
                },
                "summary": "In this paper, we introduce WorldSense, the first benchmark to assess the\nmulti-modal video understanding, that simultaneously encompasses visual, audio,\nand text inputs. In contrast to existing benchmarks, our WorldSense has several\nfeatures: (i) collaboration of omni-modality, we design the evaluation tasks to\nfeature a strong coupling of audio and video, requiring models to effectively\nutilize the synergistic perception of omni-modality; (ii) diversity of videos\nand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual\nsynchronised videos, systematically categorized into 8 primary domains and 67\nfine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice\nQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)\nhigh-quality annotations, all the QA pairs are manually labeled by 80 expert\nannotators with multiple rounds of correction to ensure quality. Based on our\nWorldSense, we extensively evaluate various state-of-the-art models. The\nexperimental results indicate that existing models face significant challenges\nin understanding real-world scenarios (48.0% best accuracy). We hope our\nWorldSense can provide a platform for evaluating the ability in constructing\nand understanding coherent contexts from omni-modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce WorldSense, the first benchmark to assess the\nmulti-modal video understanding, that simultaneously encompasses visual, audio,\nand text inputs. In contrast to existing benchmarks, our WorldSense has several\nfeatures: (i) collaboration of omni-modality, we design the evaluation tasks to\nfeature a strong coupling of audio and video, requiring models to effectively\nutilize the synergistic perception of omni-modality; (ii) diversity of videos\nand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual\nsynchronised videos, systematically categorized into 8 primary domains and 67\nfine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice\nQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)\nhigh-quality annotations, all the QA pairs are manually labeled by 80 expert\nannotators with multiple rounds of correction to ensure quality. Based on our\nWorldSense, we extensively evaluate various state-of-the-art models. The\nexperimental results indicate that existing models face significant challenges\nin understanding real-world scenarios (48.0% best accuracy). We hope our\nWorldSense can provide a platform for evaluating the ability in constructing\nand understanding coherent contexts from omni-modality."
                },
                "authors": [
                    {
                        "name": "Jack Hong"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Jiayin Cai"
                    },
                    {
                        "name": "Xiaolong Jiang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04322v1",
                "updated": "2025-02-06T18:59:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:59:02Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    2,
                    3,
                    37,
                    0
                ],
                "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions"
                },
                "summary": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions."
                },
                "authors": [
                    {
                        "name": "Yik Siu Chan"
                    },
                    {
                        "name": "Narutatsu Ri"
                    },
                    {
                        "name": "Yuxin Xiao"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    }
                ],
                "author_detail": {
                    "name": "Marzyeh Ghassemi"
                },
                "author": "Marzyeh Ghassemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04318v1",
                "updated": "2025-02-06T18:58:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    58,
                    45,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:58:45Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    58,
                    45,
                    3,
                    37,
                    0
                ],
                "title": "sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D\n  Reconstruction from Sparse-Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D\n  Reconstruction from Sparse-Views"
                },
                "summary": "Reconstructing unbounded outdoor scenes from sparse outward-facing views\nposes significant challenges due to minimal view overlap. Previous methods\noften lack cross-scene understanding and their primitive-centric formulations\noverload local features to compensate for missing global context, resulting in\nblurriness in unseen parts of the scene. We propose sshELF, a fast, single-shot\npipeline for sparse-view 3D scene reconstruction via hierarchal extrapolation\nof latent features. Our key insights is that disentangling information\nextrapolation from primitive decoding allows efficient transfer of structural\npatterns across training scenes. Our method: (1) learns cross-scene priors to\ngenerate intermediate virtual views to extrapolate to unobserved regions, (2)\noffers a two-stage network design separating virtual view generation from 3D\nprimitive decoding for efficient training and modular model design, and (3)\nintegrates a pre-trained foundation model for joint inference of latent\nfeatures and texture, improving scene understanding and generalization. sshELF\ncan reconstruct 360 degree scenes from six sparse input views and achieves\ncompetitive results on synthetic and real-world datasets. We find that sshELF\nfaithfully reconstructs occluded regions, supports real-time rendering, and\nprovides rich latent features for downstream applications. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing unbounded outdoor scenes from sparse outward-facing views\nposes significant challenges due to minimal view overlap. Previous methods\noften lack cross-scene understanding and their primitive-centric formulations\noverload local features to compensate for missing global context, resulting in\nblurriness in unseen parts of the scene. We propose sshELF, a fast, single-shot\npipeline for sparse-view 3D scene reconstruction via hierarchal extrapolation\nof latent features. Our key insights is that disentangling information\nextrapolation from primitive decoding allows efficient transfer of structural\npatterns across training scenes. Our method: (1) learns cross-scene priors to\ngenerate intermediate virtual views to extrapolate to unobserved regions, (2)\noffers a two-stage network design separating virtual view generation from 3D\nprimitive decoding for efficient training and modular model design, and (3)\nintegrates a pre-trained foundation model for joint inference of latent\nfeatures and texture, improving scene understanding and generalization. sshELF\ncan reconstruct 360 degree scenes from six sparse input views and achieves\ncompetitive results on synthetic and real-world datasets. We find that sshELF\nfaithfully reconstructs occluded regions, supports real-time rendering, and\nprovides rich latent features for downstream applications. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Eyvaz Najafli"
                    },
                    {
                        "name": "Marius Kästingschäfer"
                    },
                    {
                        "name": "Sebastian Bernhard"
                    },
                    {
                        "name": "Thomas Brox"
                    },
                    {
                        "name": "Andreas Geiger"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Geiger"
                },
                "author": "Andreas Geiger",
                "arxiv_comment": "Joint first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04315v1",
                "updated": "2025-02-06T18:57:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    57,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    57,
                    6,
                    3,
                    37,
                    0
                ],
                "title": "ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters"
                },
                "summary": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChamaleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChamaleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChamaleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChamaleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/"
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04313v1",
                "updated": "2025-02-06T18:56:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    56,
                    1,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:56:01Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    56,
                    1,
                    3,
                    37,
                    0
                ],
                "title": "Great Models Think Alike and this Undermines AI Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great Models Think Alike and this Undermines AI Oversight"
                },
                "summary": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight."
                },
                "authors": [
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Joschka Struber"
                    },
                    {
                        "name": "Ilze Amanda Auzina"
                    },
                    {
                        "name": "Karuna K Chandra"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "arxiv_comment": "60 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17460v2",
                "updated": "2025-02-06T18:55:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    55,
                    45,
                    3,
                    37,
                    0
                ],
                "published": "2024-07-24T17:57:21Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    17,
                    57,
                    21,
                    2,
                    206,
                    0
                ],
                "title": "SoNIC: Safe Social Navigation with Adaptive Conformal Inference and\n  Constrained Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoNIC: Safe Social Navigation with Adaptive Conformal Inference and\n  Constrained Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) enables social robots to generate trajectories\nwithout relying on human-designed rules or interventions, making it generally\nmore effective than rule-based systems in adapting to complex, dynamic\nreal-world scenarios. However, social navigation is a safety-critical task that\nrequires robots to avoid collisions with pedestrians, whereas existing RL-based\nsolutions often fall short of ensuring safety in complex environments. In this\npaper, we propose SoNIC, which to the best of our knowledge is the first\nalgorithm that integrates adaptive conformal inference (ACI) with constrained\nreinforcement learning (CRL) to enable safe policy learning for social\nnavigation. Specifically, our method not only augments RL observations with\nACI-generated nonconformity scores, which inform the agent of the quantified\nuncertainty but also employs these uncertainty estimates to effectively guide\nthe behaviors of RL agents by using constrained reinforcement learning. This\nintegration regulates the behaviors of RL agents and enables them to handle\nsafety-critical situations. On the standard CrowdNav benchmark, our method\nachieves a success rate of 96.93%, which is 11.67% higher than the previous\nstate-of-the-art RL method and results in 4.5 times fewer collisions and 2.8\ntimes fewer intrusions to ground-truth human future trajectories as well as\nenhanced robustness in out-of-distribution scenarios. To further validate our\napproach, we deploy our algorithm on a real robot by developing a ROS2-based\nnavigation system. Our experiments demonstrate that the system can generate\nrobust and socially polite decision-making when interacting with both sparse\nand dense crowds. The video demos can be found on our project website:\nhttps://sonic-social-nav.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) enables social robots to generate trajectories\nwithout relying on human-designed rules or interventions, making it generally\nmore effective than rule-based systems in adapting to complex, dynamic\nreal-world scenarios. However, social navigation is a safety-critical task that\nrequires robots to avoid collisions with pedestrians, whereas existing RL-based\nsolutions often fall short of ensuring safety in complex environments. In this\npaper, we propose SoNIC, which to the best of our knowledge is the first\nalgorithm that integrates adaptive conformal inference (ACI) with constrained\nreinforcement learning (CRL) to enable safe policy learning for social\nnavigation. Specifically, our method not only augments RL observations with\nACI-generated nonconformity scores, which inform the agent of the quantified\nuncertainty but also employs these uncertainty estimates to effectively guide\nthe behaviors of RL agents by using constrained reinforcement learning. This\nintegration regulates the behaviors of RL agents and enables them to handle\nsafety-critical situations. On the standard CrowdNav benchmark, our method\nachieves a success rate of 96.93%, which is 11.67% higher than the previous\nstate-of-the-art RL method and results in 4.5 times fewer collisions and 2.8\ntimes fewer intrusions to ground-truth human future trajectories as well as\nenhanced robustness in out-of-distribution scenarios. To further validate our\napproach, we deploy our algorithm on a real robot by developing a ROS2-based\nnavigation system. Our experiments demonstrate that the system can generate\nrobust and socially polite decision-making when interacting with both sparse\nand dense crowds. The video demos can be found on our project website:\nhttps://sonic-social-nav.github.io/."
                },
                "authors": [
                    {
                        "name": "Jianpeng Yao"
                    },
                    {
                        "name": "Xiaopan Zhang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Zejin Wang"
                    },
                    {
                        "name": "Amit K. Roy-Chowdhury"
                    },
                    {
                        "name": "Jiachen Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiachen Li"
                },
                "author": "Jiachen Li",
                "arxiv_comment": "Project website: https://sonic-social-nav.github.io/; 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04309v1",
                "updated": "2025-02-06T18:51:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    51,
                    28,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:51:28Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    51,
                    28,
                    3,
                    37,
                    0
                ],
                "title": "Targeted Learning for Data Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Learning for Data Fairness"
                },
                "summary": "Data and algorithms have the potential to produce and perpetuate\ndiscrimination and disparate treatment. As such, significant effort has been\ninvested in developing approaches to defining, detecting, and eliminating\nunfair outcomes in algorithms. In this paper, we focus on performing\nstatistical inference for fairness. Prior work in fairness inference has\nlargely focused on inferring the fairness properties of a given predictive\nalgorithm. Here, we expand fairness inference by evaluating fairness in the\ndata generating process itself, referred to here as data fairness. We perform\ninference on data fairness using targeted learning, a flexible framework for\nnonparametric inference. We derive estimators demographic parity, equal\nopportunity, and conditional mutual information. Additionally, we find that our\nestimators for probabilistic metrics exploit double robustness. To validate our\napproach, we perform several simulations and apply our estimators to real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and algorithms have the potential to produce and perpetuate\ndiscrimination and disparate treatment. As such, significant effort has been\ninvested in developing approaches to defining, detecting, and eliminating\nunfair outcomes in algorithms. In this paper, we focus on performing\nstatistical inference for fairness. Prior work in fairness inference has\nlargely focused on inferring the fairness properties of a given predictive\nalgorithm. Here, we expand fairness inference by evaluating fairness in the\ndata generating process itself, referred to here as data fairness. We perform\ninference on data fairness using targeted learning, a flexible framework for\nnonparametric inference. We derive estimators demographic parity, equal\nopportunity, and conditional mutual information. Additionally, we find that our\nestimators for probabilistic metrics exploit double robustness. To validate our\napproach, we perform several simulations and apply our estimators to real data."
                },
                "authors": [
                    {
                        "name": "Alexander Asemota"
                    },
                    {
                        "name": "Giles Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Giles Hooker"
                },
                "author": "Giles Hooker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04306v1",
                "updated": "2025-02-06T18:47:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    47,
                    49,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:47:49Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    47,
                    49,
                    3,
                    37,
                    0
                ],
                "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference\n  Optimization"
                },
                "summary": "Recent research has leveraged large language model multi-agent systems for\ncomplex problem-solving while trying to reduce the manual effort required to\nbuild them, driving the development of automated agent workflow optimization\nmethods. However, existing methods remain inflexible due to representational\nlimitations, a lack of adaptability, and poor scalability when relying on\ndiscrete optimization techniques. We address these challenges with ScoreFlow, a\nsimple yet high-performance framework that leverages efficient gradient-based\noptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel\nvariant of the direct preference optimization method that accounts for\nquantitative feedback. Across six benchmarks spanning question answering,\ncoding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over\nexisting baselines. Moreover, it empowers smaller models to outperform larger\nones with lower inference costs. Project:\nhttps://github.com/Gen-Verse/ScoreFlow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has leveraged large language model multi-agent systems for\ncomplex problem-solving while trying to reduce the manual effort required to\nbuild them, driving the development of automated agent workflow optimization\nmethods. However, existing methods remain inflexible due to representational\nlimitations, a lack of adaptability, and poor scalability when relying on\ndiscrete optimization techniques. We address these challenges with ScoreFlow, a\nsimple yet high-performance framework that leverages efficient gradient-based\noptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel\nvariant of the direct preference optimization method that accounts for\nquantitative feedback. Across six benchmarks spanning question answering,\ncoding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over\nexisting baselines. Moreover, it empowers smaller models to outperform larger\nones with lower inference costs. Project:\nhttps://github.com/Gen-Verse/ScoreFlow"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Bryon Aragam"
                    }
                ],
                "author_detail": {
                    "name": "Bryon Aragam"
                },
                "author": "Bryon Aragam",
                "arxiv_comment": "Project: https://github.com/Gen-Verse/ScoreFlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00672v2",
                "updated": "2025-02-06T18:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    41,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-02T05:02:42Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    2,
                    42,
                    6,
                    33,
                    0
                ],
                "title": "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of\n  Model Prediction and Scientific Understanding of Soil Organic Carbon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of\n  Model Prediction and Scientific Understanding of Soil Organic Carbon"
                },
                "summary": "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Big data and the rapid development of artificial intelligence (AI) provide\nunprecedented opportunities to enhance our understanding of the global carbon\ncycle and other biogeochemical processes. However, retrieving mechanistic\nknowledge from big data remains a challenge. Here, we develop a\nBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a\nvectorized process-based soil carbon cycle model (i.e., Community Land Model\nversion 5, CLM5) into a neural network (NN) structure to examine mechanisms\ngoverning soil organic carbon (SOC) storage from big data. BINN demonstrates\nhigh accuracy in retrieving biogeochemical parameter values from synthetic data\nin a parameter recovery experiment. We use BINN to predict six major processes\nregulating the soil carbon cycle (or components in process-based models) from\n25,925 observed SOC profiles across the conterminous US and compared them with\nthe same processes previously retrieved by a Bayesian inference-based\nPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et\nal. 2020; 2023). The high agreement between the spatial patterns of the\nretrieved processes using the two approaches with an average correlation\ncoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge\nfrom big data. Additionally, the integration of neural networks and\nprocess-based models in BINN improves computational efficiency by more than 50\ntimes over PRODA. We conclude that BINN is a transformative tool that harnesses\nthe power of both AI and process-based modeling, facilitating new scientific\ndiscoveries while improving interpretability and accuracy of Earth system\nmodels."
                },
                "authors": [
                    {
                        "name": "Haodi Xu"
                    },
                    {
                        "name": "Joshua Fan"
                    },
                    {
                        "name": "Feng Tao"
                    },
                    {
                        "name": "Lifen Jiang"
                    },
                    {
                        "name": "Fengqi You"
                    },
                    {
                        "name": "Benjamin Z. Houlton"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Carla P. Gomes"
                    },
                    {
                        "name": "Yiqi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yiqi Luo"
                },
                "author": "Yiqi Luo",
                "arxiv_comment": "60 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04295v1",
                "updated": "2025-02-06T18:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code will be available at\nhttps://github.com/HenryLau7/CFPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code will be available at\nhttps://github.com/HenryLau7/CFPO."
                },
                "authors": [
                    {
                        "name": "Yuanye Liu"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Cheng Peng"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Peng"
                },
                "author": "Cheng Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04294v1",
                "updated": "2025-02-06T18:36:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    1,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:36:01Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    1,
                    3,
                    37,
                    0
                ],
                "title": "Prediction-Powered E-Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-Powered E-Values"
                },
                "summary": "Quality statistical inference requires a sufficient amount of data, which can\nbe missing or hard to obtain. To this end, prediction-powered inference has\nrisen as a promising methodology, but existing approaches are largely limited\nto Z-estimation problems such as inference of means and quantiles. In this\npaper, we apply ideas of prediction-powered inference to e-values. By doing so,\nwe inherit all the usual benefits of e-values -- such as anytime-validity,\npost-hoc validity and versatile sequential inference -- as well as greatly\nexpand the set of inferences achievable in a prediction-powered manner. In\nparticular, we show that every inference procedure that can be framed in terms\nof e-values has a prediction-powered counterpart, given by our method. We\nshowcase the effectiveness of our framework across a wide range of inference\ntasks, from simple hypothesis testing and confidence intervals to more involved\nprocedures for change-point detection and causal discovery, which were out of\nreach of previous techniques. Our approach is modular and easily integrable\ninto existing algorithms, making it a compelling choice for practical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality statistical inference requires a sufficient amount of data, which can\nbe missing or hard to obtain. To this end, prediction-powered inference has\nrisen as a promising methodology, but existing approaches are largely limited\nto Z-estimation problems such as inference of means and quantiles. In this\npaper, we apply ideas of prediction-powered inference to e-values. By doing so,\nwe inherit all the usual benefits of e-values -- such as anytime-validity,\npost-hoc validity and versatile sequential inference -- as well as greatly\nexpand the set of inferences achievable in a prediction-powered manner. In\nparticular, we show that every inference procedure that can be framed in terms\nof e-values has a prediction-powered counterpart, given by our method. We\nshowcase the effectiveness of our framework across a wide range of inference\ntasks, from simple hypothesis testing and confidence intervals to more involved\nprocedures for change-point detection and causal discovery, which were out of\nreach of previous techniques. Our approach is modular and easily integrable\ninto existing algorithms, making it a compelling choice for practical\napplications."
                },
                "authors": [
                    {
                        "name": "Daniel Csillag"
                    },
                    {
                        "name": "Claudio José Struchiner"
                    },
                    {
                        "name": "Guilherme Tegoni Goedert"
                    }
                ],
                "author_detail": {
                    "name": "Guilherme Tegoni Goedert"
                },
                "author": "Guilherme Tegoni Goedert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04292v1",
                "updated": "2025-02-06T18:35:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    35,
                    5,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:35:05Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    35,
                    5,
                    3,
                    37,
                    0
                ],
                "title": "Magnetic Reconnection in a Compact Magnetic Dome: Peculiar Emissions and\n  High-velocity Plasma Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Reconnection in a Compact Magnetic Dome: Peculiar Emissions and\n  High-velocity Plasma Flows"
                },
                "summary": "Magnetic reconnection at small spatial scales is a fundamental driver of\nenergy release and plasma dynamics in the lower solar atmosphere. We present\nnovel observations of a brightening in an active region, captured in\nhigh-resolution data from the Daniel K. Inouye Solar Telescope (DKIST) using\nthe Visible Broadband Imager (VBI) and the Visible Spectro-Polarimeter (ViSP).\nThe event exhibits Ellerman bomb-like morphology in the H$\\beta$ filter,\nassociated with flux cancellation between a small negative polarity patch\nadjacent to opposite-polarity plage. Additionally, it displays a distinct\nannular emission pattern in Ca II K, hot jet-like structures containing\nAlfv\\'enic plasma flows, and cooler surges. We employ multi-line, non-local\nthermodynamic equilibrium (non-LTE) inversions of the spectropolarimetric data\nto infer the stratification of the physical parameters of the atmosphere.\nFurthermore, we use the photospheric vector magnetogram inferred from the ViSP\nspectra as a boundary condition for nonlinear force-free field extrapolations,\nrevealing the three-dimensional distribution of squashing factors. We find\nsignificant enhancements in temperature, velocity, and microturbulence confined\nto the upper photosphere and low chromosphere. Our findings provide\nobservational evidence of low-altitude magnetic reconnection along\nquasi-separatrix layers in a compact fan-spine-type configuration, highlighting\nthe complex interplay between magnetic topology, energy release, and plasma\nflows. Finally, we discuss the potential role of nonthermal particles in the\ndistinct emissions at different wavelengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic reconnection at small spatial scales is a fundamental driver of\nenergy release and plasma dynamics in the lower solar atmosphere. We present\nnovel observations of a brightening in an active region, captured in\nhigh-resolution data from the Daniel K. Inouye Solar Telescope (DKIST) using\nthe Visible Broadband Imager (VBI) and the Visible Spectro-Polarimeter (ViSP).\nThe event exhibits Ellerman bomb-like morphology in the H$\\beta$ filter,\nassociated with flux cancellation between a small negative polarity patch\nadjacent to opposite-polarity plage. Additionally, it displays a distinct\nannular emission pattern in Ca II K, hot jet-like structures containing\nAlfv\\'enic plasma flows, and cooler surges. We employ multi-line, non-local\nthermodynamic equilibrium (non-LTE) inversions of the spectropolarimetric data\nto infer the stratification of the physical parameters of the atmosphere.\nFurthermore, we use the photospheric vector magnetogram inferred from the ViSP\nspectra as a boundary condition for nonlinear force-free field extrapolations,\nrevealing the three-dimensional distribution of squashing factors. We find\nsignificant enhancements in temperature, velocity, and microturbulence confined\nto the upper photosphere and low chromosphere. Our findings provide\nobservational evidence of low-altitude magnetic reconnection along\nquasi-separatrix layers in a compact fan-spine-type configuration, highlighting\nthe complex interplay between magnetic topology, energy release, and plasma\nflows. Finally, we discuss the potential role of nonthermal particles in the\ndistinct emissions at different wavelengths."
                },
                "authors": [
                    {
                        "name": "J. M. da Silva Santos"
                    },
                    {
                        "name": "E. Dunnington"
                    },
                    {
                        "name": "R. Jarolim"
                    },
                    {
                        "name": "S. Danilovic"
                    },
                    {
                        "name": "S. Criscuoli"
                    }
                ],
                "author_detail": {
                    "name": "S. Criscuoli"
                },
                "author": "S. Criscuoli",
                "arxiv_comment": "submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04283v1",
                "updated": "2025-02-06T18:30:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    30,
                    57,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:30:57Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    30,
                    57,
                    3,
                    37,
                    0
                ],
                "title": "The Young Ages of 70 μm-dark Clumps Inferred from Carbon Chain\n  Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Young Ages of 70 μm-dark Clumps Inferred from Carbon Chain\n  Chemistry"
                },
                "summary": "The physical conditions of the earliest environment of high-mass star\nformation are currently poorly understood. To that end, we present observations\nof the carbon chain molecules HC$_5$N , CCS, and HC$_7$N in the 22-25 GHz band\ntowards 12 high-mass 70 micron-dark clumps (SMDC) with the Jansky Very Large\nArray (VLA). We detect HC$_5$N and CCS towards 11 of these SMDC sources. We\ncalculate column densities and abundances relative to H$_2$ for HC$_5$N and\nCCS. We do not find any clear HC$_7$N detections in the 11 sources\nindividually, but by stacking the HC$_7$N spectra, we do detect HC$_7$N on\naverage in these sources. We also calculate the ratio of the column densities\nof HC$_5$N to HC$_7$N using the stacked spectra of both species. We compare our\nmeasured abundances of HC$_5$N and our measured ratio of HC$_5$N to HC$_7$N to\nthe UMIST dark cloud chemistry models to constrain an age for the gas assuming\na fixed volume density and temperature. The chemical models favor a chemical\nevolutionary age less than 1 Myr at densities of n(H2) = 2 x 10$^4$ cm$^{-3}$.\nThe consistent carbon-chain detections and young model-derived ages support the\nconclusion that these 11 70 micron-dark clumps lack high mass protostars\nbecause they are young and not because they are inefficient and incapable of\nhigh mass star formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The physical conditions of the earliest environment of high-mass star\nformation are currently poorly understood. To that end, we present observations\nof the carbon chain molecules HC$_5$N , CCS, and HC$_7$N in the 22-25 GHz band\ntowards 12 high-mass 70 micron-dark clumps (SMDC) with the Jansky Very Large\nArray (VLA). We detect HC$_5$N and CCS towards 11 of these SMDC sources. We\ncalculate column densities and abundances relative to H$_2$ for HC$_5$N and\nCCS. We do not find any clear HC$_7$N detections in the 11 sources\nindividually, but by stacking the HC$_7$N spectra, we do detect HC$_7$N on\naverage in these sources. We also calculate the ratio of the column densities\nof HC$_5$N to HC$_7$N using the stacked spectra of both species. We compare our\nmeasured abundances of HC$_5$N and our measured ratio of HC$_5$N to HC$_7$N to\nthe UMIST dark cloud chemistry models to constrain an age for the gas assuming\na fixed volume density and temperature. The chemical models favor a chemical\nevolutionary age less than 1 Myr at densities of n(H2) = 2 x 10$^4$ cm$^{-3}$.\nThe consistent carbon-chain detections and young model-derived ages support the\nconclusion that these 11 70 micron-dark clumps lack high mass protostars\nbecause they are young and not because they are inefficient and incapable of\nhigh mass star formation."
                },
                "authors": [
                    {
                        "name": "Kadin Worthen"
                    },
                    {
                        "name": "Brian E. Svoboda"
                    },
                    {
                        "name": "David S. Meier"
                    },
                    {
                        "name": "Juergen Ott"
                    },
                    {
                        "name": "Rachel Friesen"
                    },
                    {
                        "name": "Jennifer Patience"
                    },
                    {
                        "name": "Yancy Shirley"
                    }
                ],
                "author_detail": {
                    "name": "Yancy Shirley"
                },
                "author": "Yancy Shirley",
                "arxiv_comment": "Accepted for publication in ApJ. 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04282v1",
                "updated": "2025-02-06T18:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    30,
                    30,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    30,
                    30,
                    3,
                    37,
                    0
                ],
                "title": "Isolating the hard core of phaseless inference: the Phase selection\n  formulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolating the hard core of phaseless inference: the Phase selection\n  formulation"
                },
                "summary": "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency."
                },
                "authors": [
                    {
                        "name": "Davide Straziota"
                    },
                    {
                        "name": "Luca Saglietti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Saglietti"
                },
                "author": "Luca Saglietti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19792v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19792v3",
                "updated": "2025-02-06T18:15:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    15,
                    48,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-27T18:45:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    45,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "InfAlign: Inference-aware language model alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfAlign: Inference-aware language model alignment"
                },
                "summary": "Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model alignment is a critical step in training modern generative\nlanguage models. Alignment targets to improve win rate of a sample from the\naligned model against the base model. Today, we are increasingly using\ninference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)\nto decode from language models rather than standard sampling. We show that this\ntrain/test mismatch makes standard RLHF framework sub-optimal in view of such\ninference-time methods. To this end, we propose a framework for inference-aware\nalignment (InfAlign), which aims to optimize inference-time win rate of the\naligned policy against the base model. We prove that for any inference-time\ndecoding procedure, the optimal aligned policy is the solution to the standard\nRLHF problem with a transformation of the reward. This motivates us to provide\nthe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,\nwhich involves a reward calibration step and a KL-regularized reward\nmaximization step with a transformation of the calibrated reward. For best-of-N\nsampling and best-of-N jailbreaking, we propose specific transformations\noffering up to 3-8% improvement on inference-time win rates. Finally, we also\nshow that our proposed reward calibration method is a strong baseline for\noptimizing standard win rate."
                },
                "authors": [
                    {
                        "name": "Ananth Balashankar"
                    },
                    {
                        "name": "Ziteng Sun"
                    },
                    {
                        "name": "Jonathan Berant"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    },
                    {
                        "name": "Michael Collins"
                    },
                    {
                        "name": "Adrian Hutter"
                    },
                    {
                        "name": "Jong Lee"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Flavien Prost"
                    },
                    {
                        "name": "Aradhana Sinha"
                    },
                    {
                        "name": "Ananda Theertha Suresh"
                    },
                    {
                        "name": "Ahmad Beirami"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Beirami"
                },
                "author": "Ahmad Beirami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19792v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19792v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14662v3",
                "updated": "2025-02-06T18:12:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    12,
                    57,
                    3,
                    37,
                    0
                ],
                "published": "2024-06-20T18:30:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    30,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "Advantage Alignment Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advantage Alignment Algorithms"
                },
                "summary": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation."
                },
                "authors": [
                    {
                        "name": "Juan Agustin Duque"
                    },
                    {
                        "name": "Milad Aghajohari"
                    },
                    {
                        "name": "Tim Cooijmans"
                    },
                    {
                        "name": "Razvan Ciuca"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Aaron Courville"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Courville"
                },
                "author": "Aaron Courville",
                "arxiv_comment": "25 Pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15475v2",
                "updated": "2025-02-06T18:11:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    11,
                    29,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-26T10:32:54Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    10,
                    32,
                    54,
                    6,
                    26,
                    0
                ],
                "title": "The Same Only Different: On Information Modality for Configuration\n  Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Same Only Different: On Information Modality for Configuration\n  Performance Analysis"
                },
                "summary": "Configuration in software systems helps to ensure efficient operation and\nmeet diverse user needs. Yet, some, if not all, configuration options have\nprofound implications for the system's performance. Configuration performance\nanalysis, wherein the key is to understand (or infer) the configuration\noptions' relations and their impacts on performance, is crucial. Two major\nmodalities exist that serve as the source information in the analysis: either\nthe manual or source code. However, it remains unclear what roles they play in\nconfiguration performance analysis. Much work that relies on manuals claims\ntheir benefits of information richness and naturalness; while work that trusts\nthe source code more prefers the structural information provided therein and\ncriticizes the timeliness of manuals. To fill such a gap, in this paper, we\nconduct an extensive empirical study over 10 systems, covering 1,694 options,\n106,798 words in the manual, and 22,859,552 lines-of-code for investigating the\nusefulness of manual and code in two important tasks of configuration\nperformance analysis, namely performance-sensitive options identification and\nthe associated dependencies extraction. We reveal several new findings and\ninsights, such as it is beneficial to fuse the manual and code modalities for\nboth tasks; the current automated tools that rely on a single modality are far\nfrom being practically useful and generally remain incomparable to human\nanalysis. All those pave the way for further advancing configuration\nperformance analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configuration in software systems helps to ensure efficient operation and\nmeet diverse user needs. Yet, some, if not all, configuration options have\nprofound implications for the system's performance. Configuration performance\nanalysis, wherein the key is to understand (or infer) the configuration\noptions' relations and their impacts on performance, is crucial. Two major\nmodalities exist that serve as the source information in the analysis: either\nthe manual or source code. However, it remains unclear what roles they play in\nconfiguration performance analysis. Much work that relies on manuals claims\ntheir benefits of information richness and naturalness; while work that trusts\nthe source code more prefers the structural information provided therein and\ncriticizes the timeliness of manuals. To fill such a gap, in this paper, we\nconduct an extensive empirical study over 10 systems, covering 1,694 options,\n106,798 words in the manual, and 22,859,552 lines-of-code for investigating the\nusefulness of manual and code in two important tasks of configuration\nperformance analysis, namely performance-sensitive options identification and\nthe associated dependencies extraction. We reveal several new findings and\ninsights, such as it is beneficial to fuse the manual and code modalities for\nboth tasks; the current automated tools that rely on a single modality are far\nfrom being practically useful and generally remain incomparable to human\nanalysis. All those pave the way for further advancing configuration\nperformance analysis."
                },
                "authors": [
                    {
                        "name": "Hongyuan Liang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "accepted by ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02962v2",
                "updated": "2025-02-06T17:57:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    57,
                    30,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-05T07:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    57,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "Intrinsic motivation as constrained entropy maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic motivation as constrained entropy maximization"
                },
                "summary": "\"Intrinsic motivation\" refers to the capacity for intelligent systems to be\nmotivated endogenously, i.e. by features of agential architecture itself rather\nthan by learned associations between action and reward. This paper views active\ninference, empowerment, and other formal accounts of intrinsic motivation as\nvariations on the theme of constrained maximum entropy inference, providing a\ngeneral perspective on intrinsic motivation complementary to existing\nframeworks. The connection between free energy and empowerment noted in\nprevious literature is further explored, and it is argued that the\nmaximum-occupancy approach in practice incorporates an implicit model-evidence\nconstraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Intrinsic motivation\" refers to the capacity for intelligent systems to be\nmotivated endogenously, i.e. by features of agential architecture itself rather\nthan by learned associations between action and reward. This paper views active\ninference, empowerment, and other formal accounts of intrinsic motivation as\nvariations on the theme of constrained maximum entropy inference, providing a\ngeneral perspective on intrinsic motivation complementary to existing\nframeworks. The connection between free energy and empowerment noted in\nprevious literature is further explored, and it is argued that the\nmaximum-occupancy approach in practice incorporates an implicit model-evidence\nconstraint."
                },
                "authors": [
                    {
                        "name": "Alex B. Kiefer"
                    }
                ],
                "author_detail": {
                    "name": "Alex B. Kiefer"
                },
                "author": "Alex B. Kiefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04262v1",
                "updated": "2025-02-06T17:54:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    54,
                    10,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:54:10Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    54,
                    10,
                    3,
                    37,
                    0
                ],
                "title": "Efficient Randomized Experiments Using Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Randomized Experiments Using Foundation Models"
                },
                "summary": "Randomized experiments are the preferred approach for evaluating the effects\nof interventions, but they are costly and often yield estimates with\nsubstantial uncertainty. On the other hand, in silico experiments leveraging\nfoundation models offer a cost-effective alternative that can potentially\nattain higher statistical precision. However, the benefits of in silico\nexperiments come with a significant risk: statistical inferences are not valid\nif the models fail to accurately predict experimental responses to\ninterventions. In this paper, we propose a novel approach that integrates the\npredictions from multiple foundation models with experimental data while\npreserving valid statistical inference. Our estimator is consistent and\nasymptotically normal, with asymptotic variance no larger than the standard\nestimator based on experimental data alone. Importantly, these statistical\nproperties hold even when model predictions are arbitrarily biased. Empirical\nresults across several randomized experiments show that our estimator offers\nsubstantial precision gains, equivalent to a reduction of up to 20% in the\nsample size needed to match the same precision as the standard estimator based\non experimental data alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized experiments are the preferred approach for evaluating the effects\nof interventions, but they are costly and often yield estimates with\nsubstantial uncertainty. On the other hand, in silico experiments leveraging\nfoundation models offer a cost-effective alternative that can potentially\nattain higher statistical precision. However, the benefits of in silico\nexperiments come with a significant risk: statistical inferences are not valid\nif the models fail to accurately predict experimental responses to\ninterventions. In this paper, we propose a novel approach that integrates the\npredictions from multiple foundation models with experimental data while\npreserving valid statistical inference. Our estimator is consistent and\nasymptotically normal, with asymptotic variance no larger than the standard\nestimator based on experimental data alone. Importantly, these statistical\nproperties hold even when model predictions are arbitrarily biased. Empirical\nresults across several randomized experiments show that our estimator offers\nsubstantial precision gains, equivalent to a reduction of up to 20% in the\nsample size needed to match the same precision as the standard estimator based\non experimental data alone."
                },
                "authors": [
                    {
                        "name": "Piersilvio De Bartolomeis"
                    },
                    {
                        "name": "Javier Abad"
                    },
                    {
                        "name": "Guanbo Wang"
                    },
                    {
                        "name": "Konstantin Donhauser"
                    },
                    {
                        "name": "Raymond M. Duch"
                    },
                    {
                        "name": "Fanny Yang"
                    },
                    {
                        "name": "Issa J. Dahabreh"
                    }
                ],
                "author_detail": {
                    "name": "Issa J. Dahabreh"
                },
                "author": "Issa J. Dahabreh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04259v1",
                "updated": "2025-02-06T17:43:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    43,
                    35,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:43:35Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    43,
                    35,
                    3,
                    37,
                    0
                ],
                "title": "Cognitive AI framework: advances in the simulation of human thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive AI framework: advances in the simulation of human thought"
                },
                "summary": "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields."
                },
                "authors": [
                    {
                        "name": "Rommel Salas-Guerra"
                    }
                ],
                "author_detail": {
                    "name": "Rommel Salas-Guerra"
                },
                "author": "Rommel Salas-Guerra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04251v1",
                "updated": "2025-02-06T17:40:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    40,
                    53,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:40:53Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    40,
                    53,
                    3,
                    37,
                    0
                ],
                "title": "Combining Language and App UI Analysis for the Automated Assessment of\n  Bug Reproduction Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Language and App UI Analysis for the Automated Assessment of\n  Bug Reproduction Steps"
                },
                "summary": "Bug reports are essential for developers to confirm software problems,\ninvestigate their causes, and validate fixes. Unfortunately, reports often miss\nimportant information or are written unclearly, which can cause delays,\nincreased issue resolution effort, or even the inability to solve issues. One\nof the most common components of reports that are problematic is the steps to\nreproduce the bug(s) (S2Rs), which are essential to replicate the described\nprogram failures and reason about fixes. Given the proclivity for deficiencies\nin reported S2Rs, prior work has proposed techniques that assist reporters in\nwriting or assessing the quality of S2Rs. However, automated understanding of\nS2Rs is challenging, and requires linking nuanced natural language phrases with\nspecific, semantically related program information. Prior techniques often\nstruggle to form such language to program connections - due to issues in\nlanguage variability and limitations of information gleaned from program\nanalyses.\n  To more effectively tackle the problem of S2R quality annotation, we propose\na new technique called AstroBR, which leverages the language understanding\ncapabilities of LLMs to identify and extract the S2Rs from bug reports and map\nthem to GUI interactions in a program state model derived via dynamic analysis.\nWe compared AstroBR to a related state-of-the-art approach and we found that\nAstroBR annotates S2Rs 25.2% better (in terms of F1 score) than the baseline.\nAdditionally, AstroBR suggests more accurate missing S2Rs than the baseline (by\n71.4% in terms of F1 score).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug reports are essential for developers to confirm software problems,\ninvestigate their causes, and validate fixes. Unfortunately, reports often miss\nimportant information or are written unclearly, which can cause delays,\nincreased issue resolution effort, or even the inability to solve issues. One\nof the most common components of reports that are problematic is the steps to\nreproduce the bug(s) (S2Rs), which are essential to replicate the described\nprogram failures and reason about fixes. Given the proclivity for deficiencies\nin reported S2Rs, prior work has proposed techniques that assist reporters in\nwriting or assessing the quality of S2Rs. However, automated understanding of\nS2Rs is challenging, and requires linking nuanced natural language phrases with\nspecific, semantically related program information. Prior techniques often\nstruggle to form such language to program connections - due to issues in\nlanguage variability and limitations of information gleaned from program\nanalyses.\n  To more effectively tackle the problem of S2R quality annotation, we propose\na new technique called AstroBR, which leverages the language understanding\ncapabilities of LLMs to identify and extract the S2Rs from bug reports and map\nthem to GUI interactions in a program state model derived via dynamic analysis.\nWe compared AstroBR to a related state-of-the-art approach and we found that\nAstroBR annotates S2Rs 25.2% better (in terms of F1 score) than the baseline.\nAdditionally, AstroBR suggests more accurate missing S2Rs than the baseline (by\n71.4% in terms of F1 score)."
                },
                "authors": [
                    {
                        "name": "Junayed Mahmud"
                    },
                    {
                        "name": "Antu Saha"
                    },
                    {
                        "name": "Oscar Chaparro"
                    },
                    {
                        "name": "Kevin Moran"
                    },
                    {
                        "name": "Andrian Marcus"
                    }
                ],
                "author_detail": {
                    "name": "Andrian Marcus"
                },
                "author": "Andrian Marcus",
                "arxiv_comment": "12 pages, to appear in the Proceedings of the 33rd IEEE/ACM\n  International Conference on Program Comprehension (ICPC'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06067v3",
                "updated": "2025-02-06T17:35:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    35,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2024-05-09T19:32:49Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    19,
                    32,
                    49,
                    3,
                    130,
                    0
                ],
                "title": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language\n  Processing"
                },
                "summary": "Transformer-based large language models (LLM) have been widely used in\nlanguage processing applications. However, due to the memory constraints of the\ndevices, most of them restrict the context window. Even though recurrent models\nin previous works can memorize past tokens to enable unlimited context and\nmaintain effectiveness, they have ``flat'' memory architectures. Such\narchitectures have limitations in selecting and filtering information. Since\nhumans are good at learning and self-adjustment, we believe that imitating\nbrain memory hierarchy is beneficial for model memorization. Thus, we propose\nthe Hierarchical Memory Transformer (HMT), a novel framework that facilitates a\nmodel's long-context processing ability by imitating human memorization\nbehavior. Leveraging memory-augmented segment-level recurrence, we organize the\nmemory hierarchy by preserving tokens from early input segments, passing memory\nembeddings along the sequence, and recalling relevant information from history.\nEvaluating general language modeling, question-answering tasks, and the\nsummarization task, we show that HMT consistently improves the long-context\nprocessing ability of existing models. Furthermore, HMT achieves a comparable\nor superior generation quality to long-context LLMs with $2 \\sim 57\\times$\nfewer parameters and $2.5 \\sim 116\\times$ less inference memory, significantly\noutperforming previous memory-augmented models. Code on Github:\nhttps://github.com/OswaldHe/HMT-pytorch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLM) have been widely used in\nlanguage processing applications. However, due to the memory constraints of the\ndevices, most of them restrict the context window. Even though recurrent models\nin previous works can memorize past tokens to enable unlimited context and\nmaintain effectiveness, they have ``flat'' memory architectures. Such\narchitectures have limitations in selecting and filtering information. Since\nhumans are good at learning and self-adjustment, we believe that imitating\nbrain memory hierarchy is beneficial for model memorization. Thus, we propose\nthe Hierarchical Memory Transformer (HMT), a novel framework that facilitates a\nmodel's long-context processing ability by imitating human memorization\nbehavior. Leveraging memory-augmented segment-level recurrence, we organize the\nmemory hierarchy by preserving tokens from early input segments, passing memory\nembeddings along the sequence, and recalling relevant information from history.\nEvaluating general language modeling, question-answering tasks, and the\nsummarization task, we show that HMT consistently improves the long-context\nprocessing ability of existing models. Furthermore, HMT achieves a comparable\nor superior generation quality to long-context LLMs with $2 \\sim 57\\times$\nfewer parameters and $2.5 \\sim 116\\times$ less inference memory, significantly\noutperforming previous memory-augmented models. Code on Github:\nhttps://github.com/OswaldHe/HMT-pytorch."
                },
                "authors": [
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Yingqi Cao"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_comment": "NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02138v2",
                "updated": "2025-02-06T17:32:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    32,
                    4,
                    3,
                    37,
                    0
                ],
                "published": "2024-07-02T10:33:31Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    10,
                    33,
                    31,
                    1,
                    184,
                    0
                ],
                "title": "Efficient Nearest Neighbor based Uncertainty Estimation for Natural\n  Language Processing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Nearest Neighbor based Uncertainty Estimation for Natural\n  Language Processing Tasks"
                },
                "summary": "Trustworthiness in model predictions is crucial for safety-critical\napplications in the real world. However, deep neural networks often suffer from\nthe issues of uncertainty estimation, such as miscalibration. In this study, we\npropose $k$-Nearest Neighbor Uncertainty Estimation ($k$NN-UE), which is a new\nuncertainty estimation method that uses not only the distances from the\nneighbors, but also the ratio of labels in the neighbors. Experiments on\nsentiment analysis, natural language inference, and named entity recognition\nshow that our proposed method outperforms the baselines and recent\ndensity-based methods in several calibration and uncertainty metrics. Moreover,\nour analyses indicate that approximate nearest neighbor search techniques\nreduce the inference overhead without significantly degrading the uncertainty\nestimation performance when they are appropriately combined.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in model predictions is crucial for safety-critical\napplications in the real world. However, deep neural networks often suffer from\nthe issues of uncertainty estimation, such as miscalibration. In this study, we\npropose $k$-Nearest Neighbor Uncertainty Estimation ($k$NN-UE), which is a new\nuncertainty estimation method that uses not only the distances from the\nneighbors, but also the ratio of labels in the neighbors. Experiments on\nsentiment analysis, natural language inference, and named entity recognition\nshow that our proposed method outperforms the baselines and recent\ndensity-based methods in several calibration and uncertainty metrics. Moreover,\nour analyses indicate that approximate nearest neighbor search techniques\nreduce the inference overhead without significantly degrading the uncertainty\nestimation performance when they are appropriately combined."
                },
                "authors": [
                    {
                        "name": "Wataru Hashimoto"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15510v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15510v3",
                "updated": "2025-02-06T17:16:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    16,
                    28,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-28T03:45:49Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    45,
                    49,
                    2,
                    241,
                    0
                ],
                "title": "How Reliable are Causal Probing Interventions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable are Causal Probing Interventions?"
                },
                "summary": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing."
                },
                "authors": [
                    {
                        "name": "Marc Canby"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Chirag Rastogi"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15510v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15510v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04227v1",
                "updated": "2025-02-06T17:12:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:12:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks"
                },
                "summary": "We explore the feasibility and effectiveness of using LLM-driven autonomous\nsystems for Assumed Breach penetration testing in enterprise networks. We\nintroduce a novel prototype that, driven by Large Language Models (LLMs), can\ncompromise accounts within a real-life Active Directory testbed. Our research\nprovides a comprehensive evaluation of the prototype's capabilities, and\nhighlights both strengths and limitations while executing attack. The\nevaluation uses a realistic simulation environment (Game of Active Directory,\nGOAD) to capture intricate interactions, stochastic outcomes, and timing\ndependencies that characterize live network scenarios. The study concludes that\nautonomous LLMs are able to conduct Assumed Breach simulations, potentially\ndemocratizing access to penetration testing for organizations facing budgetary\nconstraints.\n  The prototype's source code, traces, and analyzed logs are released as\nopen-source to enhance collective cybersecurity and facilitate future research\nin LLM-driven cybersecurity automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the feasibility and effectiveness of using LLM-driven autonomous\nsystems for Assumed Breach penetration testing in enterprise networks. We\nintroduce a novel prototype that, driven by Large Language Models (LLMs), can\ncompromise accounts within a real-life Active Directory testbed. Our research\nprovides a comprehensive evaluation of the prototype's capabilities, and\nhighlights both strengths and limitations while executing attack. The\nevaluation uses a realistic simulation environment (Game of Active Directory,\nGOAD) to capture intricate interactions, stochastic outcomes, and timing\ndependencies that characterize live network scenarios. The study concludes that\nautonomous LLMs are able to conduct Assumed Breach simulations, potentially\ndemocratizing access to penetration testing for organizations facing budgetary\nconstraints.\n  The prototype's source code, traces, and analyzed logs are released as\nopen-source to enhance collective cybersecurity and facilitate future research\nin LLM-driven cybersecurity automation."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04226v1",
                "updated": "2025-02-06T17:12:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    7,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:12:07Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    7,
                    3,
                    37,
                    0
                ],
                "title": "Keep It Light! Simplifying Image Clustering Via Text-Free Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep It Light! Simplifying Image Clustering Via Text-Free Adapters"
                },
                "summary": "Many competitive clustering pipelines have a multi-modal design, leveraging\nlarge language models (LLMs) or other text encoders, and text-image pairs,\nwhich are often unavailable in real-world downstream applications.\nAdditionally, such frameworks are generally complicated to train and require\nsubstantial computational resources, making widespread adoption challenging. In\nthis work, we show that in deep clustering, competitive performance with more\ncomplex state-of-the-art methods can be achieved using a text-free and highly\nsimplified training pipeline. In particular, our approach, Simple Clustering\nvia Pre-trained models (SCP), trains only a small cluster head while leveraging\npre-trained vision model feature representations and positive data pairs.\nExperiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100,\nSTL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly\ncompetitive performance. Furthermore, we provide a theoretical result\nexplaining why, at least under ideal conditions, additional text-based\nembeddings may not be necessary to achieve strong clustering performance in\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many competitive clustering pipelines have a multi-modal design, leveraging\nlarge language models (LLMs) or other text encoders, and text-image pairs,\nwhich are often unavailable in real-world downstream applications.\nAdditionally, such frameworks are generally complicated to train and require\nsubstantial computational resources, making widespread adoption challenging. In\nthis work, we show that in deep clustering, competitive performance with more\ncomplex state-of-the-art methods can be achieved using a text-free and highly\nsimplified training pipeline. In particular, our approach, Simple Clustering\nvia Pre-trained models (SCP), trains only a small cluster head while leveraging\npre-trained vision model feature representations and positive data pairs.\nExperiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100,\nSTL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly\ncompetitive performance. Furthermore, we provide a theoretical result\nexplaining why, at least under ideal conditions, additional text-based\nembeddings may not be necessary to achieve strong clustering performance in\nvision."
                },
                "authors": [
                    {
                        "name": "Yicen Li"
                    },
                    {
                        "name": "Haitz Sáez de Ocáriz Borde"
                    },
                    {
                        "name": "Anastasis Kratsios"
                    },
                    {
                        "name": "Paul D. McNicholas"
                    }
                ],
                "author_detail": {
                    "name": "Paul D. McNicholas"
                },
                "author": "Paul D. McNicholas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04223v1",
                "updated": "2025-02-06T17:07:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    7,
                    22,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:07:22Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    7,
                    22,
                    3,
                    37,
                    0
                ],
                "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Éclair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents"
                },
                "summary": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards."
                },
                "authors": [
                    {
                        "name": "Ilia Karmanov"
                    },
                    {
                        "name": "Amala Sanjay Deshmukh"
                    },
                    {
                        "name": "Lukas Voegtle"
                    },
                    {
                        "name": "Philipp Fischer"
                    },
                    {
                        "name": "Kateryna Chumachenko"
                    },
                    {
                        "name": "Timo Roman"
                    },
                    {
                        "name": "Jarno Seppänen"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Karan Sapra"
                    }
                ],
                "author_detail": {
                    "name": "Karan Sapra"
                },
                "author": "Karan Sapra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04218v1",
                "updated": "2025-02-06T17:01:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    1,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:01:00Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    1,
                    0,
                    3,
                    37,
                    0
                ],
                "title": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic\n  Data"
                },
                "summary": "Large Language Models (LLMs) have been shown to be biased in prior work, as\nthey generate text that is in line with stereotypical views of the world or\nthat is not representative of the viewpoints and values of historically\nmarginalized demographic groups. In this work, we propose using data from\nparallel men's and women's events at the Olympic Games to investigate different\nforms of gender bias in language models. We define three metrics to measure\nbias, and find that models are consistently biased against women when the\ngender is ambiguous in the prompt. In this case, the model frequently retrieves\nonly the results of the men's event with or without acknowledging them as such,\nrevealing pervasive gender bias in LLMs in the context of athletics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to be biased in prior work, as\nthey generate text that is in line with stereotypical views of the world or\nthat is not representative of the viewpoints and values of historically\nmarginalized demographic groups. In this work, we propose using data from\nparallel men's and women's events at the Olympic Games to investigate different\nforms of gender bias in language models. We define three metrics to measure\nbias, and find that models are consistently biased against women when the\ngender is ambiguous in the prompt. In this case, the model frequently retrieves\nonly the results of the men's event with or without acknowledging them as such,\nrevealing pervasive gender bias in LLMs in the context of athletics."
                },
                "authors": [
                    {
                        "name": "Laura Biester"
                    }
                ],
                "author_detail": {
                    "name": "Laura Biester"
                },
                "author": "Laura Biester",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04204v1",
                "updated": "2025-02-06T16:44:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    44,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:44:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    44,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence"
                },
                "summary": "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https://github.com/fshp971/adv-icl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https://github.com/fshp971/adv-icl."
                },
                "authors": [
                    {
                        "name": "Shaopeng Fu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04194v1",
                "updated": "2025-02-06T16:31:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "The Best Instruction-Tuning Data are Those That Fit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best Instruction-Tuning Data are Those That Fit"
                },
                "summary": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%."
                },
                "authors": [
                    {
                        "name": "Dylan Zhang"
                    },
                    {
                        "name": "Qirun Dai"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04188v1",
                "updated": "2025-02-06T16:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    22,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:22:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    22,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "Automated Microservice Pattern Instance Detection Using\n  Infrastructure-as-Code Artifacts and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Microservice Pattern Instance Detection Using\n  Infrastructure-as-Code Artifacts and Large Language Models"
                },
                "summary": "Documenting software architecture is essential to preserve architecture\nknowledge, even though it is frequently costly. Architecture pattern instances,\nincluding microservice pattern instances, provide important structural software\ninformation. Practitioners should document this information to prevent\nknowledge vaporization. However, architecture patterns may not be detectable by\nanalyzing source code artifacts, requiring the analysis of other types of\nartifacts. Moreover, many existing pattern detection instance approaches are\ncomplex to extend. This article presents our ongoing PhD research, early\nexperiments, and a prototype for a tool we call MicroPAD for automating the\ndetection of microservice pattern instances. The prototype uses Large Language\nModels (LLMs) to analyze Infrastructure-as-Code (IaC) artifacts to aid\ndetection, aiming to keep costs low and maximize the scope of detectable\npatterns. Early experiments ran the prototype thrice in 22 GitHub projects. We\nverified that 83\\% of the patterns that the prototype identified were in the\nproject. The costs of detecting the pattern instances were minimal. These\nresults indicate that the approach is likely viable and, by lowering the entry\nbarrier to automating pattern instance detection, could help democratize\ndeveloper access to this category of architecture knowledge. Finally, we\npresent our overall research methodology, planned future work, and an overview\nof MicroPAD's potential industrial impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Documenting software architecture is essential to preserve architecture\nknowledge, even though it is frequently costly. Architecture pattern instances,\nincluding microservice pattern instances, provide important structural software\ninformation. Practitioners should document this information to prevent\nknowledge vaporization. However, architecture patterns may not be detectable by\nanalyzing source code artifacts, requiring the analysis of other types of\nartifacts. Moreover, many existing pattern detection instance approaches are\ncomplex to extend. This article presents our ongoing PhD research, early\nexperiments, and a prototype for a tool we call MicroPAD for automating the\ndetection of microservice pattern instances. The prototype uses Large Language\nModels (LLMs) to analyze Infrastructure-as-Code (IaC) artifacts to aid\ndetection, aiming to keep costs low and maximize the scope of detectable\npatterns. Early experiments ran the prototype thrice in 22 GitHub projects. We\nverified that 83\\% of the patterns that the prototype identified were in the\nproject. The costs of detecting the pattern instances were minimal. These\nresults indicate that the approach is likely viable and, by lowering the entry\nbarrier to automating pattern instance detection, could help democratize\ndeveloper access to this category of architecture knowledge. Finally, we\npresent our overall research methodology, planned future work, and an overview\nof MicroPAD's potential industrial impact."
                },
                "authors": [
                    {
                        "name": "Carlos Eduardo Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Eduardo Duarte"
                },
                "author": "Carlos Eduardo Duarte",
                "arxiv_comment": "ICSA 2025 - International Conference on Software Architecture. 6\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12012v2",
                "updated": "2025-02-06T16:18:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    18,
                    9,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-21T10:06:19Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    6,
                    19,
                    1,
                    21,
                    0
                ],
                "title": "TabularARGN: A Flexible and Efficient Auto-Regressive Framework for\n  Generating High-Fidelity Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabularARGN: A Flexible and Efficient Auto-Regressive Framework for\n  Generating High-Fidelity Synthetic Data"
                },
                "summary": "Synthetic data generation for tabular datasets must balance fidelity,\nefficiency, and versatility to meet the demands of real-world applications. We\nintroduce the Tabular Auto-Regressive Generative Network (TabularARGN), a\nflexible framework designed to handle mixed-type, multivariate, and sequential\ndatasets. By training on all possible conditional probabilities, TabularARGN\nsupports advanced features such as fairness-aware generation, imputation, and\nconditional generation on any subset of columns. The framework achieves\nstate-of-the-art synthetic data quality while significantly reducing training\nand inference times, making it ideal for large-scale datasets with diverse\nstructures. Evaluated across established benchmarks, including realistic\ndatasets with complex relationships, TabularARGN demonstrates its capability to\nsynthesize high-quality data efficiently. By unifying flexibility and\nperformance, this framework paves the way for practical synthetic data\ngeneration across industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation for tabular datasets must balance fidelity,\nefficiency, and versatility to meet the demands of real-world applications. We\nintroduce the Tabular Auto-Regressive Generative Network (TabularARGN), a\nflexible framework designed to handle mixed-type, multivariate, and sequential\ndatasets. By training on all possible conditional probabilities, TabularARGN\nsupports advanced features such as fairness-aware generation, imputation, and\nconditional generation on any subset of columns. The framework achieves\nstate-of-the-art synthetic data quality while significantly reducing training\nand inference times, making it ideal for large-scale datasets with diverse\nstructures. Evaluated across established benchmarks, including realistic\ndatasets with complex relationships, TabularARGN demonstrates its capability to\nsynthesize high-quality data efficiently. By unifying flexibility and\nperformance, this framework paves the way for practical synthetic data\ngeneration across industries."
                },
                "authors": [
                    {
                        "name": "Paul Tiwald"
                    },
                    {
                        "name": "Ivona Krchova"
                    },
                    {
                        "name": "Andrey Sidorenko"
                    },
                    {
                        "name": "Mariana Vargas Vieyra"
                    },
                    {
                        "name": "Mario Scriminaci"
                    },
                    {
                        "name": "Michael Platzer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Platzer"
                },
                "author": "Michael Platzer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04184v1",
                "updated": "2025-02-06T16:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?"
                },
                "summary": "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nnotebook\\textquotesingle s executability improves by 42.7% and 28% by\ninstalling the correct modules and generating synthetic data. These findings\nchallenge prior assumptions, suggesting that notebooks have higher\nexecutability than previously reported, many of which offer valuable partial\nexecution, and that their executability should be evaluated within the\ninteractive notebook paradigm rather than through traditional software\nexecutability standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nnotebook\\textquotesingle s executability improves by 42.7% and 28% by\ninstalling the correct modules and generating synthetic data. These findings\nchallenge prior assumptions, suggesting that notebooks have higher\nexecutability than previously reported, many of which offer valuable partial\nexecution, and that their executability should be evaluated within the\ninteractive notebook paradigm rather than through traditional software\nexecutability standards."
                },
                "authors": [
                    {
                        "name": "Tien Nguyen"
                    },
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "12 pages, 10 figures, 3 tables, the 22nd International Conference on\n  Mining Software Repositories (MSR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04180v1",
                "updated": "2025-02-06T16:12:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    12,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:12:06Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    12,
                    6,
                    3,
                    37,
                    0
                ],
                "title": "Multi-agent Architecture Search via Agentic Supernet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent Architecture Search via Agentic Supernet"
                },
                "summary": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive\nboundaries of individual agents through disciplined collaboration and\ninteraction, while constructing these systems often requires labor-intensive\nmanual designs. Despite the availability of methods to automate the design of\nagentic workflows, they typically seek to identify a static, complex,\none-size-fits-all system, which, however, fails to dynamically allocate\ninference resources based on the difficulty and domain of each query. To\naddress this challenge, we shift away from the pursuit of a monolithic agentic\nsystem, instead optimizing the \\textbf{agentic supernet}, a probabilistic and\ncontinuous distribution of agentic architectures. We introduce MaAS, an\nautomated framework that samples query-dependent agentic systems from the\nsupernet, delivering high-quality solutions and tailored resource allocation\n(\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation\nacross six benchmarks demonstrates that MaAS \\textbf{(I)} requires only\n$6\\sim45\\%$ of the inference costs of existing handcrafted or automated\nmulti-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and\n\\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone\ntransferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive\nboundaries of individual agents through disciplined collaboration and\ninteraction, while constructing these systems often requires labor-intensive\nmanual designs. Despite the availability of methods to automate the design of\nagentic workflows, they typically seek to identify a static, complex,\none-size-fits-all system, which, however, fails to dynamically allocate\ninference resources based on the difficulty and domain of each query. To\naddress this challenge, we shift away from the pursuit of a monolithic agentic\nsystem, instead optimizing the \\textbf{agentic supernet}, a probabilistic and\ncontinuous distribution of agentic architectures. We introduce MaAS, an\nautomated framework that samples query-dependent agentic systems from the\nsupernet, delivering high-quality solutions and tailored resource allocation\n(\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation\nacross six benchmarks demonstrates that MaAS \\textbf{(I)} requires only\n$6\\sim45\\%$ of the inference costs of existing handcrafted or automated\nmulti-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and\n\\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone\ntransferability."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Luyang Niu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04176v1",
                "updated": "2025-02-06T16:07:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    7,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:07:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    7,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented\n  Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented\n  Multimodal Generation"
                },
                "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have shown\nremarkable performance in enhancing response accuracy and relevance by\nintegrating external knowledge into generative models. However, existing RAG\nmethods primarily focus on providing text-only answers, even in multimodal\nretrieval-augmented generation scenarios. In this work, we introduce the\nMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims\nto generate answers that combine both text and images, fully leveraging the\nmultimodal data within a corpus. Despite the importance of this task, there is\na notable absence of a comprehensive benchmark to effectively evaluate MRAMG\nperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefully\ncurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and\n4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and\nLifestyle. The dataset incorporates diverse difficulty levels and complex\nmulti-image scenarios, providing a robust foundation for evaluating multimodal\ngeneration tasks. To facilitate rigorous evaluation, our MRAMG-Bench\nincorporates a comprehensive suite of both statistical and LLM-based metrics,\nenabling a thorough analysis of the performance of popular generative models in\nthe MRAMG task. Besides, we propose an efficient multimodal answer generation\nframework that leverages both LLMs and MLLMs to generate multimodal responses.\nOur datasets are available at: https://huggingface.co/MRAMG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Retrieval-Augmented Generation (RAG) have shown\nremarkable performance in enhancing response accuracy and relevance by\nintegrating external knowledge into generative models. However, existing RAG\nmethods primarily focus on providing text-only answers, even in multimodal\nretrieval-augmented generation scenarios. In this work, we introduce the\nMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims\nto generate answers that combine both text and images, fully leveraging the\nmultimodal data within a corpus. Despite the importance of this task, there is\na notable absence of a comprehensive benchmark to effectively evaluate MRAMG\nperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefully\ncurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and\n4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and\nLifestyle. The dataset incorporates diverse difficulty levels and complex\nmulti-image scenarios, providing a robust foundation for evaluating multimodal\ngeneration tasks. To facilitate rigorous evaluation, our MRAMG-Bench\nincorporates a comprehensive suite of both statistical and LLM-based metrics,\nenabling a thorough analysis of the performance of popular generative models in\nthe MRAMG task. Besides, we propose an efficient multimodal answer generation\nframework that leverages both LLMs and MLLMs to generate multimodal responses.\nOur datasets are available at: https://huggingface.co/MRAMG."
                },
                "authors": [
                    {
                        "name": "Qinhan Yu"
                    },
                    {
                        "name": "Zhiyou Xiao"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17415v2",
                "updated": "2025-02-06T16:03:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    3,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-02-27T11:10:33Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    11,
                    10,
                    33,
                    1,
                    58,
                    0
                ],
                "title": "Rate Function Modelling of Quantum Many-Body Adiabaticity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate Function Modelling of Quantum Many-Body Adiabaticity"
                },
                "summary": "The quantum adiabatic theorem is a fundamental result in quantum mechanics,\nwith a multitude of applications, both theoretical and practical. Here, we\ninvestigate the dynamics of adiabatic processes for quantum many-body systems\n%in detail by analysing the properties of observable-free, intensive\nquantities. In particular, we study the adiabatic rate function $f(T, \\Delta\n\\lambda)$ in dependence of the ramp time $T$, which gives us a complete\ncharacterization of the many-body adiabatic fidelity as a function of $T$ and\nthe strength of the parameter displacement $\\Delta \\lambda$. $f(T, \\Delta\n\\lambda)$ quantifies the deviation from adiabaticity for a given process and\ntherefore allows us to control and define the notion of adiabaticity in\nmany-body systems. First we study $f(T, \\Delta \\lambda)$ for the 1D transverse\nfield Ising model and the Luttinger liquid, both of which are quadratic systems\nand therefore allow us to look at the thermodynamic limit. For ramps across\ngapped phases, we relate $f(T, \\Delta \\lambda)$ to the transition probability\nof the system and for ramps across a gapless point, or gapless phase we relate\nit to the excitation density of the relevant quasiparticles. Then we\ninvestigate the XXZ model which allows us to see the qualitative features that\nsurvive when interactions are turned on. Several key results in the literature\nregarding the interplay of the thermodynamic and the adiabatic limit are\nobtained as inferences from the properties of $f(T, \\Delta \\lambda)$ in the\nlarge $T$ limit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quantum adiabatic theorem is a fundamental result in quantum mechanics,\nwith a multitude of applications, both theoretical and practical. Here, we\ninvestigate the dynamics of adiabatic processes for quantum many-body systems\n%in detail by analysing the properties of observable-free, intensive\nquantities. In particular, we study the adiabatic rate function $f(T, \\Delta\n\\lambda)$ in dependence of the ramp time $T$, which gives us a complete\ncharacterization of the many-body adiabatic fidelity as a function of $T$ and\nthe strength of the parameter displacement $\\Delta \\lambda$. $f(T, \\Delta\n\\lambda)$ quantifies the deviation from adiabaticity for a given process and\ntherefore allows us to control and define the notion of adiabaticity in\nmany-body systems. First we study $f(T, \\Delta \\lambda)$ for the 1D transverse\nfield Ising model and the Luttinger liquid, both of which are quadratic systems\nand therefore allow us to look at the thermodynamic limit. For ramps across\ngapped phases, we relate $f(T, \\Delta \\lambda)$ to the transition probability\nof the system and for ramps across a gapless point, or gapless phase we relate\nit to the excitation density of the relevant quasiparticles. Then we\ninvestigate the XXZ model which allows us to see the qualitative features that\nsurvive when interactions are turned on. Several key results in the literature\nregarding the interplay of the thermodynamic and the adiabatic limit are\nobtained as inferences from the properties of $f(T, \\Delta \\lambda)$ in the\nlarge $T$ limit."
                },
                "authors": [
                    {
                        "name": "Vibhu Mishra"
                    },
                    {
                        "name": "Salvatore Manmana"
                    },
                    {
                        "name": "Stefan Kehrein"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kehrein"
                },
                "author": "Stefan Kehrein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13013v2",
                "updated": "2025-02-06T16:03:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    3,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-17T15:34:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "The Emergence of Strategic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Strategic Reasoning of Large Language Models"
                },
                "summary": "Although large language models (LLMs) have demonstrated strong reasoning\nabilities in structured tasks (e.g., coding and mathematics), it remains\nunexplored whether these abilities extend to strategic multi-agent\nenvironments. We investigate strategic reasoning capabilities -- the process of\nchoosing an optimal course of action by predicting and adapting to others'\nactions -- of LLMs by analyzing their performance in three classical games from\nbehavioral economics. We evaluate three standard LLMs (ChatGPT-4, Claude-2.1,\nGemini 1.5) and three specialized reasoning LLMs (GPT-o1, Claude-3.5-Sonnet,\nGemini Flash Thinking 2.0) using hierarchical models of bounded rationality.\nOur results show that reasoning LLMs exhibit superior strategic reasoning\ncompared to standard LLMs (which do not demonstrate substantial capabilities),\nand often match or exceed human performance. Since strategic reasoning is\nfundamental to future AI systems (including Agentic AI and Artificial General\nIntelligence), our findings demonstrate the importance of dedicated reasoning\ncapabilities in achieving effective strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated strong reasoning\nabilities in structured tasks (e.g., coding and mathematics), it remains\nunexplored whether these abilities extend to strategic multi-agent\nenvironments. We investigate strategic reasoning capabilities -- the process of\nchoosing an optimal course of action by predicting and adapting to others'\nactions -- of LLMs by analyzing their performance in three classical games from\nbehavioral economics. We evaluate three standard LLMs (ChatGPT-4, Claude-2.1,\nGemini 1.5) and three specialized reasoning LLMs (GPT-o1, Claude-3.5-Sonnet,\nGemini Flash Thinking 2.0) using hierarchical models of bounded rationality.\nOur results show that reasoning LLMs exhibit superior strategic reasoning\ncompared to standard LLMs (which do not demonstrate substantial capabilities),\nand often match or exceed human performance. Since strategic reasoning is\nfundamental to future AI systems (including Agentic AI and Artificial General\nIntelligence), our findings demonstrate the importance of dedicated reasoning\ncapabilities in achieving effective strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Dongwoo Lee"
                    },
                    {
                        "name": "Gavin Kader"
                    }
                ],
                "author_detail": {
                    "name": "Gavin Kader"
                },
                "author": "Gavin Kader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13103v2",
                "updated": "2025-02-06T15:57:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    57,
                    23,
                    3,
                    37,
                    0
                ],
                "published": "2024-06-18T23:27:46Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    23,
                    27,
                    46,
                    1,
                    170,
                    0
                ],
                "title": "A Generic Method for Fine-grained Category Discovery in Natural Language\n  Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Method for Fine-grained Category Discovery in Natural Language\n  Texts"
                },
                "summary": "Fine-grained category discovery using only coarse-grained supervision is a\ncost-effective yet challenging task. Previous training methods focus on\naligning query samples with positive samples and distancing them from\nnegatives. They often neglect intra-category and inter-category semantic\nsimilarities of fine-grained categories when navigating sample distributions in\nthe embedding space. Furthermore, some evaluation techniques that rely on\npre-collected test samples are inadequate for real-time applications. To\naddress these shortcomings, we introduce a method that successfully detects\nfine-grained clusters of semantically similar texts guided by a novel objective\nfunction. The method uses semantic similarities in a logarithmic space to guide\nsample distributions in the Euclidean space and to form distinct clusters that\nrepresent fine-grained categories. We also propose a centroid inference\nmechanism to support real-time applications. The efficacy of the method is both\ntheoretically justified and empirically confirmed on three benchmark tasks. The\nproposed objective function is integrated in multiple contrastive learning\nbased neural models. Its results surpass existing state-of-the-art approaches\nin terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of\nthe detected fine-grained categories. Code and data will be available at Code\nand data are publicly available at\nhttps://github.com/changtianluckyforever/F-grained-STAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained category discovery using only coarse-grained supervision is a\ncost-effective yet challenging task. Previous training methods focus on\naligning query samples with positive samples and distancing them from\nnegatives. They often neglect intra-category and inter-category semantic\nsimilarities of fine-grained categories when navigating sample distributions in\nthe embedding space. Furthermore, some evaluation techniques that rely on\npre-collected test samples are inadequate for real-time applications. To\naddress these shortcomings, we introduce a method that successfully detects\nfine-grained clusters of semantically similar texts guided by a novel objective\nfunction. The method uses semantic similarities in a logarithmic space to guide\nsample distributions in the Euclidean space and to form distinct clusters that\nrepresent fine-grained categories. We also propose a centroid inference\nmechanism to support real-time applications. The efficacy of the method is both\ntheoretically justified and empirically confirmed on three benchmark tasks. The\nproposed objective function is integrated in multiple contrastive learning\nbased neural models. Its results surpass existing state-of-the-art approaches\nin terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of\nthe detected fine-grained categories. Code and data will be available at Code\nand data are publicly available at\nhttps://github.com/changtianluckyforever/F-grained-STAR."
                },
                "authors": [
                    {
                        "name": "Chang Tian"
                    },
                    {
                        "name": "Matthew B. Blaschko"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Mingzhe Xing"
                    },
                    {
                        "name": "Yinliang Yue"
                    },
                    {
                        "name": "Marie-Francine Moens"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Francine Moens"
                },
                "author": "Marie-Francine Moens",
                "arxiv_comment": "contrastive learning, self-supervised learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06833v3",
                "updated": "2025-02-06T15:52:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    52,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2024-04-10T08:49:27Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    8,
                    49,
                    27,
                    2,
                    101,
                    0
                ],
                "title": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge"
                },
                "summary": "Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains."
                },
                "authors": [
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Taelin Karidi"
                    },
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Nicolas Garneau"
                    },
                    {
                        "name": "Yong Cao"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "cultural bias analysis, cultural knowledge probing, large language\n  models, cultural NLP; Accepted by NAACL2025",
                "arxiv_journal_ref": "NAACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04158v1",
                "updated": "2025-02-06T15:44:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    44,
                    5,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:44:05Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    44,
                    5,
                    3,
                    37,
                    0
                ],
                "title": "Diffusion-based mass map reconstruction from weak lensing data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based mass map reconstruction from weak lensing data"
                },
                "summary": "Diffusion models have been used in cosmological applications as a generative\nmodel for fast simulations and to reconstruct underlying cosmological fields or\nastrophysical images from noisy data. These two tasks are often treated as\nseparate: diffusion models trained for one purpose do not generalize to perform\nthe other task. In this paper, we develop a single diffusion model that can be\nused for both tasks. By using the Diffusion Posterior Sampling (DPS) approach,\nwe use a diffusion model trained to simulate weak lensing maps for the inverse\nproblem of reconstructing mass maps from noisy weak lensing data. We find that\nthe standard DPS method leads to biased inference but we correct this bias by\ndown weighting the likelihood term at early sampling time steps of the\ndiffusion. Our method give us a way to reconstruct accurate high-resolution\n(sub-arcminute) mass maps that have the correct power spectrum and a range of\nnon-Gaussian summary statistics. We discuss several applications enabled by the\ncomputational efficiency and accuracy of our model. These include generation of\nsimulation quality mass maps, aiding covariance estimation for higher order\nstatistics, and for finding filaments, voids and clusters from noisy lensing\nshear data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been used in cosmological applications as a generative\nmodel for fast simulations and to reconstruct underlying cosmological fields or\nastrophysical images from noisy data. These two tasks are often treated as\nseparate: diffusion models trained for one purpose do not generalize to perform\nthe other task. In this paper, we develop a single diffusion model that can be\nused for both tasks. By using the Diffusion Posterior Sampling (DPS) approach,\nwe use a diffusion model trained to simulate weak lensing maps for the inverse\nproblem of reconstructing mass maps from noisy weak lensing data. We find that\nthe standard DPS method leads to biased inference but we correct this bias by\ndown weighting the likelihood term at early sampling time steps of the\ndiffusion. Our method give us a way to reconstruct accurate high-resolution\n(sub-arcminute) mass maps that have the correct power spectrum and a range of\nnon-Gaussian summary statistics. We discuss several applications enabled by the\ncomputational efficiency and accuracy of our model. These include generation of\nsimulation quality mass maps, aiding covariance estimation for higher order\nstatistics, and for finding filaments, voids and clusters from noisy lensing\nshear data."
                },
                "authors": [
                    {
                        "name": "Supranta S. Boruah"
                    },
                    {
                        "name": "Michael Jacob"
                    },
                    {
                        "name": "Bhuvnesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvnesh Jain"
                },
                "author": "Bhuvnesh Jain",
                "arxiv_comment": "14 pages, 9 figures, Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04153v1",
                "updated": "2025-02-06T15:39:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    39,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:39:16Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    39,
                    16,
                    3,
                    37,
                    0
                ],
                "title": "UltraIF: Advancing Instruction Following from the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraIF: Advancing Instruction Following from the Wild"
                },
                "summary": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF."
                },
                "authors": [
                    {
                        "name": "Kaikai An"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11782v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11782v3",
                "updated": "2025-02-06T15:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    37,
                    52,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-15T17:01:21Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks"
                },
                "summary": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Dawei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Cheng"
                },
                "author": "Dawei Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11782v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11782v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02059v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02059v4",
                "updated": "2025-02-06T15:17:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    17,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2023-10-03T14:01:28Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    14,
                    1,
                    28,
                    1,
                    276,
                    0
                ],
                "title": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study"
                },
                "summary": "Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code."
                },
                "authors": [
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Jiaxin Yu"
                    },
                    {
                        "name": "Jinfu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jinfu Chen"
                },
                "author": "Jinfu Chen",
                "arxiv_comment": "Preprint accepted for publication in ACM Transactions on Software\n  Engineering and Methodology (TOSEM), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02059v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02059v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04134v1",
                "updated": "2025-02-06T15:14:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    14,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:14:02Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    14,
                    2,
                    3,
                    37,
                    0
                ],
                "title": "The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs"
                },
                "summary": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in\nclosed-source LLMs by conducting experiments across multiple tasks, including\nparaphrasing, relevance judgment, and multiple-choice questions. Our results\nshow that input order significantly affects performance across tasks, with\nshuffled inputs leading to measurable declines in output accuracy. Few-shot\nprompting demonstrates mixed effectiveness and offers partial mitigation,\nhowever, fails to fully resolve the problem. These findings highlight\npersistent risks, particularly in high-stakes applications, and point to the\nneed for more robust LLMs or improved input-handling techniques in future\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in\nclosed-source LLMs by conducting experiments across multiple tasks, including\nparaphrasing, relevance judgment, and multiple-choice questions. Our results\nshow that input order significantly affects performance across tasks, with\nshuffled inputs leading to measurable declines in output accuracy. Few-shot\nprompting demonstrates mixed effectiveness and offers partial mitigation,\nhowever, fails to fully resolve the problem. These findings highlight\npersistent risks, particularly in high-stakes applications, and point to the\nneed for more robust LLMs or improved input-handling techniques in future\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Bryan Guan"
                    },
                    {
                        "name": "Tanya Roosta"
                    },
                    {
                        "name": "Peyman Passban"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "arxiv_comment": "The first 3 authors have contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04128v1",
                "updated": "2025-02-06T15:04:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    4,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:04:00Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    4,
                    0,
                    3,
                    37,
                    0
                ],
                "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis"
                },
                "summary": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Xinsheng Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Jiahe Lei"
                    },
                    {
                        "name": "Yi Peng"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Yizhu Jin"
                    },
                    {
                        "name": "Zheqi DAI"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Liumeng Xue"
                    },
                    {
                        "name": "Yunlin Chen"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02358v3",
                "updated": "2025-02-06T15:03:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    3,
                    54,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T14:43:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    43,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm"
                },
                "summary": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion. Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions. In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion. Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions. In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziyan Guo"
                    },
                    {
                        "name": "Zeyu Hu"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "De Wen Soh"
                    }
                ],
                "author_detail": {
                    "name": "De Wen Soh"
                },
                "author": "De Wen Soh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04121v1",
                "updated": "2025-02-06T14:53:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    53,
                    21,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T14:53:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    53,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "Optimizing Perturbations for Improved Training of Machine Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Perturbations for Improved Training of Machine Learning\n  Models"
                },
                "summary": "Machine learning models have become indispensable tools in applications\nacross the physical sciences. Their training is often time-consuming, vastly\nexceeding the inference timescales. Several protocols have been developed to\nperturb the learning process and improve the training, such as shrink and\nperturb, warm restarts, and stochastic resetting. For classifiers, these\nperturbations have been shown to result in enhanced speedups or improved\ngeneralization. However, the design of such perturbations is usually done\n\\textit{ad hoc} by intuition and trial and error. To rationally optimize\ntraining protocols, we frame them as first-passage processes and consider their\nresponse to perturbations. We show that if the unperturbed learning process\nreaches a quasi-steady state, the response at a single perturbation frequency\ncan predict the behavior at a wide range of frequencies. We demonstrate that\nthis is the case when training a CIFAR-10 classifier using the ResNet-18 model\nand use this approach to identify an optimal perturbation and frequency. Our\nwork allows optimization of training protocols of machine learning models using\na statistical mechanical approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have become indispensable tools in applications\nacross the physical sciences. Their training is often time-consuming, vastly\nexceeding the inference timescales. Several protocols have been developed to\nperturb the learning process and improve the training, such as shrink and\nperturb, warm restarts, and stochastic resetting. For classifiers, these\nperturbations have been shown to result in enhanced speedups or improved\ngeneralization. However, the design of such perturbations is usually done\n\\textit{ad hoc} by intuition and trial and error. To rationally optimize\ntraining protocols, we frame them as first-passage processes and consider their\nresponse to perturbations. We show that if the unperturbed learning process\nreaches a quasi-steady state, the response at a single perturbation frequency\ncan predict the behavior at a wide range of frequencies. We demonstrate that\nthis is the case when training a CIFAR-10 classifier using the ResNet-18 model\nand use this approach to identify an optimal perturbation and frequency. Our\nwork allows optimization of training protocols of machine learning models using\na statistical mechanical approach."
                },
                "authors": [
                    {
                        "name": "Sagi Meir"
                    },
                    {
                        "name": "Tommer D. Keidar"
                    },
                    {
                        "name": "Shlomi Reuveni"
                    },
                    {
                        "name": "Barak Hirshberg"
                    }
                ],
                "author_detail": {
                    "name": "Barak Hirshberg"
                },
                "author": "Barak Hirshberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06315v3",
                "updated": "2025-02-06T14:49:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    49,
                    48,
                    3,
                    37,
                    0
                ],
                "published": "2024-06-10T14:32:10Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    32,
                    10,
                    0,
                    162,
                    0
                ],
                "title": "Inference of the Mass Composition of Cosmic Rays with energies from\n  $\\mathbf{10^{18.5}}$ to $\\mathbf{10^{20}}$ eV using the Pierre Auger\n  Observatory and Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of the Mass Composition of Cosmic Rays with energies from\n  $\\mathbf{10^{18.5}}$ to $\\mathbf{10^{20}}$ eV using the Pierre Auger\n  Observatory and Deep Learning"
                },
                "summary": "We present measurements of the atmospheric depth of the shower maximum\n$X_\\mathrm{max}$, inferred for the first time on an event-by-event level using\nthe Surface Detector of the Pierre Auger Observatory. Using deep learning, we\nwere able to extend measurements of the $X_\\mathrm{max}$ distributions up to\nenergies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements,\nproviding new insights into the mass composition of cosmic rays at extreme\nenergies. Gaining a 10-fold increase in statistics compared to the Fluorescence\nDetector data, we find evidence that the rate of change of the average\n$X_\\mathrm{max}$ with the logarithm of energy features three breaks at\n$6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm\n2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and\n$31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three\nprominent features (ankle, instep, suppression) of the cosmic-ray flux. The\nenergy evolution of the mean and standard deviation of the measured\n$X_\\mathrm{max}$ distributions indicates that the mass composition becomes\nincreasingly heavier and purer, thus being incompatible with a large fraction\nof light nuclei between 50 EeV and 100 EeV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present measurements of the atmospheric depth of the shower maximum\n$X_\\mathrm{max}$, inferred for the first time on an event-by-event level using\nthe Surface Detector of the Pierre Auger Observatory. Using deep learning, we\nwere able to extend measurements of the $X_\\mathrm{max}$ distributions up to\nenergies of 100 EeV ($10^{20}$ eV), not yet revealed by current measurements,\nproviding new insights into the mass composition of cosmic rays at extreme\nenergies. Gaining a 10-fold increase in statistics compared to the Fluorescence\nDetector data, we find evidence that the rate of change of the average\n$X_\\mathrm{max}$ with the logarithm of energy features three breaks at\n$6.5\\pm0.6~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, $11\\pm\n2~(\\mathrm{stat})\\pm1~(\\mathrm{sys})$ EeV, and\n$31\\pm5~(\\mathrm{stat})\\pm3~(\\mathrm{sys})$ EeV, in the vicinity to the three\nprominent features (ankle, instep, suppression) of the cosmic-ray flux. The\nenergy evolution of the mean and standard deviation of the measured\n$X_\\mathrm{max}$ distributions indicates that the mass composition becomes\nincreasingly heavier and purer, thus being incompatible with a large fraction\nof light nuclei between 50 EeV and 100 EeV."
                },
                "authors": [
                    {
                        "name": "The Pierre Auger Collaboration"
                    },
                    {
                        "name": "A. Abdul Halim"
                    },
                    {
                        "name": "P. Abreu"
                    },
                    {
                        "name": "M. Aglietta"
                    },
                    {
                        "name": "I. Allekotte"
                    },
                    {
                        "name": "K. Almeida Cheminant"
                    },
                    {
                        "name": "A. Almela"
                    },
                    {
                        "name": "R. Aloisio"
                    },
                    {
                        "name": "J. Alvarez-Muñiz"
                    },
                    {
                        "name": "J. Ammerman Yebra"
                    },
                    {
                        "name": "G. A. Anastasi"
                    },
                    {
                        "name": "L. Anchordoqui"
                    },
                    {
                        "name": "B. Andrada"
                    },
                    {
                        "name": "L. Andrade Dourado"
                    },
                    {
                        "name": "S. Andringa"
                    },
                    {
                        "name": "L. Apollonio"
                    },
                    {
                        "name": "C. Aramo"
                    },
                    {
                        "name": "P. R. Araújo Ferreira"
                    },
                    {
                        "name": "E. Arnone"
                    },
                    {
                        "name": "J. C. Arteaga Velázquez"
                    },
                    {
                        "name": "P. Assis"
                    },
                    {
                        "name": "G. Avila"
                    },
                    {
                        "name": "E. Avocone"
                    },
                    {
                        "name": "A. Bakalova"
                    },
                    {
                        "name": "F. Barbato"
                    },
                    {
                        "name": "A. Bartz Mocellin"
                    },
                    {
                        "name": "C. Berat"
                    },
                    {
                        "name": "M. E. Bertaina"
                    },
                    {
                        "name": "G. Bhatta"
                    },
                    {
                        "name": "M. Bianciotto"
                    },
                    {
                        "name": "P. L. Biermann"
                    },
                    {
                        "name": "V. Binet"
                    },
                    {
                        "name": "K. Bismark"
                    },
                    {
                        "name": "T. Bister"
                    },
                    {
                        "name": "J. Biteau"
                    },
                    {
                        "name": "J. Blazek"
                    },
                    {
                        "name": "C. Bleve"
                    },
                    {
                        "name": "J. Blümer"
                    },
                    {
                        "name": "M. Boháčová"
                    },
                    {
                        "name": "D. Boncioli"
                    },
                    {
                        "name": "C. Bonifazi"
                    },
                    {
                        "name": "L. Bonneau Arbeletche"
                    },
                    {
                        "name": "N. Borodai"
                    },
                    {
                        "name": "J. Brack"
                    },
                    {
                        "name": "P. G. Brichetto Orchera"
                    },
                    {
                        "name": "F. L. Briechle"
                    },
                    {
                        "name": "A. Bueno"
                    },
                    {
                        "name": "S. Buitink"
                    },
                    {
                        "name": "M. Buscemi"
                    },
                    {
                        "name": "M. Büsken"
                    },
                    {
                        "name": "A. Bwembya"
                    },
                    {
                        "name": "K. S. Caballero-Mora"
                    },
                    {
                        "name": "S. Cabana-Freire"
                    },
                    {
                        "name": "L. Caccianiga"
                    },
                    {
                        "name": "F. Campuzano"
                    },
                    {
                        "name": "R. Caruso"
                    },
                    {
                        "name": "A. Castellina"
                    },
                    {
                        "name": "F. Catalani"
                    },
                    {
                        "name": "G. Cataldi"
                    },
                    {
                        "name": "L. Cazon"
                    },
                    {
                        "name": "M. Cerda"
                    },
                    {
                        "name": "B. Čermáková"
                    },
                    {
                        "name": "A. Cermenati"
                    },
                    {
                        "name": "J. A. Chinellato"
                    },
                    {
                        "name": "J. Chudoba"
                    },
                    {
                        "name": "L. Chytka"
                    },
                    {
                        "name": "R. W. Clay"
                    },
                    {
                        "name": "A. C. Cobos Cerutti"
                    },
                    {
                        "name": "R. Colalillo"
                    },
                    {
                        "name": "M. R. Coluccia"
                    },
                    {
                        "name": "R. Conceição"
                    },
                    {
                        "name": "A. Condorelli"
                    },
                    {
                        "name": "G. Consolati"
                    },
                    {
                        "name": "M. Conte"
                    },
                    {
                        "name": "F. Convenga"
                    },
                    {
                        "name": "D. Correia dos Santos"
                    },
                    {
                        "name": "P. J. Costa"
                    },
                    {
                        "name": "C. E. Covault"
                    },
                    {
                        "name": "M. Cristinziani"
                    },
                    {
                        "name": "C. S. Cruz Sanchez"
                    },
                    {
                        "name": "S. Dasso"
                    },
                    {
                        "name": "K. Daumiller"
                    },
                    {
                        "name": "B. R. Dawson"
                    },
                    {
                        "name": "R. M. de Almeida"
                    },
                    {
                        "name": "B. de Errico"
                    },
                    {
                        "name": "J. de Jesús"
                    },
                    {
                        "name": "S. J. de Jong"
                    },
                    {
                        "name": "J. R. T. de Mello Neto"
                    },
                    {
                        "name": "I. De Mitri"
                    },
                    {
                        "name": "J. de Oliveira"
                    },
                    {
                        "name": "D. de Oliveira Franco"
                    },
                    {
                        "name": "F. de Palma"
                    },
                    {
                        "name": "V. de Souza"
                    },
                    {
                        "name": "E. De Vito"
                    },
                    {
                        "name": "A. Del Popolo"
                    },
                    {
                        "name": "O. Deligny"
                    },
                    {
                        "name": "N. Denner"
                    },
                    {
                        "name": "L. Deval"
                    },
                    {
                        "name": "A. di Matteo"
                    },
                    {
                        "name": "J. A. do"
                    },
                    {
                        "name": "M. Dobre"
                    },
                    {
                        "name": "C. Dobrigkeit"
                    },
                    {
                        "name": "J. C. D'Olivo"
                    },
                    {
                        "name": "L. M. Domingues Mendes"
                    },
                    {
                        "name": "Q. Dorosti"
                    },
                    {
                        "name": "J. C. dos Anjos"
                    },
                    {
                        "name": "R. C. dos Anjos"
                    },
                    {
                        "name": "J. Ebr"
                    },
                    {
                        "name": "F. Ellwanger"
                    },
                    {
                        "name": "M. Emam"
                    },
                    {
                        "name": "R. Engel"
                    },
                    {
                        "name": "I. Epicoco"
                    },
                    {
                        "name": "M. Erdmann"
                    },
                    {
                        "name": "A. Etchegoyen"
                    },
                    {
                        "name": "C. Evoli"
                    },
                    {
                        "name": "H. Falcke"
                    },
                    {
                        "name": "G. Farrar"
                    },
                    {
                        "name": "A. C. Fauth"
                    },
                    {
                        "name": "T. Fehler"
                    },
                    {
                        "name": "F. Feldbusch"
                    },
                    {
                        "name": "F. Fenu"
                    },
                    {
                        "name": "A. Fernandes"
                    },
                    {
                        "name": "B. Fick"
                    },
                    {
                        "name": "J. M. Figueira"
                    },
                    {
                        "name": "P. Filip"
                    },
                    {
                        "name": "A. Filipčič"
                    },
                    {
                        "name": "T. Fitoussi"
                    },
                    {
                        "name": "B. Flaggs"
                    },
                    {
                        "name": "T. Fodran"
                    },
                    {
                        "name": "T. Fujii"
                    },
                    {
                        "name": "A. Fuster"
                    },
                    {
                        "name": "C. Galea"
                    },
                    {
                        "name": "B. García"
                    },
                    {
                        "name": "C. Gaudu"
                    },
                    {
                        "name": "A. Gherghel-Lascu"
                    },
                    {
                        "name": "P. L. Ghia"
                    },
                    {
                        "name": "U. Giaccari"
                    },
                    {
                        "name": "J. Glombitza"
                    },
                    {
                        "name": "F. Gobbi"
                    },
                    {
                        "name": "F. Gollan"
                    },
                    {
                        "name": "G. Golup"
                    },
                    {
                        "name": "M. Gómez Berisso"
                    },
                    {
                        "name": "P. F. Gómez Vitale"
                    },
                    {
                        "name": "J. P. Gongora"
                    },
                    {
                        "name": "J. M. González"
                    },
                    {
                        "name": "N. González"
                    },
                    {
                        "name": "D. Góra"
                    },
                    {
                        "name": "A. Gorgi"
                    },
                    {
                        "name": "M. Gottowik"
                    },
                    {
                        "name": "F. Guarino"
                    },
                    {
                        "name": "G. P. Guedes"
                    },
                    {
                        "name": "E. Guido"
                    },
                    {
                        "name": "L. Gülzow"
                    },
                    {
                        "name": "S. Hahn"
                    },
                    {
                        "name": "P. Hamal"
                    },
                    {
                        "name": "M. R. Hampel"
                    },
                    {
                        "name": "P. Hansen"
                    },
                    {
                        "name": "D. Harari"
                    },
                    {
                        "name": "V. M. Harvey"
                    },
                    {
                        "name": "A. Haungs"
                    },
                    {
                        "name": "T. Hebbeker"
                    },
                    {
                        "name": "C. Hojvat"
                    },
                    {
                        "name": "J. R. Hörandel"
                    },
                    {
                        "name": "P. Horvath"
                    },
                    {
                        "name": "M. Hrabovský"
                    },
                    {
                        "name": "T. Huege"
                    },
                    {
                        "name": "A. Insolia"
                    },
                    {
                        "name": "P. G. Isar"
                    },
                    {
                        "name": "P. Janecek"
                    },
                    {
                        "name": "V. Jilek"
                    },
                    {
                        "name": "J. A. Johnsen"
                    },
                    {
                        "name": "J. Jurysek"
                    },
                    {
                        "name": "K. -H. Kampert"
                    },
                    {
                        "name": "B. Keilhauer"
                    },
                    {
                        "name": "A. Khakurdikar"
                    },
                    {
                        "name": "V. V. Kizakke Covilakam"
                    },
                    {
                        "name": "H. O. Klages"
                    },
                    {
                        "name": "M. Kleifges"
                    },
                    {
                        "name": "F. Knapp"
                    },
                    {
                        "name": "J. Köhler"
                    },
                    {
                        "name": "F. Krieger"
                    },
                    {
                        "name": "N. Kunka"
                    },
                    {
                        "name": "B. L. Lago"
                    },
                    {
                        "name": "N. Langner"
                    },
                    {
                        "name": "M. A. Leigui de Oliveira"
                    },
                    {
                        "name": "Y. Lema-Capeans"
                    },
                    {
                        "name": "A. Letessier-Selvon"
                    },
                    {
                        "name": "I. Lhenry-Yvon"
                    },
                    {
                        "name": "L. Lopes"
                    },
                    {
                        "name": "L. Lu"
                    },
                    {
                        "name": "Q. Luce"
                    },
                    {
                        "name": "J. P. Lundquist"
                    },
                    {
                        "name": "A. Machado Payeras"
                    },
                    {
                        "name": "M. Majercakova"
                    },
                    {
                        "name": "D. Mandat"
                    },
                    {
                        "name": "B. C. Manning"
                    },
                    {
                        "name": "P. Mantsch"
                    },
                    {
                        "name": "F. M. Mariani"
                    },
                    {
                        "name": "A. G. Mariazzi"
                    },
                    {
                        "name": "I. C. Mariş"
                    },
                    {
                        "name": "G. Marsella"
                    },
                    {
                        "name": "D. Martello"
                    },
                    {
                        "name": "S. Martinelli"
                    },
                    {
                        "name": "O. Martínez Bravo"
                    },
                    {
                        "name": "M. A. Martins"
                    },
                    {
                        "name": "H. -J. Mathes"
                    },
                    {
                        "name": "J. Matthews"
                    },
                    {
                        "name": "G. Matthiae"
                    },
                    {
                        "name": "E. Mayotte"
                    },
                    {
                        "name": "S. Mayotte"
                    },
                    {
                        "name": "P. O. Mazur"
                    },
                    {
                        "name": "G. Medina-Tanco"
                    },
                    {
                        "name": "J. Meinert"
                    },
                    {
                        "name": "D. Melo"
                    },
                    {
                        "name": "A. Menshikov"
                    },
                    {
                        "name": "C. Merx"
                    },
                    {
                        "name": "S. Michal"
                    },
                    {
                        "name": "M. I. Micheletti"
                    },
                    {
                        "name": "L. Miramonti"
                    },
                    {
                        "name": "S. Mollerach"
                    },
                    {
                        "name": "F. Montanet"
                    },
                    {
                        "name": "L. Morejon"
                    },
                    {
                        "name": "K. Mulrey"
                    },
                    {
                        "name": "R. Mussa"
                    },
                    {
                        "name": "W. M. Namasaka"
                    },
                    {
                        "name": "S. Negi"
                    },
                    {
                        "name": "L. Nellen"
                    },
                    {
                        "name": "K. Nguyen"
                    },
                    {
                        "name": "G. Nicora"
                    },
                    {
                        "name": "M. Niechciol"
                    },
                    {
                        "name": "D. Nitz"
                    },
                    {
                        "name": "D. Nosek"
                    },
                    {
                        "name": "V. Novotny"
                    },
                    {
                        "name": "L. Nožka"
                    },
                    {
                        "name": "A. Nucita"
                    },
                    {
                        "name": "L. A. Núñez"
                    },
                    {
                        "name": "C. Oliveira"
                    },
                    {
                        "name": "M. Palatka"
                    },
                    {
                        "name": "J. Pallotta"
                    },
                    {
                        "name": "S. Panja"
                    },
                    {
                        "name": "G. Parente"
                    },
                    {
                        "name": "T. Paulsen"
                    },
                    {
                        "name": "J. Pawlowsky"
                    },
                    {
                        "name": "M. Pech"
                    },
                    {
                        "name": "J. Pękala"
                    },
                    {
                        "name": "R. Pelayo"
                    },
                    {
                        "name": "V. Pelgrims"
                    },
                    {
                        "name": "L. A. S. Pereira"
                    },
                    {
                        "name": "E. E. Pereira Martins"
                    },
                    {
                        "name": "C. Pérez Bertolli"
                    },
                    {
                        "name": "L. Perrone"
                    },
                    {
                        "name": "S. Petrera"
                    },
                    {
                        "name": "C. Petrucci"
                    },
                    {
                        "name": "T. Pierog"
                    },
                    {
                        "name": "M. Pimenta"
                    },
                    {
                        "name": "M. Platino"
                    },
                    {
                        "name": "B. Pont"
                    },
                    {
                        "name": "M. Pothast"
                    },
                    {
                        "name": "M. Pourmohammad Shahvar"
                    },
                    {
                        "name": "P. Privitera"
                    },
                    {
                        "name": "M. Prouza"
                    },
                    {
                        "name": "S. Querchfeld"
                    },
                    {
                        "name": "J. Rautenberg"
                    },
                    {
                        "name": "D. Ravignani"
                    },
                    {
                        "name": "J. V. Reginatto Akim"
                    },
                    {
                        "name": "M. Reininghaus"
                    },
                    {
                        "name": "A. Reuzki"
                    },
                    {
                        "name": "J. Ridky"
                    },
                    {
                        "name": "F. Riehn"
                    },
                    {
                        "name": "M. Risse"
                    },
                    {
                        "name": "V. Rizi"
                    },
                    {
                        "name": "W. Rodrigues de Carvalho"
                    },
                    {
                        "name": "E. Rodriguez"
                    },
                    {
                        "name": "J. Rodriguez Rojo"
                    },
                    {
                        "name": "M. J. Roncoroni"
                    },
                    {
                        "name": "S. Rossoni"
                    },
                    {
                        "name": "M. Roth"
                    },
                    {
                        "name": "E. Roulet"
                    },
                    {
                        "name": "A. C. Rovero"
                    },
                    {
                        "name": "A. Saftoiu"
                    },
                    {
                        "name": "M. Saharan"
                    },
                    {
                        "name": "F. Salamida"
                    },
                    {
                        "name": "H. Salazar"
                    },
                    {
                        "name": "G. Salina"
                    },
                    {
                        "name": "J. D. Sanabria Gomez"
                    },
                    {
                        "name": "F. Sánchez"
                    },
                    {
                        "name": "E. M. Santos"
                    },
                    {
                        "name": "E. Santos"
                    },
                    {
                        "name": "F. Sarazin"
                    },
                    {
                        "name": "R. Sarmento"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "P. Savina"
                    },
                    {
                        "name": "C. M. Schäfer"
                    },
                    {
                        "name": "V. Scherini"
                    },
                    {
                        "name": "H. Schieler"
                    },
                    {
                        "name": "M. Schimassek"
                    },
                    {
                        "name": "M. Schimp"
                    },
                    {
                        "name": "D. Schmidt"
                    },
                    {
                        "name": "O. Scholten"
                    },
                    {
                        "name": "H. Schoorlemmer"
                    },
                    {
                        "name": "P. Schovánek"
                    },
                    {
                        "name": "F. G. Schröder"
                    },
                    {
                        "name": "J. Schulte"
                    },
                    {
                        "name": "T. Schulz"
                    },
                    {
                        "name": "S. J. Sciutto"
                    },
                    {
                        "name": "M. Scornavacche"
                    },
                    {
                        "name": "A. Sedoski"
                    },
                    {
                        "name": "A. Segreto"
                    },
                    {
                        "name": "S. Sehgal"
                    },
                    {
                        "name": "S. U. Shivashankara"
                    },
                    {
                        "name": "G. Sigl"
                    },
                    {
                        "name": "K. Simkova"
                    },
                    {
                        "name": "F. Simon"
                    },
                    {
                        "name": "R. Smau"
                    },
                    {
                        "name": "R. Šmída"
                    },
                    {
                        "name": "P. Sommers"
                    },
                    {
                        "name": "R. Squartini"
                    },
                    {
                        "name": "M. Stadelmaier"
                    },
                    {
                        "name": "S. Stanič"
                    },
                    {
                        "name": "J. Stasielak"
                    },
                    {
                        "name": "P. Stassi"
                    },
                    {
                        "name": "S. Strähnz"
                    },
                    {
                        "name": "M. Straub"
                    },
                    {
                        "name": "T. Suomijärvi"
                    },
                    {
                        "name": "A. D. Supanitsky"
                    },
                    {
                        "name": "Z. Svozilikova"
                    },
                    {
                        "name": "Z. Szadkowski"
                    },
                    {
                        "name": "F. Tairli"
                    },
                    {
                        "name": "A. Tapia"
                    },
                    {
                        "name": "C. Taricco"
                    },
                    {
                        "name": "C. Timmermans"
                    },
                    {
                        "name": "O. Tkachenko"
                    },
                    {
                        "name": "P. Tobiska"
                    },
                    {
                        "name": "C. J. Todero Peixoto"
                    },
                    {
                        "name": "B. Tomé"
                    },
                    {
                        "name": "Z. Torrès"
                    },
                    {
                        "name": "A. Travaini"
                    },
                    {
                        "name": "P. Travnicek"
                    },
                    {
                        "name": "M. Tueros"
                    },
                    {
                        "name": "M. Unger"
                    },
                    {
                        "name": "R. Uzeiroska"
                    },
                    {
                        "name": "L. Vaclavek"
                    },
                    {
                        "name": "M. Vacula"
                    },
                    {
                        "name": "J. F. Valdés Galicia"
                    },
                    {
                        "name": "L. Valore"
                    },
                    {
                        "name": "E. Varela"
                    },
                    {
                        "name": "V. Vašíčková"
                    },
                    {
                        "name": "A. Vásquez-Ramírez"
                    },
                    {
                        "name": "D. Veberič"
                    },
                    {
                        "name": "I. D. Vergara Quispe"
                    },
                    {
                        "name": "V. Verzi"
                    },
                    {
                        "name": "J. Vicha"
                    },
                    {
                        "name": "J. Vink"
                    },
                    {
                        "name": "S. Vorobiov"
                    },
                    {
                        "name": "C. Watanabe"
                    },
                    {
                        "name": "A. A. Watson"
                    },
                    {
                        "name": "A. Weindl"
                    },
                    {
                        "name": "L. Wiencke"
                    },
                    {
                        "name": "H. Wilczyński"
                    },
                    {
                        "name": "D. Wittkowski"
                    },
                    {
                        "name": "B. Wundheiler"
                    },
                    {
                        "name": "B. Yue"
                    },
                    {
                        "name": "A. Yushkov"
                    },
                    {
                        "name": "O. Zapparrata"
                    },
                    {
                        "name": "E. Zas"
                    },
                    {
                        "name": "D. Zavrtanik"
                    },
                    {
                        "name": "M. Zavrtanik"
                    }
                ],
                "author_detail": {
                    "name": "M. Zavrtanik"
                },
                "author": "M. Zavrtanik",
                "arxiv_doi": "10.1103/PhysRevLett.134.021001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevLett.134.021001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.06315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Version accepted for publication in Phys. Rev. Lett., 9 pages, 3\n  figures, 1 table",
                "arxiv_journal_ref": "Phys. Rev. Lett. 134, 021001 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11678v2",
                "updated": "2025-02-06T14:40:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    40,
                    52,
                    3,
                    37,
                    0
                ],
                "published": "2024-06-17T15:58:22Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    58,
                    22,
                    0,
                    169,
                    0
                ],
                "title": "TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is challenging. To tackle these\nissues, we introduce a novel documents ranking method called TourRank, which is\ninspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1)\novercome the limitation in input length and reduce the ranking latency by\nincorporating a multi-stage grouping strategy similar to the parallel group\nstage of sport tournaments; 2) improve the ranking performance and robustness\nto input orders by using a points system to ensemble multiple ranking results.\nWe test TourRank with different LLMs on the TREC DL datasets and the BEIR\nbenchmark. The experimental results demonstrate that TourRank delivers\nstate-of-the-art performance at a modest cost. The code of TourRank can be seen\non https://github.com/chenyiqun/TourRank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is challenging. To tackle these\nissues, we introduce a novel documents ranking method called TourRank, which is\ninspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1)\novercome the limitation in input length and reduce the ranking latency by\nincorporating a multi-stage grouping strategy similar to the parallel group\nstage of sport tournaments; 2) improve the ranking performance and robustness\nto input orders by using a points system to ensemble multiple ranking results.\nWe test TourRank with different LLMs on the TREC DL datasets and the BEIR\nbenchmark. The experimental results demonstrate that TourRank delivers\nstate-of-the-art performance at a modest cost. The code of TourRank can be seen\non https://github.com/chenyiqun/TourRank."
                },
                "authors": [
                    {
                        "name": "Yiqun Chen"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07111v2",
                "updated": "2025-02-06T14:33:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    33,
                    52,
                    3,
                    37,
                    0
                ],
                "published": "2024-01-13T16:20:36Z",
                "published_parsed": [
                    2024,
                    1,
                    13,
                    16,
                    20,
                    36,
                    5,
                    13,
                    0
                ],
                "title": "Bayesian Signal Matching for Transfer Learning in ERP-Based Brain\n  Computer Interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Signal Matching for Transfer Learning in ERP-Based Brain\n  Computer Interface"
                },
                "summary": "An Event-Related Potential (ERP)-based Brain-Computer Interface (BCI) Speller\nSystem assists people with disabilities to communicate by decoding\nelectroencephalogram (EEG) signals. A P300-ERP embedded in EEG signals arises\nin response to a rare, but relevant event (target) among a series of irrelevant\nevents (non-target). Different machine learning methods have constructed binary\nclassifiers to detect target events, known as calibration. The existing\ncalibration strategy uses data from participants themselves with lengthy\ntraining time. Participants feel bored and distracted, which causes biased P300\nestimation and decreased prediction accuracy. To resolve this issue, we propose\na Bayesian signal matching (BSM) framework to calibrate EEG signals from a new\nparticipant using data from source participants. BSM specifies the joint\ndistribution of stimulus-specific EEG signals among source participants via a\nBayesian hierarchical mixture model. We apply the inference strategy. If source\nand new participants are similar, they share the same set of model parameters;\notherwise, they keep their own sets of model parameters; we predict on the\ntesting data using parameters of the baseline cluster directly. Our\nhierarchical framework can be generalized to other base classifiers with\nparametric forms. We demonstrate the advantages of BSM using simulations and\nfocus on the real data analysis among participants with neuro-degenerative\ndiseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event-Related Potential (ERP)-based Brain-Computer Interface (BCI) Speller\nSystem assists people with disabilities to communicate by decoding\nelectroencephalogram (EEG) signals. A P300-ERP embedded in EEG signals arises\nin response to a rare, but relevant event (target) among a series of irrelevant\nevents (non-target). Different machine learning methods have constructed binary\nclassifiers to detect target events, known as calibration. The existing\ncalibration strategy uses data from participants themselves with lengthy\ntraining time. Participants feel bored and distracted, which causes biased P300\nestimation and decreased prediction accuracy. To resolve this issue, we propose\na Bayesian signal matching (BSM) framework to calibrate EEG signals from a new\nparticipant using data from source participants. BSM specifies the joint\ndistribution of stimulus-specific EEG signals among source participants via a\nBayesian hierarchical mixture model. We apply the inference strategy. If source\nand new participants are similar, they share the same set of model parameters;\notherwise, they keep their own sets of model parameters; we predict on the\ntesting data using parameters of the baseline cluster directly. Our\nhierarchical framework can be generalized to other base classifiers with\nparametric forms. We demonstrate the advantages of BSM using simulations and\nfocus on the real data analysis among participants with neuro-degenerative\ndiseases."
                },
                "authors": [
                    {
                        "name": "Tianwen Ma"
                    },
                    {
                        "name": "Jane E. Huggins"
                    },
                    {
                        "name": "Jian Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Kang"
                },
                "author": "Jian Kang",
                "arxiv_comment": "35 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04417v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04417v3",
                "updated": "2025-02-06T14:31:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    31,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-06T09:18:04Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    9,
                    18,
                    4,
                    6,
                    280,
                    0
                ],
                "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language\n  Model Inference"
                },
                "summary": "In vision-language models (VLMs), visual tokens usually bear a significant\namount of computational overhead despite sparsity of information in them when\ncompared to text tokens. To address this, most existing methods learn a network\nto prune redundant visual tokens using certain training data. Differently, we\npropose a text-guided training-free token optimization mechanism dubbed\nSparseVLM that eliminates the need of extra parameters or fine-tuning costs.\nGiven that visual tokens complement text tokens in VLM's linguistic reasoning,\nwe select relevant text tokens to rate the significance of visual tokens using\nself-attention matrices and, then, prune visual tokens using the proposed\nstrategy to maximize sparsity while retaining information. In particular, we\nintroduce a rank-based strategy to adaptively determine the sparsification\nratio for each layer, alongside a token recycling method that compresses pruned\ntokens into more compact representations. Experimental results show that\nSparseVLM increases the efficiency of various VLMs in a number of image and\nvideo understanding tasks. For example, LLaVA when equipped with SparseVLM\nachieves 54% reduction in FLOPs, 37% decrease in CUDA latency while maintaining\n97% of its original accuracy. Our code is available at\nhttps://github.com/Gumpest/SparseVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In vision-language models (VLMs), visual tokens usually bear a significant\namount of computational overhead despite sparsity of information in them when\ncompared to text tokens. To address this, most existing methods learn a network\nto prune redundant visual tokens using certain training data. Differently, we\npropose a text-guided training-free token optimization mechanism dubbed\nSparseVLM that eliminates the need of extra parameters or fine-tuning costs.\nGiven that visual tokens complement text tokens in VLM's linguistic reasoning,\nwe select relevant text tokens to rate the significance of visual tokens using\nself-attention matrices and, then, prune visual tokens using the proposed\nstrategy to maximize sparsity while retaining information. In particular, we\nintroduce a rank-based strategy to adaptively determine the sparsification\nratio for each layer, alongside a token recycling method that compresses pruned\ntokens into more compact representations. Experimental results show that\nSparseVLM increases the efficiency of various VLMs in a number of image and\nvideo understanding tasks. For example, LLaVA when equipped with SparseVLM\nachieves 54% reduction in FLOPs, 37% decrease in CUDA latency while maintaining\n97% of its original accuracy. Our code is available at\nhttps://github.com/Gumpest/SparseVLMs."
                },
                "authors": [
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Chun-Kai Fan"
                    },
                    {
                        "name": "Junpeng Ma"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Kuan Cheng"
                    },
                    {
                        "name": "Denis Gudovskiy"
                    },
                    {
                        "name": "Tomoyuki Okuno"
                    },
                    {
                        "name": "Yohei Nakata"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04417v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04417v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04103v1",
                "updated": "2025-02-06T14:27:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    27,
                    54,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T14:27:54Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    27,
                    54,
                    3,
                    37,
                    0
                ],
                "title": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases."
                },
                "authors": [
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Chengyu Lin"
                    },
                    {
                        "name": "Xinyi Tang"
                    },
                    {
                        "name": "Aprille Xi"
                    },
                    {
                        "name": "Canwen Wang"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Kenneth R Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R Koedinger"
                },
                "author": "Kenneth R Koedinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14425v3",
                "updated": "2025-02-06T14:26:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    26,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2024-05-23T10:48:30Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    10,
                    48,
                    30,
                    3,
                    144,
                    0
                ],
                "title": "When predict can also explain: few-shot prediction to select better\n  neural latents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predict can also explain: few-shot prediction to select better\n  neural latents"
                },
                "summary": "Latent variable models serve as powerful tools to infer underlying dynamics\nfrom observed neural activity. Ideally, the inferred dynamics should align with\ntrue ones. However, due to the absence of ground truth data, prediction\nbenchmarks are often employed as proxies. One widely-used method,\n*co-smoothing*, involves jointly estimating latent variables and predicting\nobservations along held-out channels to assess model performance. In this\nstudy, we reveal the limitations of the co-smoothing prediction framework and\npropose a remedy. In a student-teacher setup with Hidden Markov Models, we\ndemonstrate that the high co-smoothing model space encompasses models with\narbitrary extraneous dynamics in their latent representations. To address this,\nwe introduce a secondary metric -- *few-shot co-smoothing*, performing\nregression from the latent variables to held-out channels in the data using\nfewer trials. Our results indicate that among models with near-optimal\nco-smoothing, those with extraneous dynamics underperform in the few-shot\nco-smoothing compared to 'minimal' models that are devoid of such dynamics. We\nprovide analytical insights into the origin of this phenomenon and further\nvalidate our findings on real neural data using two state-of-the-art methods:\nLFADS and STNDT. In the absence of ground truth, we suggest a novel measure to\nvalidate our approach. By cross-decoding the latent variables of all model\npairs with high co-smoothing, we identify models with minimal extraneous\ndynamics. We find a correlation between few-shot co-smoothing performance and\nthis new measure. In summary, we present a novel prediction metric designed to\nyield latent variables that more accurately reflect the ground truth, offering\na significant improvement for latent dynamics inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent variable models serve as powerful tools to infer underlying dynamics\nfrom observed neural activity. Ideally, the inferred dynamics should align with\ntrue ones. However, due to the absence of ground truth data, prediction\nbenchmarks are often employed as proxies. One widely-used method,\n*co-smoothing*, involves jointly estimating latent variables and predicting\nobservations along held-out channels to assess model performance. In this\nstudy, we reveal the limitations of the co-smoothing prediction framework and\npropose a remedy. In a student-teacher setup with Hidden Markov Models, we\ndemonstrate that the high co-smoothing model space encompasses models with\narbitrary extraneous dynamics in their latent representations. To address this,\nwe introduce a secondary metric -- *few-shot co-smoothing*, performing\nregression from the latent variables to held-out channels in the data using\nfewer trials. Our results indicate that among models with near-optimal\nco-smoothing, those with extraneous dynamics underperform in the few-shot\nco-smoothing compared to 'minimal' models that are devoid of such dynamics. We\nprovide analytical insights into the origin of this phenomenon and further\nvalidate our findings on real neural data using two state-of-the-art methods:\nLFADS and STNDT. In the absence of ground truth, we suggest a novel measure to\nvalidate our approach. By cross-decoding the latent variables of all model\npairs with high co-smoothing, we identify models with minimal extraneous\ndynamics. We find a correlation between few-shot co-smoothing performance and\nthis new measure. In summary, we present a novel prediction metric designed to\nyield latent variables that more accurately reflect the ground truth, offering\na significant improvement for latent dynamics inference."
                },
                "authors": [
                    {
                        "name": "Kabir Dabholkar"
                    },
                    {
                        "name": "Omri Barak"
                    }
                ],
                "author_detail": {
                    "name": "Omri Barak"
                },
                "author": "Omri Barak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07283v2",
                "updated": "2025-02-06T14:23:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    23,
                    10,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-13T12:52:03Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    52,
                    3,
                    0,
                    13,
                    0
                ],
                "title": "Theoretical Modelling of Gamma-Ray Burst 090510",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Modelling of Gamma-Ray Burst 090510"
                },
                "summary": "Gamma-ray bursts detected at high energies provide valuable insights into the\nemission mechanisms behind these still puzzling enigmatic events. In this\nstudy, we focus on GRB 090510, which is an unusual short GRB exhibiting plateau\nemission observed by the Fermi-LAT. Using the general relativistic\nmagnetohydrodynamic code (HARM), we aim to infer the key properties of this\nGRB, such as the jet opening angle, the energetics, the Lorentz Gamma factor,\nthe jet structure and its variability, and the progenitor parameters of the\ncompact binary system. We explored both the 2D and 3D models and estimated the\nvariability timescales. Our findings show that the predicted jet opening angle\nis within $88\\%$ of the observed upper limit from observations, and the\nenergetics are in general agreement with observed values when accounting for\nthe evolution of jet opening angle with redshift. This work establishes the\nfoundation for ongoing exploration, which will further align the theoretical\nmodel simulations with observational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray bursts detected at high energies provide valuable insights into the\nemission mechanisms behind these still puzzling enigmatic events. In this\nstudy, we focus on GRB 090510, which is an unusual short GRB exhibiting plateau\nemission observed by the Fermi-LAT. Using the general relativistic\nmagnetohydrodynamic code (HARM), we aim to infer the key properties of this\nGRB, such as the jet opening angle, the energetics, the Lorentz Gamma factor,\nthe jet structure and its variability, and the progenitor parameters of the\ncompact binary system. We explored both the 2D and 3D models and estimated the\nvariability timescales. Our findings show that the predicted jet opening angle\nis within $88\\%$ of the observed upper limit from observations, and the\nenergetics are in general agreement with observed values when accounting for\nthe evolution of jet opening angle with redshift. This work establishes the\nfoundation for ongoing exploration, which will further align the theoretical\nmodel simulations with observational data."
                },
                "authors": [
                    {
                        "name": "Joseph Saji"
                    },
                    {
                        "name": "Agnieszka Janiuk"
                    },
                    {
                        "name": "Maria Giovanna Dainotti"
                    },
                    {
                        "name": "Shubham Bhardwaj"
                    },
                    {
                        "name": "Gerardo Urrutia"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Urrutia"
                },
                "author": "Gerardo Urrutia",
                "arxiv_comment": "accepted in the Proceedings of the MarcellGrossmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04095v1",
                "updated": "2025-02-06T14:12:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    12,
                    41,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T14:12:41Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    12,
                    41,
                    3,
                    37,
                    0
                ],
                "title": "LLMs to Support a Domain Specific Knowledge Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs to Support a Domain Specific Knowledge Assistant"
                },
                "summary": "This work presents a custom approach to developing a domain specific\nknowledge assistant for sustainability reporting using the International\nFinancial Reporting Standards (IFRS). In this domain, there is no publicly\navailable question-answer dataset, which has impeded the development of a\nhigh-quality chatbot to support companies with IFRS reporting. The two key\ncontributions of this project therefore are:\n  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS\nsustainability standards, created using a novel generation and evaluation\npipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse\nQA pairs that address a wide spectrum of potential user queries in\nsustainability reporting. Various LLM-based techniques are employed to create\nthe dataset, including chain-of-thought reasoning and few-shot prompting. A\ncustom evaluation framework is developed to assess question and answer quality\nacross multiple dimensions, including faithfulness, relevance, and domain\nspecificity. The dataset averages a score range of 8.16 out of 10 on these\nmetrics.\n  (2) Two architectures for question-answering in the sustainability reporting\ndomain - a RAG pipeline and a fully LLM-based pipeline. The architectures are\ndeveloped by experimenting, fine-tuning, and training on the QA dataset. The\nfinal pipelines feature an LLM fine-tuned on domain specific data and an\nindustry classification component to improve the handling of complex queries.\nThe RAG architecture achieves an accuracy of 85.32% on single-industry and\n72.15% on cross-industry multiple-choice questions, outperforming the baseline\napproach by 4.67 and 19.21 percentage points, respectively. The LLM-based\npipeline achieves an accuracy of 93.45% on single-industry and 80.30% on\ncross-industry multiple-choice questions, an improvement of 12.80 and 27.36\npercentage points over the baseline, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a custom approach to developing a domain specific\nknowledge assistant for sustainability reporting using the International\nFinancial Reporting Standards (IFRS). In this domain, there is no publicly\navailable question-answer dataset, which has impeded the development of a\nhigh-quality chatbot to support companies with IFRS reporting. The two key\ncontributions of this project therefore are:\n  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS\nsustainability standards, created using a novel generation and evaluation\npipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse\nQA pairs that address a wide spectrum of potential user queries in\nsustainability reporting. Various LLM-based techniques are employed to create\nthe dataset, including chain-of-thought reasoning and few-shot prompting. A\ncustom evaluation framework is developed to assess question and answer quality\nacross multiple dimensions, including faithfulness, relevance, and domain\nspecificity. The dataset averages a score range of 8.16 out of 10 on these\nmetrics.\n  (2) Two architectures for question-answering in the sustainability reporting\ndomain - a RAG pipeline and a fully LLM-based pipeline. The architectures are\ndeveloped by experimenting, fine-tuning, and training on the QA dataset. The\nfinal pipelines feature an LLM fine-tuned on domain specific data and an\nindustry classification component to improve the handling of complex queries.\nThe RAG architecture achieves an accuracy of 85.32% on single-industry and\n72.15% on cross-industry multiple-choice questions, outperforming the baseline\napproach by 4.67 and 19.21 percentage points, respectively. The LLM-based\npipeline achieves an accuracy of 93.45% on single-industry and 80.30% on\ncross-industry multiple-choice questions, an improvement of 12.80 and 27.36\npercentage points over the baseline, respectively."
                },
                "authors": [
                    {
                        "name": "Maria-Flavia Lovin"
                    }
                ],
                "author_detail": {
                    "name": "Maria-Flavia Lovin"
                },
                "author": "Maria-Flavia Lovin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04089v1",
                "updated": "2025-02-06T14:02:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    2,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T14:02:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    2,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "Revisiting symbiotic binaries with interferometry. I. The PIONIER\n  archival collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting symbiotic binaries with interferometry. I. The PIONIER\n  archival collection"
                },
                "summary": "Symbiotic stars serve as exceptional laboratories for investigating mass\ntransfer processes in binary systems. However, the dominant mechanism of mass\ntransfer from the red giant donor to the compact accretor - typically a white\ndwarf or, in rare cases, a neutron star - remains unclear. It is uncertain\nwhether it is driven primarily by the stellar wind, Roche-lobe overflow, or a\ncombination of the two. While radii inferred from rotational velocities or\nspectral types suggest smaller Roche-lobe filling factors, the presence of\nellipsoidal variability, presumably caused by tidally deformed giants in many\nsymbiotic systems, indicates the opposite. Interferometric observations of\nsymbiotic giants, combined with distance measurements provided by the Gaia\nmission, offer a promising avenue to resolve this discrepancy. In this first\npaper of the series, we (re)analyze VLTI/PIONIER observations of six symbiotic\nstars: AG Peg, FG Ser, ER Del, V1261 Ori, RW Hya, and V399 Pav. With the\nexception of the uncertain case of V399 Pav, we find that the giants in these\nsystems remain well within their canonical Roche lobes, even in V1261 Ori and\nRW Hya, where ellipsoidal variability is observed. All six stars appear to be\nrather luminous and likely located on the asymptotic giant branch, although the\npossibility of some of them being at the tip of the first red giant branch\ncannot be ruled out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbiotic stars serve as exceptional laboratories for investigating mass\ntransfer processes in binary systems. However, the dominant mechanism of mass\ntransfer from the red giant donor to the compact accretor - typically a white\ndwarf or, in rare cases, a neutron star - remains unclear. It is uncertain\nwhether it is driven primarily by the stellar wind, Roche-lobe overflow, or a\ncombination of the two. While radii inferred from rotational velocities or\nspectral types suggest smaller Roche-lobe filling factors, the presence of\nellipsoidal variability, presumably caused by tidally deformed giants in many\nsymbiotic systems, indicates the opposite. Interferometric observations of\nsymbiotic giants, combined with distance measurements provided by the Gaia\nmission, offer a promising avenue to resolve this discrepancy. In this first\npaper of the series, we (re)analyze VLTI/PIONIER observations of six symbiotic\nstars: AG Peg, FG Ser, ER Del, V1261 Ori, RW Hya, and V399 Pav. With the\nexception of the uncertain case of V399 Pav, we find that the giants in these\nsystems remain well within their canonical Roche lobes, even in V1261 Ori and\nRW Hya, where ellipsoidal variability is observed. All six stars appear to be\nrather luminous and likely located on the asymptotic giant branch, although the\npossibility of some of them being at the tip of the first red giant branch\ncannot be ruled out."
                },
                "authors": [
                    {
                        "name": "Jaroslav Merc"
                    },
                    {
                        "name": "Henri M. J. Boffin"
                    }
                ],
                "author_detail": {
                    "name": "Henri M. J. Boffin"
                },
                "author": "Henri M. J. Boffin",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, additional figures in the appendices;\n  accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04088v1",
                "updated": "2025-02-06T13:57:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    57,
                    19,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:57:19Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    57,
                    19,
                    3,
                    37,
                    0
                ],
                "title": "Quantifying imperfect cognition via achieved information gain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying imperfect cognition via achieved information gain"
                },
                "summary": "Cognition, the process of information processing in form of inference,\ncommunication, and memorization, is the central activity of any intelligence.\nIts physical realization in a brain, computer, or in any other intelligent\nsystem requires resources like time, energy, memory, bandwidth, money, and\nothers. Due to limited resources, many real world intelligent systems perform\nonly imperfect cognition. For understanding the trade-off between accuracy and\nresource investments in existing systems, e.g. in biology, as well as for the\nresource-aware optimal design of information processing systems, like computer\nalgorithms and artificial neural networks, a quantification of information\nobtained in an imperfect cognitive operation is desirable. To this end, we\npropose the concept of achieved information gain (AIG) of a belief update,\nwhich is given by the amount of information obtained by updating from the\ninitial knowledge state to the ideal one, minus the amount a change from the\nimperfect to the ideal state would yield. AIG has many properties desired for\nquantifying imperfect cognition. The ratio of achieved to ideally obtainable\ninformation measures cognitive fidelity and that of AIG to the necessary\ncognitive effort measures cognitive efficiency. We provide an axiomatic\nderivation of AIG, illustrate its application at common scenarios of posterior\ninaccuracies, and discuss the implication of cognitive efficiency for\nsustainable resource allocation in computational inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognition, the process of information processing in form of inference,\ncommunication, and memorization, is the central activity of any intelligence.\nIts physical realization in a brain, computer, or in any other intelligent\nsystem requires resources like time, energy, memory, bandwidth, money, and\nothers. Due to limited resources, many real world intelligent systems perform\nonly imperfect cognition. For understanding the trade-off between accuracy and\nresource investments in existing systems, e.g. in biology, as well as for the\nresource-aware optimal design of information processing systems, like computer\nalgorithms and artificial neural networks, a quantification of information\nobtained in an imperfect cognitive operation is desirable. To this end, we\npropose the concept of achieved information gain (AIG) of a belief update,\nwhich is given by the amount of information obtained by updating from the\ninitial knowledge state to the ideal one, minus the amount a change from the\nimperfect to the ideal state would yield. AIG has many properties desired for\nquantifying imperfect cognition. The ratio of achieved to ideally obtainable\ninformation measures cognitive fidelity and that of AIG to the necessary\ncognitive effort measures cognitive efficiency. We provide an axiomatic\nderivation of AIG, illustrate its application at common scenarios of posterior\ninaccuracies, and discuss the implication of cognitive efficiency for\nsustainable resource allocation in computational inference."
                },
                "authors": [
                    {
                        "name": "Torsten Enßlin"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Enßlin"
                },
                "author": "Torsten Enßlin",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94A15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v8",
                "updated": "2025-02-06T13:42:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    42,
                    31,
                    3,
                    37,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation"
                },
                "summary": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04078v1",
                "updated": "2025-02-06T13:42:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    42,
                    7,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:42:07Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    42,
                    7,
                    3,
                    37,
                    0
                ],
                "title": "CDIO: Cross-Domain Inference Optimization with Resource Preference\n  Prediction for Edge-Cloud Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDIO: Cross-Domain Inference Optimization with Resource Preference\n  Prediction for Edge-Cloud Collaboration"
                },
                "summary": "Currently, massive video tasks are processed by edge-cloud collaboration.\nHowever, the diversity of task requirements and the dynamics of resources pose\ngreat challenges to efficient inference, resulting in many wasted resources. In\nthis paper, we present CDIO, a cross-domain inference optimization framework\ndesigned for edge-cloud collaboration. For diverse input tasks, CDIO can\npredict resource preference types by analyzing spatial complexity and\nprocessing requirements of the task. Subsequently, a cross-domain collaborative\noptimization algorithm is employed to guide resource allocation in the\nedge-cloud system. By ensuring that each task is matched with the ideal\nservers, the edge-cloud system can achieve higher efficiency inference. The\nevaluation results on public datasets demonstrate that CDIO can effectively\nmeet the accuracy and delay requirements for task processing. Compared to\nstate-of-the-art edge-cloud solutions, CDIO achieves a computing and bandwidth\nconsumption reduction of 20%-40%. And it can reduce energy consumption by more\nthan 40%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, massive video tasks are processed by edge-cloud collaboration.\nHowever, the diversity of task requirements and the dynamics of resources pose\ngreat challenges to efficient inference, resulting in many wasted resources. In\nthis paper, we present CDIO, a cross-domain inference optimization framework\ndesigned for edge-cloud collaboration. For diverse input tasks, CDIO can\npredict resource preference types by analyzing spatial complexity and\nprocessing requirements of the task. Subsequently, a cross-domain collaborative\noptimization algorithm is employed to guide resource allocation in the\nedge-cloud system. By ensuring that each task is matched with the ideal\nservers, the edge-cloud system can achieve higher efficiency inference. The\nevaluation results on public datasets demonstrate that CDIO can effectively\nmeet the accuracy and delay requirements for task processing. Compared to\nstate-of-the-art edge-cloud solutions, CDIO achieves a computing and bandwidth\nconsumption reduction of 20%-40%. And it can reduce energy consumption by more\nthan 40%."
                },
                "authors": [
                    {
                        "name": "Zheming Yang"
                    },
                    {
                        "name": "Wen Ji"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Dieli Hu"
                    },
                    {
                        "name": "Chang Zhao"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Xuanlei Zhao"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Chaoyu Gong"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05675v2",
                "updated": "2025-02-06T13:39:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    39,
                    31,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-07T14:44:22Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    14,
                    44,
                    22,
                    5,
                    342,
                    0
                ],
                "title": "M$^3$PC: Test-time Model Predictive Control for Pretrained Masked\n  Trajectory Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M$^3$PC: Test-time Model Predictive Control for Pretrained Masked\n  Trajectory Model"
                },
                "summary": "Recent work in Offline Reinforcement Learning (RL) has shown that a unified\nTransformer trained under a masked auto-encoding objective can effectively\ncapture the relationships between different modalities (e.g., states, actions,\nrewards) within given trajectory datasets. However, this information has not\nbeen fully exploited during the inference phase, where the agent needs to\ngenerate an optimal policy instead of just reconstructing masked components\nfrom unmasked ones. Given that a pretrained trajectory model can act as both a\nPolicy Model and a World Model with appropriate mask patterns, we propose using\nModel Predictive Control (MPC) at test time to leverage the model's own\npredictive capability to guide its action selection. Empirical results on D4RL\nand RoboMimic show that our inference-phase MPC significantly improves the\ndecision-making performance of a pretrained trajectory model without any\nadditional parameter training. Furthermore, our framework can be adapted to\nOffline to Online (O2O) RL and Goal Reaching RL, resulting in more substantial\nperformance gains when an additional online interaction budget is provided, and\nbetter generalization capabilities when different task targets are specified.\nCode is available: https://github.com/wkh923/m3pc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work in Offline Reinforcement Learning (RL) has shown that a unified\nTransformer trained under a masked auto-encoding objective can effectively\ncapture the relationships between different modalities (e.g., states, actions,\nrewards) within given trajectory datasets. However, this information has not\nbeen fully exploited during the inference phase, where the agent needs to\ngenerate an optimal policy instead of just reconstructing masked components\nfrom unmasked ones. Given that a pretrained trajectory model can act as both a\nPolicy Model and a World Model with appropriate mask patterns, we propose using\nModel Predictive Control (MPC) at test time to leverage the model's own\npredictive capability to guide its action selection. Empirical results on D4RL\nand RoboMimic show that our inference-phase MPC significantly improves the\ndecision-making performance of a pretrained trajectory model without any\nadditional parameter training. Furthermore, our framework can be adapted to\nOffline to Online (O2O) RL and Goal Reaching RL, resulting in more substantial\nperformance gains when an additional online interaction budget is provided, and\nbetter generalization capabilities when different task targets are specified.\nCode is available: https://github.com/wkh923/m3pc."
                },
                "authors": [
                    {
                        "name": "Kehan Wen"
                    },
                    {
                        "name": "Yutong Hu"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Lei Ke"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ke"
                },
                "author": "Lei Ke",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04075v1",
                "updated": "2025-02-06T13:38:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    38,
                    57,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    38,
                    57,
                    3,
                    37,
                    0
                ],
                "title": "Controllable Emotion Generation with Emotion Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Emotion Generation with Emotion Vectors"
                },
                "summary": "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method."
                },
                "authors": [
                    {
                        "name": "Yurui Dong"
                    },
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Yao Yang"
                    },
                    {
                        "name": "Bingjie Lu"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Zhi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Liu"
                },
                "author": "Zhi Liu",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04067v1",
                "updated": "2025-02-06T13:24:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    24,
                    29,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:24:29Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    24,
                    29,
                    3,
                    37,
                    0
                ],
                "title": "Generalised Bayesian distance-based phylogenetics for the genomics era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalised Bayesian distance-based phylogenetics for the genomics era"
                },
                "summary": "As whole genomes become widely available, maximum likelihood and Bayesian\nphylogenetic methods are demonstrating their limits in meeting the escalating\ncomputational demands. Conversely, distance-based phylogenetic methods are\nefficient, but are rarely favoured due to their inferior performance. Here, we\nextend distance-based phylogenetics using an entropy-based likelihood of the\nevolution among pairs of taxa, allowing for fast Bayesian inference in\ngenome-scale datasets. We provide evidence of a close link between the\ninference criteria used in distance methods and Felsenstein's likelihood, such\nthat the methods are expected to have comparable performance in practice. Using\nthe entropic likelihood, we perform Bayesian inference on three phylogenetic\nbenchmark datasets and find that estimates closely correspond with previous\ninferences. We also apply this rapid inference approach to a 60-million-site\nalignment from 363 avian taxa, covering most avian families. The method has\noutstanding performance and reveals substantial uncertainty in the avian\ndiversification events immediately after the K-Pg transition event. The\nentropic likelihood allows for efficient Bayesian phylogenetic inference,\naccommodating the analysis demands of the genomic era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As whole genomes become widely available, maximum likelihood and Bayesian\nphylogenetic methods are demonstrating their limits in meeting the escalating\ncomputational demands. Conversely, distance-based phylogenetic methods are\nefficient, but are rarely favoured due to their inferior performance. Here, we\nextend distance-based phylogenetics using an entropy-based likelihood of the\nevolution among pairs of taxa, allowing for fast Bayesian inference in\ngenome-scale datasets. We provide evidence of a close link between the\ninference criteria used in distance methods and Felsenstein's likelihood, such\nthat the methods are expected to have comparable performance in practice. Using\nthe entropic likelihood, we perform Bayesian inference on three phylogenetic\nbenchmark datasets and find that estimates closely correspond with previous\ninferences. We also apply this rapid inference approach to a 60-million-site\nalignment from 363 avian taxa, covering most avian families. The method has\noutstanding performance and reveals substantial uncertainty in the avian\ndiversification events immediately after the K-Pg transition event. The\nentropic likelihood allows for efficient Bayesian phylogenetic inference,\naccommodating the analysis demands of the genomic era."
                },
                "authors": [
                    {
                        "name": "Matthew J. Penn"
                    },
                    {
                        "name": "Neil Scheidwasser"
                    },
                    {
                        "name": "Mark P. Khurana"
                    },
                    {
                        "name": "Christl A. Donnelly"
                    },
                    {
                        "name": "David A. Duchêne"
                    },
                    {
                        "name": "Samir Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Samir Bhatt"
                },
                "author": "Samir Bhatt",
                "arxiv_comment": "53 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06809v2",
                "updated": "2025-02-06T13:21:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    21,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-09T12:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    9,
                    30,
                    2,
                    283,
                    0
                ],
                "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level"
                },
                "summary": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xinyi Zeng"
                    },
                    {
                        "name": "Yuying Shang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04050v1",
                "updated": "2025-02-06T13:08:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    8,
                    43,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    8,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models"
                },
                "summary": "We present the first text-based image editing approach for object parts based\non pre-trained diffusion models. Diffusion-based image editing approaches\ncapitalized on the deep understanding of diffusion models of image semantics to\nperform a variety of edits. However, existing diffusion models lack sufficient\nunderstanding of many object parts, hindering fine-grained edits requested by\nusers. To address this, we propose to expand the knowledge of pre-trained\ndiffusion models to allow them to understand various object parts, enabling\nthem to perform fine-grained edits. We achieve this by learning special textual\ntokens that correspond to different object parts through an efficient token\noptimization process. These tokens are optimized to produce reliable\nlocalization masks at each inference step to localize the editing region.\nLeveraging these masks, we design feature-blending and adaptive thresholding\nstrategies to execute the edits seamlessly. To evaluate our approach, we\nestablish a benchmark and an evaluation protocol for part editing. Experiments\nshow that our approach outperforms existing editing methods on all metrics and\nis preferred by users 77-90% of the time in conducted user studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first text-based image editing approach for object parts based\non pre-trained diffusion models. Diffusion-based image editing approaches\ncapitalized on the deep understanding of diffusion models of image semantics to\nperform a variety of edits. However, existing diffusion models lack sufficient\nunderstanding of many object parts, hindering fine-grained edits requested by\nusers. To address this, we propose to expand the knowledge of pre-trained\ndiffusion models to allow them to understand various object parts, enabling\nthem to perform fine-grained edits. We achieve this by learning special textual\ntokens that correspond to different object parts through an efficient token\noptimization process. These tokens are optimized to produce reliable\nlocalization masks at each inference step to localize the editing region.\nLeveraging these masks, we design feature-blending and adaptive thresholding\nstrategies to execute the edits seamlessly. To evaluate our approach, we\nestablish a benchmark and an evaluation protocol for part editing. Experiments\nshow that our approach outperforms existing editing methods on all metrics and\nis preferred by users 77-90% of the time in conducted user studies."
                },
                "authors": [
                    {
                        "name": "Aleksandar Cvejic"
                    },
                    {
                        "name": "Abdelrahman Eldesokey"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "arxiv_affiliation": "KAUST",
                "author": "Peter Wonka",
                "arxiv_comment": "Project page: https://partedit.github.io/PartEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04043v1",
                "updated": "2025-02-06T13:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    3,
                    5,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:03:05Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    3,
                    5,
                    3,
                    37,
                    0
                ],
                "title": "Probe-Free Low-Rank Activation Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probe-Free Low-Rank Activation Intervention"
                },
                "summary": "Language models (LMs) can produce texts that appear accurate and coherent but\ncontain untruthful or toxic content. Inference-time interventions that edit the\nhidden activations have shown promising results in steering the LMs towards\ndesirable generations. Existing activation intervention methods often comprise\nan activation probe to detect undesirable generation, triggering the activation\nmodification to steer subsequent generation. This paper proposes a probe-free\nintervention method FLORAIN for all attention heads in a specific activation\nlayer. It eliminates the need to train classifiers for probing purposes. The\nintervention function is parametrized by a sample-wise nonlinear low-rank\nmapping, which is trained by minimizing the distance between the modified\nactivations and their projection onto the manifold of desirable content. Under\nspecific constructions of the manifold and projection distance, we show that\nthe intervention strategy can be computed efficiently by solving a smooth\noptimization problem. The empirical results, benchmarked on multiple base\nmodels, demonstrate that FLORAIN consistently outperforms several baseline\nmethods in enhancing model truthfulness and quality across generation and\nmultiple-choice tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) can produce texts that appear accurate and coherent but\ncontain untruthful or toxic content. Inference-time interventions that edit the\nhidden activations have shown promising results in steering the LMs towards\ndesirable generations. Existing activation intervention methods often comprise\nan activation probe to detect undesirable generation, triggering the activation\nmodification to steer subsequent generation. This paper proposes a probe-free\nintervention method FLORAIN for all attention heads in a specific activation\nlayer. It eliminates the need to train classifiers for probing purposes. The\nintervention function is parametrized by a sample-wise nonlinear low-rank\nmapping, which is trained by minimizing the distance between the modified\nactivations and their projection onto the manifold of desirable content. Under\nspecific constructions of the manifold and projection distance, we show that\nthe intervention strategy can be computed efficiently by solving a smooth\noptimization problem. The empirical results, benchmarked on multiple base\nmodels, demonstrate that FLORAIN consistently outperforms several baseline\nmethods in enhancing model truthfulness and quality across generation and\nmultiple-choice tasks."
                },
                "authors": [
                    {
                        "name": "Chonghe Jiang"
                    },
                    {
                        "name": "Bao Nguyen"
                    },
                    {
                        "name": "Anthony Man-Cho So"
                    },
                    {
                        "name": "Viet Anh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Viet Anh Nguyen"
                },
                "author": "Viet Anh Nguyen",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04040v1",
                "updated": "2025-02-06T13:01:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    1,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:01:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    1,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for\n  Enhancing Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for\n  Enhancing Safety Alignment"
                },
                "summary": "Training safe LLMs is one of the most critical research challenge. However,\nthe commonly used method, Refusal Training (RT), struggles to generalize\nagainst various OOD jailbreaking attacks. Many safety training methods have\nbeen proposed to address this issue. While they offer valuable insights, we aim\nto complement this line of research by investigating whether OOD attacks truly\nexceed the capability of RT model. Conducting evaluation with BoN, we observe\nsignificant improvements on generalization as N increases. This underscores\nthat the model possesses sufficient safety-related latent knowledge, but RT\nfails to consistently elicit this knowledge when addressing OOD attacks.\nFurther analysis based on domain adaptation reveals that training with direct\nrefusal causes model to rely on superficial shortcuts, resulting in learning of\nnon-robust representation mappings. Based on our findings, we propose training\nmodel to perform safety reasoning for each query. Reasoning supervision\nencourages model to perform more computations, explicitly eliciting and using\nlatent knowledge through reasoning. To achieve this, we synthesize reasoning\nsupervision based on pre-guidelines, training the model to reason in alignment\nwith them, thereby effectively eliciting and utilizing latent knowledge from\ndiverse perspectives. Extensive experiments show that our method significantly\nimproves generalization performance against OOD attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training safe LLMs is one of the most critical research challenge. However,\nthe commonly used method, Refusal Training (RT), struggles to generalize\nagainst various OOD jailbreaking attacks. Many safety training methods have\nbeen proposed to address this issue. While they offer valuable insights, we aim\nto complement this line of research by investigating whether OOD attacks truly\nexceed the capability of RT model. Conducting evaluation with BoN, we observe\nsignificant improvements on generalization as N increases. This underscores\nthat the model possesses sufficient safety-related latent knowledge, but RT\nfails to consistently elicit this knowledge when addressing OOD attacks.\nFurther analysis based on domain adaptation reveals that training with direct\nrefusal causes model to rely on superficial shortcuts, resulting in learning of\nnon-robust representation mappings. Based on our findings, we propose training\nmodel to perform safety reasoning for each query. Reasoning supervision\nencourages model to perform more computations, explicitly eliciting and using\nlatent knowledge through reasoning. To achieve this, we synthesize reasoning\nsupervision based on pre-guidelines, training the model to reason in alignment\nwith them, thereby effectively eliciting and utilizing latent knowledge from\ndiverse perspectives. Extensive experiments show that our method significantly\nimproves generalization performance against OOD attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04037v1",
                "updated": "2025-02-06T12:57:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    57,
                    50,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:57:50Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    57,
                    50,
                    3,
                    37,
                    0
                ],
                "title": "Exploring Imbalanced Annotations for Effective In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Imbalanced Annotations for Effective In-Context Learning"
                },
                "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. Existing selection methods may\nhinge on the distribution of annotated datasets, which can often be long-tailed\nin real-world scenarios. In this work, we show that imbalanced class\ndistributions in annotated datasets significantly degrade the performance of\nICL across various tasks and selection methods. Moreover, traditional rebalance\nmethods fail to ameliorate the issue of class imbalance in ICL. Our method is\nmotivated by decomposing the distributional differences between annotated and\ntest datasets into two-component weights: class-wise weights and conditional\nbias. The key idea behind our method is to estimate the conditional bias by\nminimizing the empirical error on a balanced validation dataset and to employ\nthe two-component weights to modify the original scoring functions during\nselection. Our approach can prevent selecting too many demonstrations from a\nsingle class while preserving the effectiveness of the original selection\nmethods. Extensive experiments demonstrate the effectiveness of our method,\nimproving the average accuracy by up to 5.46 on common benchmarks with\nimbalanced datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. Existing selection methods may\nhinge on the distribution of annotated datasets, which can often be long-tailed\nin real-world scenarios. In this work, we show that imbalanced class\ndistributions in annotated datasets significantly degrade the performance of\nICL across various tasks and selection methods. Moreover, traditional rebalance\nmethods fail to ameliorate the issue of class imbalance in ICL. Our method is\nmotivated by decomposing the distributional differences between annotated and\ntest datasets into two-component weights: class-wise weights and conditional\nbias. The key idea behind our method is to estimate the conditional bias by\nminimizing the empirical error on a balanced validation dataset and to employ\nthe two-component weights to modify the original scoring functions during\nselection. Our approach can prevent selecting too many demonstrations from a\nsingle class while preserving the effectiveness of the original selection\nmethods. Extensive experiments demonstrate the effectiveness of our method,\nimproving the average accuracy by up to 5.46 on common benchmarks with\nimbalanced datasets."
                },
                "authors": [
                    {
                        "name": "Hongfu Gao"
                    },
                    {
                        "name": "Feipeng Zhang"
                    },
                    {
                        "name": "Hao Zeng"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11468v2",
                "updated": "2025-02-06T12:55:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    55,
                    33,
                    3,
                    37,
                    0
                ],
                "published": "2024-03-18T04:41:38Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    4,
                    41,
                    38,
                    0,
                    78,
                    0
                ],
                "title": "CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with\n  GPT-4V",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with\n  GPT-4V"
                },
                "summary": "Recent advancements in generative AI have suggested that by taking visual\nprompts, GPT-4V can demonstrate significant proficiency in visual recognition\ntasks. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier to its wide use. To address\nthis challenge, we propose a budget-friendly collage prompting task that\ncollages multiple images into a single visual prompt and makes GPT-4V perform\nvisual recognition on several images simultaneously, thereby reducing the cost.\nWe collect a dataset of various collage prompts to assess its performance in\nGPT-4V's visual recognition. Our evaluations reveal several key findings: 1)\nRecognition accuracy varies with different positions in the collage. 2)\nGrouping images of the same category together leads to better visual\nrecognition results. 3) Incorrect labels often come from adjacent images. These\nfindings highlight the importance of image arrangement within collage prompt.\nTo this end, we construct a benchmark called CollagePrompt, which offers a\nplatform for designing collage prompt to achieve more cost-effective visual\nrecognition with GPT-4V. A baseline method derived from genetic algorithms to\noptimize collage layouts is proposed and two metrics are introduced to measure\nthe efficiency of the optimized collage prompt. Our benchmark enables\nresearchers to better optimize collage prompts, thus making GPT-4V more\ncost-effective in visual recognition. The code and data are available at this\nproject page https://collageprompting.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative AI have suggested that by taking visual\nprompts, GPT-4V can demonstrate significant proficiency in visual recognition\ntasks. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier to its wide use. To address\nthis challenge, we propose a budget-friendly collage prompting task that\ncollages multiple images into a single visual prompt and makes GPT-4V perform\nvisual recognition on several images simultaneously, thereby reducing the cost.\nWe collect a dataset of various collage prompts to assess its performance in\nGPT-4V's visual recognition. Our evaluations reveal several key findings: 1)\nRecognition accuracy varies with different positions in the collage. 2)\nGrouping images of the same category together leads to better visual\nrecognition results. 3) Incorrect labels often come from adjacent images. These\nfindings highlight the importance of image arrangement within collage prompt.\nTo this end, we construct a benchmark called CollagePrompt, which offers a\nplatform for designing collage prompt to achieve more cost-effective visual\nrecognition with GPT-4V. A baseline method derived from genetic algorithms to\noptimize collage layouts is proposed and two metrics are introduced to measure\nthe efficiency of the optimized collage prompt. Our benchmark enables\nresearchers to better optimize collage prompts, thus making GPT-4V more\ncost-effective in visual recognition. The code and data are available at this\nproject page https://collageprompting.github.io/."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Daochang Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted by NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07770v2",
                "updated": "2025-02-06T12:52:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    52,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2024-02-12T16:32:37Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    16,
                    32,
                    37,
                    0,
                    43,
                    0
                ],
                "title": "Had enough of experts? Quantitative knowledge retrieval from large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Had enough of experts? Quantitative knowledge retrieval from large\n  language models"
                },
                "summary": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. Here we\nexplore the feasibility of LLMs as a mechanism for quantitative knowledge\nretrieval to aid two data analysis tasks: elicitation of prior distributions\nfor Bayesian models and imputation of missing data. We introduce a framework\nthat leverages LLMs to enhance Bayesian workflows by eliciting expert-like\nprior knowledge and imputing missing data. Tested on diverse datasets, this\napproach can improve predictive accuracy and reduce data requirements, offering\nsignificant potential in healthcare, environmental science and engineering\napplications. We discuss the implications and challenges of treating LLMs as\n'experts'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. Here we\nexplore the feasibility of LLMs as a mechanism for quantitative knowledge\nretrieval to aid two data analysis tasks: elicitation of prior distributions\nfor Bayesian models and imputation of missing data. We introduce a framework\nthat leverages LLMs to enhance Bayesian workflows by eliciting expert-like\nprior knowledge and imputing missing data. Tested on diverse datasets, this\napproach can improve predictive accuracy and reduce data requirements, offering\nsignificant potential in healthcare, environmental science and engineering\napplications. We discuss the implications and challenges of treating LLMs as\n'experts'."
                },
                "authors": [
                    {
                        "name": "David Selby"
                    },
                    {
                        "name": "Kai Spriestersbach"
                    },
                    {
                        "name": "Yuichiro Iwashita"
                    },
                    {
                        "name": "Mohammad Saad"
                    },
                    {
                        "name": "Dennis Bappert"
                    },
                    {
                        "name": "Archana Warrier"
                    },
                    {
                        "name": "Sumantrak Mukherjee"
                    },
                    {
                        "name": "Koichi Kise"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Vollmer"
                },
                "author": "Sebastian Vollmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04030v1",
                "updated": "2025-02-06T12:47:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    47,
                    25,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:47:25Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    47,
                    25,
                    3,
                    37,
                    0
                ],
                "title": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging"
                },
                "summary": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04029v1",
                "updated": "2025-02-06T12:43:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    43,
                    9,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:43:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    43,
                    9,
                    3,
                    37,
                    0
                ],
                "title": "Echo-Teddy: Preliminary Design and Development of Large Language\n  Model-based Social Robot for Autistic Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo-Teddy: Preliminary Design and Development of Large Language\n  Model-based Social Robot for Autistic Students"
                },
                "summary": "Autistic students often face challenges in social interaction, which can\nhinder their educational and personal development. This study introduces\nEcho-Teddy, a Large Language Model (LLM)-based social robot designed to support\nautistic students in developing social and communication skills. Unlike\nprevious chatbot-based solutions, Echo-Teddy leverages advanced LLM\ncapabilities to provide more natural and adaptive interactions. The research\naddresses two key questions: (1) What are the design principles and initial\nprototype characteristics of an effective LLM-based social robot for autistic\nstudents? (2) What improvements can be made based on developer\nreflection-on-action and expert interviews? The study employed a mixed-methods\napproach, combining prototype development with qualitative analysis of\ndeveloper reflections and expert interviews. Key design principles identified\ninclude customizability, ethical considerations, and age-appropriate\ninteractions. The initial prototype, built on a Raspberry Pi platform, features\ncustom speech components and basic motor functions. Evaluation of the prototype\nrevealed potential improvements in areas such as user interface, educational\nvalue, and practical implementation in educational settings. This research\ncontributes to the growing field of AI-assisted special education by\ndemonstrating the potential of LLM-based social robots in supporting autistic\nstudents. The findings provide valuable insights for future developments in\naccessible and effective social support tools for special education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autistic students often face challenges in social interaction, which can\nhinder their educational and personal development. This study introduces\nEcho-Teddy, a Large Language Model (LLM)-based social robot designed to support\nautistic students in developing social and communication skills. Unlike\nprevious chatbot-based solutions, Echo-Teddy leverages advanced LLM\ncapabilities to provide more natural and adaptive interactions. The research\naddresses two key questions: (1) What are the design principles and initial\nprototype characteristics of an effective LLM-based social robot for autistic\nstudents? (2) What improvements can be made based on developer\nreflection-on-action and expert interviews? The study employed a mixed-methods\napproach, combining prototype development with qualitative analysis of\ndeveloper reflections and expert interviews. Key design principles identified\ninclude customizability, ethical considerations, and age-appropriate\ninteractions. The initial prototype, built on a Raspberry Pi platform, features\ncustom speech components and basic motor functions. Evaluation of the prototype\nrevealed potential improvements in areas such as user interface, educational\nvalue, and practical implementation in educational settings. This research\ncontributes to the growing field of AI-assisted special education by\ndemonstrating the potential of LLM-based social robots in supporting autistic\nstudents. The findings provide valuable insights for future developments in\naccessible and effective social support tools for special education."
                },
                "authors": [
                    {
                        "name": "Unggi Lee"
                    },
                    {
                        "name": "Hansung Kim"
                    },
                    {
                        "name": "Juhong Eom"
                    },
                    {
                        "name": "Hyeonseo Jeong"
                    },
                    {
                        "name": "Seungyeon Lee"
                    },
                    {
                        "name": "Gyuri Byun"
                    },
                    {
                        "name": "Yunseo Lee"
                    },
                    {
                        "name": "Minji Kang"
                    },
                    {
                        "name": "Gospel Kim"
                    },
                    {
                        "name": "Jihoi Na"
                    },
                    {
                        "name": "Jewoong Moon"
                    },
                    {
                        "name": "Hyeoncheol Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeoncheol Kim"
                },
                "author": "Hyeoncheol Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12428v2",
                "updated": "2025-02-06T12:42:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    42,
                    45,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-21T13:53:16Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    53,
                    16,
                    1,
                    21,
                    0
                ],
                "title": "SplitQuant: Layer Splitting for Low-Bit Neural Network Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SplitQuant: Layer Splitting for Low-Bit Neural Network Quantization"
                },
                "summary": "Quantization for deep neural networks (DNNs) is the process of mapping the\nparameter values of DNNs from original data types to other data types of lower\nprecision to reduce model sizes and make inference faster. Quantization often\nmaps different original values to a single quantized value because the range of\nthe original values is larger than the range of the quantized values. This\nleads to the degradation of the accuracy of the quantized DNNs. Outliers are a\nmain cause of the degradation of quantization resolution because they enlarge\nthe range of original values. To solve the problem, the percentile method is\noften used to clip outliers. However, clipping the outliers has another problem\nof removing the important and strong signals in the DNNs. This paper proposes\nSplitQuant to keep the outliers and improve the quantization resolution at the\nsame time. SplitQuant narrows down the range of the original values and\nmitigates the effect of outliers by splitting each quantizable layer into three\nmathematically equivalent layers and applies different scaling factors.\nEspecially, weights and biases are clustered into lower, middle and upper\nclusters for optimized split. By preprocessing DNNs with SplitQuant,\nquantization algorithms can achieve better results. SplitQuant was applied on\ntwo BERT-Tiny models and improved the accuracy of INT2 quantization by 3.3%p\nand 2.1%p, achieving accuracies comparable to those of the original FP32\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization for deep neural networks (DNNs) is the process of mapping the\nparameter values of DNNs from original data types to other data types of lower\nprecision to reduce model sizes and make inference faster. Quantization often\nmaps different original values to a single quantized value because the range of\nthe original values is larger than the range of the quantized values. This\nleads to the degradation of the accuracy of the quantized DNNs. Outliers are a\nmain cause of the degradation of quantization resolution because they enlarge\nthe range of original values. To solve the problem, the percentile method is\noften used to clip outliers. However, clipping the outliers has another problem\nof removing the important and strong signals in the DNNs. This paper proposes\nSplitQuant to keep the outliers and improve the quantization resolution at the\nsame time. SplitQuant narrows down the range of the original values and\nmitigates the effect of outliers by splitting each quantizable layer into three\nmathematically equivalent layers and applies different scaling factors.\nEspecially, weights and biases are clustered into lower, middle and upper\nclusters for optimized split. By preprocessing DNNs with SplitQuant,\nquantization algorithms can achieve better results. SplitQuant was applied on\ntwo BERT-Tiny models and improved the accuracy of INT2 quantization by 3.3%p\nand 2.1%p, achieving accuracies comparable to those of the original FP32\nmodels."
                },
                "authors": [
                    {
                        "name": "Jaewoo Song"
                    },
                    {
                        "name": "Fangzhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhen Lin"
                },
                "author": "Fangzhen Lin",
                "arxiv_comment": "Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00094v3",
                "updated": "2025-02-06T12:40:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    40,
                    17,
                    3,
                    37,
                    0
                ],
                "published": "2023-10-31T19:16:07Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    19,
                    16,
                    7,
                    1,
                    304,
                    0
                ],
                "title": "A Tractable Inference Perspective of Offline RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tractable Inference Perspective of Offline RL"
                },
                "summary": "A popular paradigm for offline Reinforcement Learning (RL) tasks is to first\nfit the offline trajectories to a sequence model, and then prompt the model for\nactions that lead to high expected return. In addition to obtaining accurate\nsequence models, this paper highlights that tractability, the ability to\nexactly and efficiently answer various probabilistic queries, plays an\nimportant role in offline RL. Specifically, due to the fundamental\nstochasticity from the offline data-collection policies and the environment\ndynamics, highly non-trivial conditional/constrained generation is required to\nelicit rewarding actions. it is still possible to approximate such queries, we\nobserve that such crude estimates significantly undermine the benefits brought\nby expressive sequence models. To overcome this problem, this paper proposes\nTrifle (Tractable Inference for Offline RL), which leverages modern Tractable\nProbabilistic Models (TPMs) to bridge the gap between good sequence models and\nhigh expected returns at evaluation time. Empirically, Trifle achieves the most\nstate-of-the-art scores in 9 Gym-MuJoCo benchmarks against strong baselines.\nFurther, owing to its tractability, Trifle significantly outperforms prior\napproaches in stochastic environments and safe RL tasks (e.g. with action\nconstraints) with minimum algorithmic modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A popular paradigm for offline Reinforcement Learning (RL) tasks is to first\nfit the offline trajectories to a sequence model, and then prompt the model for\nactions that lead to high expected return. In addition to obtaining accurate\nsequence models, this paper highlights that tractability, the ability to\nexactly and efficiently answer various probabilistic queries, plays an\nimportant role in offline RL. Specifically, due to the fundamental\nstochasticity from the offline data-collection policies and the environment\ndynamics, highly non-trivial conditional/constrained generation is required to\nelicit rewarding actions. it is still possible to approximate such queries, we\nobserve that such crude estimates significantly undermine the benefits brought\nby expressive sequence models. To overcome this problem, this paper proposes\nTrifle (Tractable Inference for Offline RL), which leverages modern Tractable\nProbabilistic Models (TPMs) to bridge the gap between good sequence models and\nhigh expected returns at evaluation time. Empirically, Trifle achieves the most\nstate-of-the-art scores in 9 Gym-MuJoCo benchmarks against strong baselines.\nFurther, owing to its tractability, Trifle significantly outperforms prior\napproaches in stochastic environments and safe RL tasks (e.g. with action\nconstraints) with minimum algorithmic modifications."
                },
                "authors": [
                    {
                        "name": "Xuejie Liu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.00094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08001v3",
                "updated": "2025-02-06T12:37:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    37,
                    15,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-10T14:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation"
                },
                "summary": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/"
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "Project page: https://opendrivelab.com/RoboDual/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04027v1",
                "updated": "2025-02-06T12:31:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    31,
                    17,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:31:17Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    31,
                    17,
                    3,
                    37,
                    0
                ],
                "title": "High-Frequency Market Manipulation Detection with a Markov-modulated\n  Hawkes process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Frequency Market Manipulation Detection with a Markov-modulated\n  Hawkes process"
                },
                "summary": "This work focuses on a self-exciting point process defined by a Hawkes-like\nintensity and a switching mechanism based on a hidden Markov chain. Previous\nworks in such a setting assume constant intensities between consecutive events.\nWe extend the model to general Hawkes excitation kernels that are piecewise\nconstant between events. We develop an expectation-maximization algorithm for\nthe statistical inference of the Hawkes intensities parameters as well as the\nstate transition probabilities. The numerical convergence of the estimators is\nextensively tested on simulated data. Using high-frequency cryptocurrency data\non a top centralized exchange, we apply the model to the detection of anomalous\nbursts of trades. We benchmark the goodness-of-fit of the model with the\nMarkov-modulated Poisson process and demonstrate the relevance of the model in\ndetecting suspicious activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on a self-exciting point process defined by a Hawkes-like\nintensity and a switching mechanism based on a hidden Markov chain. Previous\nworks in such a setting assume constant intensities between consecutive events.\nWe extend the model to general Hawkes excitation kernels that are piecewise\nconstant between events. We develop an expectation-maximization algorithm for\nthe statistical inference of the Hawkes intensities parameters as well as the\nstate transition probabilities. The numerical convergence of the estimators is\nextensively tested on simulated data. Using high-frequency cryptocurrency data\non a top centralized exchange, we apply the model to the detection of anomalous\nbursts of trades. We benchmark the goodness-of-fit of the model with the\nMarkov-modulated Poisson process and demonstrate the relevance of the model in\ndetecting suspicious activities."
                },
                "authors": [
                    {
                        "name": "Timothée Fabre"
                    },
                    {
                        "name": "Ioane Muni Toke"
                    }
                ],
                "author_detail": {
                    "name": "Ioane Muni Toke"
                },
                "author": "Ioane Muni Toke",
                "arxiv_comment": "35 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04022v1",
                "updated": "2025-02-06T12:25:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    25,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:25:16Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    25,
                    16,
                    3,
                    37,
                    0
                ],
                "title": "Quantification of Biodiversity from Historical Survey Text with\n  LLM-based Best-Worst Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantification of Biodiversity from Historical Survey Text with\n  LLM-based Best-Worst Scaling"
                },
                "summary": "In this study, we evaluate methods to determine the frequency of species via\nquantity estimation from historical survey text. To that end, we formulate\nclassification tasks and finally show that this problem can be adequately\nframed as a regression task using Best-Worst Scaling (BWS) with Large Language\nModels (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the\nlatter two have reasonable agreement with humans and each other. We conclude\nthat this approach is more cost-effective and similarly robust compared to a\nfine-grained multi-class approach, allowing automated quantity estimation\nacross species.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we evaluate methods to determine the frequency of species via\nquantity estimation from historical survey text. To that end, we formulate\nclassification tasks and finally show that this problem can be adequately\nframed as a regression task using Best-Worst Scaling (BWS) with Large Language\nModels (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the\nlatter two have reasonable agreement with humans and each other. We conclude\nthat this approach is more cost-effective and similarly robust compared to a\nfine-grained multi-class approach, allowing automated quantity estimation\nacross species."
                },
                "authors": [
                    {
                        "name": "Thomas Haider"
                    },
                    {
                        "name": "Tobias Perschl"
                    },
                    {
                        "name": "Malte Rehbein"
                    }
                ],
                "author_detail": {
                    "name": "Malte Rehbein"
                },
                "author": "Malte Rehbein",
                "arxiv_comment": "NoDaLiDa 2025, EcoNLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11551v3",
                "updated": "2025-02-06T12:17:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    17,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-20T15:39:39Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    15,
                    39,
                    39,
                    0,
                    20,
                    0
                ],
                "title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation"
                },
                "summary": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "38 pages, 18 figures, technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04008v1",
                "updated": "2025-02-06T12:10:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    10,
                    1,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:10:01Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    10,
                    1,
                    3,
                    37,
                    0
                ],
                "title": "Automating a Complete Software Test Process Using LLMs: An Automotive\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating a Complete Software Test Process Using LLMs: An Automotive\n  Case Study"
                },
                "summary": "Vehicle API testing verifies whether the interactions between a vehicle's\ninternal systems and external applications meet expectations, ensuring that\nusers can access and control various vehicle functions and data. However, this\ntask is inherently complex, requiring the alignment and coordination of API\nsystems, communication protocols, and even vehicle simulation systems to\ndevelop valid test cases. In practical industrial scenarios, inconsistencies,\nambiguities, and interdependencies across various documents and system\nspecifications pose significant challenges. This paper presents a system\ndesigned for the automated testing of in-vehicle APIs. By clearly defining and\nsegmenting the testing process, we enable Large Language Models (LLMs) to focus\non specific tasks, ensuring a stable and controlled testing workflow.\nExperiments conducted on over 100 APIs demonstrate that our system effectively\nautomates vehicle API testing. The results also confirm that LLMs can\nefficiently handle mundane tasks requiring human judgment, making them suitable\nfor complete automation in similar industrial contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle API testing verifies whether the interactions between a vehicle's\ninternal systems and external applications meet expectations, ensuring that\nusers can access and control various vehicle functions and data. However, this\ntask is inherently complex, requiring the alignment and coordination of API\nsystems, communication protocols, and even vehicle simulation systems to\ndevelop valid test cases. In practical industrial scenarios, inconsistencies,\nambiguities, and interdependencies across various documents and system\nspecifications pose significant challenges. This paper presents a system\ndesigned for the automated testing of in-vehicle APIs. By clearly defining and\nsegmenting the testing process, we enable Large Language Models (LLMs) to focus\non specific tasks, ensuring a stable and controlled testing workflow.\nExperiments conducted on over 100 APIs demonstrate that our system effectively\nautomates vehicle API testing. The results also confirm that LLMs can\nefficiently handle mundane tasks requiring human judgment, making them suitable\nfor complete automation in similar industrial contexts."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Dhasarathy Parthasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Dhasarathy Parthasarathy"
                },
                "author": "Dhasarathy Parthasarathy",
                "arxiv_comment": "Accepted by International Conference on Software Engineering (ICSE)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09243v3",
                "updated": "2025-02-06T12:03:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    3,
                    33,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-12T12:53:30Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "title": "SPRec: Self-Play to Debias LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRec: Self-Play to Debias LLM-based Recommendation"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current work primarily applies supervised fine-tuning\n(SFT) to adapt the model for recommendation tasks. However, SFT on positive\nexamples only limits the model's ability to align with user preference. To\naddress this, researchers recently introduced Direct Preference Optimization\n(DPO), which explicitly aligns LLMs with user preferences using offline\npreference ranking data. However, we found that DPO inherently biases the model\ntowards a few items, exacerbating the filter bubble issue and ultimately\ndegrading user experience.\n  In this paper, we propose SPRec, a novel self-play framework designed to\nmitigate over-recommendation and improve fairness without requiring additional\ndata or manual intervention. In each self-play iteration, the model undergoes\nan SFT step followed by a DPO step, treating offline interaction data as\npositive samples and the predicted outputs from the previous iteration as\nnegative samples. This effectively re-weights the DPO loss function using the\nmodel's logits, adaptively suppressing biased items. Extensive experiments on\nmultiple real-world datasets demonstrate SPRec's effectiveness in enhancing\nrecommendation accuracy and fairness. The implementation is available via\nhttps://github.com/RegionCh/SPRec",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current work primarily applies supervised fine-tuning\n(SFT) to adapt the model for recommendation tasks. However, SFT on positive\nexamples only limits the model's ability to align with user preference. To\naddress this, researchers recently introduced Direct Preference Optimization\n(DPO), which explicitly aligns LLMs with user preferences using offline\npreference ranking data. However, we found that DPO inherently biases the model\ntowards a few items, exacerbating the filter bubble issue and ultimately\ndegrading user experience.\n  In this paper, we propose SPRec, a novel self-play framework designed to\nmitigate over-recommendation and improve fairness without requiring additional\ndata or manual intervention. In each self-play iteration, the model undergoes\nan SFT step followed by a DPO step, treating offline interaction data as\npositive samples and the predicted outputs from the previous iteration as\nnegative samples. This effectively re-weights the DPO loss function using the\nmodel's logits, adaptively suppressing biased items. Extensive experiments on\nmultiple real-world datasets demonstrate SPRec's effectiveness in enhancing\nrecommendation accuracy and fairness. The implementation is available via\nhttps://github.com/RegionCh/SPRec"
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_doi": "10.1145/3696410.3714524",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714524",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03625v3",
                "updated": "2025-02-06T12:00:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    0,
                    19,
                    3,
                    37,
                    0
                ],
                "published": "2024-11-06T02:46:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    46,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "Identification and Inference in General Bunching Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Inference in General Bunching Designs"
                },
                "summary": "This paper develops an econometric framework and tools for the identification\nand inference of a structural parameter in general bunching designs. We present\npoint and partial identification results, which generalize previous approaches\nin the literature. The key assumption for point identification is the\nanalyticity of the counterfactual density, which defines a broader class of\ndistributions than many commonly used parametric families. In the partial\nidentification approach, the analyticity condition is relaxed and various\ninequality restrictions can be incorporated. Both of our identification\napproaches allow for observed covariates in the model, which has previously\nbeen permitted only in limited ways. These covariates allow us to account for\nobservable factors that influence decisions regarding the running variable. We\nprovide a suite of counterfactual estimation and inference methods, termed the\ngeneralized polynomial strategy. Our method restores the merits of the original\npolynomial strategy proposed by Chetty et al. (2011) while addressing several\nweaknesses in the widespread practice. The efficacy of the proposed method is\ndemonstrated compared to the polynomial estimator in a series of Monte Carlo\nstudies within the augmented isoelastic model. We revisit the data used in Saez\n(2010) and find substantially different results relative to those from the\npolynomial strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops an econometric framework and tools for the identification\nand inference of a structural parameter in general bunching designs. We present\npoint and partial identification results, which generalize previous approaches\nin the literature. The key assumption for point identification is the\nanalyticity of the counterfactual density, which defines a broader class of\ndistributions than many commonly used parametric families. In the partial\nidentification approach, the analyticity condition is relaxed and various\ninequality restrictions can be incorporated. Both of our identification\napproaches allow for observed covariates in the model, which has previously\nbeen permitted only in limited ways. These covariates allow us to account for\nobservable factors that influence decisions regarding the running variable. We\nprovide a suite of counterfactual estimation and inference methods, termed the\ngeneralized polynomial strategy. Our method restores the merits of the original\npolynomial strategy proposed by Chetty et al. (2011) while addressing several\nweaknesses in the widespread practice. The efficacy of the proposed method is\ndemonstrated compared to the polynomial estimator in a series of Monte Carlo\nstudies within the augmented isoelastic model. We revisit the data used in Saez\n(2010) and find substantially different results relative to those from the\npolynomial strategy."
                },
                "authors": [
                    {
                        "name": "Myunghyun Song"
                    }
                ],
                "author_detail": {
                    "name": "Myunghyun Song"
                },
                "author": "Myunghyun Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03997v1",
                "updated": "2025-02-06T11:57:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    57,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:57:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    57,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing"
                },
                "summary": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively."
                },
                "authors": [
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08144v2",
                "updated": "2025-02-06T11:56:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    56,
                    23,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-15T13:28:18Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    28,
                    18,
                    3,
                    228,
                    0
                ],
                "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU"
                },
                "summary": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "So-Eon Kim"
                    },
                    {
                        "name": "Seong-Bae Park"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    }
                ],
                "author_detail": {
                    "name": "Soyeon Caren Han"
                },
                "author": "Soyeon Caren Han",
                "arxiv_comment": "Accepted by NAACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01866v2",
                "updated": "2025-02-06T11:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    54,
                    35,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-02T11:54:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    11,
                    54,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "House of Cards: Massive Weights in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "House of Cards: Massive Weights in LLMs"
                },
                "summary": "Massive activations, which manifest in specific feature dimensions of hidden\nstates, introduce a significant bias in large language models (LLMs), leading\nto an overemphasis on the corresponding token. In this paper, we identify that\nmassive activations originate not from the hidden state but from the\nintermediate state of a feed-forward network module in an early layer.\nExpanding on the previous observation that massive activations occur only in\nspecific feature dimensions, we dive deep into the weights that cause massive\nactivations. Specifically, we define top-$k$ massive weights as the weights\nthat contribute to the dimensions with the top-$k$ magnitudes in the\nintermediate state. When these massive weights are set to zero, the\nfunctionality of LLMs is entirely disrupted. However, when all weights except\nfor massive weights are set to zero, it results in a relatively minor\nperformance drop, even though a much larger number of weights are set to zero.\nThis implies that during the pre-training process, learning is dominantly\nfocused on massive weights. Building on this observation, we propose a simple\nplug-and-play method called MacDrop (massive weights curriculum dropout), to\nrely less on massive weights during parameter-efficient fine-tuning. This\nmethod applies dropout to the pre-trained massive weights, starting with a high\ndropout probability and gradually decreasing it as fine-tuning progresses.\nThrough various experiments, including zero-shot downstream tasks, long-context\ntasks, and ablation studies, we demonstrate that \\texttt{MacDrop} generally\nimproves performance and strengthens robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive activations, which manifest in specific feature dimensions of hidden\nstates, introduce a significant bias in large language models (LLMs), leading\nto an overemphasis on the corresponding token. In this paper, we identify that\nmassive activations originate not from the hidden state but from the\nintermediate state of a feed-forward network module in an early layer.\nExpanding on the previous observation that massive activations occur only in\nspecific feature dimensions, we dive deep into the weights that cause massive\nactivations. Specifically, we define top-$k$ massive weights as the weights\nthat contribute to the dimensions with the top-$k$ magnitudes in the\nintermediate state. When these massive weights are set to zero, the\nfunctionality of LLMs is entirely disrupted. However, when all weights except\nfor massive weights are set to zero, it results in a relatively minor\nperformance drop, even though a much larger number of weights are set to zero.\nThis implies that during the pre-training process, learning is dominantly\nfocused on massive weights. Building on this observation, we propose a simple\nplug-and-play method called MacDrop (massive weights curriculum dropout), to\nrely less on massive weights during parameter-efficient fine-tuning. This\nmethod applies dropout to the pre-trained massive weights, starting with a high\ndropout probability and gradually decreasing it as fine-tuning progresses.\nThrough various experiments, including zero-shot downstream tasks, long-context\ntasks, and ablation studies, we demonstrate that \\texttt{MacDrop} generally\nimproves performance and strengthens robustness."
                },
                "authors": [
                    {
                        "name": "Jaehoon Oh"
                    },
                    {
                        "name": "Seungjun Shin"
                    },
                    {
                        "name": "Dokwan Oh"
                    }
                ],
                "author_detail": {
                    "name": "Dokwan Oh"
                },
                "author": "Dokwan Oh",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03992v1",
                "updated": "2025-02-06T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    47,
                    58,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:47:58Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    47,
                    58,
                    3,
                    37,
                    0
                ],
                "title": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge\n  Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge\n  Graph Question Answering"
                },
                "summary": "Most existing Knowledge Graph Question Answering (KGQA) approaches are\ndesigned for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the\nheterogeneity of the underlying graph schema, topology and assertions, most\nKGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without\nresource-intensive training data. We present OntoSCPrompt, a novel Large\nLanguage Model (LLM)-based KGQA approach with a two-stage architecture that\nseparates semantic parsing from KG-dependent interactions. OntoSCPrompt first\ngenerates a SPARQL query structure (including SPARQL keywords such as SELECT,\nASK, WHERE and placeholders for missing tokens) and then fills them with\nKG-specific information. To enhance the understanding of the underlying KG, we\npresent an ontology-guided, hybrid prompt learning strategy that integrates KG\nontology into the learning process of hybrid prompts (e.g., discrete and\ncontinuous vectors). We also present several task-specific decoding strategies\nto ensure the correctness and executability of generated SPARQL queries in both\nstages. Experimental results demonstrate that OntoSCPrompt performs as well as\nSOTA approaches without retraining on a number of KGQA datasets such as CWQ,\nWebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well\nto unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:\n\\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing Knowledge Graph Question Answering (KGQA) approaches are\ndesigned for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the\nheterogeneity of the underlying graph schema, topology and assertions, most\nKGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without\nresource-intensive training data. We present OntoSCPrompt, a novel Large\nLanguage Model (LLM)-based KGQA approach with a two-stage architecture that\nseparates semantic parsing from KG-dependent interactions. OntoSCPrompt first\ngenerates a SPARQL query structure (including SPARQL keywords such as SELECT,\nASK, WHERE and placeholders for missing tokens) and then fills them with\nKG-specific information. To enhance the understanding of the underlying KG, we\npresent an ontology-guided, hybrid prompt learning strategy that integrates KG\nontology into the learning process of hybrid prompts (e.g., discrete and\ncontinuous vectors). We also present several task-specific decoding strategies\nto ensure the correctness and executability of generated SPARQL queries in both\nstages. Experimental results demonstrate that OntoSCPrompt performs as well as\nSOTA approaches without retraining on a number of KGQA datasets such as CWQ,\nWebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well\nto unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:\n\\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}"
                },
                "authors": [
                    {
                        "name": "Longquan Jiang"
                    },
                    {
                        "name": "Junbo Huang"
                    },
                    {
                        "name": "Cedric Möller"
                    },
                    {
                        "name": "Ricardo Usbeck"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Usbeck"
                },
                "author": "Ricardo Usbeck",
                "arxiv_comment": "Accepted By ICSC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12837v2",
                "updated": "2025-02-06T11:47:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    47,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-17T12:02:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    2,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Scrutinizing the Vulnerability of Decentralized Learning to Membership\n  Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scrutinizing the Vulnerability of Decentralized Learning to Membership\n  Inference Attacks"
                },
                "summary": "The primary promise of decentralized learning is to allow users to engage in\nthe training of machine learning models in a collaborative manner while keeping\ntheir data on their premises and without relying on any central entity.\nHowever, this paradigm necessitates the exchange of model parameters or\ngradients between peers. Such exchanges can be exploited to infer sensitive\ninformation about training data, which is achieved through privacy attacks (e.g\nMembership Inference Attacks -- MIA). In order to devise effective defense\nmechanisms, it is important to understand the factors that increase/reduce the\nvulnerability of a given decentralized learning architecture to MIA. In this\nstudy, we extensively explore the vulnerability to MIA of various decentralized\nlearning architectures by varying the graph structure (e.g number of\nneighbors), the graph dynamics, and the aggregation strategy, across diverse\ndatasets and data distributions. Our key finding, which to the best of our\nknowledge we are the first to report, is that the vulnerability to MIA is\nheavily correlated to (i) the local model mixing strategy performed by each\nnode upon reception of models from neighboring nodes and (ii) the global mixing\nproperties of the communication graph. We illustrate these results\nexperimentally using four datasets and by theoretically analyzing the mixing\nproperties of various decentralized architectures. Our paper draws a set of\nlessons learned for devising decentralized learning systems that reduce by\ndesign the vulnerability to MIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary promise of decentralized learning is to allow users to engage in\nthe training of machine learning models in a collaborative manner while keeping\ntheir data on their premises and without relying on any central entity.\nHowever, this paradigm necessitates the exchange of model parameters or\ngradients between peers. Such exchanges can be exploited to infer sensitive\ninformation about training data, which is achieved through privacy attacks (e.g\nMembership Inference Attacks -- MIA). In order to devise effective defense\nmechanisms, it is important to understand the factors that increase/reduce the\nvulnerability of a given decentralized learning architecture to MIA. In this\nstudy, we extensively explore the vulnerability to MIA of various decentralized\nlearning architectures by varying the graph structure (e.g number of\nneighbors), the graph dynamics, and the aggregation strategy, across diverse\ndatasets and data distributions. Our key finding, which to the best of our\nknowledge we are the first to report, is that the vulnerability to MIA is\nheavily correlated to (i) the local model mixing strategy performed by each\nnode upon reception of models from neighboring nodes and (ii) the global mixing\nproperties of the communication graph. We illustrate these results\nexperimentally using four datasets and by theoretically analyzing the mixing\nproperties of various decentralized architectures. Our paper draws a set of\nlessons learned for devising decentralized learning systems that reduce by\ndesign the vulnerability to MIA."
                },
                "authors": [
                    {
                        "name": "Ousmane Touat"
                    },
                    {
                        "name": "Jezekael Brunon"
                    },
                    {
                        "name": "Yacine Belal"
                    },
                    {
                        "name": "Julien Nicolas"
                    },
                    {
                        "name": "Mohamed Maouche"
                    },
                    {
                        "name": "César Sabater"
                    },
                    {
                        "name": "Sonia Ben Mokhtar"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Ben Mokhtar"
                },
                "author": "Sonia Ben Mokhtar",
                "arxiv_comment": "Adding acknowledgments 12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03984v1",
                "updated": "2025-02-06T11:34:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    34,
                    41,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:34:41Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    34,
                    41,
                    3,
                    37,
                    0
                ],
                "title": "PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation"
                },
                "summary": "Large pretrained language models such as BERT suffer from slow inference and\nhigh memory usage, due to their huge size. Recent approaches to compressing\nBERT rely on iterative pruning and knowledge distillation, which, however, are\noften too complicated and computationally intensive. This paper proposes a\nnovel semi-structured one-shot pruning method for BERT, called\n$\\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high\ncompression efficiency and sparsity while preserving accuracy. To this end, PGB\nidentifies important groups of individual weights by permutation and prunes all\nother weights as a structure in both multi-head attention and feed-forward\nlayers. Furthermore, if no important group is formed in a particular layer, PGB\ndrops the entire layer to produce an even more compact model. Our experimental\nresults on BERT$_{\\text{BASE}}$ demonstrate that PGB outperforms the\nstate-of-the-art structured pruning methods in terms of computational cost and\naccuracy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pretrained language models such as BERT suffer from slow inference and\nhigh memory usage, due to their huge size. Recent approaches to compressing\nBERT rely on iterative pruning and knowledge distillation, which, however, are\noften too complicated and computationally intensive. This paper proposes a\nnovel semi-structured one-shot pruning method for BERT, called\n$\\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high\ncompression efficiency and sparsity while preserving accuracy. To this end, PGB\nidentifies important groups of individual weights by permutation and prunes all\nother weights as a structure in both multi-head attention and feed-forward\nlayers. Furthermore, if no important group is formed in a particular layer, PGB\ndrops the entire layer to produce an even more compact model. Our experimental\nresults on BERT$_{\\text{BASE}}$ demonstrate that PGB outperforms the\nstate-of-the-art structured pruning methods in terms of computational cost and\naccuracy preservation."
                },
                "authors": [
                    {
                        "name": "Hyemin Lim"
                    },
                    {
                        "name": "Jaeyeon Lee"
                    },
                    {
                        "name": "Dong-Wan Choi"
                    }
                ],
                "author_detail": {
                    "name": "Dong-Wan Choi"
                },
                "author": "Dong-Wan Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03983v1",
                "updated": "2025-02-06T11:31:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "Time delay interferometry with minimal null frequencies and shortened\n  time span",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time delay interferometry with minimal null frequencies and shortened\n  time span"
                },
                "summary": "In Paper I, we introduced an alternative second-generation time-delay\ninterferometry (TDI) configuration, hybrid Relay, designed to minimize null\nfrequencies and enhance data analysis for massive binary black hole (MBBH). In\nPaper II, we further improved its performance in noise characterization by\nreplacing its null stream with a specialized stable channel, $C^{12}_3$. In\nthis work, we propose a novel TDI configuration, labeled PD4L, which features\nminimal null frequencies and a reduced time span. Unlike the hybrid Relay or\nthe second-generation Michelson, which require a maximum delay of $7L$ (where\n$L$ is the ranging time of interferometric arm), the PD4L synthesizes data only\nwithin $3L$ delay. This shorter time span brings several advantages: 1)\nreducing margins at boundaries of data segments, 2) mitigating frequency\naliasing in the high frequency band, and 3) shortening the tail at the end of a\nsignal. To assess its effectiveness in data analysis, we perform parameter\ninference for a rapidly chirping gravitational wave signal from a MBBH. As a\nmore compact TDI structure, PD4L achieves more accurate parameters estimation\nin the frequency-domain compared to the hybrid Relay. Additionally, PD4L's null\nstream exhibits minimal null frequencies, identical to its science channels,\nwhile maintaining a more stable noise spectrum than the $C^{12}_3$. We further\nevaluate its capability in noise characterization. The results demonstrate that\nalthough the stability of noise spectra in science channels is slightly lower\ncompared to that of hybrid Relay, PD4L can still reliably infer noise\nparameters for data durations of up to four months. These investigations and\ncomparisons suggest that PD4L is a promising TDI scheme, particularly for the\nhigher frequency band.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Paper I, we introduced an alternative second-generation time-delay\ninterferometry (TDI) configuration, hybrid Relay, designed to minimize null\nfrequencies and enhance data analysis for massive binary black hole (MBBH). In\nPaper II, we further improved its performance in noise characterization by\nreplacing its null stream with a specialized stable channel, $C^{12}_3$. In\nthis work, we propose a novel TDI configuration, labeled PD4L, which features\nminimal null frequencies and a reduced time span. Unlike the hybrid Relay or\nthe second-generation Michelson, which require a maximum delay of $7L$ (where\n$L$ is the ranging time of interferometric arm), the PD4L synthesizes data only\nwithin $3L$ delay. This shorter time span brings several advantages: 1)\nreducing margins at boundaries of data segments, 2) mitigating frequency\naliasing in the high frequency band, and 3) shortening the tail at the end of a\nsignal. To assess its effectiveness in data analysis, we perform parameter\ninference for a rapidly chirping gravitational wave signal from a MBBH. As a\nmore compact TDI structure, PD4L achieves more accurate parameters estimation\nin the frequency-domain compared to the hybrid Relay. Additionally, PD4L's null\nstream exhibits minimal null frequencies, identical to its science channels,\nwhile maintaining a more stable noise spectrum than the $C^{12}_3$. We further\nevaluate its capability in noise characterization. The results demonstrate that\nalthough the stability of noise spectra in science channels is slightly lower\ncompared to that of hybrid Relay, PD4L can still reliably infer noise\nparameters for data durations of up to four months. These investigations and\ncomparisons suggest that PD4L is a promising TDI scheme, particularly for the\nhigher frequency band."
                },
                "authors": [
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "14 pages, 9 figures, A follow-up to the works arXiv:2403.01490 and\n  arXiv:2406.11305",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01269v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01269v4",
                "updated": "2025-02-06T11:12:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    12,
                    9,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-02T08:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search"
                },
                "summary": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines."
                },
                "authors": [
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Baijun Ji"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Zhongyi Liu"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01269v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01269v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03964v1",
                "updated": "2025-02-06T10:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    57,
                    5,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    57,
                    5,
                    3,
                    37,
                    0
                ],
                "title": "\"It Warned Me Just at the Right Moment\": Exploring LLM-based Real-time\n  Detection of Phone Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It Warned Me Just at the Right Moment\": Exploring LLM-based Real-time\n  Detection of Phone Scams"
                },
                "summary": "Despite living in the era of the internet, phone-based scams remain one of\nthe most prevalent forms of scams. These scams aim to exploit victims for\nfinancial gain, causing both monetary losses and psychological distress. While\ngovernments, industries, and academia have actively introduced various\ncountermeasures, scammers also continue to evolve their tactics, making phone\nscams a persistent threat. To combat these increasingly sophisticated scams,\ndetection technologies must also advance. In this work, we propose a framework\nfor modeling scam calls and introduce an LLM-based real-time detection\napproach, which assesses fraudulent intent in conversations, further providing\nimmediate warnings to users to mitigate harm. Through experiments, we evaluate\nthe method's performance and analyze key factors influencing its effectiveness.\nThis analysis enables us to refine the method to improve precision while\nexploring the trade-off between recall and timeliness, paving the way for\nfuture directions in this critical area of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite living in the era of the internet, phone-based scams remain one of\nthe most prevalent forms of scams. These scams aim to exploit victims for\nfinancial gain, causing both monetary losses and psychological distress. While\ngovernments, industries, and academia have actively introduced various\ncountermeasures, scammers also continue to evolve their tactics, making phone\nscams a persistent threat. To combat these increasingly sophisticated scams,\ndetection technologies must also advance. In this work, we propose a framework\nfor modeling scam calls and introduce an LLM-based real-time detection\napproach, which assesses fraudulent intent in conversations, further providing\nimmediate warnings to users to mitigate harm. Through experiments, we evaluate\nthe method's performance and analyze key factors influencing its effectiveness.\nThis analysis enables us to refine the method to improve precision while\nexploring the trade-off between recall and timeliness, paving the way for\nfuture directions in this critical area of research."
                },
                "authors": [
                    {
                        "name": "Zitong Shen"
                    },
                    {
                        "name": "Sineng Yan"
                    },
                    {
                        "name": "Youqian Zhang"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Grace Ngai"
                    },
                    {
                        "name": "Eugene Yujun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Yujun Fu"
                },
                "author": "Eugene Yujun Fu",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16788v2",
                "updated": "2025-02-06T10:48:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    48,
                    23,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-28T08:35:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    8,
                    35,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Statistical biases in parameterized searches for gravitational-wave\n  polarizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical biases in parameterized searches for gravitational-wave\n  polarizations"
                },
                "summary": "In tests of gravity using gravitational waves (GWs), GW events analyzed are\noften selected based on specific criteria, particularly the signal-to-noise\nratio (SNR). However, such event selection can introduce bias into parameter\nestimation unless the selection effect is appropriately taken into account in\nthe analysis. In this paper, we investigate how event selection with certain\nprior information affects parameter inference within the scalar-tensor\npolarization framework, focusing on the measurement of the scalar mode\namplitude parameters. We find that for the Tensor+Scalar(dipole) model, the\namplitude of the scalar dipole radiation is overestimated when its true value\nis nonzero while there is no false deviation in the absence of the scalar mode.\nThe same bias is expected to occur also for the Tensor+Scalar(quadrupole)\nmodel. However, error typically exceeds the bias as the scalar quadrupole mode\nis difficult to be distinguished from the tensor mode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In tests of gravity using gravitational waves (GWs), GW events analyzed are\noften selected based on specific criteria, particularly the signal-to-noise\nratio (SNR). However, such event selection can introduce bias into parameter\nestimation unless the selection effect is appropriately taken into account in\nthe analysis. In this paper, we investigate how event selection with certain\nprior information affects parameter inference within the scalar-tensor\npolarization framework, focusing on the measurement of the scalar mode\namplitude parameters. We find that for the Tensor+Scalar(dipole) model, the\namplitude of the scalar dipole radiation is overestimated when its true value\nis nonzero while there is no false deviation in the absence of the scalar mode.\nThe same bias is expected to occur also for the Tensor+Scalar(quadrupole)\nmodel. However, error typically exceeds the bias as the scalar quadrupole mode\nis difficult to be distinguished from the tensor mode."
                },
                "authors": [
                    {
                        "name": "Hayato Imafuku"
                    },
                    {
                        "name": "Hiroki Takeda"
                    },
                    {
                        "name": "Atsushi Nishizawa"
                    },
                    {
                        "name": "Daiki Watarai"
                    },
                    {
                        "name": "Kipp Cannon"
                    }
                ],
                "author_detail": {
                    "name": "Kipp Cannon"
                },
                "author": "Kipp Cannon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03957v1",
                "updated": "2025-02-06T10:47:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    47,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:47:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    47,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "Improving the Perturbation-Based Explanation of Deepfake Detectors\n  Through the Use of Adversarially-Generated Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Perturbation-Based Explanation of Deepfake Detectors\n  Through the Use of Adversarially-Generated Samples"
                },
                "summary": "In this paper, we introduce the idea of using adversarially-generated samples\nof the input images that were classified as deepfakes by a detector, to form\nperturbation masks for inferring the importance of different input features and\nproduce visual explanations. We generate these samples based on Natural\nEvolution Strategies, aiming to flip the original deepfake detector's decision\nand classify these samples as real. We apply this idea to four\nperturbation-based explanation methods (LIME, SHAP, SOBOL and RISE) and\nevaluate the performance of the resulting modified methods using a SOTA\ndeepfake detection model, a benchmarking dataset (FaceForensics++) and a\ncorresponding explanation evaluation framework. Our quantitative assessments\ndocument the mostly positive contribution of the proposed perturbation approach\nin the performance of explanation methods. Our qualitative analysis shows the\ncapacity of the modified explanation methods to demarcate the manipulated image\nregions more accurately, and thus to provide more useful explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the idea of using adversarially-generated samples\nof the input images that were classified as deepfakes by a detector, to form\nperturbation masks for inferring the importance of different input features and\nproduce visual explanations. We generate these samples based on Natural\nEvolution Strategies, aiming to flip the original deepfake detector's decision\nand classify these samples as real. We apply this idea to four\nperturbation-based explanation methods (LIME, SHAP, SOBOL and RISE) and\nevaluate the performance of the resulting modified methods using a SOTA\ndeepfake detection model, a benchmarking dataset (FaceForensics++) and a\ncorresponding explanation evaluation framework. Our quantitative assessments\ndocument the mostly positive contribution of the proposed perturbation approach\nin the performance of explanation methods. Our qualitative analysis shows the\ncapacity of the modified explanation methods to demarcate the manipulated image\nregions more accurately, and thus to provide more useful explanations."
                },
                "authors": [
                    {
                        "name": "Konstantinos Tsigos"
                    },
                    {
                        "name": "Evlampios Apostolidis"
                    },
                    {
                        "name": "Vasileios Mezaris"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Mezaris"
                },
                "author": "Vasileios Mezaris",
                "arxiv_comment": "Accepted for publication, AI4MFDD Workshop @ IEEE/CVF Winter\n  Conference on Applications of Computer Vision (WACV 2025), Tucson, AZ, USA,\n  Feb. 2025. This is the authors' \"accepted version\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03954v1",
                "updated": "2025-02-06T10:46:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    46,
                    19,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:46:19Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    46,
                    19,
                    3,
                    37,
                    0
                ],
                "title": "MAQInstruct: Instruction-based Unified Event Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQInstruct: Instruction-based Unified Event Relation Extraction"
                },
                "summary": "Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "Accepted by WWW 2025 short",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01584v2",
                "updated": "2025-02-06T10:45:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    45,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T18:10:38Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    10,
                    38,
                    0,
                    34,
                    0
                ],
                "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language\n  Models"
                },
                "summary": "Existing benchmarks for frontier models often test specialized, ``PhD-level''\nknowledge that is difficult for non-experts to grasp. In contrast, we present a\nbenchmark based on the NPR Sunday Puzzle Challenge that requires only general\nknowledge. Our benchmark is challenging for both humans and models, however\ncorrect solutions are easy to verify, and models' mistakes are easy to spot.\n  Our work reveals capability gaps that are not evident in existing benchmarks:\nOpenAI o1 significantly outperforms other reasoning models that are on par on\nbenchmarks that test specialized knowledge. Furthermore, our analysis of\nreasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance,\noften concedes with ``I give up'' before providing an answer that it knows is\nwrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases,\nit does not ``finish thinking,'' which suggests the need for an inference-time\ntechnique to ``wrap up'' before the context window limit is reached. We also\nquantify the effectiveness of reasoning longer with R1 and Gemini Thinking to\nidentify the point beyond which more reasoning is unlikely to improve accuracy\non our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for frontier models often test specialized, ``PhD-level''\nknowledge that is difficult for non-experts to grasp. In contrast, we present a\nbenchmark based on the NPR Sunday Puzzle Challenge that requires only general\nknowledge. Our benchmark is challenging for both humans and models, however\ncorrect solutions are easy to verify, and models' mistakes are easy to spot.\n  Our work reveals capability gaps that are not evident in existing benchmarks:\nOpenAI o1 significantly outperforms other reasoning models that are on par on\nbenchmarks that test specialized knowledge. Furthermore, our analysis of\nreasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance,\noften concedes with ``I give up'' before providing an answer that it knows is\nwrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases,\nit does not ``finish thinking,'' which suggests the need for an inference-time\ntechnique to ``wrap up'' before the context window limit is reached. We also\nquantify the effectiveness of reasoning longer with R1 and Gemini Thinking to\nidentify the point beyond which more reasoning is unlikely to improve accuracy\non our benchmark."
                },
                "authors": [
                    {
                        "name": "Carolyn Jane Anderson"
                    },
                    {
                        "name": "Joydeep Biswas"
                    },
                    {
                        "name": "Aleksander Boruch-Gruszecki"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Molly Q Feldman"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Zixuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zixuan Wu"
                },
                "author": "Zixuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03952v1",
                "updated": "2025-02-06T10:43:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    43,
                    55,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:43:55Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    43,
                    55,
                    3,
                    37,
                    0
                ],
                "title": "Bridging the inference gap in Mutimodal Variational Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the inference gap in Mutimodal Variational Autoencoders"
                },
                "summary": "From medical diagnosis to autonomous vehicles, critical applications rely on\nthe integration of multiple heterogeneous data modalities. Multimodal\nVariational Autoencoders offer versatile and scalable methods for generating\nunobserved modalities from observed ones. Recent models using\nmixturesof-experts aggregation suffer from theoretically grounded limitations\nthat restrict their generation quality on complex datasets. In this article, we\npropose a novel interpretable model able to learn both joint and conditional\ndistributions without introducing mixture aggregation. Our model follows a\nmultistage training process: first modeling the joint distribution with\nvariational inference and then modeling the conditional distributions with\nNormalizing Flows to better approximate true posteriors. Importantly, we also\npropose to extract and leverage the information shared between modalities to\nimprove the conditional coherence of generated samples. Our method achieves\nstate-of-the-art results on several benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From medical diagnosis to autonomous vehicles, critical applications rely on\nthe integration of multiple heterogeneous data modalities. Multimodal\nVariational Autoencoders offer versatile and scalable methods for generating\nunobserved modalities from observed ones. Recent models using\nmixturesof-experts aggregation suffer from theoretically grounded limitations\nthat restrict their generation quality on complex datasets. In this article, we\npropose a novel interpretable model able to learn both joint and conditional\ndistributions without introducing mixture aggregation. Our model follows a\nmultistage training process: first modeling the joint distribution with\nvariational inference and then modeling the conditional distributions with\nNormalizing Flows to better approximate true posteriors. Importantly, we also\npropose to extract and leverage the information shared between modalities to\nimprove the conditional coherence of generated samples. Our method achieves\nstate-of-the-art results on several benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Agathe Senellart"
                    },
                    {
                        "name": "Stéphanie Allassonnière"
                    }
                ],
                "author_detail": {
                    "name": "Stéphanie Allassonnière"
                },
                "author": "Stéphanie Allassonnière",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19920v3",
                "updated": "2025-02-06T10:41:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    41,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2024-05-30T10:32:59Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    10,
                    32,
                    59,
                    3,
                    151,
                    0
                ],
                "title": "The ARR2 prior: flexible predictive prior definition for Bayesian\n  auto-regressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ARR2 prior: flexible predictive prior definition for Bayesian\n  auto-regressions"
                },
                "summary": "We present the ARR2 prior, a joint prior over the auto-regressive components\nin Bayesian time-series models and their induced $R^2$. Compared to other\npriors designed for times-series models, the ARR2 prior allows for flexible and\nintuitive shrinkage. We derive the prior for pure auto-regressive models, and\nextend it to auto-regressive models with exogenous inputs, and state-space\nmodels. Through both simulations and real-world modelling exercises, we\ndemonstrate the efficacy of the ARR2 prior in improving sparse and reliable\ninference, while showing greater inference quality and predictive performance\nthan other shrinkage priors. An open-source implementation of the prior is\nprovided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the ARR2 prior, a joint prior over the auto-regressive components\nin Bayesian time-series models and their induced $R^2$. Compared to other\npriors designed for times-series models, the ARR2 prior allows for flexible and\nintuitive shrinkage. We derive the prior for pure auto-regressive models, and\nextend it to auto-regressive models with exogenous inputs, and state-space\nmodels. Through both simulations and real-world modelling exercises, we\ndemonstrate the efficacy of the ARR2 prior in improving sparse and reliable\ninference, while showing greater inference quality and predictive performance\nthan other shrinkage priors. An open-source implementation of the prior is\nprovided."
                },
                "authors": [
                    {
                        "name": "David Kohns"
                    },
                    {
                        "name": "Noa Kallioinen"
                    },
                    {
                        "name": "Yann McLatchie"
                    },
                    {
                        "name": "Aki Vehtari"
                    }
                ],
                "author_detail": {
                    "name": "Aki Vehtari"
                },
                "author": "Aki Vehtari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03949v1",
                "updated": "2025-02-06T10:39:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    39,
                    30,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:39:30Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    39,
                    30,
                    3,
                    37,
                    0
                ],
                "title": "Semantic Feature Division Multiple Access for Digital Semantic Broadcast\n  Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Feature Division Multiple Access for Digital Semantic Broadcast\n  Channels"
                },
                "summary": "In this paper, we propose a digital semantic feature division multiple access\n(SFDMA) paradigm in multi-user broadcast (BC) networks for the inference and\nthe image reconstruction tasks. In this SFDMA scheme, the multi-user semantic\ninformation is encoded into discrete approximately orthogonal representations,\nand the encoded semantic features of multiple users can be simultaneously\ntransmitted in the same time-frequency resource. Specifically, for inference\ntasks, we design a SFDMA digital BC network based on robust information\nbottleneck (RIB), which can achieve a tradeoff between inference performance,\ndata compression and multi-user interference. Moreover, for image\nreconstruction tasks, we develop a SFDMA digital BC network by utilizing a Swin\nTransformer, which significantly reduces multi-user interference. More\nimportantly, SFDMA can protect the privacy of users' semantic information, in\nwhich each receiver can only decode its own semantic information. Furthermore,\nwe establish a relationship between performance and signal to interference plus\nnoise ratio (SINR), which is fitted by an Alpha-Beta-Gamma (ABG) function.\nFurthermore, an optimal power allocation method is developed for the inference\nand reconstruction tasks. Extensive simulations verify the effectiveness and\nsuperiority of our proposed SFDMA scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a digital semantic feature division multiple access\n(SFDMA) paradigm in multi-user broadcast (BC) networks for the inference and\nthe image reconstruction tasks. In this SFDMA scheme, the multi-user semantic\ninformation is encoded into discrete approximately orthogonal representations,\nand the encoded semantic features of multiple users can be simultaneously\ntransmitted in the same time-frequency resource. Specifically, for inference\ntasks, we design a SFDMA digital BC network based on robust information\nbottleneck (RIB), which can achieve a tradeoff between inference performance,\ndata compression and multi-user interference. Moreover, for image\nreconstruction tasks, we develop a SFDMA digital BC network by utilizing a Swin\nTransformer, which significantly reduces multi-user interference. More\nimportantly, SFDMA can protect the privacy of users' semantic information, in\nwhich each receiver can only decode its own semantic information. Furthermore,\nwe establish a relationship between performance and signal to interference plus\nnoise ratio (SINR), which is fitted by an Alpha-Beta-Gamma (ABG) function.\nFurthermore, an optimal power allocation method is developed for the inference\nand reconstruction tasks. Extensive simulations verify the effectiveness and\nsuperiority of our proposed SFDMA scheme."
                },
                "authors": [
                    {
                        "name": "Shuai Ma"
                    },
                    {
                        "name": "Zhiye Sun"
                    },
                    {
                        "name": "Bin Shen"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Guangming Shi"
                    },
                    {
                        "name": "Shiyin Li"
                    },
                    {
                        "name": "Naofal Al-Dhahir"
                    }
                ],
                "author_detail": {
                    "name": "Naofal Al-Dhahir"
                },
                "author": "Naofal Al-Dhahir",
                "arxiv_doi": "10.1109/JIOT.2025.3538764",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JIOT.2025.3538764",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20005v2",
                "updated": "2025-02-06T10:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    37,
                    17,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-28T04:01:30Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    4,
                    1,
                    30,
                    5,
                    363,
                    0
                ],
                "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\n  System"
                },
                "summary": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4."
                },
                "authors": [
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Xiangyuan Ru"
                    },
                    {
                        "name": "Kangwei Liu"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "WWW 2025 Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.04328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04328v1",
                "updated": "2025-02-06T18:59:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    55,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:59:55Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    55,
                    3,
                    37,
                    0
                ],
                "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment"
                },
                "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola."
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Yongming Rao"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Rao"
                },
                "author": "Yongming Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04326v1",
                "updated": "2025-02-06T18:59:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    40,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:59:40Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    40,
                    3,
                    37,
                    0
                ],
                "title": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal\n  LLMs"
                },
                "summary": "In this paper, we introduce WorldSense, the first benchmark to assess the\nmulti-modal video understanding, that simultaneously encompasses visual, audio,\nand text inputs. In contrast to existing benchmarks, our WorldSense has several\nfeatures: (i) collaboration of omni-modality, we design the evaluation tasks to\nfeature a strong coupling of audio and video, requiring models to effectively\nutilize the synergistic perception of omni-modality; (ii) diversity of videos\nand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual\nsynchronised videos, systematically categorized into 8 primary domains and 67\nfine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice\nQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)\nhigh-quality annotations, all the QA pairs are manually labeled by 80 expert\nannotators with multiple rounds of correction to ensure quality. Based on our\nWorldSense, we extensively evaluate various state-of-the-art models. The\nexperimental results indicate that existing models face significant challenges\nin understanding real-world scenarios (48.0% best accuracy). We hope our\nWorldSense can provide a platform for evaluating the ability in constructing\nand understanding coherent contexts from omni-modality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce WorldSense, the first benchmark to assess the\nmulti-modal video understanding, that simultaneously encompasses visual, audio,\nand text inputs. In contrast to existing benchmarks, our WorldSense has several\nfeatures: (i) collaboration of omni-modality, we design the evaluation tasks to\nfeature a strong coupling of audio and video, requiring models to effectively\nutilize the synergistic perception of omni-modality; (ii) diversity of videos\nand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual\nsynchronised videos, systematically categorized into 8 primary domains and 67\nfine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice\nQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)\nhigh-quality annotations, all the QA pairs are manually labeled by 80 expert\nannotators with multiple rounds of correction to ensure quality. Based on our\nWorldSense, we extensively evaluate various state-of-the-art models. The\nexperimental results indicate that existing models face significant challenges\nin understanding real-world scenarios (48.0% best accuracy). We hope our\nWorldSense can provide a platform for evaluating the ability in constructing\nand understanding coherent contexts from omni-modality."
                },
                "authors": [
                    {
                        "name": "Jack Hong"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Jiayin Cai"
                    },
                    {
                        "name": "Xiaolong Jiang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04322v1",
                "updated": "2025-02-06T18:59:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:59:02Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    2,
                    3,
                    37,
                    0
                ],
                "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions"
                },
                "summary": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions."
                },
                "authors": [
                    {
                        "name": "Yik Siu Chan"
                    },
                    {
                        "name": "Narutatsu Ri"
                    },
                    {
                        "name": "Yuxin Xiao"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    }
                ],
                "author_detail": {
                    "name": "Marzyeh Ghassemi"
                },
                "author": "Marzyeh Ghassemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04315v1",
                "updated": "2025-02-06T18:57:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    57,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    57,
                    6,
                    3,
                    37,
                    0
                ],
                "title": "ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters"
                },
                "summary": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChamaleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChamaleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChamaleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChamaleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/"
                },
                "authors": [
                    {
                        "name": "Kamer Ali Yuksel"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04313v1",
                "updated": "2025-02-06T18:56:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    56,
                    1,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:56:01Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    56,
                    1,
                    3,
                    37,
                    0
                ],
                "title": "Great Models Think Alike and this Undermines AI Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great Models Think Alike and this Undermines AI Oversight"
                },
                "summary": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight."
                },
                "authors": [
                    {
                        "name": "Shashwat Goel"
                    },
                    {
                        "name": "Joschka Struber"
                    },
                    {
                        "name": "Ilze Amanda Auzina"
                    },
                    {
                        "name": "Karuna K Chandra"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "arxiv_comment": "60 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04306v1",
                "updated": "2025-02-06T18:47:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    47,
                    49,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:47:49Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    47,
                    49,
                    3,
                    37,
                    0
                ],
                "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference\n  Optimization"
                },
                "summary": "Recent research has leveraged large language model multi-agent systems for\ncomplex problem-solving while trying to reduce the manual effort required to\nbuild them, driving the development of automated agent workflow optimization\nmethods. However, existing methods remain inflexible due to representational\nlimitations, a lack of adaptability, and poor scalability when relying on\ndiscrete optimization techniques. We address these challenges with ScoreFlow, a\nsimple yet high-performance framework that leverages efficient gradient-based\noptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel\nvariant of the direct preference optimization method that accounts for\nquantitative feedback. Across six benchmarks spanning question answering,\ncoding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over\nexisting baselines. Moreover, it empowers smaller models to outperform larger\nones with lower inference costs. Project:\nhttps://github.com/Gen-Verse/ScoreFlow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has leveraged large language model multi-agent systems for\ncomplex problem-solving while trying to reduce the manual effort required to\nbuild them, driving the development of automated agent workflow optimization\nmethods. However, existing methods remain inflexible due to representational\nlimitations, a lack of adaptability, and poor scalability when relying on\ndiscrete optimization techniques. We address these challenges with ScoreFlow, a\nsimple yet high-performance framework that leverages efficient gradient-based\noptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel\nvariant of the direct preference optimization method that accounts for\nquantitative feedback. Across six benchmarks spanning question answering,\ncoding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over\nexisting baselines. Moreover, it empowers smaller models to outperform larger\nones with lower inference costs. Project:\nhttps://github.com/Gen-Verse/ScoreFlow"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Bryon Aragam"
                    }
                ],
                "author_detail": {
                    "name": "Bryon Aragam"
                },
                "author": "Bryon Aragam",
                "arxiv_comment": "Project: https://github.com/Gen-Verse/ScoreFlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04295v1",
                "updated": "2025-02-06T18:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T18:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code will be available at\nhttps://github.com/HenryLau7/CFPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code will be available at\nhttps://github.com/HenryLau7/CFPO."
                },
                "authors": [
                    {
                        "name": "Yuanye Liu"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Cheng Peng"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Peng"
                },
                "author": "Cheng Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14662v3",
                "updated": "2025-02-06T18:12:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    12,
                    57,
                    3,
                    37,
                    0
                ],
                "published": "2024-06-20T18:30:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    30,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "Advantage Alignment Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advantage Alignment Algorithms"
                },
                "summary": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation."
                },
                "authors": [
                    {
                        "name": "Juan Agustin Duque"
                    },
                    {
                        "name": "Milad Aghajohari"
                    },
                    {
                        "name": "Tim Cooijmans"
                    },
                    {
                        "name": "Razvan Ciuca"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Aaron Courville"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Courville"
                },
                "author": "Aaron Courville",
                "arxiv_comment": "25 Pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04251v1",
                "updated": "2025-02-06T17:40:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    40,
                    53,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:40:53Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    40,
                    53,
                    3,
                    37,
                    0
                ],
                "title": "Combining Language and App UI Analysis for the Automated Assessment of\n  Bug Reproduction Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Language and App UI Analysis for the Automated Assessment of\n  Bug Reproduction Steps"
                },
                "summary": "Bug reports are essential for developers to confirm software problems,\ninvestigate their causes, and validate fixes. Unfortunately, reports often miss\nimportant information or are written unclearly, which can cause delays,\nincreased issue resolution effort, or even the inability to solve issues. One\nof the most common components of reports that are problematic is the steps to\nreproduce the bug(s) (S2Rs), which are essential to replicate the described\nprogram failures and reason about fixes. Given the proclivity for deficiencies\nin reported S2Rs, prior work has proposed techniques that assist reporters in\nwriting or assessing the quality of S2Rs. However, automated understanding of\nS2Rs is challenging, and requires linking nuanced natural language phrases with\nspecific, semantically related program information. Prior techniques often\nstruggle to form such language to program connections - due to issues in\nlanguage variability and limitations of information gleaned from program\nanalyses.\n  To more effectively tackle the problem of S2R quality annotation, we propose\na new technique called AstroBR, which leverages the language understanding\ncapabilities of LLMs to identify and extract the S2Rs from bug reports and map\nthem to GUI interactions in a program state model derived via dynamic analysis.\nWe compared AstroBR to a related state-of-the-art approach and we found that\nAstroBR annotates S2Rs 25.2% better (in terms of F1 score) than the baseline.\nAdditionally, AstroBR suggests more accurate missing S2Rs than the baseline (by\n71.4% in terms of F1 score).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug reports are essential for developers to confirm software problems,\ninvestigate their causes, and validate fixes. Unfortunately, reports often miss\nimportant information or are written unclearly, which can cause delays,\nincreased issue resolution effort, or even the inability to solve issues. One\nof the most common components of reports that are problematic is the steps to\nreproduce the bug(s) (S2Rs), which are essential to replicate the described\nprogram failures and reason about fixes. Given the proclivity for deficiencies\nin reported S2Rs, prior work has proposed techniques that assist reporters in\nwriting or assessing the quality of S2Rs. However, automated understanding of\nS2Rs is challenging, and requires linking nuanced natural language phrases with\nspecific, semantically related program information. Prior techniques often\nstruggle to form such language to program connections - due to issues in\nlanguage variability and limitations of information gleaned from program\nanalyses.\n  To more effectively tackle the problem of S2R quality annotation, we propose\na new technique called AstroBR, which leverages the language understanding\ncapabilities of LLMs to identify and extract the S2Rs from bug reports and map\nthem to GUI interactions in a program state model derived via dynamic analysis.\nWe compared AstroBR to a related state-of-the-art approach and we found that\nAstroBR annotates S2Rs 25.2% better (in terms of F1 score) than the baseline.\nAdditionally, AstroBR suggests more accurate missing S2Rs than the baseline (by\n71.4% in terms of F1 score)."
                },
                "authors": [
                    {
                        "name": "Junayed Mahmud"
                    },
                    {
                        "name": "Antu Saha"
                    },
                    {
                        "name": "Oscar Chaparro"
                    },
                    {
                        "name": "Kevin Moran"
                    },
                    {
                        "name": "Andrian Marcus"
                    }
                ],
                "author_detail": {
                    "name": "Andrian Marcus"
                },
                "author": "Andrian Marcus",
                "arxiv_comment": "12 pages, to appear in the Proceedings of the 33rd IEEE/ACM\n  International Conference on Program Comprehension (ICPC'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04248v1",
                "updated": "2025-02-06T17:38:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    38,
                    41,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:38:41Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    38,
                    41,
                    3,
                    37,
                    0
                ],
                "title": "Adapting to Evolving Adversaries with Regularized Continual Robust\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting to Evolving Adversaries with Regularized Continual Robust\n  Training"
                },
                "summary": "Robust training methods typically defend against specific attack types, such\nas Lp attacks with fixed budgets, and rarely account for the fact that\ndefenders may encounter new attacks over time. A natural solution is to adapt\nthe defended model to new adversaries as they arise via fine-tuning, a method\nwhich we call continual robust training (CRT). However, when implemented\nnaively, fine-tuning on new attacks degrades robustness on previous attacks.\nThis raises the question: how can we improve the initial training and\nfine-tuning of the model to simultaneously achieve robustness against previous\nand new attacks? We present theoretical results which show that the gap in a\nmodel's robustness against different attacks is bounded by how far each attack\nperturbs a sample in the model's logit space, suggesting that regularizing with\nrespect to this logit space distance can help maintain robustness against\nprevious attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and\nImageNette) and over 100 attack combinations demonstrate that the proposed\nregularization improves robust accuracy with little overhead in training time.\nOur findings and open-source code lay the groundwork for the deployment of\nmodels robust to evolving attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust training methods typically defend against specific attack types, such\nas Lp attacks with fixed budgets, and rarely account for the fact that\ndefenders may encounter new attacks over time. A natural solution is to adapt\nthe defended model to new adversaries as they arise via fine-tuning, a method\nwhich we call continual robust training (CRT). However, when implemented\nnaively, fine-tuning on new attacks degrades robustness on previous attacks.\nThis raises the question: how can we improve the initial training and\nfine-tuning of the model to simultaneously achieve robustness against previous\nand new attacks? We present theoretical results which show that the gap in a\nmodel's robustness against different attacks is bounded by how far each attack\nperturbs a sample in the model's logit space, suggesting that regularizing with\nrespect to this logit space distance can help maintain robustness against\nprevious attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and\nImageNette) and over 100 attack combinations demonstrate that the proposed\nregularization improves robust accuracy with little overhead in training time.\nOur findings and open-source code lay the groundwork for the deployment of\nmodels robust to evolving attacks."
                },
                "authors": [
                    {
                        "name": "Sihui Dai"
                    },
                    {
                        "name": "Christian Cianfarani"
                    },
                    {
                        "name": "Arjun Bhagoji"
                    },
                    {
                        "name": "Vikash Sehwag"
                    },
                    {
                        "name": "Prateek Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Mittal"
                },
                "author": "Prateek Mittal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06067v3",
                "updated": "2025-02-06T17:35:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    35,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2024-05-09T19:32:49Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    19,
                    32,
                    49,
                    3,
                    130,
                    0
                ],
                "title": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language\n  Processing"
                },
                "summary": "Transformer-based large language models (LLM) have been widely used in\nlanguage processing applications. However, due to the memory constraints of the\ndevices, most of them restrict the context window. Even though recurrent models\nin previous works can memorize past tokens to enable unlimited context and\nmaintain effectiveness, they have ``flat'' memory architectures. Such\narchitectures have limitations in selecting and filtering information. Since\nhumans are good at learning and self-adjustment, we believe that imitating\nbrain memory hierarchy is beneficial for model memorization. Thus, we propose\nthe Hierarchical Memory Transformer (HMT), a novel framework that facilitates a\nmodel's long-context processing ability by imitating human memorization\nbehavior. Leveraging memory-augmented segment-level recurrence, we organize the\nmemory hierarchy by preserving tokens from early input segments, passing memory\nembeddings along the sequence, and recalling relevant information from history.\nEvaluating general language modeling, question-answering tasks, and the\nsummarization task, we show that HMT consistently improves the long-context\nprocessing ability of existing models. Furthermore, HMT achieves a comparable\nor superior generation quality to long-context LLMs with $2 \\sim 57\\times$\nfewer parameters and $2.5 \\sim 116\\times$ less inference memory, significantly\noutperforming previous memory-augmented models. Code on Github:\nhttps://github.com/OswaldHe/HMT-pytorch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLM) have been widely used in\nlanguage processing applications. However, due to the memory constraints of the\ndevices, most of them restrict the context window. Even though recurrent models\nin previous works can memorize past tokens to enable unlimited context and\nmaintain effectiveness, they have ``flat'' memory architectures. Such\narchitectures have limitations in selecting and filtering information. Since\nhumans are good at learning and self-adjustment, we believe that imitating\nbrain memory hierarchy is beneficial for model memorization. Thus, we propose\nthe Hierarchical Memory Transformer (HMT), a novel framework that facilitates a\nmodel's long-context processing ability by imitating human memorization\nbehavior. Leveraging memory-augmented segment-level recurrence, we organize the\nmemory hierarchy by preserving tokens from early input segments, passing memory\nembeddings along the sequence, and recalling relevant information from history.\nEvaluating general language modeling, question-answering tasks, and the\nsummarization task, we show that HMT consistently improves the long-context\nprocessing ability of existing models. Furthermore, HMT achieves a comparable\nor superior generation quality to long-context LLMs with $2 \\sim 57\\times$\nfewer parameters and $2.5 \\sim 116\\times$ less inference memory, significantly\noutperforming previous memory-augmented models. Code on Github:\nhttps://github.com/OswaldHe/HMT-pytorch."
                },
                "authors": [
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Yingqi Cao"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_comment": "NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19457v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19457v3",
                "updated": "2025-02-06T17:27:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    27,
                    3,
                    3,
                    37,
                    0
                ],
                "published": "2024-09-28T21:11:25Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    21,
                    11,
                    25,
                    5,
                    272,
                    0
                ],
                "title": "A Parameter-Efficient Tuning Framework for Language-guided Object\n  Grounding and Robot Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Parameter-Efficient Tuning Framework for Language-guided Object\n  Grounding and Robot Grasping"
                },
                "summary": "The language-guided robot grasping task requires a robot agent to integrate\nmultimodal information from both visual and linguistic inputs to predict\nactions for target-driven grasping. While recent approaches utilizing\nMultimodal Large Language Models (MLLMs) have shown promising results, their\nextensive computation and data demands limit the feasibility of local\ndeployment and customization. To address this, we propose a novel CLIP-based\nmultimodal parameter-efficient tuning (PET) framework designed for three\nlanguage-guided object grounding and grasping tasks: (1) Referring Expression\nSegmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring\nGrasp Affordance (RGA). Our approach introduces two key innovations: a\nbi-directional vision-language adapter that aligns multimodal inputs for\npixel-level language understanding and a depth fusion branch that incorporates\ngeometric cues to facilitate robot grasping predictions. Experiment results\ndemonstrate superior performance in the RES object grounding task compared with\nexisting CLIP-based full-model tuning or PET approaches. In the RGS and RGA\ntasks, our model not only effectively interprets object attributes based on\nsimple language descriptions but also shows strong potential for comprehending\ncomplex spatial reasoning scenarios, such as multiple identical objects present\nin the workspace. Project page: https://z.umn.edu/etog-etrg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The language-guided robot grasping task requires a robot agent to integrate\nmultimodal information from both visual and linguistic inputs to predict\nactions for target-driven grasping. While recent approaches utilizing\nMultimodal Large Language Models (MLLMs) have shown promising results, their\nextensive computation and data demands limit the feasibility of local\ndeployment and customization. To address this, we propose a novel CLIP-based\nmultimodal parameter-efficient tuning (PET) framework designed for three\nlanguage-guided object grounding and grasping tasks: (1) Referring Expression\nSegmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring\nGrasp Affordance (RGA). Our approach introduces two key innovations: a\nbi-directional vision-language adapter that aligns multimodal inputs for\npixel-level language understanding and a depth fusion branch that incorporates\ngeometric cues to facilitate robot grasping predictions. Experiment results\ndemonstrate superior performance in the RES object grounding task compared with\nexisting CLIP-based full-model tuning or PET approaches. In the RGS and RGA\ntasks, our model not only effectively interprets object attributes based on\nsimple language descriptions but also shows strong potential for comprehending\ncomplex spatial reasoning scenarios, such as multiple identical objects present\nin the workspace. Project page: https://z.umn.edu/etog-etrg"
                },
                "authors": [
                    {
                        "name": "Houjian Yu"
                    },
                    {
                        "name": "Mingen Li"
                    },
                    {
                        "name": "Alireza Rezazadeh"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Changhyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Changhyun Choi"
                },
                "author": "Changhyun Choi",
                "arxiv_comment": "Accepted for ICRA 2025. Project page:\n  https://sites.google.com/umn.edu/etog-etrg/home",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19457v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19457v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15510v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15510v3",
                "updated": "2025-02-06T17:16:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    16,
                    28,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-28T03:45:49Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    45,
                    49,
                    2,
                    241,
                    0
                ],
                "title": "How Reliable are Causal Probing Interventions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable are Causal Probing Interventions?"
                },
                "summary": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal probing aims to analyze foundation models by examining how intervening\non their representation of various latent properties impacts their outputs.\nRecent works have cast doubt on the theoretical basis of several leading causal\nprobing methods, but it has been unclear how to systematically evaluate the\neffectiveness of these methods in practice. To address this, we define two key\ncausal probing desiderata: completeness (how thoroughly the representation of\nthe target property has been transformed) and selectivity (how little\nnon-targeted properties have been impacted). We find that there is an inherent\ntradeoff between the two, which we define as reliability, their harmonic mean.\nWe introduce an empirical analysis framework to measure and evaluate these\nquantities, allowing us to make the first direct comparisons between different\nfamilies of leading causal probing methods (e.g., linear vs. nonlinear, or\nconcept removal vs. counterfactual interventions). We find that: (1) no method\nis reliable across all layers; (2) more reliable methods have a greater impact\non LLM behavior; (3) nonlinear interventions are more reliable in early and\nintermediate layers, and linear interventions are more reliable in later\nlayers; and (4) concept removal methods are far less reliable than\ncounterfactual interventions, suggesting that they may not be an effective\napproach to causal probing."
                },
                "authors": [
                    {
                        "name": "Marc Canby"
                    },
                    {
                        "name": "Adam Davies"
                    },
                    {
                        "name": "Chirag Rastogi"
                    },
                    {
                        "name": "Julia Hockenmaier"
                    }
                ],
                "author_detail": {
                    "name": "Julia Hockenmaier"
                },
                "author": "Julia Hockenmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15510v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15510v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04227v1",
                "updated": "2025-02-06T17:12:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:12:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks"
                },
                "summary": "We explore the feasibility and effectiveness of using LLM-driven autonomous\nsystems for Assumed Breach penetration testing in enterprise networks. We\nintroduce a novel prototype that, driven by Large Language Models (LLMs), can\ncompromise accounts within a real-life Active Directory testbed. Our research\nprovides a comprehensive evaluation of the prototype's capabilities, and\nhighlights both strengths and limitations while executing attack. The\nevaluation uses a realistic simulation environment (Game of Active Directory,\nGOAD) to capture intricate interactions, stochastic outcomes, and timing\ndependencies that characterize live network scenarios. The study concludes that\nautonomous LLMs are able to conduct Assumed Breach simulations, potentially\ndemocratizing access to penetration testing for organizations facing budgetary\nconstraints.\n  The prototype's source code, traces, and analyzed logs are released as\nopen-source to enhance collective cybersecurity and facilitate future research\nin LLM-driven cybersecurity automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the feasibility and effectiveness of using LLM-driven autonomous\nsystems for Assumed Breach penetration testing in enterprise networks. We\nintroduce a novel prototype that, driven by Large Language Models (LLMs), can\ncompromise accounts within a real-life Active Directory testbed. Our research\nprovides a comprehensive evaluation of the prototype's capabilities, and\nhighlights both strengths and limitations while executing attack. The\nevaluation uses a realistic simulation environment (Game of Active Directory,\nGOAD) to capture intricate interactions, stochastic outcomes, and timing\ndependencies that characterize live network scenarios. The study concludes that\nautonomous LLMs are able to conduct Assumed Breach simulations, potentially\ndemocratizing access to penetration testing for organizations facing budgetary\nconstraints.\n  The prototype's source code, traces, and analyzed logs are released as\nopen-source to enhance collective cybersecurity and facilitate future research\nin LLM-driven cybersecurity automation."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04226v1",
                "updated": "2025-02-06T17:12:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    7,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:12:07Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    7,
                    3,
                    37,
                    0
                ],
                "title": "Keep It Light! Simplifying Image Clustering Via Text-Free Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep It Light! Simplifying Image Clustering Via Text-Free Adapters"
                },
                "summary": "Many competitive clustering pipelines have a multi-modal design, leveraging\nlarge language models (LLMs) or other text encoders, and text-image pairs,\nwhich are often unavailable in real-world downstream applications.\nAdditionally, such frameworks are generally complicated to train and require\nsubstantial computational resources, making widespread adoption challenging. In\nthis work, we show that in deep clustering, competitive performance with more\ncomplex state-of-the-art methods can be achieved using a text-free and highly\nsimplified training pipeline. In particular, our approach, Simple Clustering\nvia Pre-trained models (SCP), trains only a small cluster head while leveraging\npre-trained vision model feature representations and positive data pairs.\nExperiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100,\nSTL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly\ncompetitive performance. Furthermore, we provide a theoretical result\nexplaining why, at least under ideal conditions, additional text-based\nembeddings may not be necessary to achieve strong clustering performance in\nvision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many competitive clustering pipelines have a multi-modal design, leveraging\nlarge language models (LLMs) or other text encoders, and text-image pairs,\nwhich are often unavailable in real-world downstream applications.\nAdditionally, such frameworks are generally complicated to train and require\nsubstantial computational resources, making widespread adoption challenging. In\nthis work, we show that in deep clustering, competitive performance with more\ncomplex state-of-the-art methods can be achieved using a text-free and highly\nsimplified training pipeline. In particular, our approach, Simple Clustering\nvia Pre-trained models (SCP), trains only a small cluster head while leveraging\npre-trained vision model feature representations and positive data pairs.\nExperiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100,\nSTL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly\ncompetitive performance. Furthermore, we provide a theoretical result\nexplaining why, at least under ideal conditions, additional text-based\nembeddings may not be necessary to achieve strong clustering performance in\nvision."
                },
                "authors": [
                    {
                        "name": "Yicen Li"
                    },
                    {
                        "name": "Haitz Sáez de Ocáriz Borde"
                    },
                    {
                        "name": "Anastasis Kratsios"
                    },
                    {
                        "name": "Paul D. McNicholas"
                    }
                ],
                "author_detail": {
                    "name": "Paul D. McNicholas"
                },
                "author": "Paul D. McNicholas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04223v1",
                "updated": "2025-02-06T17:07:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    7,
                    22,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:07:22Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    7,
                    22,
                    3,
                    37,
                    0
                ],
                "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Éclair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents"
                },
                "summary": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards."
                },
                "authors": [
                    {
                        "name": "Ilia Karmanov"
                    },
                    {
                        "name": "Amala Sanjay Deshmukh"
                    },
                    {
                        "name": "Lukas Voegtle"
                    },
                    {
                        "name": "Philipp Fischer"
                    },
                    {
                        "name": "Kateryna Chumachenko"
                    },
                    {
                        "name": "Timo Roman"
                    },
                    {
                        "name": "Jarno Seppänen"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Karan Sapra"
                    }
                ],
                "author_detail": {
                    "name": "Karan Sapra"
                },
                "author": "Karan Sapra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04218v1",
                "updated": "2025-02-06T17:01:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    1,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T17:01:00Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    1,
                    0,
                    3,
                    37,
                    0
                ],
                "title": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sports and Women's Sports: Gender Bias in Text Generation with Olympic\n  Data"
                },
                "summary": "Large Language Models (LLMs) have been shown to be biased in prior work, as\nthey generate text that is in line with stereotypical views of the world or\nthat is not representative of the viewpoints and values of historically\nmarginalized demographic groups. In this work, we propose using data from\nparallel men's and women's events at the Olympic Games to investigate different\nforms of gender bias in language models. We define three metrics to measure\nbias, and find that models are consistently biased against women when the\ngender is ambiguous in the prompt. In this case, the model frequently retrieves\nonly the results of the men's event with or without acknowledging them as such,\nrevealing pervasive gender bias in LLMs in the context of athletics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to be biased in prior work, as\nthey generate text that is in line with stereotypical views of the world or\nthat is not representative of the viewpoints and values of historically\nmarginalized demographic groups. In this work, we propose using data from\nparallel men's and women's events at the Olympic Games to investigate different\nforms of gender bias in language models. We define three metrics to measure\nbias, and find that models are consistently biased against women when the\ngender is ambiguous in the prompt. In this case, the model frequently retrieves\nonly the results of the men's event with or without acknowledging them as such,\nrevealing pervasive gender bias in LLMs in the context of athletics."
                },
                "authors": [
                    {
                        "name": "Laura Biester"
                    }
                ],
                "author_detail": {
                    "name": "Laura Biester"
                },
                "author": "Laura Biester",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04206v1",
                "updated": "2025-02-06T16:47:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    47,
                    21,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:47:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    47,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "Ensuring Reliability via Hyperparameter Selection: Review and Advances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring Reliability via Hyperparameter Selection: Review and Advances"
                },
                "summary": "Hyperparameter selection is a critical step in the deployment of artificial\nintelligence (AI) models, particularly in the current era of foundational,\npre-trained, models. By framing hyperparameter selection as a multiple\nhypothesis testing problem, recent research has shown that it is possible to\nprovide statistical guarantees on population risk measures attained by the\nselected hyperparameter. This paper reviews the Learn-Then-Test (LTT)\nframework, which formalizes this approach, and explores several extensions\ntailored to engineering-relevant scenarios. These extensions encompass\ndifferent risk measures and statistical guarantees, multi-objective\noptimization, the incorporation of prior knowledge and dependency structures\ninto the hyperparameter selection process, as well as adaptivity. The paper\nalso includes illustrative applications for communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter selection is a critical step in the deployment of artificial\nintelligence (AI) models, particularly in the current era of foundational,\npre-trained, models. By framing hyperparameter selection as a multiple\nhypothesis testing problem, recent research has shown that it is possible to\nprovide statistical guarantees on population risk measures attained by the\nselected hyperparameter. This paper reviews the Learn-Then-Test (LTT)\nframework, which formalizes this approach, and explores several extensions\ntailored to engineering-relevant scenarios. These extensions encompass\ndifferent risk measures and statistical guarantees, multi-objective\noptimization, the incorporation of prior knowledge and dependency structures\ninto the hyperparameter selection process, as well as adaptivity. The paper\nalso includes illustrative applications for communication systems."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Farzaneh"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04205v1",
                "updated": "2025-02-06T16:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    45,
                    12,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:45:12Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    45,
                    12,
                    3,
                    37,
                    0
                ],
                "title": "Beyond 2050: From deployment to renewal of the global solar PV system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond 2050: From deployment to renewal of the global solar PV system"
                },
                "summary": "The global energy transition relies heavily on the large-scale deployment of\nPV capacity with deployment targets typically defined for 2050. However,\nsustaining the PV system beyond 2050 will require continuous renewal. This\nresearch explores the overlooked industrial transition from the initial\nimplementation phase to the long-term renewal phase, emphasizing the\nconsequences of this dynamic shift. Using streamlined modelling we estimate the\nannual production needed to both expand and maintain the global PV system. Our\nresults indicate that PV panel production dynamics during this transition are\nvery sensitive to two key factors: deployment speed and panel lifespan. If\ndeployment occurs over a shorter period than the average panel lifespan,\nproduction initially overshoots and exhibits an endogenous damped oscillatory\nbehavior due to a succession of installation and replacement cycles.\nConversely, if deployment is more gradual, production increases smoothly before\nstabilizing at the renewal rate. Given the current deployment scenarios and\nlifespan estimates, the PV industry is likely to face significant production\ndamped oscillations, up to 60%. These oscillations, corresponding to\nover/underproduction, are further amplified by the increasingly ambitious\nenergy transition targets, which accelerate deployment rates. Panel lifespan\nconversly remains a less flexible parameter. This study discusses oscillations\nfrom a systemic perspective and how they could exacerbate challenges for the\nlong-term sustainability of PV, including industrial, workforce, economic and\ngeopolitical dimensions. Beyond the case of PV, this study underscores a\nbroader issue in the energy transition: the shift from infrastructure expansion\nto long-term maintenance through renewal. Addressing this often-overlooked\nphase is essential for ensuring the sustainability of renewable energy systems\nbeyond 2050.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global energy transition relies heavily on the large-scale deployment of\nPV capacity with deployment targets typically defined for 2050. However,\nsustaining the PV system beyond 2050 will require continuous renewal. This\nresearch explores the overlooked industrial transition from the initial\nimplementation phase to the long-term renewal phase, emphasizing the\nconsequences of this dynamic shift. Using streamlined modelling we estimate the\nannual production needed to both expand and maintain the global PV system. Our\nresults indicate that PV panel production dynamics during this transition are\nvery sensitive to two key factors: deployment speed and panel lifespan. If\ndeployment occurs over a shorter period than the average panel lifespan,\nproduction initially overshoots and exhibits an endogenous damped oscillatory\nbehavior due to a succession of installation and replacement cycles.\nConversely, if deployment is more gradual, production increases smoothly before\nstabilizing at the renewal rate. Given the current deployment scenarios and\nlifespan estimates, the PV industry is likely to face significant production\ndamped oscillations, up to 60%. These oscillations, corresponding to\nover/underproduction, are further amplified by the increasingly ambitious\nenergy transition targets, which accelerate deployment rates. Panel lifespan\nconversly remains a less flexible parameter. This study discusses oscillations\nfrom a systemic perspective and how they could exacerbate challenges for the\nlong-term sustainability of PV, including industrial, workforce, economic and\ngeopolitical dimensions. Beyond the case of PV, this study underscores a\nbroader issue in the energy transition: the shift from infrastructure expansion\nto long-term maintenance through renewal. Addressing this often-overlooked\nphase is essential for ensuring the sustainability of renewable energy systems\nbeyond 2050."
                },
                "authors": [
                    {
                        "name": "Joseph Le Bihan"
                    },
                    {
                        "name": "Thomas Lapi"
                    },
                    {
                        "name": "José Halloy"
                    }
                ],
                "author_detail": {
                    "name": "José Halloy"
                },
                "author": "José Halloy",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93-10, 91B74",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04204v1",
                "updated": "2025-02-06T16:44:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    44,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:44:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    44,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence"
                },
                "summary": "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https://github.com/fshp971/adv-icl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https://github.com/fshp971/adv-icl."
                },
                "authors": [
                    {
                        "name": "Shaopeng Fu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04194v1",
                "updated": "2025-02-06T16:31:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "The Best Instruction-Tuning Data are Those That Fit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Best Instruction-Tuning Data are Those That Fit"
                },
                "summary": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%."
                },
                "authors": [
                    {
                        "name": "Dylan Zhang"
                    },
                    {
                        "name": "Qirun Dai"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04188v1",
                "updated": "2025-02-06T16:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    22,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:22:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    22,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "Automated Microservice Pattern Instance Detection Using\n  Infrastructure-as-Code Artifacts and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Microservice Pattern Instance Detection Using\n  Infrastructure-as-Code Artifacts and Large Language Models"
                },
                "summary": "Documenting software architecture is essential to preserve architecture\nknowledge, even though it is frequently costly. Architecture pattern instances,\nincluding microservice pattern instances, provide important structural software\ninformation. Practitioners should document this information to prevent\nknowledge vaporization. However, architecture patterns may not be detectable by\nanalyzing source code artifacts, requiring the analysis of other types of\nartifacts. Moreover, many existing pattern detection instance approaches are\ncomplex to extend. This article presents our ongoing PhD research, early\nexperiments, and a prototype for a tool we call MicroPAD for automating the\ndetection of microservice pattern instances. The prototype uses Large Language\nModels (LLMs) to analyze Infrastructure-as-Code (IaC) artifacts to aid\ndetection, aiming to keep costs low and maximize the scope of detectable\npatterns. Early experiments ran the prototype thrice in 22 GitHub projects. We\nverified that 83\\% of the patterns that the prototype identified were in the\nproject. The costs of detecting the pattern instances were minimal. These\nresults indicate that the approach is likely viable and, by lowering the entry\nbarrier to automating pattern instance detection, could help democratize\ndeveloper access to this category of architecture knowledge. Finally, we\npresent our overall research methodology, planned future work, and an overview\nof MicroPAD's potential industrial impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Documenting software architecture is essential to preserve architecture\nknowledge, even though it is frequently costly. Architecture pattern instances,\nincluding microservice pattern instances, provide important structural software\ninformation. Practitioners should document this information to prevent\nknowledge vaporization. However, architecture patterns may not be detectable by\nanalyzing source code artifacts, requiring the analysis of other types of\nartifacts. Moreover, many existing pattern detection instance approaches are\ncomplex to extend. This article presents our ongoing PhD research, early\nexperiments, and a prototype for a tool we call MicroPAD for automating the\ndetection of microservice pattern instances. The prototype uses Large Language\nModels (LLMs) to analyze Infrastructure-as-Code (IaC) artifacts to aid\ndetection, aiming to keep costs low and maximize the scope of detectable\npatterns. Early experiments ran the prototype thrice in 22 GitHub projects. We\nverified that 83\\% of the patterns that the prototype identified were in the\nproject. The costs of detecting the pattern instances were minimal. These\nresults indicate that the approach is likely viable and, by lowering the entry\nbarrier to automating pattern instance detection, could help democratize\ndeveloper access to this category of architecture knowledge. Finally, we\npresent our overall research methodology, planned future work, and an overview\nof MicroPAD's potential industrial impact."
                },
                "authors": [
                    {
                        "name": "Carlos Eduardo Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Eduardo Duarte"
                },
                "author": "Carlos Eduardo Duarte",
                "arxiv_comment": "ICSA 2025 - International Conference on Software Architecture. 6\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04184v1",
                "updated": "2025-02-06T16:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?"
                },
                "summary": "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nnotebook\\textquotesingle s executability improves by 42.7% and 28% by\ninstalling the correct modules and generating synthetic data. These findings\nchallenge prior assumptions, suggesting that notebooks have higher\nexecutability than previously reported, many of which offer valuable partial\nexecution, and that their executability should be evaluated within the\ninteractive notebook paradigm rather than through traditional software\nexecutability standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nnotebook\\textquotesingle s executability improves by 42.7% and 28% by\ninstalling the correct modules and generating synthetic data. These findings\nchallenge prior assumptions, suggesting that notebooks have higher\nexecutability than previously reported, many of which offer valuable partial\nexecution, and that their executability should be evaluated within the\ninteractive notebook paradigm rather than through traditional software\nexecutability standards."
                },
                "authors": [
                    {
                        "name": "Tien Nguyen"
                    },
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "12 pages, 10 figures, 3 tables, the 22nd International Conference on\n  Mining Software Repositories (MSR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04180v1",
                "updated": "2025-02-06T16:12:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    12,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:12:06Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    12,
                    6,
                    3,
                    37,
                    0
                ],
                "title": "Multi-agent Architecture Search via Agentic Supernet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent Architecture Search via Agentic Supernet"
                },
                "summary": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive\nboundaries of individual agents through disciplined collaboration and\ninteraction, while constructing these systems often requires labor-intensive\nmanual designs. Despite the availability of methods to automate the design of\nagentic workflows, they typically seek to identify a static, complex,\none-size-fits-all system, which, however, fails to dynamically allocate\ninference resources based on the difficulty and domain of each query. To\naddress this challenge, we shift away from the pursuit of a monolithic agentic\nsystem, instead optimizing the \\textbf{agentic supernet}, a probabilistic and\ncontinuous distribution of agentic architectures. We introduce MaAS, an\nautomated framework that samples query-dependent agentic systems from the\nsupernet, delivering high-quality solutions and tailored resource allocation\n(\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation\nacross six benchmarks demonstrates that MaAS \\textbf{(I)} requires only\n$6\\sim45\\%$ of the inference costs of existing handcrafted or automated\nmulti-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and\n\\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone\ntransferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive\nboundaries of individual agents through disciplined collaboration and\ninteraction, while constructing these systems often requires labor-intensive\nmanual designs. Despite the availability of methods to automate the design of\nagentic workflows, they typically seek to identify a static, complex,\none-size-fits-all system, which, however, fails to dynamically allocate\ninference resources based on the difficulty and domain of each query. To\naddress this challenge, we shift away from the pursuit of a monolithic agentic\nsystem, instead optimizing the \\textbf{agentic supernet}, a probabilistic and\ncontinuous distribution of agentic architectures. We introduce MaAS, an\nautomated framework that samples query-dependent agentic systems from the\nsupernet, delivering high-quality solutions and tailored resource allocation\n(\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation\nacross six benchmarks demonstrates that MaAS \\textbf{(I)} requires only\n$6\\sim45\\%$ of the inference costs of existing handcrafted or automated\nmulti-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and\n\\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone\ntransferability."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Luyang Niu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04176v1",
                "updated": "2025-02-06T16:07:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    7,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T16:07:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    7,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented\n  Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented\n  Multimodal Generation"
                },
                "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have shown\nremarkable performance in enhancing response accuracy and relevance by\nintegrating external knowledge into generative models. However, existing RAG\nmethods primarily focus on providing text-only answers, even in multimodal\nretrieval-augmented generation scenarios. In this work, we introduce the\nMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims\nto generate answers that combine both text and images, fully leveraging the\nmultimodal data within a corpus. Despite the importance of this task, there is\na notable absence of a comprehensive benchmark to effectively evaluate MRAMG\nperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefully\ncurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and\n4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and\nLifestyle. The dataset incorporates diverse difficulty levels and complex\nmulti-image scenarios, providing a robust foundation for evaluating multimodal\ngeneration tasks. To facilitate rigorous evaluation, our MRAMG-Bench\nincorporates a comprehensive suite of both statistical and LLM-based metrics,\nenabling a thorough analysis of the performance of popular generative models in\nthe MRAMG task. Besides, we propose an efficient multimodal answer generation\nframework that leverages both LLMs and MLLMs to generate multimodal responses.\nOur datasets are available at: https://huggingface.co/MRAMG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Retrieval-Augmented Generation (RAG) have shown\nremarkable performance in enhancing response accuracy and relevance by\nintegrating external knowledge into generative models. However, existing RAG\nmethods primarily focus on providing text-only answers, even in multimodal\nretrieval-augmented generation scenarios. In this work, we introduce the\nMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims\nto generate answers that combine both text and images, fully leveraging the\nmultimodal data within a corpus. Despite the importance of this task, there is\na notable absence of a comprehensive benchmark to effectively evaluate MRAMG\nperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefully\ncurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and\n4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and\nLifestyle. The dataset incorporates diverse difficulty levels and complex\nmulti-image scenarios, providing a robust foundation for evaluating multimodal\ngeneration tasks. To facilitate rigorous evaluation, our MRAMG-Bench\nincorporates a comprehensive suite of both statistical and LLM-based metrics,\nenabling a thorough analysis of the performance of popular generative models in\nthe MRAMG task. Besides, we propose an efficient multimodal answer generation\nframework that leverages both LLMs and MLLMs to generate multimodal responses.\nOur datasets are available at: https://huggingface.co/MRAMG."
                },
                "authors": [
                    {
                        "name": "Qinhan Yu"
                    },
                    {
                        "name": "Zhiyou Xiao"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Zhengren Wang"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13013v2",
                "updated": "2025-02-06T16:03:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    3,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-17T15:34:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    15,
                    34,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "The Emergence of Strategic Reasoning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Emergence of Strategic Reasoning of Large Language Models"
                },
                "summary": "Although large language models (LLMs) have demonstrated strong reasoning\nabilities in structured tasks (e.g., coding and mathematics), it remains\nunexplored whether these abilities extend to strategic multi-agent\nenvironments. We investigate strategic reasoning capabilities -- the process of\nchoosing an optimal course of action by predicting and adapting to others'\nactions -- of LLMs by analyzing their performance in three classical games from\nbehavioral economics. We evaluate three standard LLMs (ChatGPT-4, Claude-2.1,\nGemini 1.5) and three specialized reasoning LLMs (GPT-o1, Claude-3.5-Sonnet,\nGemini Flash Thinking 2.0) using hierarchical models of bounded rationality.\nOur results show that reasoning LLMs exhibit superior strategic reasoning\ncompared to standard LLMs (which do not demonstrate substantial capabilities),\nand often match or exceed human performance. Since strategic reasoning is\nfundamental to future AI systems (including Agentic AI and Artificial General\nIntelligence), our findings demonstrate the importance of dedicated reasoning\ncapabilities in achieving effective strategic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated strong reasoning\nabilities in structured tasks (e.g., coding and mathematics), it remains\nunexplored whether these abilities extend to strategic multi-agent\nenvironments. We investigate strategic reasoning capabilities -- the process of\nchoosing an optimal course of action by predicting and adapting to others'\nactions -- of LLMs by analyzing their performance in three classical games from\nbehavioral economics. We evaluate three standard LLMs (ChatGPT-4, Claude-2.1,\nGemini 1.5) and three specialized reasoning LLMs (GPT-o1, Claude-3.5-Sonnet,\nGemini Flash Thinking 2.0) using hierarchical models of bounded rationality.\nOur results show that reasoning LLMs exhibit superior strategic reasoning\ncompared to standard LLMs (which do not demonstrate substantial capabilities),\nand often match or exceed human performance. Since strategic reasoning is\nfundamental to future AI systems (including Agentic AI and Artificial General\nIntelligence), our findings demonstrate the importance of dedicated reasoning\ncapabilities in achieving effective strategic reasoning."
                },
                "authors": [
                    {
                        "name": "Dongwoo Lee"
                    },
                    {
                        "name": "Gavin Kader"
                    }
                ],
                "author_detail": {
                    "name": "Gavin Kader"
                },
                "author": "Gavin Kader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06833v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06833v3",
                "updated": "2025-02-06T15:52:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    52,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2024-04-10T08:49:27Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    8,
                    49,
                    27,
                    2,
                    101,
                    0
                ],
                "title": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge"
                },
                "summary": "Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains."
                },
                "authors": [
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Taelin Karidi"
                    },
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Nicolas Garneau"
                    },
                    {
                        "name": "Yong Cao"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "cultural bias analysis, cultural knowledge probing, large language\n  models, cultural NLP; Accepted by NAACL2025",
                "arxiv_journal_ref": "NAACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06833v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06833v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12964v2",
                "updated": "2025-02-06T15:51:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    51,
                    59,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-31T13:10:48Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    13,
                    10,
                    48,
                    5,
                    244,
                    0
                ],
                "title": "OpenRANet: Neuralized Spectrum Access by Joint Subcarrier and Power\n  Allocation with Optimization-based Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenRANet: Neuralized Spectrum Access by Joint Subcarrier and Power\n  Allocation with Optimization-based Deep Learning"
                },
                "summary": "The next-generation radio access network (RAN), known as Open RAN, is poised\nto feature an AI-native interface for wireless cellular networks, including\nemerging satellite-terrestrial systems, making deep learning integral to its\noperation. In this paper, we address the nonconvex optimization challenge of\njoint subcarrier and power allocation in Open RAN, with the objective of\nminimizing the total power consumption while ensuring users meet their\ntransmission data rate requirements. We propose OpenRANet, an\noptimization-based deep learning model that integrates machine-learning\ntechniques with iterative optimization algorithms. We start by transforming the\noriginal nonconvex problem into convex subproblems through decoupling, variable\ntransformation, and relaxation techniques. These subproblems are then\nefficiently solved using iterative methods within the standard interference\nfunction framework, enabling the derivation of primal-dual solutions. These\nsolutions integrate seamlessly as a convex optimization layer within OpenRANet,\nenhancing constraint adherence, solution accuracy, and computational efficiency\nby combining machine learning with convex analysis, as shown in numerical\nexperiments. OpenRANet also serves as a foundation for designing\nresource-constrained AI-native wireless optimization strategies for broader\nscenarios like multi-cell systems, satellite-terrestrial networks, and future\nOpen RAN deployments with complex power consumption requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next-generation radio access network (RAN), known as Open RAN, is poised\nto feature an AI-native interface for wireless cellular networks, including\nemerging satellite-terrestrial systems, making deep learning integral to its\noperation. In this paper, we address the nonconvex optimization challenge of\njoint subcarrier and power allocation in Open RAN, with the objective of\nminimizing the total power consumption while ensuring users meet their\ntransmission data rate requirements. We propose OpenRANet, an\noptimization-based deep learning model that integrates machine-learning\ntechniques with iterative optimization algorithms. We start by transforming the\noriginal nonconvex problem into convex subproblems through decoupling, variable\ntransformation, and relaxation techniques. These subproblems are then\nefficiently solved using iterative methods within the standard interference\nfunction framework, enabling the derivation of primal-dual solutions. These\nsolutions integrate seamlessly as a convex optimization layer within OpenRANet,\nenhancing constraint adherence, solution accuracy, and computational efficiency\nby combining machine learning with convex analysis, as shown in numerical\nexperiments. OpenRANet also serves as a foundation for designing\nresource-constrained AI-native wireless optimization strategies for broader\nscenarios like multi-cell systems, satellite-terrestrial networks, and future\nOpen RAN deployments with complex power consumption requirements."
                },
                "authors": [
                    {
                        "name": "Siya Chen"
                    },
                    {
                        "name": "Chee Wei Tan"
                    },
                    {
                        "name": "Xiangping Zhai"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "arxiv_comment": "This paper has been accepted by the IEEE Transactions on Green\n  Communications and Networking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08580v2",
                "updated": "2025-02-06T15:39:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    39,
                    22,
                    3,
                    37,
                    0
                ],
                "published": "2024-07-11T15:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    15,
                    5,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Collaborative Object Manipulation on the Water Surface by a UAV-USV Team\n  Using Tethers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Object Manipulation on the Water Surface by a UAV-USV Team\n  Using Tethers"
                },
                "summary": "This paper introduces an innovative methodology for object manipulation on\nthe surface of water through the collaboration of an Unmanned Aerial Vehicle\n(UAV) and an Unmanned Surface Vehicle (USV) connected to the object by tethers.\nWe propose a novel mathematical model of a robotic system that combines the\nUAV, USV, and the tethered floating object. A novel Model Predictive Control\n(MPC) framework is designed for using this model to achieve precise control and\nguidance for this collaborative robotic system. Extensive simulations in the\nrealistic robotic simulator Gazebo demonstrate the system's readiness for\nreal-world deployment, highlighting its versatility and effectiveness. Our\nmulti-robot system overcomes the state-of-the-art single-robot approach,\nexhibiting smaller control errors during the tracking of the floating object's\nreference. Additionally, our multi-robot system demonstrates a shorter recovery\ntime from a disturbance compared to the single-robot approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative methodology for object manipulation on\nthe surface of water through the collaboration of an Unmanned Aerial Vehicle\n(UAV) and an Unmanned Surface Vehicle (USV) connected to the object by tethers.\nWe propose a novel mathematical model of a robotic system that combines the\nUAV, USV, and the tethered floating object. A novel Model Predictive Control\n(MPC) framework is designed for using this model to achieve precise control and\nguidance for this collaborative robotic system. Extensive simulations in the\nrealistic robotic simulator Gazebo demonstrate the system's readiness for\nreal-world deployment, highlighting its versatility and effectiveness. Our\nmulti-robot system overcomes the state-of-the-art single-robot approach,\nexhibiting smaller control errors during the tracking of the floating object's\nreference. Additionally, our multi-robot system demonstrates a shorter recovery\ntime from a disturbance compared to the single-robot approach."
                },
                "authors": [
                    {
                        "name": "Filip Novák"
                    },
                    {
                        "name": "Tomáš Báča"
                    },
                    {
                        "name": "Martin Saska"
                    }
                ],
                "author_detail": {
                    "name": "Martin Saska"
                },
                "author": "Martin Saska",
                "arxiv_doi": "10.1109/IROS58592.2024.10802469",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10802469",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2024). p. 3425-3432. ISSN 2153-0866. ISBN 979-8-3503-7770-5",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04153v1",
                "updated": "2025-02-06T15:39:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    39,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:39:16Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    39,
                    16,
                    3,
                    37,
                    0
                ],
                "title": "UltraIF: Advancing Instruction Following from the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraIF: Advancing Instruction Following from the Wild"
                },
                "summary": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF."
                },
                "authors": [
                    {
                        "name": "Kaikai An"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11782v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11782v3",
                "updated": "2025-02-06T15:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    37,
                    52,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-15T17:01:21Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks"
                },
                "summary": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Dawei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Cheng"
                },
                "author": "Dawei Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11782v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11782v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02059v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02059v4",
                "updated": "2025-02-06T15:17:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    17,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2023-10-03T14:01:28Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    14,
                    1,
                    28,
                    1,
                    276,
                    0
                ],
                "title": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study"
                },
                "summary": "Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code."
                },
                "authors": [
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Jiaxin Yu"
                    },
                    {
                        "name": "Jinfu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jinfu Chen"
                },
                "author": "Jinfu Chen",
                "arxiv_comment": "Preprint accepted for publication in ACM Transactions on Software\n  Engineering and Methodology (TOSEM), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02059v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02059v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04134v1",
                "updated": "2025-02-06T15:14:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    14,
                    2,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:14:02Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    14,
                    2,
                    3,
                    37,
                    0
                ],
                "title": "The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs"
                },
                "summary": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in\nclosed-source LLMs by conducting experiments across multiple tasks, including\nparaphrasing, relevance judgment, and multiple-choice questions. Our results\nshow that input order significantly affects performance across tasks, with\nshuffled inputs leading to measurable declines in output accuracy. Few-shot\nprompting demonstrates mixed effectiveness and offers partial mitigation,\nhowever, fails to fully resolve the problem. These findings highlight\npersistent risks, particularly in high-stakes applications, and point to the\nneed for more robust LLMs or improved input-handling techniques in future\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in\nclosed-source LLMs by conducting experiments across multiple tasks, including\nparaphrasing, relevance judgment, and multiple-choice questions. Our results\nshow that input order significantly affects performance across tasks, with\nshuffled inputs leading to measurable declines in output accuracy. Few-shot\nprompting demonstrates mixed effectiveness and offers partial mitigation,\nhowever, fails to fully resolve the problem. These findings highlight\npersistent risks, particularly in high-stakes applications, and point to the\nneed for more robust LLMs or improved input-handling techniques in future\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Bryan Guan"
                    },
                    {
                        "name": "Tanya Roosta"
                    },
                    {
                        "name": "Peyman Passban"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "arxiv_comment": "The first 3 authors have contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04128v1",
                "updated": "2025-02-06T15:04:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    4,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:04:00Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    4,
                    0,
                    3,
                    37,
                    0
                ],
                "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis"
                },
                "summary": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Xinsheng Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Jiahe Lei"
                    },
                    {
                        "name": "Yi Peng"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Yizhu Jin"
                    },
                    {
                        "name": "Zheqi DAI"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Liumeng Xue"
                    },
                    {
                        "name": "Yunlin Chen"
                    },
                    {
                        "name": "Zhifei Li"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11678v2",
                "updated": "2025-02-06T14:40:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    40,
                    52,
                    3,
                    37,
                    0
                ],
                "published": "2024-06-17T15:58:22Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    58,
                    22,
                    0,
                    169,
                    0
                ],
                "title": "TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TourRank: Utilizing Large Language Models for Documents Ranking with a\n  Tournament-Inspired Strategy"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is challenging. To tackle these\nissues, we introduce a novel documents ranking method called TourRank, which is\ninspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1)\novercome the limitation in input length and reduce the ranking latency by\nincorporating a multi-stage grouping strategy similar to the parallel group\nstage of sport tournaments; 2) improve the ranking performance and robustness\nto input orders by using a points system to ensemble multiple ranking results.\nWe test TourRank with different LLMs on the TREC DL datasets and the BEIR\nbenchmark. The experimental results demonstrate that TourRank delivers\nstate-of-the-art performance at a modest cost. The code of TourRank can be seen\non https://github.com/chenyiqun/TourRank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in zero-shot documents\nranking, yielding commendable results. However, several significant challenges\nstill persist in LLMs for ranking: (1) LLMs are constrained by limited input\nlength, precluding them from processing a large number of documents\nsimultaneously; (2) The output document sequence is influenced by the input\norder of documents, resulting in inconsistent ranking outcomes; (3) Achieving a\nbalance between cost and ranking performance is challenging. To tackle these\nissues, we introduce a novel documents ranking method called TourRank, which is\ninspired by the sport tournaments, such as FIFA World Cup. Specifically, we 1)\novercome the limitation in input length and reduce the ranking latency by\nincorporating a multi-stage grouping strategy similar to the parallel group\nstage of sport tournaments; 2) improve the ranking performance and robustness\nto input orders by using a points system to ensemble multiple ranking results.\nWe test TourRank with different LLMs on the TREC DL datasets and the BEIR\nbenchmark. The experimental results demonstrate that TourRank delivers\nstate-of-the-art performance at a modest cost. The code of TourRank can be seen\non https://github.com/chenyiqun/TourRank."
                },
                "authors": [
                    {
                        "name": "Yiqun Chen"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04103v1",
                "updated": "2025-02-06T14:27:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    27,
                    54,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T14:27:54Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    27,
                    54,
                    3,
                    37,
                    0
                ],
                "title": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTutor: An Open-Source SDK for Generative AI-Powered Animated\n  Pedagogical Agents with Multi-Media Output"
                },
                "summary": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases."
                },
                "authors": [
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Chengyu Lin"
                    },
                    {
                        "name": "Xinyi Tang"
                    },
                    {
                        "name": "Aprille Xi"
                    },
                    {
                        "name": "Canwen Wang"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Kenneth R Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R Koedinger"
                },
                "author": "Kenneth R Koedinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04095v1",
                "updated": "2025-02-06T14:12:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    12,
                    41,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T14:12:41Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    12,
                    41,
                    3,
                    37,
                    0
                ],
                "title": "LLMs to Support a Domain Specific Knowledge Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs to Support a Domain Specific Knowledge Assistant"
                },
                "summary": "This work presents a custom approach to developing a domain specific\nknowledge assistant for sustainability reporting using the International\nFinancial Reporting Standards (IFRS). In this domain, there is no publicly\navailable question-answer dataset, which has impeded the development of a\nhigh-quality chatbot to support companies with IFRS reporting. The two key\ncontributions of this project therefore are:\n  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS\nsustainability standards, created using a novel generation and evaluation\npipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse\nQA pairs that address a wide spectrum of potential user queries in\nsustainability reporting. Various LLM-based techniques are employed to create\nthe dataset, including chain-of-thought reasoning and few-shot prompting. A\ncustom evaluation framework is developed to assess question and answer quality\nacross multiple dimensions, including faithfulness, relevance, and domain\nspecificity. The dataset averages a score range of 8.16 out of 10 on these\nmetrics.\n  (2) Two architectures for question-answering in the sustainability reporting\ndomain - a RAG pipeline and a fully LLM-based pipeline. The architectures are\ndeveloped by experimenting, fine-tuning, and training on the QA dataset. The\nfinal pipelines feature an LLM fine-tuned on domain specific data and an\nindustry classification component to improve the handling of complex queries.\nThe RAG architecture achieves an accuracy of 85.32% on single-industry and\n72.15% on cross-industry multiple-choice questions, outperforming the baseline\napproach by 4.67 and 19.21 percentage points, respectively. The LLM-based\npipeline achieves an accuracy of 93.45% on single-industry and 80.30% on\ncross-industry multiple-choice questions, an improvement of 12.80 and 27.36\npercentage points over the baseline, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a custom approach to developing a domain specific\nknowledge assistant for sustainability reporting using the International\nFinancial Reporting Standards (IFRS). In this domain, there is no publicly\navailable question-answer dataset, which has impeded the development of a\nhigh-quality chatbot to support companies with IFRS reporting. The two key\ncontributions of this project therefore are:\n  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS\nsustainability standards, created using a novel generation and evaluation\npipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse\nQA pairs that address a wide spectrum of potential user queries in\nsustainability reporting. Various LLM-based techniques are employed to create\nthe dataset, including chain-of-thought reasoning and few-shot prompting. A\ncustom evaluation framework is developed to assess question and answer quality\nacross multiple dimensions, including faithfulness, relevance, and domain\nspecificity. The dataset averages a score range of 8.16 out of 10 on these\nmetrics.\n  (2) Two architectures for question-answering in the sustainability reporting\ndomain - a RAG pipeline and a fully LLM-based pipeline. The architectures are\ndeveloped by experimenting, fine-tuning, and training on the QA dataset. The\nfinal pipelines feature an LLM fine-tuned on domain specific data and an\nindustry classification component to improve the handling of complex queries.\nThe RAG architecture achieves an accuracy of 85.32% on single-industry and\n72.15% on cross-industry multiple-choice questions, outperforming the baseline\napproach by 4.67 and 19.21 percentage points, respectively. The LLM-based\npipeline achieves an accuracy of 93.45% on single-industry and 80.30% on\ncross-industry multiple-choice questions, an improvement of 12.80 and 27.36\npercentage points over the baseline, respectively."
                },
                "authors": [
                    {
                        "name": "Maria-Flavia Lovin"
                    }
                ],
                "author_detail": {
                    "name": "Maria-Flavia Lovin"
                },
                "author": "Maria-Flavia Lovin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v8",
                "updated": "2025-02-06T13:42:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    42,
                    31,
                    3,
                    37,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation"
                },
                "summary": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04075v1",
                "updated": "2025-02-06T13:38:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    38,
                    57,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    38,
                    57,
                    3,
                    37,
                    0
                ],
                "title": "Controllable Emotion Generation with Emotion Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Emotion Generation with Emotion Vectors"
                },
                "summary": "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method."
                },
                "authors": [
                    {
                        "name": "Yurui Dong"
                    },
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Yao Yang"
                    },
                    {
                        "name": "Bingjie Lu"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Zhi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Liu"
                },
                "author": "Zhi Liu",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06809v2",
                "updated": "2025-02-06T13:21:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    21,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-09T12:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    12,
                    9,
                    30,
                    2,
                    283,
                    0
                ],
                "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level"
                },
                "summary": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xinyi Zeng"
                    },
                    {
                        "name": "Yuying Shang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "arxiv_comment": "19 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04057v1",
                "updated": "2025-02-06T13:17:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    17,
                    3,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:17:03Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    17,
                    3,
                    3,
                    37,
                    0
                ],
                "title": "Smart IoT Security: Lightweight Machine Learning Techniques for\n  Multi-Class Attack Detection in IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart IoT Security: Lightweight Machine Learning Techniques for\n  Multi-Class Attack Detection in IoT Networks"
                },
                "summary": "In the growing terrain of the Internet of Things (IoT), it is vital that\nnetworks are secure to protect against a range of cyber threats. Based on the\nstrong machine learning framework, this study proposes novel lightweight\nensemble approaches for improving multi-class attack detection of IoT devices.\nUsing the large CICIoT 2023 dataset with 34 attack types distributed amongst 10\nattack categories, we systematically evaluated the performance of a wide\nvariety of modern machine learning methods with the aim of establishing the\nbest-performing algorithmic choice to secure IoT applications. In particular,\nwe explore approaches based on ML classifiers to tackle the biocharges\ncharacterized by the challenging and heterogeneous nature of attack vectors in\nIoT environments. The method that performed best was the Decision Tree, with an\naccuracy of 99.56% and an F1 score of 99.62%, showing that this model is\ncapable of accurately and reliably detecting threats.The Random Forest model\nwas the next best-performing model with 98.22% and an F1 score of 98.24%,\nsuggesting that ML methods are quite effective in a situation of\nhigh-dimensional data. Our results highlight the potential for using ML\nclassifiers in bolstering security for IoT devices and also serve as\nmotivations for future investigations targeting scalable, keystroke-based\nattack detection systems. We believe that our method provides a new path to\ndevelop complex machine learning algorithms for low-resource IoT devices,\nbalancing both accuracy and time efficiency needs. In summary, these\ncontributions enrich the state of the art of the IoT security literature,\nlaying down solid ground and guidelines for the deployment of smart, adaptive\nsecurity in IoT settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the growing terrain of the Internet of Things (IoT), it is vital that\nnetworks are secure to protect against a range of cyber threats. Based on the\nstrong machine learning framework, this study proposes novel lightweight\nensemble approaches for improving multi-class attack detection of IoT devices.\nUsing the large CICIoT 2023 dataset with 34 attack types distributed amongst 10\nattack categories, we systematically evaluated the performance of a wide\nvariety of modern machine learning methods with the aim of establishing the\nbest-performing algorithmic choice to secure IoT applications. In particular,\nwe explore approaches based on ML classifiers to tackle the biocharges\ncharacterized by the challenging and heterogeneous nature of attack vectors in\nIoT environments. The method that performed best was the Decision Tree, with an\naccuracy of 99.56% and an F1 score of 99.62%, showing that this model is\ncapable of accurately and reliably detecting threats.The Random Forest model\nwas the next best-performing model with 98.22% and an F1 score of 98.24%,\nsuggesting that ML methods are quite effective in a situation of\nhigh-dimensional data. Our results highlight the potential for using ML\nclassifiers in bolstering security for IoT devices and also serve as\nmotivations for future investigations targeting scalable, keystroke-based\nattack detection systems. We believe that our method provides a new path to\ndevelop complex machine learning algorithms for low-resource IoT devices,\nbalancing both accuracy and time efficiency needs. In summary, these\ncontributions enrich the state of the art of the IoT security literature,\nlaying down solid ground and guidelines for the deployment of smart, adaptive\nsecurity in IoT settings."
                },
                "authors": [
                    {
                        "name": "Shahran Rahman Alve"
                    },
                    {
                        "name": "Muhammad Zawad Mahmud"
                    },
                    {
                        "name": "Samiha Islam"
                    },
                    {
                        "name": "Md. Asaduzzaman Chowdhury"
                    },
                    {
                        "name": "Jahirul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Jahirul Islam"
                },
                "author": "Jahirul Islam",
                "arxiv_comment": "Accepted in an international conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04040v1",
                "updated": "2025-02-06T13:01:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    1,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:01:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    1,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for\n  Enhancing Safety Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for\n  Enhancing Safety Alignment"
                },
                "summary": "Training safe LLMs is one of the most critical research challenge. However,\nthe commonly used method, Refusal Training (RT), struggles to generalize\nagainst various OOD jailbreaking attacks. Many safety training methods have\nbeen proposed to address this issue. While they offer valuable insights, we aim\nto complement this line of research by investigating whether OOD attacks truly\nexceed the capability of RT model. Conducting evaluation with BoN, we observe\nsignificant improvements on generalization as N increases. This underscores\nthat the model possesses sufficient safety-related latent knowledge, but RT\nfails to consistently elicit this knowledge when addressing OOD attacks.\nFurther analysis based on domain adaptation reveals that training with direct\nrefusal causes model to rely on superficial shortcuts, resulting in learning of\nnon-robust representation mappings. Based on our findings, we propose training\nmodel to perform safety reasoning for each query. Reasoning supervision\nencourages model to perform more computations, explicitly eliciting and using\nlatent knowledge through reasoning. To achieve this, we synthesize reasoning\nsupervision based on pre-guidelines, training the model to reason in alignment\nwith them, thereby effectively eliciting and utilizing latent knowledge from\ndiverse perspectives. Extensive experiments show that our method significantly\nimproves generalization performance against OOD attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training safe LLMs is one of the most critical research challenge. However,\nthe commonly used method, Refusal Training (RT), struggles to generalize\nagainst various OOD jailbreaking attacks. Many safety training methods have\nbeen proposed to address this issue. While they offer valuable insights, we aim\nto complement this line of research by investigating whether OOD attacks truly\nexceed the capability of RT model. Conducting evaluation with BoN, we observe\nsignificant improvements on generalization as N increases. This underscores\nthat the model possesses sufficient safety-related latent knowledge, but RT\nfails to consistently elicit this knowledge when addressing OOD attacks.\nFurther analysis based on domain adaptation reveals that training with direct\nrefusal causes model to rely on superficial shortcuts, resulting in learning of\nnon-robust representation mappings. Based on our findings, we propose training\nmodel to perform safety reasoning for each query. Reasoning supervision\nencourages model to perform more computations, explicitly eliciting and using\nlatent knowledge through reasoning. To achieve this, we synthesize reasoning\nsupervision based on pre-guidelines, training the model to reason in alignment\nwith them, thereby effectively eliciting and utilizing latent knowledge from\ndiverse perspectives. Extensive experiments show that our method significantly\nimproves generalization performance against OOD attacks."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Xueqian Wang"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04039v1",
                "updated": "2025-02-06T13:01:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    1,
                    27,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:01:27Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    1,
                    27,
                    3,
                    37,
                    0
                ],
                "title": "A Cloud-native Agile approach to cyber platform prototyping and\n  integration for astronomy: the ENGAGE SKA case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cloud-native Agile approach to cyber platform prototyping and\n  integration for astronomy: the ENGAGE SKA case"
                },
                "summary": "The Square Kilometre Array (SKA) Observatory is gearing up the formal\nconstruction of its two radio interferometers in Australia and South Africa\nafter the end of design and pre-construction phases. Agile methodologies, the\nCloud native Computing technologies and the DevOps software ideas are\ninfluencing the design of compute infrastructures that will be key to reduce\nthe operational costs of SKA while improving the control and monitoring of the\nSKA antennas and ancillary systems, Correlators, HPC facilities or related data\ncentre tiered systems. These tools will likely include advanced power metering\ntechnologies and efficient distribution automation and Network Operation\nCentres (NOC). SKA will become the world's largest radio telescope and is\nexpected to achieve its first science by 2026. To cope with this dimension and\ncomplexity, a key part of this distributed Observatory is the overall software\ncontrol and monitoring system embodied in the Observatory Management and\nControl (OMC) and the Services Teams that requires specialized Agile Teams to\nassist in software and cyber infrastructure building using an Agile development\nenvironment that includes test automation, Continuous Integration, and\nContinuous Deployment. To manage such a large and distributed machine, the\nAgile approach was adopted for the core software package of the SKA Telescope\naimed at scheduling observations, controlling their execution, monitoring the\ntelescope status and ensuring scalability and reliability. Here, we report on\nthe ENGAGE SKA ciberinfrastructure prototyping support to the SKA Agile\nSoftware Development Life Cycle (SDLC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Square Kilometre Array (SKA) Observatory is gearing up the formal\nconstruction of its two radio interferometers in Australia and South Africa\nafter the end of design and pre-construction phases. Agile methodologies, the\nCloud native Computing technologies and the DevOps software ideas are\ninfluencing the design of compute infrastructures that will be key to reduce\nthe operational costs of SKA while improving the control and monitoring of the\nSKA antennas and ancillary systems, Correlators, HPC facilities or related data\ncentre tiered systems. These tools will likely include advanced power metering\ntechnologies and efficient distribution automation and Network Operation\nCentres (NOC). SKA will become the world's largest radio telescope and is\nexpected to achieve its first science by 2026. To cope with this dimension and\ncomplexity, a key part of this distributed Observatory is the overall software\ncontrol and monitoring system embodied in the Observatory Management and\nControl (OMC) and the Services Teams that requires specialized Agile Teams to\nassist in software and cyber infrastructure building using an Agile development\nenvironment that includes test automation, Continuous Integration, and\nContinuous Deployment. To manage such a large and distributed machine, the\nAgile approach was adopted for the core software package of the SKA Telescope\naimed at scheduling observations, controlling their execution, monitoring the\ntelescope status and ensuring scalability and reliability. Here, we report on\nthe ENGAGE SKA ciberinfrastructure prototyping support to the SKA Agile\nSoftware Development Life Cycle (SDLC)."
                },
                "authors": [
                    {
                        "name": "Domingos Barbosa"
                    },
                    {
                        "name": "Diogo Regateiro"
                    },
                    {
                        "name": "João Paulo Barraca"
                    },
                    {
                        "name": "Dzianis Bartashevich"
                    },
                    {
                        "name": "Marco Bartolini"
                    },
                    {
                        "name": "Matteo di Carlo"
                    },
                    {
                        "name": "Piers Harding"
                    },
                    {
                        "name": "Dalmiro Maia"
                    },
                    {
                        "name": "Bruno Morgado"
                    },
                    {
                        "name": "Domingos Nunes"
                    },
                    {
                        "name": "Bruno Ribeiro"
                    },
                    {
                        "name": "Bruno Coelho"
                    },
                    {
                        "name": "Valério Ribeiro"
                    },
                    {
                        "name": "Allan K. de Almeida Jr"
                    },
                    {
                        "name": "Timothée Vaillant"
                    },
                    {
                        "name": "Uğur Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Uğur Yilmaz"
                },
                "author": "Uğur Yilmaz",
                "arxiv_comment": "21 pages, 6 figures. Submitted as Technical Report article to the The\n  Journal of Instrumentation (JINST )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04037v1",
                "updated": "2025-02-06T12:57:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    57,
                    50,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:57:50Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    57,
                    50,
                    3,
                    37,
                    0
                ],
                "title": "Exploring Imbalanced Annotations for Effective In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Imbalanced Annotations for Effective In-Context Learning"
                },
                "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. Existing selection methods may\nhinge on the distribution of annotated datasets, which can often be long-tailed\nin real-world scenarios. In this work, we show that imbalanced class\ndistributions in annotated datasets significantly degrade the performance of\nICL across various tasks and selection methods. Moreover, traditional rebalance\nmethods fail to ameliorate the issue of class imbalance in ICL. Our method is\nmotivated by decomposing the distributional differences between annotated and\ntest datasets into two-component weights: class-wise weights and conditional\nbias. The key idea behind our method is to estimate the conditional bias by\nminimizing the empirical error on a balanced validation dataset and to employ\nthe two-component weights to modify the original scoring functions during\nselection. Our approach can prevent selecting too many demonstrations from a\nsingle class while preserving the effectiveness of the original selection\nmethods. Extensive experiments demonstrate the effectiveness of our method,\nimproving the average accuracy by up to 5.46 on common benchmarks with\nimbalanced datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. Existing selection methods may\nhinge on the distribution of annotated datasets, which can often be long-tailed\nin real-world scenarios. In this work, we show that imbalanced class\ndistributions in annotated datasets significantly degrade the performance of\nICL across various tasks and selection methods. Moreover, traditional rebalance\nmethods fail to ameliorate the issue of class imbalance in ICL. Our method is\nmotivated by decomposing the distributional differences between annotated and\ntest datasets into two-component weights: class-wise weights and conditional\nbias. The key idea behind our method is to estimate the conditional bias by\nminimizing the empirical error on a balanced validation dataset and to employ\nthe two-component weights to modify the original scoring functions during\nselection. Our approach can prevent selecting too many demonstrations from a\nsingle class while preserving the effectiveness of the original selection\nmethods. Extensive experiments demonstrate the effectiveness of our method,\nimproving the average accuracy by up to 5.46 on common benchmarks with\nimbalanced datasets."
                },
                "authors": [
                    {
                        "name": "Hongfu Gao"
                    },
                    {
                        "name": "Feipeng Zhang"
                    },
                    {
                        "name": "Hao Zeng"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07770v2",
                "updated": "2025-02-06T12:52:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    52,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2024-02-12T16:32:37Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    16,
                    32,
                    37,
                    0,
                    43,
                    0
                ],
                "title": "Had enough of experts? Quantitative knowledge retrieval from large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Had enough of experts? Quantitative knowledge retrieval from large\n  language models"
                },
                "summary": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. Here we\nexplore the feasibility of LLMs as a mechanism for quantitative knowledge\nretrieval to aid two data analysis tasks: elicitation of prior distributions\nfor Bayesian models and imputation of missing data. We introduce a framework\nthat leverages LLMs to enhance Bayesian workflows by eliciting expert-like\nprior knowledge and imputing missing data. Tested on diverse datasets, this\napproach can improve predictive accuracy and reduce data requirements, offering\nsignificant potential in healthcare, environmental science and engineering\napplications. We discuss the implications and challenges of treating LLMs as\n'experts'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been extensively studied for their\nabilities to generate convincing natural language sequences, however their\nutility for quantitative information retrieval is less well understood. Here we\nexplore the feasibility of LLMs as a mechanism for quantitative knowledge\nretrieval to aid two data analysis tasks: elicitation of prior distributions\nfor Bayesian models and imputation of missing data. We introduce a framework\nthat leverages LLMs to enhance Bayesian workflows by eliciting expert-like\nprior knowledge and imputing missing data. Tested on diverse datasets, this\napproach can improve predictive accuracy and reduce data requirements, offering\nsignificant potential in healthcare, environmental science and engineering\napplications. We discuss the implications and challenges of treating LLMs as\n'experts'."
                },
                "authors": [
                    {
                        "name": "David Selby"
                    },
                    {
                        "name": "Kai Spriestersbach"
                    },
                    {
                        "name": "Yuichiro Iwashita"
                    },
                    {
                        "name": "Mohammad Saad"
                    },
                    {
                        "name": "Dennis Bappert"
                    },
                    {
                        "name": "Archana Warrier"
                    },
                    {
                        "name": "Sumantrak Mukherjee"
                    },
                    {
                        "name": "Koichi Kise"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Vollmer"
                },
                "author": "Sebastian Vollmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04030v1",
                "updated": "2025-02-06T12:47:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    47,
                    25,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:47:25Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    47,
                    25,
                    3,
                    37,
                    0
                ],
                "title": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated\n  Model Merging"
                },
                "summary": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04029v1",
                "updated": "2025-02-06T12:43:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    43,
                    9,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:43:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    43,
                    9,
                    3,
                    37,
                    0
                ],
                "title": "Echo-Teddy: Preliminary Design and Development of Large Language\n  Model-based Social Robot for Autistic Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo-Teddy: Preliminary Design and Development of Large Language\n  Model-based Social Robot for Autistic Students"
                },
                "summary": "Autistic students often face challenges in social interaction, which can\nhinder their educational and personal development. This study introduces\nEcho-Teddy, a Large Language Model (LLM)-based social robot designed to support\nautistic students in developing social and communication skills. Unlike\nprevious chatbot-based solutions, Echo-Teddy leverages advanced LLM\ncapabilities to provide more natural and adaptive interactions. The research\naddresses two key questions: (1) What are the design principles and initial\nprototype characteristics of an effective LLM-based social robot for autistic\nstudents? (2) What improvements can be made based on developer\nreflection-on-action and expert interviews? The study employed a mixed-methods\napproach, combining prototype development with qualitative analysis of\ndeveloper reflections and expert interviews. Key design principles identified\ninclude customizability, ethical considerations, and age-appropriate\ninteractions. The initial prototype, built on a Raspberry Pi platform, features\ncustom speech components and basic motor functions. Evaluation of the prototype\nrevealed potential improvements in areas such as user interface, educational\nvalue, and practical implementation in educational settings. This research\ncontributes to the growing field of AI-assisted special education by\ndemonstrating the potential of LLM-based social robots in supporting autistic\nstudents. The findings provide valuable insights for future developments in\naccessible and effective social support tools for special education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autistic students often face challenges in social interaction, which can\nhinder their educational and personal development. This study introduces\nEcho-Teddy, a Large Language Model (LLM)-based social robot designed to support\nautistic students in developing social and communication skills. Unlike\nprevious chatbot-based solutions, Echo-Teddy leverages advanced LLM\ncapabilities to provide more natural and adaptive interactions. The research\naddresses two key questions: (1) What are the design principles and initial\nprototype characteristics of an effective LLM-based social robot for autistic\nstudents? (2) What improvements can be made based on developer\nreflection-on-action and expert interviews? The study employed a mixed-methods\napproach, combining prototype development with qualitative analysis of\ndeveloper reflections and expert interviews. Key design principles identified\ninclude customizability, ethical considerations, and age-appropriate\ninteractions. The initial prototype, built on a Raspberry Pi platform, features\ncustom speech components and basic motor functions. Evaluation of the prototype\nrevealed potential improvements in areas such as user interface, educational\nvalue, and practical implementation in educational settings. This research\ncontributes to the growing field of AI-assisted special education by\ndemonstrating the potential of LLM-based social robots in supporting autistic\nstudents. The findings provide valuable insights for future developments in\naccessible and effective social support tools for special education."
                },
                "authors": [
                    {
                        "name": "Unggi Lee"
                    },
                    {
                        "name": "Hansung Kim"
                    },
                    {
                        "name": "Juhong Eom"
                    },
                    {
                        "name": "Hyeonseo Jeong"
                    },
                    {
                        "name": "Seungyeon Lee"
                    },
                    {
                        "name": "Gyuri Byun"
                    },
                    {
                        "name": "Yunseo Lee"
                    },
                    {
                        "name": "Minji Kang"
                    },
                    {
                        "name": "Gospel Kim"
                    },
                    {
                        "name": "Jihoi Na"
                    },
                    {
                        "name": "Jewoong Moon"
                    },
                    {
                        "name": "Hyeoncheol Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeoncheol Kim"
                },
                "author": "Hyeoncheol Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08001v3",
                "updated": "2025-02-06T12:37:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    37,
                    15,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-10T14:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    57,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic\n  Manipulation"
                },
                "summary": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for versatile robotic systems to operate in diverse and\ndynamic environments has emphasized the importance of a generalist policy,\nwhich leverages a large cross-embodiment data corpus to facilitate broad\nadaptability and high-level reasoning. However, the generalist would struggle\nwith inefficient inference and cost-expensive training. The specialist policy,\ninstead, is curated for specific domain data and excels at task-level precision\nwith efficiency. Yet, it lacks the generalization capacity for a wide range of\napplications. Inspired by these observations, we introduce RoboDual, a\nsynergistic dual-system that supplements the merits of both generalist and\nspecialist policy. A diffusion transformer-based specialist is devised for\nmulti-step action rollouts, exquisitely conditioned on the high-level task\nunderstanding and discretized action output of a vision-language-action (VLA)\nbased generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in\nreal-world setting and 12% gain on CALVIN by introducing a specialist policy\nwith merely 20M trainable parameters. It maintains strong performance with 5%\nof demonstration data only, and enables a 3.8 times higher control frequency in\nreal-world deployment. Code would be made publicly available. Our project page\nis hosted at: https://opendrivelab.com/RoboDual/"
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Jia Zeng"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "Project page: https://opendrivelab.com/RoboDual/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04022v1",
                "updated": "2025-02-06T12:25:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    25,
                    16,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:25:16Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    25,
                    16,
                    3,
                    37,
                    0
                ],
                "title": "Quantification of Biodiversity from Historical Survey Text with\n  LLM-based Best-Worst Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantification of Biodiversity from Historical Survey Text with\n  LLM-based Best-Worst Scaling"
                },
                "summary": "In this study, we evaluate methods to determine the frequency of species via\nquantity estimation from historical survey text. To that end, we formulate\nclassification tasks and finally show that this problem can be adequately\nframed as a regression task using Best-Worst Scaling (BWS) with Large Language\nModels (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the\nlatter two have reasonable agreement with humans and each other. We conclude\nthat this approach is more cost-effective and similarly robust compared to a\nfine-grained multi-class approach, allowing automated quantity estimation\nacross species.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we evaluate methods to determine the frequency of species via\nquantity estimation from historical survey text. To that end, we formulate\nclassification tasks and finally show that this problem can be adequately\nframed as a regression task using Best-Worst Scaling (BWS) with Large Language\nModels (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the\nlatter two have reasonable agreement with humans and each other. We conclude\nthat this approach is more cost-effective and similarly robust compared to a\nfine-grained multi-class approach, allowing automated quantity estimation\nacross species."
                },
                "authors": [
                    {
                        "name": "Thomas Haider"
                    },
                    {
                        "name": "Tobias Perschl"
                    },
                    {
                        "name": "Malte Rehbein"
                    }
                ],
                "author_detail": {
                    "name": "Malte Rehbein"
                },
                "author": "Malte Rehbein",
                "arxiv_comment": "NoDaLiDa 2025, EcoNLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11551v3",
                "updated": "2025-02-06T12:17:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    17,
                    6,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-20T15:39:39Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    15,
                    39,
                    39,
                    0,
                    20,
                    0
                ],
                "title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation"
                },
                "summary": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "38 pages, 18 figures, technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04009v1",
                "updated": "2025-02-06T12:11:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    11,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:11:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    11,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "A Critical Analysis of Deployed Use Cases for Quantum Key Distribution\n  and Comparison with Post-Quantum Cryptography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Analysis of Deployed Use Cases for Quantum Key Distribution\n  and Comparison with Post-Quantum Cryptography"
                },
                "summary": "Quantum Key Distribution (QKD) is currently being discussed as a technology\nto safeguard communication in a future where quantum computers compromise\ntraditional public-key cryptosystems. In this paper, we conduct a comprehensive\nsecurity evaluation of QKD-based solutions, focusing on real-world use cases\nsourced from academic literature and industry reports. We analyze these use\ncases, assess their security and identify the possible advantages of deploying\nQKD-based solutions. We further compare QKD-based solutions with Post-Quantum\nCryptography (PQC), the alternative approach to achieving security when quantum\ncomputers compromise traditional public-key cryptosystems, evaluating their\nrespective suitability for each scenario. Based on this comparative analysis,\nwe critically discuss and comment on which use cases QKD is suited for,\nconsidering factors such as implementation complexity, scalability, and\nlong-term security. Our findings contribute to a better understanding of the\nrole QKD could play in future cryptographic infrastructures and offer guidance\nto decision-makers considering the deployment of QKD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) is currently being discussed as a technology\nto safeguard communication in a future where quantum computers compromise\ntraditional public-key cryptosystems. In this paper, we conduct a comprehensive\nsecurity evaluation of QKD-based solutions, focusing on real-world use cases\nsourced from academic literature and industry reports. We analyze these use\ncases, assess their security and identify the possible advantages of deploying\nQKD-based solutions. We further compare QKD-based solutions with Post-Quantum\nCryptography (PQC), the alternative approach to achieving security when quantum\ncomputers compromise traditional public-key cryptosystems, evaluating their\nrespective suitability for each scenario. Based on this comparative analysis,\nwe critically discuss and comment on which use cases QKD is suited for,\nconsidering factors such as implementation complexity, scalability, and\nlong-term security. Our findings contribute to a better understanding of the\nrole QKD could play in future cryptographic infrastructures and offer guidance\nto decision-makers considering the deployment of QKD."
                },
                "authors": [
                    {
                        "name": "Nick Aquina"
                    },
                    {
                        "name": "Bruno Cimoli"
                    },
                    {
                        "name": "Soumya Das"
                    },
                    {
                        "name": "Kathrin Hövelmanns"
                    },
                    {
                        "name": "Fiona Johanna Weber"
                    },
                    {
                        "name": "Chigo Okonkwo"
                    },
                    {
                        "name": "Simon Rommel"
                    },
                    {
                        "name": "Boris Škorić"
                    },
                    {
                        "name": "Idelfonso Tafur Monroy"
                    },
                    {
                        "name": "Sebastian Verschoor"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Verschoor"
                },
                "author": "Sebastian Verschoor",
                "arxiv_comment": "Submitted to EPJ Quantum Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04008v1",
                "updated": "2025-02-06T12:10:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    10,
                    1,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:10:01Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    10,
                    1,
                    3,
                    37,
                    0
                ],
                "title": "Automating a Complete Software Test Process Using LLMs: An Automotive\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating a Complete Software Test Process Using LLMs: An Automotive\n  Case Study"
                },
                "summary": "Vehicle API testing verifies whether the interactions between a vehicle's\ninternal systems and external applications meet expectations, ensuring that\nusers can access and control various vehicle functions and data. However, this\ntask is inherently complex, requiring the alignment and coordination of API\nsystems, communication protocols, and even vehicle simulation systems to\ndevelop valid test cases. In practical industrial scenarios, inconsistencies,\nambiguities, and interdependencies across various documents and system\nspecifications pose significant challenges. This paper presents a system\ndesigned for the automated testing of in-vehicle APIs. By clearly defining and\nsegmenting the testing process, we enable Large Language Models (LLMs) to focus\non specific tasks, ensuring a stable and controlled testing workflow.\nExperiments conducted on over 100 APIs demonstrate that our system effectively\nautomates vehicle API testing. The results also confirm that LLMs can\nefficiently handle mundane tasks requiring human judgment, making them suitable\nfor complete automation in similar industrial contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle API testing verifies whether the interactions between a vehicle's\ninternal systems and external applications meet expectations, ensuring that\nusers can access and control various vehicle functions and data. However, this\ntask is inherently complex, requiring the alignment and coordination of API\nsystems, communication protocols, and even vehicle simulation systems to\ndevelop valid test cases. In practical industrial scenarios, inconsistencies,\nambiguities, and interdependencies across various documents and system\nspecifications pose significant challenges. This paper presents a system\ndesigned for the automated testing of in-vehicle APIs. By clearly defining and\nsegmenting the testing process, we enable Large Language Models (LLMs) to focus\non specific tasks, ensuring a stable and controlled testing workflow.\nExperiments conducted on over 100 APIs demonstrate that our system effectively\nautomates vehicle API testing. The results also confirm that LLMs can\nefficiently handle mundane tasks requiring human judgment, making them suitable\nfor complete automation in similar industrial contexts."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yinan Yu"
                    },
                    {
                        "name": "Robert Feldt"
                    },
                    {
                        "name": "Dhasarathy Parthasarathy"
                    }
                ],
                "author_detail": {
                    "name": "Dhasarathy Parthasarathy"
                },
                "author": "Dhasarathy Parthasarathy",
                "arxiv_comment": "Accepted by International Conference on Software Engineering (ICSE)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09243v3",
                "updated": "2025-02-06T12:03:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    3,
                    33,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-12T12:53:30Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    53,
                    30,
                    3,
                    347,
                    0
                ],
                "title": "SPRec: Self-Play to Debias LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRec: Self-Play to Debias LLM-based Recommendation"
                },
                "summary": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current work primarily applies supervised fine-tuning\n(SFT) to adapt the model for recommendation tasks. However, SFT on positive\nexamples only limits the model's ability to align with user preference. To\naddress this, researchers recently introduced Direct Preference Optimization\n(DPO), which explicitly aligns LLMs with user preferences using offline\npreference ranking data. However, we found that DPO inherently biases the model\ntowards a few items, exacerbating the filter bubble issue and ultimately\ndegrading user experience.\n  In this paper, we propose SPRec, a novel self-play framework designed to\nmitigate over-recommendation and improve fairness without requiring additional\ndata or manual intervention. In each self-play iteration, the model undergoes\nan SFT step followed by a DPO step, treating offline interaction data as\npositive samples and the predicted outputs from the previous iteration as\nnegative samples. This effectively re-weights the DPO loss function using the\nmodel's logits, adaptively suppressing biased items. Extensive experiments on\nmultiple real-world datasets demonstrate SPRec's effectiveness in enhancing\nrecommendation accuracy and fairness. The implementation is available via\nhttps://github.com/RegionCh/SPRec",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have attracted significant attention in\nrecommendation systems. Current work primarily applies supervised fine-tuning\n(SFT) to adapt the model for recommendation tasks. However, SFT on positive\nexamples only limits the model's ability to align with user preference. To\naddress this, researchers recently introduced Direct Preference Optimization\n(DPO), which explicitly aligns LLMs with user preferences using offline\npreference ranking data. However, we found that DPO inherently biases the model\ntowards a few items, exacerbating the filter bubble issue and ultimately\ndegrading user experience.\n  In this paper, we propose SPRec, a novel self-play framework designed to\nmitigate over-recommendation and improve fairness without requiring additional\ndata or manual intervention. In each self-play iteration, the model undergoes\nan SFT step followed by a DPO step, treating offline interaction data as\npositive samples and the predicted outputs from the previous iteration as\nnegative samples. This effectively re-weights the DPO loss function using the\nmodel's logits, adaptively suppressing biased items. Extensive experiments on\nmultiple real-world datasets demonstrate SPRec's effectiveness in enhancing\nrecommendation accuracy and fairness. The implementation is available via\nhttps://github.com/RegionCh/SPRec"
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Yuanqing Yu"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_doi": "10.1145/3696410.3714524",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714524",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03997v1",
                "updated": "2025-02-06T11:57:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    57,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:57:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    57,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing"
                },
                "summary": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer Aided Design (CAD) is indispensable across various industries.\n\\emph{Text-based CAD editing}, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce \\emph{CAD-Editor}, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively."
                },
                "authors": [
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08144v2",
                "updated": "2025-02-06T11:56:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    56,
                    23,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-15T13:28:18Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    28,
                    18,
                    3,
                    228,
                    0
                ],
                "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for\n  Multi-turn NLU"
                },
                "summary": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models(LLMs) can generate coherent and contextually\nrelevant text, they often struggle to recognise the intent behind the human\nuser's query. Natural Language Understanding (NLU) models, however, interpret\nthe purpose and key information of user's input to enable responsive\ninteractions. Existing NLU models generally map individual utterances to a\ndual-level semantic frame, involving sentence-level intent and word-level slot\nlabels. However, real-life conversations primarily consist of multi-turn\nconversations, involving the interpretation of complex and extended dialogues.\nResearchers encounter challenges addressing all facets of multi-turn dialogue\nconversations using a unified single NLU model. This paper introduces a novel\napproach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. To achieve this, we construct distinct\nteachers for varying levels of conversation knowledge, namely, sentence-level\nintent detection, word-level slot filling, and conversation-level domain\nclassification. These teachers are then fine-tuned to acquire specific\nknowledge of their designated levels. A multi-teacher loss is proposed to\nfacilitate the combination of these multi-level teachers, guiding a student\nmodel in multi-turn dialogue tasks. The experimental results demonstrate the\nefficacy of our model in improving the overall multi-turn conversation\nunderstanding, showcasing the potential for advancements in NLU models through\nthe incorporation of multi-level dialogue knowledge distillation techniques."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "So-Eon Kim"
                    },
                    {
                        "name": "Seong-Bae Park"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    }
                ],
                "author_detail": {
                    "name": "Soyeon Caren Han"
                },
                "author": "Soyeon Caren Han",
                "arxiv_comment": "Accepted by NAACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01866v2",
                "updated": "2025-02-06T11:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    54,
                    35,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-02T11:54:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    11,
                    54,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "House of Cards: Massive Weights in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "House of Cards: Massive Weights in LLMs"
                },
                "summary": "Massive activations, which manifest in specific feature dimensions of hidden\nstates, introduce a significant bias in large language models (LLMs), leading\nto an overemphasis on the corresponding token. In this paper, we identify that\nmassive activations originate not from the hidden state but from the\nintermediate state of a feed-forward network module in an early layer.\nExpanding on the previous observation that massive activations occur only in\nspecific feature dimensions, we dive deep into the weights that cause massive\nactivations. Specifically, we define top-$k$ massive weights as the weights\nthat contribute to the dimensions with the top-$k$ magnitudes in the\nintermediate state. When these massive weights are set to zero, the\nfunctionality of LLMs is entirely disrupted. However, when all weights except\nfor massive weights are set to zero, it results in a relatively minor\nperformance drop, even though a much larger number of weights are set to zero.\nThis implies that during the pre-training process, learning is dominantly\nfocused on massive weights. Building on this observation, we propose a simple\nplug-and-play method called MacDrop (massive weights curriculum dropout), to\nrely less on massive weights during parameter-efficient fine-tuning. This\nmethod applies dropout to the pre-trained massive weights, starting with a high\ndropout probability and gradually decreasing it as fine-tuning progresses.\nThrough various experiments, including zero-shot downstream tasks, long-context\ntasks, and ablation studies, we demonstrate that \\texttt{MacDrop} generally\nimproves performance and strengthens robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive activations, which manifest in specific feature dimensions of hidden\nstates, introduce a significant bias in large language models (LLMs), leading\nto an overemphasis on the corresponding token. In this paper, we identify that\nmassive activations originate not from the hidden state but from the\nintermediate state of a feed-forward network module in an early layer.\nExpanding on the previous observation that massive activations occur only in\nspecific feature dimensions, we dive deep into the weights that cause massive\nactivations. Specifically, we define top-$k$ massive weights as the weights\nthat contribute to the dimensions with the top-$k$ magnitudes in the\nintermediate state. When these massive weights are set to zero, the\nfunctionality of LLMs is entirely disrupted. However, when all weights except\nfor massive weights are set to zero, it results in a relatively minor\nperformance drop, even though a much larger number of weights are set to zero.\nThis implies that during the pre-training process, learning is dominantly\nfocused on massive weights. Building on this observation, we propose a simple\nplug-and-play method called MacDrop (massive weights curriculum dropout), to\nrely less on massive weights during parameter-efficient fine-tuning. This\nmethod applies dropout to the pre-trained massive weights, starting with a high\ndropout probability and gradually decreasing it as fine-tuning progresses.\nThrough various experiments, including zero-shot downstream tasks, long-context\ntasks, and ablation studies, we demonstrate that \\texttt{MacDrop} generally\nimproves performance and strengthens robustness."
                },
                "authors": [
                    {
                        "name": "Jaehoon Oh"
                    },
                    {
                        "name": "Seungjun Shin"
                    },
                    {
                        "name": "Dokwan Oh"
                    }
                ],
                "author_detail": {
                    "name": "Dokwan Oh"
                },
                "author": "Dokwan Oh",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03994v1",
                "updated": "2025-02-06T11:51:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    51,
                    36,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:51:36Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    51,
                    36,
                    3,
                    37,
                    0
                ],
                "title": "Pre-Optimized Irregular Arrays versus Moveable Antennas in Multi-User\n  MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Optimized Irregular Arrays versus Moveable Antennas in Multi-User\n  MIMO Systems"
                },
                "summary": "Massive multiple-input multiple-output (MIMO) systems exploit the spatial\ndiversity achieved with an array of many antennas to perform spatial\nmultiplexing of many users. Similar performance can be achieved using fewer\nantennas if movable antenna (MA) elements are used instead. MA-enabled arrays\ncan dynamically change the antenna locations, mechanically or electrically, to\nachieve maximum spatial diversity for the current propagation conditions.\nHowever, optimizing the antenna locations for each channel realization is\ncomputationally excessive, requires channel knowledge for all conceivable\nlocations, and requires rapid antenna movements, thus making real-time\nimplementation cumbersome. To overcome these challenges, we propose a\npre-optimized irregular array (PIA) concept, where the antenna locations at the\nbase station are optimized a priori for a given coverage area. The objective is\nto maximize the average sum rate and we take a particle swarm optimization\napproach to solve it. Simulation results show that PIA achieves performance\ncomparable to MA-enabled arrays while outperforming traditional uniform arrays.\nHence, PIA offers a fixed yet efficient array deployment approach without the\ncomplexities associated with MA-enabled arrays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive multiple-input multiple-output (MIMO) systems exploit the spatial\ndiversity achieved with an array of many antennas to perform spatial\nmultiplexing of many users. Similar performance can be achieved using fewer\nantennas if movable antenna (MA) elements are used instead. MA-enabled arrays\ncan dynamically change the antenna locations, mechanically or electrically, to\nachieve maximum spatial diversity for the current propagation conditions.\nHowever, optimizing the antenna locations for each channel realization is\ncomputationally excessive, requires channel knowledge for all conceivable\nlocations, and requires rapid antenna movements, thus making real-time\nimplementation cumbersome. To overcome these challenges, we propose a\npre-optimized irregular array (PIA) concept, where the antenna locations at the\nbase station are optimized a priori for a given coverage area. The objective is\nto maximize the average sum rate and we take a particle swarm optimization\napproach to solve it. Simulation results show that PIA achieves performance\ncomparable to MA-enabled arrays while outperforming traditional uniform arrays.\nHence, PIA offers a fixed yet efficient array deployment approach without the\ncomplexities associated with MA-enabled arrays."
                },
                "authors": [
                    {
                        "name": "Amna Irshad"
                    },
                    {
                        "name": "Alva Kosasih"
                    },
                    {
                        "name": "Vitaly Petrov"
                    },
                    {
                        "name": "Emil Björnson"
                    }
                ],
                "author_detail": {
                    "name": "Emil Björnson"
                },
                "author": "Emil Björnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03992v1",
                "updated": "2025-02-06T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    47,
                    58,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T11:47:58Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    47,
                    58,
                    3,
                    37,
                    0
                ],
                "title": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge\n  Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge\n  Graph Question Answering"
                },
                "summary": "Most existing Knowledge Graph Question Answering (KGQA) approaches are\ndesigned for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the\nheterogeneity of the underlying graph schema, topology and assertions, most\nKGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without\nresource-intensive training data. We present OntoSCPrompt, a novel Large\nLanguage Model (LLM)-based KGQA approach with a two-stage architecture that\nseparates semantic parsing from KG-dependent interactions. OntoSCPrompt first\ngenerates a SPARQL query structure (including SPARQL keywords such as SELECT,\nASK, WHERE and placeholders for missing tokens) and then fills them with\nKG-specific information. To enhance the understanding of the underlying KG, we\npresent an ontology-guided, hybrid prompt learning strategy that integrates KG\nontology into the learning process of hybrid prompts (e.g., discrete and\ncontinuous vectors). We also present several task-specific decoding strategies\nto ensure the correctness and executability of generated SPARQL queries in both\nstages. Experimental results demonstrate that OntoSCPrompt performs as well as\nSOTA approaches without retraining on a number of KGQA datasets such as CWQ,\nWebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well\nto unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:\n\\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing Knowledge Graph Question Answering (KGQA) approaches are\ndesigned for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the\nheterogeneity of the underlying graph schema, topology and assertions, most\nKGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without\nresource-intensive training data. We present OntoSCPrompt, a novel Large\nLanguage Model (LLM)-based KGQA approach with a two-stage architecture that\nseparates semantic parsing from KG-dependent interactions. OntoSCPrompt first\ngenerates a SPARQL query structure (including SPARQL keywords such as SELECT,\nASK, WHERE and placeholders for missing tokens) and then fills them with\nKG-specific information. To enhance the understanding of the underlying KG, we\npresent an ontology-guided, hybrid prompt learning strategy that integrates KG\nontology into the learning process of hybrid prompts (e.g., discrete and\ncontinuous vectors). We also present several task-specific decoding strategies\nto ensure the correctness and executability of generated SPARQL queries in both\nstages. Experimental results demonstrate that OntoSCPrompt performs as well as\nSOTA approaches without retraining on a number of KGQA datasets such as CWQ,\nWebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well\nto unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:\n\\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}"
                },
                "authors": [
                    {
                        "name": "Longquan Jiang"
                    },
                    {
                        "name": "Junbo Huang"
                    },
                    {
                        "name": "Cedric Möller"
                    },
                    {
                        "name": "Ricardo Usbeck"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Usbeck"
                },
                "author": "Ricardo Usbeck",
                "arxiv_comment": "Accepted By ICSC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01269v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01269v4",
                "updated": "2025-02-06T11:12:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    12,
                    9,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-02T08:35:54Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    35,
                    54,
                    0,
                    337,
                    0
                ],
                "title": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPRM: A LLM-based Continual Pre-training Framework for Relevance\n  Modeling in Commercial Search"
                },
                "summary": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance modeling between queries and items stands as a pivotal component in\ncommercial search engines, directly affecting the user experience. Given the\nremarkable achievements of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, LLM-based relevance modeling is gradually\nbeing adopted within industrial search systems. Nevertheless, foundational LLMs\nlack domain-specific knowledge and do not fully exploit the potential of\nin-context learning. Furthermore, structured item text remains underutilized,\nand there is a shortage in the supply of corresponding queries and background\nknowledge. We thereby propose CPRM (Continual Pre-training for Relevance\nModeling), a framework designed for the continual pre-training of LLMs to\naddress these issues. Our CPRM framework includes three modules: 1) employing\nboth queries and multi-field item to jointly pre-train for enhancing domain\nknowledge, 2) applying in-context pre-training, a novel approach where LLMs are\npre-trained on a sequence of related queries or items, and 3) conducting\nreading comprehension on items to produce associated domain knowledge and\nbackground information (e.g., generating summaries and corresponding queries)\nto further strengthen LLMs. Results on offline experiments and online A/B\ntesting demonstrate that our model achieves convincing performance compared to\nstrong baselines."
                },
                "authors": [
                    {
                        "name": "Kaixin Wu"
                    },
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Baijun Ji"
                    },
                    {
                        "name": "Jia Xu"
                    },
                    {
                        "name": "Zhongyi Liu"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Linjian Mo"
                    }
                ],
                "author_detail": {
                    "name": "Linjian Mo"
                },
                "author": "Linjian Mo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01269v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01269v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17311v2",
                "updated": "2025-02-06T10:59:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    59,
                    25,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-28T21:48:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    48,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on\n  Scaled Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLPP: A Residual Method for Zero-Shot Real-World Autonomous Racing on\n  Scaled Platforms"
                },
                "summary": "Autonomous racing presents a complex environment requiring robust controllers\ncapable of making rapid decisions under dynamic conditions. While traditional\ncontrollers based on tire models are reliable, they often demand extensive\ntuning or system identification. Reinforcement Learning (RL) methods offer\nsignificant potential due to their ability to learn directly from interaction,\nyet they typically suffer from the sim-to-real gap, where policies trained in\nsimulation fail to perform effectively in the real world. In this paper, we\npropose RLPP, a residual RL framework that enhances a Pure Pursuit (PP)\ncontroller with an RL-based residual. This hybrid approach leverages the\nreliability and interpretability of PP while using RL to fine-tune the\ncontroller's performance in real-world scenarios. Extensive testing on the\nF1TENTH platform demonstrates that RLPP improves lap times of the baseline\ncontrollers by up to 6.37 %, closing the gap to the State-of-the-Art methods by\nmore than 52 % and providing reliable performance in zero-shot real-world\ndeployment, overcoming key challenges associated with the sim-to-real transfer\nand reducing the performance gap from simulation to reality by more than 8-fold\nwhen compared to the baseline RL controller. The RLPP framework is made\navailable as an open-source tool, encouraging further exploration and\nadvancement in autonomous racing research. The code is available at:\nwww.github.com/forzaeth/rlpp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous racing presents a complex environment requiring robust controllers\ncapable of making rapid decisions under dynamic conditions. While traditional\ncontrollers based on tire models are reliable, they often demand extensive\ntuning or system identification. Reinforcement Learning (RL) methods offer\nsignificant potential due to their ability to learn directly from interaction,\nyet they typically suffer from the sim-to-real gap, where policies trained in\nsimulation fail to perform effectively in the real world. In this paper, we\npropose RLPP, a residual RL framework that enhances a Pure Pursuit (PP)\ncontroller with an RL-based residual. This hybrid approach leverages the\nreliability and interpretability of PP while using RL to fine-tune the\ncontroller's performance in real-world scenarios. Extensive testing on the\nF1TENTH platform demonstrates that RLPP improves lap times of the baseline\ncontrollers by up to 6.37 %, closing the gap to the State-of-the-Art methods by\nmore than 52 % and providing reliable performance in zero-shot real-world\ndeployment, overcoming key challenges associated with the sim-to-real transfer\nand reducing the performance gap from simulation to reality by more than 8-fold\nwhen compared to the baseline RL controller. The RLPP framework is made\navailable as an open-source tool, encouraging further exploration and\nadvancement in autonomous racing research. The code is available at:\nwww.github.com/forzaeth/rlpp."
                },
                "authors": [
                    {
                        "name": "Edoardo Ghignone"
                    },
                    {
                        "name": "Nicolas Baumann"
                    },
                    {
                        "name": "Cheng Hu"
                    },
                    {
                        "name": "Jonathan Wang"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Andrea Carron"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_comment": "This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025. The\n  code is available at: www.github.com/forzaeth/rlpp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03964v1",
                "updated": "2025-02-06T10:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    57,
                    5,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    57,
                    5,
                    3,
                    37,
                    0
                ],
                "title": "\"It Warned Me Just at the Right Moment\": Exploring LLM-based Real-time\n  Detection of Phone Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It Warned Me Just at the Right Moment\": Exploring LLM-based Real-time\n  Detection of Phone Scams"
                },
                "summary": "Despite living in the era of the internet, phone-based scams remain one of\nthe most prevalent forms of scams. These scams aim to exploit victims for\nfinancial gain, causing both monetary losses and psychological distress. While\ngovernments, industries, and academia have actively introduced various\ncountermeasures, scammers also continue to evolve their tactics, making phone\nscams a persistent threat. To combat these increasingly sophisticated scams,\ndetection technologies must also advance. In this work, we propose a framework\nfor modeling scam calls and introduce an LLM-based real-time detection\napproach, which assesses fraudulent intent in conversations, further providing\nimmediate warnings to users to mitigate harm. Through experiments, we evaluate\nthe method's performance and analyze key factors influencing its effectiveness.\nThis analysis enables us to refine the method to improve precision while\nexploring the trade-off between recall and timeliness, paving the way for\nfuture directions in this critical area of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite living in the era of the internet, phone-based scams remain one of\nthe most prevalent forms of scams. These scams aim to exploit victims for\nfinancial gain, causing both monetary losses and psychological distress. While\ngovernments, industries, and academia have actively introduced various\ncountermeasures, scammers also continue to evolve their tactics, making phone\nscams a persistent threat. To combat these increasingly sophisticated scams,\ndetection technologies must also advance. In this work, we propose a framework\nfor modeling scam calls and introduce an LLM-based real-time detection\napproach, which assesses fraudulent intent in conversations, further providing\nimmediate warnings to users to mitigate harm. Through experiments, we evaluate\nthe method's performance and analyze key factors influencing its effectiveness.\nThis analysis enables us to refine the method to improve precision while\nexploring the trade-off between recall and timeliness, paving the way for\nfuture directions in this critical area of research."
                },
                "authors": [
                    {
                        "name": "Zitong Shen"
                    },
                    {
                        "name": "Sineng Yan"
                    },
                    {
                        "name": "Youqian Zhang"
                    },
                    {
                        "name": "Xiapu Luo"
                    },
                    {
                        "name": "Grace Ngai"
                    },
                    {
                        "name": "Eugene Yujun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Yujun Fu"
                },
                "author": "Eugene Yujun Fu",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03954v1",
                "updated": "2025-02-06T10:46:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    46,
                    19,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:46:19Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    46,
                    19,
                    3,
                    37,
                    0
                ],
                "title": "MAQInstruct: Instruction-based Unified Event Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQInstruct: Instruction-based Unified Event Relation Extraction"
                },
                "summary": "Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "Accepted by WWW 2025 short",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20005v2",
                "updated": "2025-02-06T10:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    37,
                    17,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-28T04:01:30Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    4,
                    1,
                    30,
                    5,
                    363,
                    0
                ],
                "title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction\n  System"
                },
                "summary": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneKE, a dockerized schema-guided knowledge extraction system,\nwhich can extract knowledge from the Web and raw PDF Books, and support various\ndomains (science, news, etc.). Specifically, we design OneKE with multiple\nagents and a configure knowledge base. Different agents perform their\nrespective roles, enabling support for various extraction scenarios. The\nconfigure knowledge base facilitates schema configuration, error case debugging\nand correction, further improving the performance. Empirical evaluations on\nbenchmark datasets demonstrate OneKE's efficacy, while case studies further\nelucidate its adaptability to diverse tasks across multiple domains,\nhighlighting its potential for broad applications. We have open-sourced the\nCode at https://github.com/zjunlp/OneKE and released a Video at\nhttp://oneke.openkg.cn/demo.mp4."
                },
                "authors": [
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Xiangyuan Ru"
                    },
                    {
                        "name": "Kangwei Liu"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "WWW 2025 Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03945v1",
                "updated": "2025-02-06T10:33:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    33,
                    7,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T10:33:07Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    33,
                    7,
                    3,
                    37,
                    0
                ],
                "title": "Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English\n  Conversations in Healthcare and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English\n  Conversations in Healthcare and Beyond"
                },
                "summary": "Speech technologies are transforming interactions across various sectors,\nfrom healthcare to call centers and robots, yet their performance on\nAfrican-accented conversations remains underexplored. We introduce\nAfrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical\nAfrican-accented English conversations, designed to evaluate automatic speech\nrecognition (ASR) and related technologies. We assess state-of-the-art (SOTA)\nspeaker diarization and ASR systems on long-form, accented speech, comparing\ntheir performance with native accents and discover a 10%+ performance\ndegradation. Additionally, we explore medical conversation summarization\ncapabilities of large language models (LLMs) to demonstrate the impact of ASR\nerrors on downstream medical summaries, providing insights into the challenges\nand opportunities for speech technologies in the Global South. Our work\nhighlights the need for more inclusive datasets to advance conversational AI in\nlow-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech technologies are transforming interactions across various sectors,\nfrom healthcare to call centers and robots, yet their performance on\nAfrican-accented conversations remains underexplored. We introduce\nAfrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical\nAfrican-accented English conversations, designed to evaluate automatic speech\nrecognition (ASR) and related technologies. We assess state-of-the-art (SOTA)\nspeaker diarization and ASR systems on long-form, accented speech, comparing\ntheir performance with native accents and discover a 10%+ performance\ndegradation. Additionally, we explore medical conversation summarization\ncapabilities of large language models (LLMs) to demonstrate the impact of ASR\nerrors on downstream medical summaries, providing insights into the challenges\nand opportunities for speech technologies in the Global South. Our work\nhighlights the need for more inclusive datasets to advance conversational AI in\nlow-resource settings."
                },
                "authors": [
                    {
                        "name": "Mardhiyah Sanni"
                    },
                    {
                        "name": "Tassallah Abdullahi"
                    },
                    {
                        "name": "Devendra D. Kayande"
                    },
                    {
                        "name": "Emmanuel Ayodele"
                    },
                    {
                        "name": "Naome A. Etori"
                    },
                    {
                        "name": "Michael S. Mollel"
                    },
                    {
                        "name": "Moshood Yekini"
                    },
                    {
                        "name": "Chibuzor Okocha"
                    },
                    {
                        "name": "Lukman E. Ismaila"
                    },
                    {
                        "name": "Folafunmi Omofoye"
                    },
                    {
                        "name": "Boluwatife A. Adewale"
                    },
                    {
                        "name": "Tobi Olatunji"
                    }
                ],
                "author_detail": {
                    "name": "Tobi Olatunji"
                },
                "author": "Tobi Olatunji",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v6",
                "updated": "2025-02-06T09:56:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    56,
                    31,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "NAACL 2025 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03916v1",
                "updated": "2025-02-06T09:48:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    48,
                    4,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T09:48:04Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    48,
                    4,
                    3,
                    37,
                    0
                ],
                "title": "Experiments with Large Language Models on Retrieval-Augmented Generation\n  for Closed-Source Simulation Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments with Large Language Models on Retrieval-Augmented Generation\n  for Closed-Source Simulation Software"
                },
                "summary": "Large Language Models (LLMs) are increasingly helpful in text generation,\neven writing code in programming languages based on user prompts written in\nnatural language. They are even applied to generate simulation models for\nmultibody systems from natural language. Research results suggest that LLMs\nsurpass the mere replication of existing code examples, where some LLMs have\nbeen trained on an open-source multibody simulation code. However, for\nclosed-source simulation software, such results are not to be expected as their\nideas and concepts might differ from other publicly available ones. LLMs can\nhallucinate for knowledge-intensive tasks, such as model creation, which can\nlead to wrong responses. This is especially the case for the LLM unknown\nclosed-source simulation software. The same applies to other internal knowledge\nkept private to protect intellectual property or data privacy. The\nRetrieval-Augmented Generation (RAG) approach might yield a solution for these\nknowledge-intensive tasks. This paper explores the application of RAG to\nclosed-source simulation software and presents first experiments. After a brief\nintroduction to LLMs, the RAG approach, and the simulation method applied by\nthe close-source simulation software, several examples are provided to test\nLLMs' knowledge of the simulation software and the creation of simulation\nmodels using two RAG systems. The examples show promising results indicating\nthe benefits of applying RAG systems to closed-source simulation software,\nhelping to access their knowledge. Nevertheless, they also reveal gaps in the\napplied information and open questions for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly helpful in text generation,\neven writing code in programming languages based on user prompts written in\nnatural language. They are even applied to generate simulation models for\nmultibody systems from natural language. Research results suggest that LLMs\nsurpass the mere replication of existing code examples, where some LLMs have\nbeen trained on an open-source multibody simulation code. However, for\nclosed-source simulation software, such results are not to be expected as their\nideas and concepts might differ from other publicly available ones. LLMs can\nhallucinate for knowledge-intensive tasks, such as model creation, which can\nlead to wrong responses. This is especially the case for the LLM unknown\nclosed-source simulation software. The same applies to other internal knowledge\nkept private to protect intellectual property or data privacy. The\nRetrieval-Augmented Generation (RAG) approach might yield a solution for these\nknowledge-intensive tasks. This paper explores the application of RAG to\nclosed-source simulation software and presents first experiments. After a brief\nintroduction to LLMs, the RAG approach, and the simulation method applied by\nthe close-source simulation software, several examples are provided to test\nLLMs' knowledge of the simulation software and the creation of simulation\nmodels using two RAG systems. The examples show promising results indicating\nthe benefits of applying RAG systems to closed-source simulation software,\nhelping to access their knowledge. Nevertheless, they also reveal gaps in the\napplied information and open questions for further research."
                },
                "authors": [
                    {
                        "name": "Andreas Baumann"
                    },
                    {
                        "name": "Peter Eberhard"
                    }
                ],
                "author_detail": {
                    "name": "Peter Eberhard"
                },
                "author": "Peter Eberhard",
                "arxiv_comment": "11 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00764v3",
                "updated": "2025-02-06T09:17:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    17,
                    39,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-01T17:59:46Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    17,
                    59,
                    46,
                    3,
                    214,
                    0
                ],
                "title": "AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation"
                },
                "summary": "Large Language Model-based agents have garnered significant attention and are\nbecoming increasingly popular. Furthermore, planning ability is a crucial\ncomponent of an LLM-based agent, which generally entails achieving a desired\ngoal from an initial state. This paper investigates enhancing the planning\nabilities of LLMs through instruction tuning, referred to as agent training.\nRecent studies have demonstrated that utilizing expert-level trajectory for\ninstruction-tuning LLMs effectively enhances their planning capabilities.\nHowever, existing work primarily focuses on synthesizing trajectories from\nmanually designed planning tasks and environments. The labor-intensive nature\nof creating these environments and tasks impedes the generation of sufficiently\nvaried and extensive trajectories. To address this limitation, this paper\nexplores the automated synthesis of diverse environments and a gradual range of\nplanning tasks, from easy to difficult. We introduce a framework, AgentGen,\nthat leverages LLMs first to generate environments and subsequently generate\nplanning tasks conditioned on these environments. Specifically, to improve\nenvironmental diversity, we propose using an inspiration corpus composed of\nvarious domain-specific text segments as the context for synthesizing\nenvironments. Moreover, to increase the difficulty diversity of generated\nplanning tasks, we propose a bidirectional evolution method, Bi-Evol, that\nevolves planning tasks from easier and harder directions to synthesize a task\nset with a smoother difficulty curve. The evaluation results derived from\nAgentBoard show that AgentGen greatly improves LLMs' planning ability, e.g.,\nthe AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall\nperformance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves\nstate-of-the-art results in planning tasks. Project page:\nhttps://agent-gen.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based agents have garnered significant attention and are\nbecoming increasingly popular. Furthermore, planning ability is a crucial\ncomponent of an LLM-based agent, which generally entails achieving a desired\ngoal from an initial state. This paper investigates enhancing the planning\nabilities of LLMs through instruction tuning, referred to as agent training.\nRecent studies have demonstrated that utilizing expert-level trajectory for\ninstruction-tuning LLMs effectively enhances their planning capabilities.\nHowever, existing work primarily focuses on synthesizing trajectories from\nmanually designed planning tasks and environments. The labor-intensive nature\nof creating these environments and tasks impedes the generation of sufficiently\nvaried and extensive trajectories. To address this limitation, this paper\nexplores the automated synthesis of diverse environments and a gradual range of\nplanning tasks, from easy to difficult. We introduce a framework, AgentGen,\nthat leverages LLMs first to generate environments and subsequently generate\nplanning tasks conditioned on these environments. Specifically, to improve\nenvironmental diversity, we propose using an inspiration corpus composed of\nvarious domain-specific text segments as the context for synthesizing\nenvironments. Moreover, to increase the difficulty diversity of generated\nplanning tasks, we propose a bidirectional evolution method, Bi-Evol, that\nevolves planning tasks from easier and harder directions to synthesize a task\nset with a smoother difficulty curve. The evaluation results derived from\nAgentBoard show that AgentGen greatly improves LLMs' planning ability, e.g.,\nthe AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall\nperformance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves\nstate-of-the-art results in planning tasks. Project page:\nhttps://agent-gen.github.io/."
                },
                "authors": [
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Jianguang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "Accepted by KDD 2025 (Research Track). Project page:\n  https://agent-gen.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03885v1",
                "updated": "2025-02-06T09:01:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    1,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T09:01:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    1,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfinitePOD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers"
                },
                "summary": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfinitePOD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfinitePOD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfinitePOD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfinitePOD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfinitePOD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfinitePOD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node)."
                },
                "authors": [
                    {
                        "name": "Chenchen Shou"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Huaiyu Meng"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yinmin Jiang"
                    },
                    {
                        "name": "Wenqing Lv"
                    },
                    {
                        "name": "Yelong Xu"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yichen Shen"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03884v1",
                "updated": "2025-02-06T08:58:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    58,
                    3,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T08:58:03Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    58,
                    3,
                    3,
                    37,
                    0
                ],
                "title": "Rank Also Matters: Hierarchical Configuration for Mixture of Adapter\n  Experts in LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank Also Matters: Hierarchical Configuration for Mixture of Adapter\n  Experts in LLM Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable success across\nvarious tasks, accompanied by a continuous increase in their parameter size.\nParameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), address the challenges of fine-tuning LLMs by significantly reducing\nthe number of trainable parameters. Recent studies have integrated LoRA with\nMixture of Experts (MoE) architectures, leveraging multiple adapter experts and\ngating mechanisms to further improve fine-tuning performance. However, existing\napproaches primarily focus on adjusting the allocations of adapter experts per\nlayer to optimize the introduced trainable parameter size, while neglecting a\ncritical factor of adapters' rank. To this end, we propose a hierarchical\nscheme for expert allocation and rank configuration, HILO, which dynamically\nadjusts the number and rank of adapter experts across layers, matching the\nvarying representational complexity of model layers in adapter-granularity.\nExtensive experiments on multiple benchmark tasks demonstrate that HILO\noutperforms existing methods in accuracy while introducing fewer trainable\nparameters, providing an efficient and practical solution for fine-tuning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable success across\nvarious tasks, accompanied by a continuous increase in their parameter size.\nParameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), address the challenges of fine-tuning LLMs by significantly reducing\nthe number of trainable parameters. Recent studies have integrated LoRA with\nMixture of Experts (MoE) architectures, leveraging multiple adapter experts and\ngating mechanisms to further improve fine-tuning performance. However, existing\napproaches primarily focus on adjusting the allocations of adapter experts per\nlayer to optimize the introduced trainable parameter size, while neglecting a\ncritical factor of adapters' rank. To this end, we propose a hierarchical\nscheme for expert allocation and rank configuration, HILO, which dynamically\nadjusts the number and rank of adapter experts across layers, matching the\nvarying representational complexity of model layers in adapter-granularity.\nExtensive experiments on multiple benchmark tasks demonstrate that HILO\noutperforms existing methods in accuracy while introducing fewer trainable\nparameters, providing an efficient and practical solution for fine-tuning LLMs."
                },
                "authors": [
                    {
                        "name": "Peizhuang Cong"
                    },
                    {
                        "name": "Wenpu Liu"
                    },
                    {
                        "name": "Wenhan Yu"
                    },
                    {
                        "name": "Haochen Zhao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03882v1",
                "updated": "2025-02-06T08:55:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    55,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T08:55:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    55,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "Hierarchical Entropic Diffusion for Ransomware Detection: A\n  Probabilistic Approach to Behavioral Anomaly Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Entropic Diffusion for Ransomware Detection: A\n  Probabilistic Approach to Behavioral Anomaly Isolation"
                },
                "summary": "The increasing complexity of cryptographic extortion techniques has\nnecessitated the development of adaptive detection frameworks capable of\nidentifying adversarial encryption behaviors without reliance on predefined\nsignatures. Hierarchical Entropic Diffusion (HED) introduces a structured\nentropy-based anomaly classification mechanism that systematically tracks\nfluctuations in entropy evolution to differentiate between benign cryptographic\nprocesses and unauthorized encryption attempts. The integration of hierarchical\nclustering, entropy profiling, and probabilistic diffusion modeling refines\ndetection granularity, ensuring that encryption anomalies are identified\ndespite obfuscation strategies or incremental execution methodologies.\nExperimental evaluations demonstrated that HED maintained high classification\naccuracy across diverse ransomware families, outperforming traditional\nheuristic-based and signature-driven approaches while reducing false positive\noccurrences. Comparative analysis highlighted that entropy-driven anomaly\nsegmentation improved detection efficiency under variable system workload\nconditions, ensuring real-time classification feasibility. The computational\noverhead associated with entropy anomaly detection remained within operational\nconstraints, reinforcing the suitability of entropy-driven classification for\nlarge-scale deployment. The ability to identify adversarial entropy\nmanipulations before encryption completion contributes to broader cybersecurity\ndefenses, offering a structured methodology for isolating unauthorized\ncryptographic activities within heterogeneous computing environments. The\nresults further emphasized that entropy evolution modeling facilitates\npredictive anomaly detection, enhancing resilience against encryption evasion\ntechniques designed to circumvent traditional detection mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of cryptographic extortion techniques has\nnecessitated the development of adaptive detection frameworks capable of\nidentifying adversarial encryption behaviors without reliance on predefined\nsignatures. Hierarchical Entropic Diffusion (HED) introduces a structured\nentropy-based anomaly classification mechanism that systematically tracks\nfluctuations in entropy evolution to differentiate between benign cryptographic\nprocesses and unauthorized encryption attempts. The integration of hierarchical\nclustering, entropy profiling, and probabilistic diffusion modeling refines\ndetection granularity, ensuring that encryption anomalies are identified\ndespite obfuscation strategies or incremental execution methodologies.\nExperimental evaluations demonstrated that HED maintained high classification\naccuracy across diverse ransomware families, outperforming traditional\nheuristic-based and signature-driven approaches while reducing false positive\noccurrences. Comparative analysis highlighted that entropy-driven anomaly\nsegmentation improved detection efficiency under variable system workload\nconditions, ensuring real-time classification feasibility. The computational\noverhead associated with entropy anomaly detection remained within operational\nconstraints, reinforcing the suitability of entropy-driven classification for\nlarge-scale deployment. The ability to identify adversarial entropy\nmanipulations before encryption completion contributes to broader cybersecurity\ndefenses, offering a structured methodology for isolating unauthorized\ncryptographic activities within heterogeneous computing environments. The\nresults further emphasized that entropy evolution modeling facilitates\npredictive anomaly detection, enhancing resilience against encryption evasion\ntechniques designed to circumvent traditional detection mechanisms."
                },
                "authors": [
                    {
                        "name": "Vasili Iskorohodov"
                    },
                    {
                        "name": "Maximilian Ravensdale"
                    },
                    {
                        "name": "Matthias von Holstein"
                    },
                    {
                        "name": "Hugo Petrovic"
                    },
                    {
                        "name": "Adrian Yardley"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Yardley"
                },
                "author": "Adrian Yardley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14495v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14495v3",
                "updated": "2025-02-06T08:43:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    43,
                    15,
                    3,
                    37,
                    0
                ],
                "published": "2024-09-22T15:44:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    44,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation\n  for Logical Reading Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation\n  for Logical Reading Comprehension"
                },
                "summary": "Logical reading comprehension is a challenging task that entails grasping the\nunderlying semantics of text and applying reasoning to deduce the correct\nanswer. Prior researches have primarily focused on enhancing logical reasoning\ncapabilities through Chain-of-Thought (CoT) or data augmentation. However,\nprevious work constructing chain-of-thought rationales concentrates solely on\nanalyzing correct options, neglecting the incorrect alternatives. Addtionally,\nearlier efforts on data augmentation by altering contexts rely on rule-based\nmethods, which result in generated contexts that lack diversity and coherence.\nTo address these issues, we propose a Premise-Oriented Data Augmentation (PODA)\nframework. This framework can generate CoT rationales including analyses for\nboth correct and incorrect options, while constructing diverse and high-quality\ncounterfactual contexts from incorrect candidate options. We integrate\nsummarizing premises and identifying premises for each option into rationales.\nSubsequently, we employ multi-step prompts with identified premises to\nconstruct counterfactual context. To facilitate the model's capabilities to\nbetter differentiate the reasoning process associated with each option, we\nintroduce a novel thought-path contrastive learning method that compares\nreasoning paths between the original and counterfactual samples. Experimental\nresults on three representative LLMs demonstrate that our method can improve\nthe baselines substantially across two challenging logical reasoning benchmarks\n(ReClor and LogiQA 2.0). The data and code are released at\nhttps://github.com/lalalamdbf/TPReasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reading comprehension is a challenging task that entails grasping the\nunderlying semantics of text and applying reasoning to deduce the correct\nanswer. Prior researches have primarily focused on enhancing logical reasoning\ncapabilities through Chain-of-Thought (CoT) or data augmentation. However,\nprevious work constructing chain-of-thought rationales concentrates solely on\nanalyzing correct options, neglecting the incorrect alternatives. Addtionally,\nearlier efforts on data augmentation by altering contexts rely on rule-based\nmethods, which result in generated contexts that lack diversity and coherence.\nTo address these issues, we propose a Premise-Oriented Data Augmentation (PODA)\nframework. This framework can generate CoT rationales including analyses for\nboth correct and incorrect options, while constructing diverse and high-quality\ncounterfactual contexts from incorrect candidate options. We integrate\nsummarizing premises and identifying premises for each option into rationales.\nSubsequently, we employ multi-step prompts with identified premises to\nconstruct counterfactual context. To facilitate the model's capabilities to\nbetter differentiate the reasoning process associated with each option, we\nintroduce a novel thought-path contrastive learning method that compares\nreasoning paths between the original and counterfactual samples. Experimental\nresults on three representative LLMs demonstrate that our method can improve\nthe baselines substantially across two challenging logical reasoning benchmarks\n(ReClor and LogiQA 2.0). The data and code are released at\nhttps://github.com/lalalamdbf/TPReasoner."
                },
                "authors": [
                    {
                        "name": "Chenxu Wang"
                    },
                    {
                        "name": "Ping Jian"
                    },
                    {
                        "name": "Zhen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Yang"
                },
                "author": "Zhen Yang",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14495v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14495v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03868v1",
                "updated": "2025-02-06T08:28:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    28,
                    41,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T08:28:41Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    28,
                    41,
                    3,
                    37,
                    0
                ],
                "title": "Time-based GNSS attack detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-based GNSS attack detection"
                },
                "summary": "To safeguard Civilian Global Navigation Satellite Systems (GNSS) external\ninformation available to the platform encompassing the GNSS receiver can be\nused to detect attacks. Cross-checking the GNSS-provided time against\nalternative multiple trusted time sources can lead to attack detection aiming\nat controlling the GNSS receiver time. Leveraging external, network-connected\nsecure time providers and onboard clock references, we achieve detection even\nunder fine-grained time attacks. We provide an extensive evaluation of our\nmulti-layered defense against adversaries mounting attacks against the GNSS\nreceiver along with controlling the network link. We implement adversaries\nspanning from simplistic spoofers to advanced ones synchronized with the GNSS\nconstellation. We demonstrate attack detection is possible in all tested cases\n(sharp discontinuity, smooth take-over, and coordinated network manipulation)\nwithout changes to the structure of the GNSS receiver. Leveraging the diversity\nof the reference time sources, detection of take-over time push as low as 150us\nis possible. Smooth take-overs forcing variations as low as 30ns are also\ndetected based on on-board precision oscillators. The method (and thus the\nevaluation) is largely agnostic to the satellite constellation and the attacker\ntype, making time-based data validation of GNSS information compatible with\nexisting receivers and readily deployable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To safeguard Civilian Global Navigation Satellite Systems (GNSS) external\ninformation available to the platform encompassing the GNSS receiver can be\nused to detect attacks. Cross-checking the GNSS-provided time against\nalternative multiple trusted time sources can lead to attack detection aiming\nat controlling the GNSS receiver time. Leveraging external, network-connected\nsecure time providers and onboard clock references, we achieve detection even\nunder fine-grained time attacks. We provide an extensive evaluation of our\nmulti-layered defense against adversaries mounting attacks against the GNSS\nreceiver along with controlling the network link. We implement adversaries\nspanning from simplistic spoofers to advanced ones synchronized with the GNSS\nconstellation. We demonstrate attack detection is possible in all tested cases\n(sharp discontinuity, smooth take-over, and coordinated network manipulation)\nwithout changes to the structure of the GNSS receiver. Leveraging the diversity\nof the reference time sources, detection of take-over time push as low as 150us\nis possible. Smooth take-overs forcing variations as low as 30ns are also\ndetected based on on-board precision oscillators. The method (and thus the\nevaluation) is largely agnostic to the satellite constellation and the attacker\ntype, making time-based data validation of GNSS information compatible with\nexisting receivers and readily deployable."
                },
                "authors": [
                    {
                        "name": "Marco Spanghero"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_doi": "10.1109/TAES.2024.3516708",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TAES.2024.3516708",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE Transactions on Aerospace and Electronic Systems (Early Access)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03860v1",
                "updated": "2025-02-06T08:19:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    19,
                    59,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T08:19:59Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    19,
                    59,
                    3,
                    37,
                    0
                ],
                "title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without\n  Distillation"
                },
                "summary": "Large language models (LLMs), such as o1 from OpenAI, have demonstrated\nremarkable reasoning capabilities. o1 generates a long chain-of-thought\n(LongCoT) before answering a question. LongCoT allows LLMs to analyze problems,\ndevise plans, reflect, and backtrack effectively. These actions empower LLM to\nsolve complex problems. After the release of o1, many teams have attempted to\nreplicate its LongCoT and reasoning capabilities. In terms of methods, they\nprimarily rely on knowledge distillation with data from existing models with\nLongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving\nsignificant uncertainties on systematically developing such reasoning\nabilities. In terms of data domains, these works focus narrowly on math while a\nfew others include coding, limiting their generalizability. This paper\nintroduces a novel approach to enable LLM's LongCoT capacity without\ndistillation from o1-like models or expensive human annotations, where we\nbootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three\nstages: 1) LongCoT data bootstrapping with in-context learning on a standard\ninstruct model; 2) LongCoT supervised finetuning; 3) online training to further\nrefine LongCoT capacities. In BOLT, only a few in-context examples need to be\nconstructed during the bootstrapping stage; in our experiments, we created 10\nexamples, demonstrating the feasibility of this approach. We use\nLlama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various\nmodel scales (7B, 8B, 70B). We achieve impressive performance on a variety of\nbenchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which\nevaluate diverse task-solving and reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as o1 from OpenAI, have demonstrated\nremarkable reasoning capabilities. o1 generates a long chain-of-thought\n(LongCoT) before answering a question. LongCoT allows LLMs to analyze problems,\ndevise plans, reflect, and backtrack effectively. These actions empower LLM to\nsolve complex problems. After the release of o1, many teams have attempted to\nreplicate its LongCoT and reasoning capabilities. In terms of methods, they\nprimarily rely on knowledge distillation with data from existing models with\nLongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving\nsignificant uncertainties on systematically developing such reasoning\nabilities. In terms of data domains, these works focus narrowly on math while a\nfew others include coding, limiting their generalizability. This paper\nintroduces a novel approach to enable LLM's LongCoT capacity without\ndistillation from o1-like models or expensive human annotations, where we\nbootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three\nstages: 1) LongCoT data bootstrapping with in-context learning on a standard\ninstruct model; 2) LongCoT supervised finetuning; 3) online training to further\nrefine LongCoT capacities. In BOLT, only a few in-context examples need to be\nconstructed during the bootstrapping stage; in our experiments, we created 10\nexamples, demonstrating the feasibility of this approach. We use\nLlama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various\nmodel scales (7B, 8B, 70B). We achieve impressive performance on a variety of\nbenchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which\nevaluate diverse task-solving and reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "36 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v4",
                "updated": "2025-02-06T07:56:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    56,
                    1,
                    3,
                    37,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: an investigation of Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: an investigation of Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "arxiv_comment": "This manuscript has been accepted for publication in PeerJ Computer\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03843v1",
                "updated": "2025-02-06T07:53:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    53,
                    40,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T07:53:40Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    53,
                    40,
                    3,
                    37,
                    0
                ],
                "title": "Improving Natural Language Understanding for LLMs via Large-Scale\n  Instruction Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Natural Language Understanding for LLMs via Large-Scale\n  Instruction Synthesis"
                },
                "summary": "High-quality, large-scale instructions are crucial for aligning large\nlanguage models (LLMs), however, there is a severe shortage of instruction in\nthe field of natural language understanding (NLU). Previous works on\nconstructing NLU instructions mainly focus on information extraction (IE),\nneglecting tasks such as machine reading comprehension, question answering, and\ntext classification. Furthermore, the lack of diversity in the data has led to\na decreased generalization ability of trained LLMs in other NLU tasks and a\nnoticeable decline in the fundamental model's general capabilities. To address\nthis issue, we propose Hum, a large-scale, high-quality synthetic instruction\ncorpus for NLU tasks, designed to enhance the NLU capabilities of LLMs.\nSpecifically, Hum includes IE (either close IE or open IE), machine reading\ncomprehension, text classification, and instruction generalist tasks, thereby\nenriching task diversity. Additionally, we introduce a human-LLMs collaborative\nmechanism to synthesize instructions, which enriches instruction diversity by\nincorporating guidelines, preference rules, and format variants. We conduct\nextensive experiments on 5 NLU tasks and 28 general capability evaluation\ndatasets for LLMs. Experimental results show that Hum enhances the NLU\ncapabilities of six LLMs by an average of 3.1\\%, with no significant decline\nobserved in other general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality, large-scale instructions are crucial for aligning large\nlanguage models (LLMs), however, there is a severe shortage of instruction in\nthe field of natural language understanding (NLU). Previous works on\nconstructing NLU instructions mainly focus on information extraction (IE),\nneglecting tasks such as machine reading comprehension, question answering, and\ntext classification. Furthermore, the lack of diversity in the data has led to\na decreased generalization ability of trained LLMs in other NLU tasks and a\nnoticeable decline in the fundamental model's general capabilities. To address\nthis issue, we propose Hum, a large-scale, high-quality synthetic instruction\ncorpus for NLU tasks, designed to enhance the NLU capabilities of LLMs.\nSpecifically, Hum includes IE (either close IE or open IE), machine reading\ncomprehension, text classification, and instruction generalist tasks, thereby\nenriching task diversity. Additionally, we introduce a human-LLMs collaborative\nmechanism to synthesize instructions, which enriches instruction diversity by\nincorporating guidelines, preference rules, and format variants. We conduct\nextensive experiments on 5 NLU tasks and 28 general capability evaluation\ndatasets for LLMs. Experimental results show that Hum enhances the NLU\ncapabilities of six LLMs by an average of 3.1\\%, with no significant decline\nobserved in other general capabilities."
                },
                "authors": [
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Honghao Gui"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03826v1",
                "updated": "2025-02-06T07:22:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    22,
                    57,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T07:22:57Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    22,
                    57,
                    3,
                    37,
                    0
                ],
                "title": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large\n  Language Model-Assisted Detection and Attribute Rebalancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large\n  Language Model-Assisted Detection and Attribute Rebalancing"
                },
                "summary": "The proliferation of Text-to-Image (T2I) models has revolutionized content\ncreation, providing powerful tools for diverse applications ranging from\nartistic expression to educational material development and marketing. Despite\nthese technological advancements, significant ethical concerns arise from these\nmodels' reliance on large-scale datasets that often contain inherent societal\nbiases. These biases are further amplified when AI-generated content is\nincluded in training data, potentially reinforcing and perpetuating stereotypes\nin the generated outputs. In this paper, we introduce FairT2I, a novel\nframework that harnesses large language models to detect and mitigate social\nbiases in T2I generation. Our framework comprises two key components: (1) an\nLLM-based bias detection module that identifies potential social biases in\ngenerated images based on text prompts, and (2) an attribute rebalancing module\nthat fine-tunes sensitive attributes within the T2I model to mitigate\nidentified biases. Our extensive experiments across various T2I models and\ndatasets show that FairT2I can significantly reduce bias while maintaining\nhigh-quality image generation. We conducted both qualitative user studies and\nquantitative non-parametric analyses in the generated image feature space,\nbuilding upon the occupational dataset introduced in the Stable Bias study. Our\nresults show that FairT2I successfully mitigates social biases and enhances the\ndiversity of sensitive attributes in generated images. We further demonstrate,\nusing the P2 dataset, that our framework can detect subtle biases that are\nchallenging for human observers to perceive, extending beyond\noccupation-related prompts. On the basis of these findings, we introduce a new\nbenchmark dataset for evaluating bias in T2I models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Text-to-Image (T2I) models has revolutionized content\ncreation, providing powerful tools for diverse applications ranging from\nartistic expression to educational material development and marketing. Despite\nthese technological advancements, significant ethical concerns arise from these\nmodels' reliance on large-scale datasets that often contain inherent societal\nbiases. These biases are further amplified when AI-generated content is\nincluded in training data, potentially reinforcing and perpetuating stereotypes\nin the generated outputs. In this paper, we introduce FairT2I, a novel\nframework that harnesses large language models to detect and mitigate social\nbiases in T2I generation. Our framework comprises two key components: (1) an\nLLM-based bias detection module that identifies potential social biases in\ngenerated images based on text prompts, and (2) an attribute rebalancing module\nthat fine-tunes sensitive attributes within the T2I model to mitigate\nidentified biases. Our extensive experiments across various T2I models and\ndatasets show that FairT2I can significantly reduce bias while maintaining\nhigh-quality image generation. We conducted both qualitative user studies and\nquantitative non-parametric analyses in the generated image feature space,\nbuilding upon the occupational dataset introduced in the Stable Bias study. Our\nresults show that FairT2I successfully mitigates social biases and enhances the\ndiversity of sensitive attributes in generated images. We further demonstrate,\nusing the P2 dataset, that our framework can detect subtle biases that are\nchallenging for human observers to perceive, extending beyond\noccupation-related prompts. On the basis of these findings, we introduce a new\nbenchmark dataset for evaluating bias in T2I models."
                },
                "authors": [
                    {
                        "name": "Jinya Sakurai"
                    },
                    {
                        "name": "Issei Sato"
                    }
                ],
                "author_detail": {
                    "name": "Issei Sato"
                },
                "author": "Issei Sato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03824v1",
                "updated": "2025-02-06T07:19:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    19,
                    59,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T07:19:59Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    19,
                    59,
                    3,
                    37,
                    0
                ],
                "title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs"
                },
                "summary": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}."
                },
                "authors": [
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Seungjun Baek"
                    }
                ],
                "author_detail": {
                    "name": "Seungjun Baek"
                },
                "author": "Seungjun Baek",
                "arxiv_comment": "the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03821v1",
                "updated": "2025-02-06T07:17:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    17,
                    12,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T07:17:12Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    17,
                    12,
                    3,
                    37,
                    0
                ],
                "title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyPlay: Personality-Infused Role-Playing Conversational Agents"
                },
                "summary": "The current research on Role-Playing Conversational Agents (RPCAs) with Large\nLanguage Models (LLMs) primarily focuses on imitating specific speaking styles\nand utilizing character backgrounds, neglecting the depiction of deeper\npersonality traits.~In this study, we introduce personality-infused\nrole-playing for LLM agents, which encourages agents to accurately portray\ntheir designated personality traits during dialogues. We then propose PsyPlay,\na dialogue generation framework that facilitates the expression of rich\npersonalities among multiple LLM agents. Specifically, PsyPlay enables agents\nto assume roles with distinct personality traits and engage in discussions\ncentered around specific topics, consistently exhibiting their designated\npersonality traits throughout the interactions. Validation on generated\ndialogue data demonstrates that PsyPlay can accurately portray the intended\npersonality traits, achieving an overall success rate of 80.31% on GPT-3.5.\nNotably, we observe that LLMs aligned with positive values are more successful\nin portraying positive personality roles compared to negative ones. Moreover,\nwe construct a dialogue corpus for personality-infused role-playing, called\nPsyPlay-Bench. The corpus, which consists of 4745 instances of correctly\nportrayed dialogues using PsyPlay, aims to further facilitate research in\npersonalized role-playing and dialogue personality detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current research on Role-Playing Conversational Agents (RPCAs) with Large\nLanguage Models (LLMs) primarily focuses on imitating specific speaking styles\nand utilizing character backgrounds, neglecting the depiction of deeper\npersonality traits.~In this study, we introduce personality-infused\nrole-playing for LLM agents, which encourages agents to accurately portray\ntheir designated personality traits during dialogues. We then propose PsyPlay,\na dialogue generation framework that facilitates the expression of rich\npersonalities among multiple LLM agents. Specifically, PsyPlay enables agents\nto assume roles with distinct personality traits and engage in discussions\ncentered around specific topics, consistently exhibiting their designated\npersonality traits throughout the interactions. Validation on generated\ndialogue data demonstrates that PsyPlay can accurately portray the intended\npersonality traits, achieving an overall success rate of 80.31% on GPT-3.5.\nNotably, we observe that LLMs aligned with positive values are more successful\nin portraying positive personality roles compared to negative ones. Moreover,\nwe construct a dialogue corpus for personality-infused role-playing, called\nPsyPlay-Bench. The corpus, which consists of 4745 instances of correctly\nportrayed dialogues using PsyPlay, aims to further facilitate research in\npersonalized role-playing and dialogue personality detection."
                },
                "authors": [
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yuhua Zhu"
                    },
                    {
                        "name": "Xiaojun Quan"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Qifan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qifan Wang"
                },
                "author": "Qifan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06387v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06387v4",
                "updated": "2025-02-06T07:07:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    7,
                    28,
                    3,
                    37,
                    0
                ],
                "published": "2024-11-10T08:11:05Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    11,
                    5,
                    6,
                    315,
                    0
                ],
                "title": "Self-Training Meets Consistency: Improving LLMs' Reasoning with\n  Consistency-Driven Rationale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Meets Consistency: Improving LLMs' Reasoning with\n  Consistency-Driven Rationale Evaluation"
                },
                "summary": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches."
                },
                "authors": [
                    {
                        "name": "Jaehyeok Lee"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "JinYeong Bak"
                    }
                ],
                "author_detail": {
                    "name": "JinYeong Bak"
                },
                "author": "JinYeong Bak",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06387v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06387v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03814v1",
                "updated": "2025-02-06T06:52:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    52,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:52:14Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    52,
                    14,
                    3,
                    37,
                    0
                ],
                "title": "Large Language Models for Multi-Robot Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multi-Robot Systems: A Survey"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\npossibilities in Multi-Robot Systems (MRS), enabling enhanced communication,\ntask planning, and human-robot interaction. Unlike traditional single-robot and\nmulti-agent systems, MRS poses unique challenges, including coordination,\nscalability, and real-world adaptability. This survey provides the first\ncomprehensive exploration of LLM integration into MRS. It systematically\ncategorizes their applications across high-level task allocation, mid-level\nmotion planning, low-level action generation, and human intervention. We\nhighlight key applications in diverse domains, such as household robotics,\nconstruction, formation control, target tracking, and robot games, showcasing\nthe versatility and transformative potential of LLMs in MRS. Furthermore, we\nexamine the challenges that limit adapting LLMs in MRS, including mathematical\nreasoning limitations, hallucination, latency issues, and the need for robust\nbenchmarking systems. Finally, we outline opportunities for future research,\nemphasizing advancements in fine-tuning, reasoning techniques, and\ntask-specific models. This survey aims to guide researchers in the intelligence\nand real-world deployment of MRS powered by LLMs. Based on the fast-evolving\nnature of research in the field, we keep updating the papers in the open-source\nGithub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened new\npossibilities in Multi-Robot Systems (MRS), enabling enhanced communication,\ntask planning, and human-robot interaction. Unlike traditional single-robot and\nmulti-agent systems, MRS poses unique challenges, including coordination,\nscalability, and real-world adaptability. This survey provides the first\ncomprehensive exploration of LLM integration into MRS. It systematically\ncategorizes their applications across high-level task allocation, mid-level\nmotion planning, low-level action generation, and human intervention. We\nhighlight key applications in diverse domains, such as household robotics,\nconstruction, formation control, target tracking, and robot games, showcasing\nthe versatility and transformative potential of LLMs in MRS. Furthermore, we\nexamine the challenges that limit adapting LLMs in MRS, including mathematical\nreasoning limitations, hallucination, latency issues, and the need for robust\nbenchmarking systems. Finally, we outline opportunities for future research,\nemphasizing advancements in fine-tuning, reasoning techniques, and\ntask-specific models. This survey aims to guide researchers in the intelligence\nand real-world deployment of MRS powered by LLMs. Based on the fast-evolving\nnature of research in the field, we keep updating the papers in the open-source\nGithub repository."
                },
                "authors": [
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Zijian An"
                    },
                    {
                        "name": "Shams Abrar"
                    },
                    {
                        "name": "Lifeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Zhou"
                },
                "author": "Lifeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01720v3",
                "updated": "2025-02-06T06:42:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    42,
                    17,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-02T16:32:05Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    32,
                    5,
                    2,
                    276,
                    0
                ],
                "title": "Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Theoretical Understanding of Synthetic Data in LLM\n  Post-Training: A Reverse-Bottleneck Perspective"
                },
                "summary": "Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open-source\nour code at\nhttps://github.com/ZyGan1999/Towards-a-Theoretical-Understanding-of-Synthetic-Data-in-LLM-Post-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data has become a pivotal resource in post-training tasks for large\nlanguage models (LLMs) due to the scarcity of high-quality, specific data.\nWhile various methods have been developed to generate synthetic data, there\nremains a discernible gap between the practical effects of synthetic data and\nour theoretical comprehension. To address this challenge, we commence by\npresenting a detailed modeling of the prevalent synthetic data generation\nprocess. Building upon this modeling, we demonstrate that the generalization\ncapability of the post-trained model is critically determined by the\ninformation gain derived from the generative model, as analyzed from a novel\nreverse-bottleneck perspective. Moreover, we introduce the concept of\nGeneralization Gain via Mutual Information (GGMI) and elucidate the\nrelationship between generalization gain and information gain. This analysis\nserves as a theoretical foundation for synthetic data generation and further\nhighlights its connection with the generalization capability of post-trained\nmodels, offering an understanding about the design of synthetic data generation\ntechniques and the optimization of the post-training process. We open-source\nour code at\nhttps://github.com/ZyGan1999/Towards-a-Theoretical-Understanding-of-Synthetic-Data-in-LLM-Post-Training."
                },
                "authors": [
                    {
                        "name": "Zeyu Gan"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03804v1",
                "updated": "2025-02-06T06:27:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    27,
                    9,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:27:09Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    27,
                    9,
                    3,
                    37,
                    0
                ],
                "title": "Understanding and Supporting Formal Email Exchange by Answering\n  AI-Generated Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Supporting Formal Email Exchange by Answering\n  AI-Generated Questions"
                },
                "summary": "Replying to formal emails is time-consuming and cognitively demanding, as it\nrequires polite phrasing and ensuring an adequate response to the sender's\ndemands. Although systems with Large Language Models (LLM) were designed to\nsimplify the email replying process, users still needed to provide detailed\nprompts to obtain the expected output. Therefore, we proposed and evaluated an\nLLM-powered question-and-answer (QA)-based approach for users to reply to\nemails by answering a set of simple and short questions generated from the\nincoming email. We developed a prototype system, ResQ, and conducted controlled\nand field experiments with 12 and 8 participants. Our results demonstrated that\nQA-based approach improves the efficiency of replying to emails and reduces\nworkload while maintaining email quality compared to a conventional\nprompt-based approach that requires users to craft appropriate prompts to\nobtain email drafts. We discuss how QA-based approach influences the email\nreply process and interpersonal relationship dynamics, as well as the\nopportunities and challenges associated with using a QA-based approach in\nAI-mediated communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replying to formal emails is time-consuming and cognitively demanding, as it\nrequires polite phrasing and ensuring an adequate response to the sender's\ndemands. Although systems with Large Language Models (LLM) were designed to\nsimplify the email replying process, users still needed to provide detailed\nprompts to obtain the expected output. Therefore, we proposed and evaluated an\nLLM-powered question-and-answer (QA)-based approach for users to reply to\nemails by answering a set of simple and short questions generated from the\nincoming email. We developed a prototype system, ResQ, and conducted controlled\nand field experiments with 12 and 8 participants. Our results demonstrated that\nQA-based approach improves the efficiency of replying to emails and reduces\nworkload while maintaining email quality compared to a conventional\nprompt-based approach that requires users to craft appropriate prompts to\nobtain email drafts. We discuss how QA-based approach influences the email\nreply process and interpersonal relationship dynamics, as well as the\nopportunities and challenges associated with using a QA-based approach in\nAI-mediated communication."
                },
                "authors": [
                    {
                        "name": "Yusuke Miura"
                    },
                    {
                        "name": "Chi-Lan Yang"
                    },
                    {
                        "name": "Masaki Kuribayashi"
                    },
                    {
                        "name": "Keigo Matsumoto"
                    },
                    {
                        "name": "Hideaki Kuzuoka"
                    },
                    {
                        "name": "Shigeo Morishima"
                    }
                ],
                "author_detail": {
                    "name": "Shigeo Morishima"
                },
                "author": "Shigeo Morishima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16701v2",
                "updated": "2025-02-06T06:22:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    22,
                    14,
                    3,
                    37,
                    0
                ],
                "published": "2024-09-25T07:47:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "Vulnerability-Triggering Test Case Generation from Third-Party Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability-Triggering Test Case Generation from Third-Party Libraries"
                },
                "summary": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "arxiv_comment": "Published in 2nd Conference on AI Foundation Models and Software\n  Engineering (FORGE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14259v2",
                "updated": "2025-02-06T06:19:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    19,
                    10,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-18T08:14:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    14,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement"
                },
                "summary": "The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-LLM collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and two multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-LLM collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and two multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies."
                },
                "authors": [
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_doi": "10.1145/3696410.3714770",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714770",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Social Media, Large Language Models, LLM-generated Text Detection,\n  AI-assisted News Detection; Accepted by WWW2025",
                "arxiv_journal_ref": "Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May\n  2, 2025, Sydney, NSW, Australia",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03799v1",
                "updated": "2025-02-06T06:02:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    2,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:02:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    2,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Enhancing Hallucination Detection through Noise Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Hallucination Detection through Noise Injection"
                },
                "summary": "Large Language Models (LLMs) are prone to generating plausible yet incorrect\nresponses, known as hallucinations. Effectively detecting hallucinations is\ntherefore crucial for the safe deployment of LLMs. Recent research has linked\nhallucinations to model uncertainty, suggesting that hallucinations can be\ndetected by measuring dispersion over answer distributions obtained from a set\nof samples drawn from a model. While drawing from the distribution over tokens\ndefined by the model is a natural way to obtain samples, in this work, we argue\nthat it is sub-optimal for the purpose of detecting hallucinations. We show\nthat detection can be improved significantly by taking into account model\nuncertainty in the Bayesian sense. To this end, we propose a very simple and\nefficient approach that perturbs an appropriate subset of model parameters, or\nequivalently hidden unit activations, during sampling. We demonstrate its\neffectiveness across a wide range of datasets and model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating plausible yet incorrect\nresponses, known as hallucinations. Effectively detecting hallucinations is\ntherefore crucial for the safe deployment of LLMs. Recent research has linked\nhallucinations to model uncertainty, suggesting that hallucinations can be\ndetected by measuring dispersion over answer distributions obtained from a set\nof samples drawn from a model. While drawing from the distribution over tokens\ndefined by the model is a natural way to obtain samples, in this work, we argue\nthat it is sub-optimal for the purpose of detecting hallucinations. We show\nthat detection can be improved significantly by taking into account model\nuncertainty in the Bayesian sense. To this end, we propose a very simple and\nefficient approach that perturbs an appropriate subset of model parameters, or\nequivalently hidden unit activations, during sampling. We demonstrate its\neffectiveness across a wide range of datasets and model architectures."
                },
                "authors": [
                    {
                        "name": "Litian Liu"
                    },
                    {
                        "name": "Reza Pourreza"
                    },
                    {
                        "name": "Sunny Panchal"
                    },
                    {
                        "name": "Apratim Bhattacharyya"
                    },
                    {
                        "name": "Yao Qin"
                    },
                    {
                        "name": "Roland Memisevic"
                    }
                ],
                "author_detail": {
                    "name": "Roland Memisevic"
                },
                "author": "Roland Memisevic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03793v1",
                "updated": "2025-02-06T05:47:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    5,
                    47,
                    37,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T05:47:37Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    5,
                    47,
                    37,
                    3,
                    37,
                    0
                ],
                "title": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like\n  Masked Language Models As Generative Classifiers"
                },
                "summary": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements."
                },
                "authors": [
                    {
                        "name": "Benjamin Clavié"
                    },
                    {
                        "name": "Nathan Cooper"
                    },
                    {
                        "name": "Benjamin Warner"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Warner"
                },
                "author": "Benjamin Warner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04683v2",
                "updated": "2025-02-06T05:44:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    5,
                    44,
                    29,
                    3,
                    37,
                    0
                ],
                "published": "2024-12-06T00:46:20Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    0,
                    46,
                    20,
                    4,
                    341,
                    0
                ],
                "title": "From Principles to Practice: A Deep Dive into AI Ethics and Regulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Principles to Practice: A Deep Dive into AI Ethics and Regulations"
                },
                "summary": "In the rapidly evolving domain of Artificial Intelligence (AI), the complex\ninteraction between innovation and regulation has become an emerging focus of\nour society. Despite tremendous advancements in AI's capabilities to excel in\nspecific tasks and contribute to diverse sectors, establishing a high degree of\ntrust in AI-generated outputs and decisions necessitates meticulous caution and\ncontinuous oversight. A broad spectrum of stakeholders, including governmental\nbodies, private sector corporations, academic institutions, and individuals,\nhave launched significant initiatives. These efforts include developing ethical\nguidelines for AI and engaging in vibrant discussions on AI ethics, both among\nAI practitioners and within the broader society. This article thoroughly\nanalyzes the ground-breaking AI regulatory framework proposed by the European\nUnion. It delves into the fundamental ethical principles of safety,\ntransparency, non-discrimination, traceability, and environmental\nsustainability for AI developments and deployments. Considering the technical\nefforts and strategies undertaken by academics and industry to uphold these\nprinciples, we explore the synergies and conflicts among the five ethical\nprinciples. Through this lens, work presents a forward-looking perspective on\nthe future of AI regulations, advocating for a harmonized approach that\nsafeguards societal values while encouraging technological advancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of Artificial Intelligence (AI), the complex\ninteraction between innovation and regulation has become an emerging focus of\nour society. Despite tremendous advancements in AI's capabilities to excel in\nspecific tasks and contribute to diverse sectors, establishing a high degree of\ntrust in AI-generated outputs and decisions necessitates meticulous caution and\ncontinuous oversight. A broad spectrum of stakeholders, including governmental\nbodies, private sector corporations, academic institutions, and individuals,\nhave launched significant initiatives. These efforts include developing ethical\nguidelines for AI and engaging in vibrant discussions on AI ethics, both among\nAI practitioners and within the broader society. This article thoroughly\nanalyzes the ground-breaking AI regulatory framework proposed by the European\nUnion. It delves into the fundamental ethical principles of safety,\ntransparency, non-discrimination, traceability, and environmental\nsustainability for AI developments and deployments. Considering the technical\nefforts and strategies undertaken by academics and industry to uphold these\nprinciples, we explore the synergies and conflicts among the five ethical\nprinciples. Through this lens, work presents a forward-looking perspective on\nthe future of AI regulations, advocating for a harmonized approach that\nsafeguards societal values while encouraging technological advancement."
                },
                "authors": [
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Yuantian Miao"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Ming Ding"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Submitted to JAIR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06224v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06224v3",
                "updated": "2025-02-06T04:58:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    58,
                    18,
                    3,
                    37,
                    0
                ],
                "published": "2025-01-07T09:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    21,
                    20,
                    1,
                    7,
                    0
                ],
                "title": "Detection, Retrieval, and Explanation Unified: A Violence Detection\n  System Based on Knowledge Graphs and GAT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection, Retrieval, and Explanation Unified: A Violence Detection\n  System Based on Knowledge Graphs and GAT"
                },
                "summary": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease."
                },
                "authors": [
                    {
                        "name": "Wen-Dong Jiang"
                    },
                    {
                        "name": "Chih-Yung Chang"
                    },
                    {
                        "name": "Diptendu Sinha Roy"
                    }
                ],
                "author_detail": {
                    "name": "Diptendu Sinha Roy"
                },
                "author": "Diptendu Sinha Roy",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06224v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06224v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00412v3",
                "updated": "2025-02-06T04:18:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    18,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2024-11-01T07:18:31Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    18,
                    31,
                    4,
                    306,
                    0
                ],
                "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation"
                },
                "summary": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nsimple scientific problems but, even with domain-specific fine-tuning, often\nproduce hallucinations for complex ones. While integrating LLMs with tools can\nmitigate this reliability issue, models finetuned on tool usage only often\nover-rely on them, incurring unnecessary costs from resource-intensive\nscientific tools even for simpler problems. Inspired by how human experts\nassess the complexity of the problem before choosing the solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tools-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we classify questions as easy or hard based on the\nWKL-trained model's accuracy, and train it to maintain direct reasoning for\nsimple problems while switching to tools for challenging ones. We validate our\nmethod on 6 scientific benchmark datasets in climate science, epidemiology, and\nmathematics. Compared to the base 8B model, our trained models achieve 28.27%\nhigher answer accuracy and 13.76% better tool usage accuracy, even surpassing\nstate-of-the-art models including GPT-4 and Claude-3.5 on 4 custom-created\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nsimple scientific problems but, even with domain-specific fine-tuning, often\nproduce hallucinations for complex ones. While integrating LLMs with tools can\nmitigate this reliability issue, models finetuned on tool usage only often\nover-rely on them, incurring unnecessary costs from resource-intensive\nscientific tools even for simpler problems. Inspired by how human experts\nassess the complexity of the problem before choosing the solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tools-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we classify questions as easy or hard based on the\nWKL-trained model's accuracy, and train it to maintain direct reasoning for\nsimple problems while switching to tools for challenging ones. We validate our\nmethod on 6 scientific benchmark datasets in climate science, epidemiology, and\nmathematics. Compared to the base 8B model, our trained models achieve 28.27%\nhigher answer accuracy and 13.76% better tool usage accuracy, even surpassing\nstate-of-the-art models including GPT-4 and Claude-3.5 on 4 custom-created\ndatasets."
                },
                "authors": [
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Yadi Cao"
                    },
                    {
                        "name": "Duncan Watson-Parris"
                    },
                    {
                        "name": "Leon Bergen"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "arxiv_comment": "32 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12032v2",
                "updated": "2025-02-06T04:16:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    22,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-15T20:06:33Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    20,
                    6,
                    33,
                    1,
                    289,
                    0
                ],
                "title": "MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning\n  Systems from Microwatts to Megawatts for Sustainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning\n  Systems from Microwatts to Megawatts for Sustainable AI"
                },
                "summary": "Rapid adoption of machine learning (ML) technologies has led to a surge in\npower consumption across diverse systems, from tiny IoT devices to massive\ndatacenter clusters. Benchmarking the energy efficiency of these systems is\ncrucial for optimization, but presents novel challenges due to the variety of\nhardware platforms, workload characteristics, and system-level interactions.\nThis paper introduces MLPerf Power, a comprehensive benchmarking methodology\nwith capabilities to evaluate the energy efficiency of ML systems at power\nlevels ranging from microwatts to megawatts. Developed by a consortium of\nindustry professionals from more than 20 organizations, MLPerf Power\nestablishes rules and best practices to ensure comparability across diverse\narchitectures. We use representative workloads from the MLPerf benchmark suite\nto collect 1,841 reproducible measurements from 60 systems across the entire\nrange of ML deployment scales. Our analysis reveals trade-offs between\nperformance, complexity, and energy efficiency across this wide range of\nsystems, providing actionable insights for designing optimized ML solutions\nfrom the smallest edge devices to the largest cloud infrastructures. This work\nemphasizes the importance of energy efficiency as a key metric in the\nevaluation and comparison of the ML system, laying the foundation for future\nresearch in this critical area. We discuss the implications for developing\nsustainable AI solutions and standardizing energy efficiency benchmarking for\nML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adoption of machine learning (ML) technologies has led to a surge in\npower consumption across diverse systems, from tiny IoT devices to massive\ndatacenter clusters. Benchmarking the energy efficiency of these systems is\ncrucial for optimization, but presents novel challenges due to the variety of\nhardware platforms, workload characteristics, and system-level interactions.\nThis paper introduces MLPerf Power, a comprehensive benchmarking methodology\nwith capabilities to evaluate the energy efficiency of ML systems at power\nlevels ranging from microwatts to megawatts. Developed by a consortium of\nindustry professionals from more than 20 organizations, MLPerf Power\nestablishes rules and best practices to ensure comparability across diverse\narchitectures. We use representative workloads from the MLPerf benchmark suite\nto collect 1,841 reproducible measurements from 60 systems across the entire\nrange of ML deployment scales. Our analysis reveals trade-offs between\nperformance, complexity, and energy efficiency across this wide range of\nsystems, providing actionable insights for designing optimized ML solutions\nfrom the smallest edge devices to the largest cloud infrastructures. This work\nemphasizes the importance of energy efficiency as a key metric in the\nevaluation and comparison of the ML system, laying the foundation for future\nresearch in this critical area. We discuss the implications for developing\nsustainable AI solutions and standardizing energy efficiency benchmarking for\nML systems."
                },
                "authors": [
                    {
                        "name": "Arya Tschand"
                    },
                    {
                        "name": "Arun Tejusve Raghunath Rajan"
                    },
                    {
                        "name": "Sachin Idgunji"
                    },
                    {
                        "name": "Anirban Ghosh"
                    },
                    {
                        "name": "Jeremy Holleman"
                    },
                    {
                        "name": "Csaba Kiraly"
                    },
                    {
                        "name": "Pawan Ambalkar"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Ramesh Chukka"
                    },
                    {
                        "name": "Trevor Cockrell"
                    },
                    {
                        "name": "Oliver Curtis"
                    },
                    {
                        "name": "Grigori Fursin"
                    },
                    {
                        "name": "Miro Hodak"
                    },
                    {
                        "name": "Hiwot Kassa"
                    },
                    {
                        "name": "Anton Lokhmotov"
                    },
                    {
                        "name": "Dejan Miskovic"
                    },
                    {
                        "name": "Yuechao Pan"
                    },
                    {
                        "name": "Manu Prasad Manmathan"
                    },
                    {
                        "name": "Liz Raymond"
                    },
                    {
                        "name": "Tom St. John"
                    },
                    {
                        "name": "Arjun Suresh"
                    },
                    {
                        "name": "Rowan Taubitz"
                    },
                    {
                        "name": "Sean Zhan"
                    },
                    {
                        "name": "Scott Wasson"
                    },
                    {
                        "name": "David Kanter"
                    },
                    {
                        "name": "Vijay Janapa Reddi"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Janapa Reddi"
                },
                "author": "Vijay Janapa Reddi",
                "arxiv_comment": "16 pages, 11 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01070v2",
                "updated": "2025-02-06T04:04:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    4,
                    51,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T05:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    26,
                    22,
                    0,
                    34,
                    0
                ],
                "title": "An Investigation of FP8 Across Accelerators for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of FP8 Across Accelerators for LLM Inference"
                },
                "summary": "The introduction of 8-bit floating-point (FP8) computation units in modern AI\naccelerators has generated significant interest in FP8-based large language\nmodel (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep\nlearning requires a shared scaling factor. Additionally, while E4M3 and E5M2\nare well-defined at the individual value level, their scaling and accumulation\nmethods remain unspecified and vary across hardware and software\nimplementations. As a result, FP8 behaves more like a quantization format than\na standard numeric representation. In this work, we provide the first\ncomprehensive analysis of FP8 computation and acceleration on two AI\naccelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that\nthe Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency\nduring LLM inference, offering valuable insights into the practical\nimplications of FP8 adoption for datacenter-scale LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of 8-bit floating-point (FP8) computation units in modern AI\naccelerators has generated significant interest in FP8-based large language\nmodel (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep\nlearning requires a shared scaling factor. Additionally, while E4M3 and E5M2\nare well-defined at the individual value level, their scaling and accumulation\nmethods remain unspecified and vary across hardware and software\nimplementations. As a result, FP8 behaves more like a quantization format than\na standard numeric representation. In this work, we provide the first\ncomprehensive analysis of FP8 computation and acceleration on two AI\naccelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that\nthe Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency\nduring LLM inference, offering valuable insights into the practical\nimplications of FP8 adoption for datacenter-scale LLM serving."
                },
                "authors": [
                    {
                        "name": "Jiwoo Kim"
                    },
                    {
                        "name": "Joonhyung Lee"
                    },
                    {
                        "name": "Gunho Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    },
                    {
                        "name": "Youngjoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Lee"
                },
                "author": "Youngjoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03759v1",
                "updated": "2025-02-06T03:43:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    43,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:43:44Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    43,
                    44,
                    3,
                    37,
                    0
                ],
                "title": "Drone Beam Mapping of the TONE Radio Dish Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone Beam Mapping of the TONE Radio Dish Array"
                },
                "summary": "Drone-based beam measurements are a promising avenue to tackle the critical\nchallenge of calibration for 21 cm cosmology telescopes. In this paper, we\nintroduce a new drone-based calibration system for 400-800 MHz radio\nobservatories, describing its instrumentation and first deployment. We discuss\nmeasurements of the TONE array, a CHIME/FRB outrigger pathfinder, and present\nresults, including full 2D high spatial resolution beam maps in both co- and\ncross-polarization, as well as comparisons to simulations. The polarized beam\nmaps cover a 70 degree by 70 degree grid, capturing the first two sidelobes and\nmeasuring the TONE main beam and first sidelobe with 7-9% statistical errors.\nWe investigate polarization angle alignment with frequency, finding significant\npolarization leakage in the TONE antennas at frequencies above 600 MHz, and a\npolarization axis rotation with frequency. We describe statistical and\nsystematic errors, as well as measurements of radio frequency interference from\nthe drone and equipment. Our drone system is the first to incorporate a\nbroad-band switched calibration source in the drone payload, enabling\nbackground subtraction and direct measurements of the RFI emitted by the drone.\nThe results presented are the first drone-based 2D measurements of cross-polar\nbeam structure and of polarization alignment of an array. The high frequency\nand spatial resolution achieved with this system have revealed the rich\nstructure of the beam of each antenna, and enabled comparisons between\nindividual dishes and to electromagnetic simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone-based beam measurements are a promising avenue to tackle the critical\nchallenge of calibration for 21 cm cosmology telescopes. In this paper, we\nintroduce a new drone-based calibration system for 400-800 MHz radio\nobservatories, describing its instrumentation and first deployment. We discuss\nmeasurements of the TONE array, a CHIME/FRB outrigger pathfinder, and present\nresults, including full 2D high spatial resolution beam maps in both co- and\ncross-polarization, as well as comparisons to simulations. The polarized beam\nmaps cover a 70 degree by 70 degree grid, capturing the first two sidelobes and\nmeasuring the TONE main beam and first sidelobe with 7-9% statistical errors.\nWe investigate polarization angle alignment with frequency, finding significant\npolarization leakage in the TONE antennas at frequencies above 600 MHz, and a\npolarization axis rotation with frequency. We describe statistical and\nsystematic errors, as well as measurements of radio frequency interference from\nthe drone and equipment. Our drone system is the first to incorporate a\nbroad-band switched calibration source in the drone payload, enabling\nbackground subtraction and direct measurements of the RFI emitted by the drone.\nThe results presented are the first drone-based 2D measurements of cross-polar\nbeam structure and of polarization alignment of an array. The high frequency\nand spatial resolution achieved with this system have revealed the rich\nstructure of the beam of each antenna, and enabled comparisons between\nindividual dishes and to electromagnetic simulations."
                },
                "authors": [
                    {
                        "name": "Emily R. Kuhn"
                    },
                    {
                        "name": "Will Tyndall"
                    },
                    {
                        "name": "Benjamin R. B. Saliwanchik"
                    },
                    {
                        "name": "Anna Rose Polish"
                    },
                    {
                        "name": "Maile Harris"
                    },
                    {
                        "name": "Laura B. Newburgh"
                    }
                ],
                "author_detail": {
                    "name": "Laura B. Newburgh"
                },
                "author": "Laura B. Newburgh",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01768v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01768v3",
                "updated": "2025-02-06T03:33:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    33,
                    8,
                    3,
                    37,
                    0
                ],
                "published": "2024-05-02T22:37:38Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    22,
                    37,
                    38,
                    3,
                    123,
                    0
                ],
                "title": "Context Steering: Controllable Personalization at Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Steering: Controllable Personalization at Inference Time"
                },
                "summary": "To deliver high-quality, personalized responses, large language models (LLMs)\nmust effectively incorporate context -- personal, demographic, and cultural\ninformation specific to an end-user. For example, asking the model to explain\nNewton's second law with the context \"I am a toddler\" should produce a response\ndifferent from when the context is \"I am a physics professor\". However,\nleveraging the context in practice is a nuanced and challenging task, and is\noften dependent on the specific situation or user base. The model must strike a\nbalance between providing specific, personalized responses and maintaining\ngeneral applicability. Current solutions, such as prompt-engineering and\nfine-tuning, require collection of contextually appropriate responses as\nexamples, making them time-consuming and less flexible to use across different\ncontexts. In this work, we introduce Context Steering (CoS) -- a simple,\ntraining-free decoding approach that amplifies the influence of the context in\nnext token predictions. CoS computes contextual influence by comparing the\noutput probabilities from two LLM forward passes: one that includes the context\nand one that does not. By linearly scaling the contextual influence, CoS allows\npractitioners to flexibly control the degree of personalization for different\nuse cases. We show that CoS can be applied to autoregressive LLMs, and\ndemonstrates strong performance in personalized recommendations. Additionally,\nwe show that CoS can function as a Bayesian Generative model to infer and\nquantify correlations between open-ended texts, broadening its potential\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To deliver high-quality, personalized responses, large language models (LLMs)\nmust effectively incorporate context -- personal, demographic, and cultural\ninformation specific to an end-user. For example, asking the model to explain\nNewton's second law with the context \"I am a toddler\" should produce a response\ndifferent from when the context is \"I am a physics professor\". However,\nleveraging the context in practice is a nuanced and challenging task, and is\noften dependent on the specific situation or user base. The model must strike a\nbalance between providing specific, personalized responses and maintaining\ngeneral applicability. Current solutions, such as prompt-engineering and\nfine-tuning, require collection of contextually appropriate responses as\nexamples, making them time-consuming and less flexible to use across different\ncontexts. In this work, we introduce Context Steering (CoS) -- a simple,\ntraining-free decoding approach that amplifies the influence of the context in\nnext token predictions. CoS computes contextual influence by comparing the\noutput probabilities from two LLM forward passes: one that includes the context\nand one that does not. By linearly scaling the contextual influence, CoS allows\npractitioners to flexibly control the degree of personalization for different\nuse cases. We show that CoS can be applied to autoregressive LLMs, and\ndemonstrates strong performance in personalized recommendations. Additionally,\nwe show that CoS can function as a Bayesian Generative model to infer and\nquantify correlations between open-ended texts, broadening its potential\napplications."
                },
                "authors": [
                    {
                        "name": "Jerry Zhi-Yang He"
                    },
                    {
                        "name": "Sashrika Pandey"
                    },
                    {
                        "name": "Mariah L. Schrum"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01768v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01768v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09281v2",
                "updated": "2025-02-06T03:31:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    31,
                    50,
                    3,
                    37,
                    0
                ],
                "published": "2024-09-14T03:11:00Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    3,
                    11,
                    0,
                    5,
                    258,
                    0
                ],
                "title": "Language Models \"Grok\" to Copy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models \"Grok\" to Copy"
                },
                "summary": "We examine the pre-training dynamics of language models, focusing on their\nability to copy text from preceding context--a fundamental skill for various\nLLM applications, including in-context learning (ICL) and retrieval-augmented\ngeneration (RAG). We propose a novel perspective that Transformer-based\nlanguage models develop copying abilities similarly to grokking, which refers\nto sudden generalization on test set long after the model fit to the training\nset. Our experiments yield three arguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of models initially lags and then\nabruptly saturates. (2) The speed of developing copying ability is independent\nof the number of tokens trained, similarly to how grokking speed is unaffected\nby dataset size as long as the data distribution is preserved. (3) Induction\nheads, the attention heads responsible for copying, form from shallow to deep\nlayers during training, mirroring the development of circuits in deeper layers\nduring grokking. We contend that the connection between grokking and context\ncopying can provide valuable insights for more effective language model\ntraining, ultimately improving in-context performance. For example, we\ndemonstrated that techniques that enhance grokking, such as regularization,\neither accelerate or enhance the development of context copying.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the pre-training dynamics of language models, focusing on their\nability to copy text from preceding context--a fundamental skill for various\nLLM applications, including in-context learning (ICL) and retrieval-augmented\ngeneration (RAG). We propose a novel perspective that Transformer-based\nlanguage models develop copying abilities similarly to grokking, which refers\nto sudden generalization on test set long after the model fit to the training\nset. Our experiments yield three arguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of models initially lags and then\nabruptly saturates. (2) The speed of developing copying ability is independent\nof the number of tokens trained, similarly to how grokking speed is unaffected\nby dataset size as long as the data distribution is preserved. (3) Induction\nheads, the attention heads responsible for copying, form from shallow to deep\nlayers during training, mirroring the development of circuits in deeper layers\nduring grokking. We contend that the connection between grokking and context\ncopying can provide valuable insights for more effective language model\ntraining, ultimately improving in-context performance. For example, we\ndemonstrated that techniques that enhance grokking, such as regularization,\neither accelerate or enhance the development of context copying."
                },
                "authors": [
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "NAACL 2025 main conference, short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20672v2",
                "updated": "2025-02-06T03:23:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    23,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T02:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    2,
                    15,
                    45,
                    0,
                    302,
                    0
                ],
                "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA"
                },
                "summary": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Seungyeon Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Tal Schuster"
                },
                "author": "Tal Schuster",
                "arxiv_comment": "ICLR 2025; 47 pages, 17 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03748v1",
                "updated": "2025-02-06T03:20:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    20,
                    17,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:20:17Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    20,
                    17,
                    3,
                    37,
                    0
                ],
                "title": "Rethinking the Residual Distribution of Locate-then-Editing Methods in\n  Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Residual Distribution of Locate-then-Editing Methods in\n  Model Editing"
                },
                "summary": "Model editing is a powerful technique for updating the knowledge of Large\nLanguage Models (LLMs). Locate-then-edit methods are a popular class of\napproaches that first identify the critical layers storing knowledge, then\ncompute the residual of the last critical layer based on the edited knowledge,\nand finally perform multi-layer updates using a least-squares solution by\nevenly distributing the residual from the first critical layer to the last.\nAlthough these methods achieve promising results, they have been shown to\ndegrade the original knowledge of LLMs. We argue that residual distribution\nleads to this issue. To explore this, we conduct a comprehensive analysis of\nresidual distribution in locate-then-edit methods from both empirical and\ntheoretical perspectives, revealing that residual distribution introduces\nediting errors, leading to inaccurate edits. To address this issue, we propose\nthe Boundary Layer UpdatE (BLUE) strategy to enhance locate-then-edit methods.\nSequential batch editing experiments on three LLMs and two datasets demonstrate\nthat BLUE not only delivers an average performance improvement of 35.59\\%,\nsignificantly advancing the state of the art in model editing, but also\nenhances the preservation of LLMs' general capabilities. Our code is available\nat https://github.com/xpq-tech/BLUE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing is a powerful technique for updating the knowledge of Large\nLanguage Models (LLMs). Locate-then-edit methods are a popular class of\napproaches that first identify the critical layers storing knowledge, then\ncompute the residual of the last critical layer based on the edited knowledge,\nand finally perform multi-layer updates using a least-squares solution by\nevenly distributing the residual from the first critical layer to the last.\nAlthough these methods achieve promising results, they have been shown to\ndegrade the original knowledge of LLMs. We argue that residual distribution\nleads to this issue. To explore this, we conduct a comprehensive analysis of\nresidual distribution in locate-then-edit methods from both empirical and\ntheoretical perspectives, revealing that residual distribution introduces\nediting errors, leading to inaccurate edits. To address this issue, we propose\nthe Boundary Layer UpdatE (BLUE) strategy to enhance locate-then-edit methods.\nSequential batch editing experiments on three LLMs and two datasets demonstrate\nthat BLUE not only delivers an average performance improvement of 35.59\\%,\nsignificantly advancing the state of the art in model editing, but also\nenhances the preservation of LLMs' general capabilities. Our code is available\nat https://github.com/xpq-tech/BLUE."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shanwen Wang"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Jie Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yu"
                },
                "author": "Jie Yu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]